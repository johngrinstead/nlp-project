,repo,language,readme_contents
0,microsoft/roosterjs-react,TypeScript,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
1,microsoft/vscode-azure-iot-toolkit,HTML,"# Azure IoT Hub

[![Join the chat at https://gitter.im/Microsoft/azure-iot-toolkit](https://badges.gitter.im/Microsoft/azure-iot-toolkit.svg)](https://gitter.im/Microsoft/azure-iot-toolkit?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) [![Marketplace Version](https://vsmarketplacebadge.apphb.com/version-short/vsciot-vscode.azure-iot-toolkit.svg)](https://marketplace.visualstudio.com/items?itemName=vsciot-vscode.azure-iot-toolkit)

***[Azure IoT Hub extension](https://marketplace.visualstudio.com/items?itemName=vsciot-vscode.azure-iot-toolkit) is now a part of [Azure IoT Tools](https://marketplace.visualstudio.com/items?itemName=vsciot-vscode.azure-iot-tools) extension pack. We highly recommend installing [Azure IoT Tools](https://marketplace.visualstudio.com/items?itemName=vsciot-vscode.azure-iot-tools) extension pack, which makes it easy to discover and interact with Azure IoT Hub that power your IoT Edge and device applications.** This extension pack can help you:*
- *Develop and connect your [Azure IoT Applications](https://azure.microsoft.com/en-us/overview/iot/) to Azure. With this extension, you can interact with an Azure IoT Hub, manage connected devices, and enable distributed tracing for your Azure IoT applications.*
- *Develop and debug [Certifies Azure IoT Devices](https://catalog.azureiotsolutions.com/alldevices) (including [MXChip IoT DevKit](https://aka.ms/iot-devkit), [ESP32](https://catalog.azureiotsolutions.com/details?title=ESP32_DevKitC&source=all-devices-page), [Raspberry Pi](https://www.adafruit.com/category/288)) to Azure. This extension pack makes it easy to code, build, deploy and debug your IoT applications with popular IoT development boards.*
- *Develop and deploy artificial intelligence and your custom logic to [Azure IoT Edge](https://azure.microsoft.com/en-us/services/iot-edge/). This extension pack makes it easy to code, build, deploy, and debug your IoT Edge applications.*

## Overview

Interact with Azure IoT Hub, IoT Device Management, IoT Edge Management, IoT Hub Device Simulation, IoT Hub Code Generation and IoT Hub Device Provisioning Service.

## Device Explorer
The [Wiki page](https://github.com/Microsoft/vscode-azure-iot-toolkit/wiki) includes a comprehensive getting started guide as well as  detailed usage instructions of the following features:

* IoT Hub management
    * Create IoT Hub
    * Select IoT Hub
    * Copy IoT Hub Connection String
    * Generate SAS Token for IoT Hub
* Device management
    * List devices
    * Get device info
    * Create IoT device
    * Create Edge device
    * Delete device
    * Copy Device Connection String
    * Generate SAS Token for Device
* Module management
    * List Modules
    * Get Module Info
    * Create Module
    * Edit Module Twin
    * Invoke Module Direct Method
    * Copy Module Connection String
    * Delete Module
* Interact with Azure IoT Hub
    * Generate Code for C#, F#, Go, Java, Node.js, PHP, Python, Ruby or REST API
    * Send D2C message to IoT Hub
    * Monitor Built-in Event Endpoint
    * Send C2D message to device
    * Receive C2D message from IoT Hub
    * Invoke Device Direct Method
    * Edit Device Twin
    * Manage Azure IoT distributed tracing
* Interact with Azure IoT Edge (Install [Azure IoT Edge](https://marketplace.visualstudio.com/items?itemName=vsciot-vscode.azure-iot-edge) for more IoT Edge support)
    * List Modules 
    * Edit Module Twin
    * Create deployment for Single Device
    * Create Deployment at Scale
* Endpoints management
    * List Built-in and Custom Endpoints
    * Monitor Custom Event Hub Endpoint

### Prerequisites

1. In Explorer of VS Code, click ""Azure IoT Hub"" in the bottom left corner.

  ![Click Device Explorer](images/device-explorer-click.png)

2. Click ""Set IoT Hub Connection String"" in context menu.

  ![Set Connection String](images/set-connection-string.png)

3. An input box will pop up, then enter your IoT Hub Connection String (It is one-time configuration, and please make sure it is **IoT Hub Connection String** not **Device Connection String**. The format is `HostName=<my-hub>.azure-devices.net;SharedAccessKeyName=<my-policy>;SharedAccessKey=<my-policy-key>`).

  ![Enter Connection String](images/enter-connection-string.png)

4. The devices list will be shown.

  ![Device Explorer](images/device-explorer.png)

### Sign in to Azure

Instead of copying and pasting to set IoT Hub Connection String, you could sign in to Azure to select IoT Hub from your Azure Subscription.

1. Click ""Select IoT Hub"" in context menu.

  ![Select IoT Hub](images/select-iot-hub.png)

2. If you have not signed in to Azure, a pop-up will show to let you sign in to Azure.
3. After you sign in, your Azure Subscription list will be shown, then select an Azure Subscription.
4. Your IoT Hub list will be shown, then select an IoT Hub.
5. The devices and endpoints list will be shown.

  ![IoT Hub Explorer](images/iot-hub-explorer.png)

## Device Provisioning Service Explorer

1. Open ""Azure"" view on the Activity Bar, and expand ""IOT HUB DEVICE PROVISIONING SERVICE"".

![DPS Explorer](images/dps-explorer.png)

2. If you're not signed in, click ""Sign in to Azure..."" to sign in.

3. Expand one subscription to start exploring your device provisioning services.


## Code Generation

![Code Generation](images/code-generation.gif)

## Code Snippets

| Trigger | Content |
| ---- | ---- |
| iotSendD2CMessage | Send D2C message to IoT Hub |
| iotMonitorD2CMessage | Monitor D2C message for IoT Hub |
| iotSendC2DMessage | Send C2D message to device |
| iotMonitorC2DMessage | Monitor C2D message from IoT Hub |
| iotCallDirectMethods | Send direct methods to device |
| iotReceiveDirectMethods | Receive direct methods from IoT Hub |

![Snippet](images/snippet.gif)

> After code snippet is created, you need to install corresponding npm package (e.g. [azure-iot-device-mqtt](https://www.npmjs.com/package/azure-iot-device-mqtt)) to run the code snippet.
> If you want to 'Run Code' directly, you need to install [Code Runner](https://marketplace.visualstudio.com/items?itemName=formulahendry.code-runner).

## Configuration

IoT Hub Consumer Group (default is `""$Default""`):
```json
{
    ""azure-iot-toolkit.iotHubConsumerGroup"": ""$Default""
}
```

The time span (in minutes) of monitoring D2C message before current time (default is `0`):
```json
{
    ""azure-iot-toolkit.monitorD2CBeforeNowInMinutes"": 0
}
```

Whether to show verbose info when monitoring messages (default is `false`):
```json
{
    ""azure-iot-toolkit.showVerboseMessage"": false
}
```

Whether to stringify device-to-cloud messages (default is `false`):
```json
{ 
    ""azure-iot-toolkit.iotHubD2CMessageStringify"": false
}
```

Whether to show IoT Hub info when IoT Hub Connection String is not set (default is `true`):
```json
{ 
    ""azure-iot-toolkit.showIoTHubInfo"": true
}
```

Whether to enable auto refresh of tree view (default is `false`):
```json
{ 
    ""azure-iot-toolkit.treeViewAutoRefreshEnable"": false
}
```

Time interval in seconds for tree view auto refresh, auto refresh has to be enabled for it to work. (default is `60`):
```json
{ 
    ""azure-iot-toolkit.treeViewAutoRefreshIntervalInSeconds"": 60
}
```

## Resources
- [Channel 9 video: Walkthrough of Azure IoT Hub extension](https://channel9.msdn.com/Shows/Internet-of-Things-Show/Azure-IoT-Toolkit-extension-for-Visual-Studio-Code)
- [Channel 9 video: What's new in the IoT Hub extension for VS Code](https://channel9.msdn.com/Shows/Internet-of-Things-Show/Whats-new-in-the-IoT-Toolkit-extension-for-VS-Code)
- [Create an IoT hub using the Azure IoT Tools for Visual Studio Code](https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-create-use-iot-toolkit)
- [Use Azure IoT Tools to send and receive messages between your device and IoT Hub](https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-vscode-iot-toolkit-cloud-device-messaging)
- [Use Azure IoT Tools for Azure IoT Hub device management](https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-device-management-iot-toolkit)
- [Quickly build your Azure IoT application with Node.js, Python or REST API](https://devblogs.microsoft.com/iotdev/azure-iot-toolkit-1-2-0-quickly-build-your-azure-iot-application-in-vs-code-with-node-js-python-or-rest-api/)
- [Azure IoT Hub extension supports C#, Go, Java, Node.js, PHP, Python and Ruby to develop Azure IoT application in VS Code](https://devblogs.microsoft.com/iotdev/azure-iot-toolkit-supports-c-go-java-node-js-php-python-and-ruby-to-develop-azure-iot-application-in-vs-code/)
- [Use VS Code as IoT Hub Device Simulator](https://blogs.msdn.microsoft.com/iotdev/2018/07/12/use-vs-code-as-iot-hub-device-simulator-say-hello-to-azure-iot-hub-in-5-minutes/)
- [Use VS Code to call Azure IoT Hub REST APIs](https://blogs.msdn.microsoft.com/iotdev/2018/07/19/call-azure-iot-hub-rest-apis-in-vs-code/)
- [Create and control an IoT device connected to an IoT hub (Node.js)](https://github.com/Microsoft/vscode-azure-iot-toolkit/wiki/Quickstart-Node.js)
- [Create and control an IoT device connected to an IoT hub (.NET)](https://github.com/Microsoft/vscode-azure-iot-toolkit/wiki/Quickstart-.NET)
- [Handy Tool When You Develop With Azure IoT](https://blogs.msdn.microsoft.com/iotdev/2017/09/01/handy-tool-when-you-develop-with-azure-iot/)
- [Azure IoT Hub extension for Visual Studio Code generally available for managing Azure IoT Hub and Devices with ease](https://blogs.msdn.microsoft.com/iotdev/2018/06/30/azure-iot-toolkit-for-visual-studio-code-generally-available-for-managing-azure-iot-hub-and-devices-with-ease/)

## ❤️ Contributors

Thanks to all the [contributors](https://github.com/Microsoft/vscode-azure-iot-toolkit/graphs/contributors)!



## Data/Telemetry
This project collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](http://go.microsoft.com/fwlink/?LinkId=521839) to learn more. 
If you don’t wish to send usage data to Microsoft, you can set the `telemetry.enableTelemetry` setting to `false`. Learn more in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).
"
2,microsoft/vscode-azuretools,TypeScript,"# VSCode Azure SDK for Node.js

[![Build Status](https://dev.azure.com/ms-azuretools/AzCode/_apis/build/status/vscode-azuretools)](https://dev.azure.com/ms-azuretools/AzCode/_build/latest?definitionId=17)

This project provides Node.js packages that make it easy to consume and manage Azure Services in Visual Studio Code.

## Modules

* [Azure Kudu](kudu/)
* [Azure App Service](appservice/)
* [Azure UI](ui/)
* [Azure Dev](dev/)

## Developing locally

In order to quickly develop and debug these packages locally, follow these instructions:
1. Navigate to the package you are developing and run `npm install`, `npm run build`, and `npm link`
1. Navigate to the project that references the package you're developing and run `npm link <name of package>`

Example:
```
    cd ~/repos/vscode-azuretools/ui
    npm install
    npm run build
    npm link
    cd ~/repos/vscode-azurestorage
    npm link vscode-azureextensionui
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License
[MIT](LICENSE.md)"
3,microsoft/knack,Python,"Knack
=====

.. image:: https://img.shields.io/pypi/v/knack.svg
    :target: https://pypi.python.org/pypi/knack

.. image:: https://img.shields.io/pypi/pyversions/knack.svg
    :target: https://pypi.python.org/pypi/knack

.. image:: https://dev.azure.com/azure-sdk/public/_apis/build/status/cli/microsoft.knack?branchName=dev
    :target: https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=1643&branchName=dev


------------


::

    _                     _
   | | ___ __   __ _  ___| | __
   | |/ / '_ \ / _` |/ __| |/ /
   |   <| | | | (_| | (__|   <
   |_|\_\_| |_|\__,_|\___|_|\_\


**A Command-Line Interface framework**

Installation is easy via pip:

.. code-block:: bash

    pip install knack

Knack can be installed as a non-privileged user to your home directory by adding ""--user"" as below:

.. code-block:: bash

    pip install knack --user

------------

.. note:: The project is in `initial development phase <https://semver.org/#how-should-i-deal-with-revisions-in-the-0yz-initial-development-phase>`__. We recommend pinning to at least a specific minor version when marking **knack** as a dependency in your project.

------------


Usage
=====


.. code-block:: python

    import sys
    from collections import OrderedDict

    from knack import CLI, ArgumentsContext, CLICommandsLoader
    from knack.commands import CommandGroup


    def abc_str(length=3):
        import string
        return string.ascii_lowercase[:length]


    class MyCommandsLoader(CLICommandsLoader):
        def load_command_table(self, args):
            with CommandGroup(self, 'abc', '__main__#{}') as g:
                g.command('str', 'abc_str')
            return OrderedDict(self.command_table)

        def load_arguments(self, command):
            with ArgumentsContext(self, 'abc str') as ac:
                ac.argument('length', type=int)
            super(MyCommandsLoader, self).load_arguments(command)


    mycli = CLI(cli_name='mycli', commands_loader_cls=MyCommandsLoader)
    exit_code = mycli.invoke(sys.argv[1:])
    sys.exit(exit_code)

    # $ python mycli.py abc str
    # ""abc""

    # $ python mycli.py abc str --length 5
    # ""abcde""

    # $ python mycli.py abc str --length 100
    # ""abcdefghijklmnopqrstuvwxyz""


More samples and snippets are available at `examples <https://github.com/Microsoft/knack/tree/dev/examples>`__.


Documentation
=============

Documentation is available at `docs <https://github.com/Microsoft/knack/tree/dev/docs>`__.

Developer Setup
===============

In a virtual environment, install the `requirements.txt` file.

.. code-block:: bash

    pip install -r requirements.txt
    pip install -e .

Run Automation
==============

This project supports running automation using `tox <https://tox.readthedocs.io/en/latest/>`__.

.. code-block:: bash

    pip install tox
    tox


Real-world uses
===============

- `Azure CLI <https://github.com/Azure/azure-cli/>`__: The Azure CLI 2.0 is Azure's new command line experience for managing Azure resources.
- `VSTS CLI <https://github.com/Microsoft/vsts-cli>`__: A command-line interface for Visual Studio Team Services (VSTS) and Team Foundation Server (TFS). With the VSTS CLI, you can manage and work with resources including pull requests, work items, builds, and more.
- `Service Fabric CLI <https://github.com/Azure/service-fabric-cli>`__: A command-line interface for interacting with Azure Service Fabric clusters and their related entities.

Do you use knack in your CLI as well? Open a pull request to include it here. We would love to have it in our list.


Release History
===============

See `GitHub Releases <https://github.com/Microsoft/knack/releases>`__.


Contribute Code
===============

This project has adopted the `Microsoft Open Source Code of Conduct <https://opensource.microsoft.com/codeofconduct/>`__.

For more information see the `Code of Conduct FAQ <https://opensource.microsoft.com/codeofconduct/faq/>`__ or contact `opencode@microsoft.com <mailto:opencode@microsoft.com>`__ with any additional questions or comments.

If you would like to become an active contributor to this project, please
follow the instructions provided in `Contribution License Agreement <https://cla.microsoft.com/>`__.


License
=======

Knack is licensed under `MIT <LICENSE>`__.
"
4,microsoft/browsecloud,TypeScript,"**BrowseCloud - Public Demo**

[Try out BrowseCloud with a demonstration model trained on the English dictionary here.](https://aka.ms/browsecloud-demo)

**BrowseCloud - Microsoft Internal**

[If you're a Microsoft full-time employee, try out our full site.](https://aka.ms/browsecloud)

It supports creating custom visualizations with your own data set and correlate metadata with topics. This site also has a Gallery of models and visualizations with data such as the Microsoft employee engagement survey, called MSPoll, and feedback on the Windows Engineering System.

# BrowseCloud [![Build Status](https://dev.azure.com/ms/browsecloud/_apis/build/status/microsoft.browsecloud?branchName=master)](https://dev.azure.com/ms/browsecloud/_build/latest?definitionId=161&branchName=master)
![alt text](https://github.com/microsoft/browsecloud/blob/master/Images/browsecloud-screenshot.png ""A screenshot of the BrowseCloud visualization of feedback on the Windows & Devices Group Engineering Systems in 2018."")

It's a laborious task to collect and synthesize the perspectives of customers.
There's an immense amount of customer data from a variety of digital channels: survey data, StackOverflow, Reddit, email, etc.
Even for internal tools teams at Microsoft, there are at least 10,000 user feedback documents generated per quarter.

To help solve this problem, BrowseCloud is an application that summarizes feedback data via smart word clouds, called counting grids.
On a word cloud, the size of the text simply scales with the frequency of the word.
Text is scattered randomly on word clouds. In BrowseCloud, we have a word cloud where the position of the word matters.
As the user scans along the visualization, themes smoothly transition between each other.


<a href=""https://www.youtube.com/watch?v=pcsZPozC9uA""><img src=""https://raw.githubusercontent.com/microsoft/browsecloud/master/Images/IntroToBrowseCloudYouTube.PNG"" alt=""Introduction to BrowseCloud"" /></a>

<a href=""https://www.youtube.com/watch?v=OjHaiafkZXs""><img src=""https://raw.githubusercontent.com/microsoft/browsecloud/master/Images/BrowseCloudTutorial.PNG"" alt=""BrowseCloud Tutorial"" /></a>

## Features
- Add your custom text data set to the site. &ast;
- Visualize the text data by inspecting the largest words in clusters around the screen.
- Drop a pin by clicking on the visualization to view a ranked list of verbatims (shown on the far right-hand side of the screen) related to the micro-topic you pinned!
- Search for a word to narrow down the visualization and ranked list further.
- Correlate topics with positive or negative sentiment on the screen by looking at the color of the the words in a region, after applying the sentiment analysis job. &ast;
- Correlate your own custom metadata with topic. We support numeric data, nominal data with two categories, and ordinal data. &ast;
- Download the relevant verbatims into Excel!

&ast; <sub><sup>These features are not supported in the demo application. They are in the full version.</sup></sub>

## Getting Started
Our documentation is available on this repository's [wiki](https://github.com/microsoft/browsecloud/wiki).

# Build and Test
We have Azure Pipelines set up on the pull request workflow for pre-check-in validation. The pipeline will also deploy the demo site on merge with master.

Note that it is not required that you use the service to get up and running with the app.
You can quickly visualize your data by using the Python command line application to train your data,
and copying the resulting model files to the `/browsecloud-client/src/assets/demo` folder.
You can then run the demo client app by following the client setup steps and running `npm run start:demo`.

## Client

The client is a simple Angular CLI generated application.

- Ensure you have Node and NPM installed according to [Angular CLI requirements](https://angular.io/guide/setup-local).
- change directories to `/browsecloud-client`.
- run `npm install` and then `npm start`.
- open http://localhost:4200 in your browser.

At this point the client should load in your browser for local development.
You will need to adjust some of the values in `src/environments/environment.ts` in order to login with your AAD app and point the app to the correct service URL.
For more information on how to create an AAD app,
visit the [azure docs](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).

If instead you want to build to host on your own webserver, you can run `npm run build` or `npm run build:prod`. You can then host these files in a simple Azure App Service or elsewhere.

There are currently no tests, but we would love it if someone would contribute some 😉

## Service

The service is an ASP.NET Core application that has many Azure dependencies. We will first get these dependencies set up.

- Visit the Azure Portal and create a new resource of type ""Template Deployment"".
On the next page, select ""Build your own template in the editor"", and upload the template file `/deployment/az-service-template.json`.
On the next page, fill in the resource and resource group names. Purchase this resource group.
- Create an AAD app for the service. For more information on how to create an AAD app,
visit the [azure docs](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).
- Perform some setup tasks on these resources.
  - Visit the newly created Azure KeyVault, and add yourself to access the secrets in the ""Access policies"" pane.
  - In the KeyVault's ""Secrets"" pane, you will find some secret names have been generated.
  Populate these secrets with a secret for your AAD app, your Document DB secret,
  and your Redis connection string (all generated by the template file). After setting up the Azure Batch infrastructure for training the models,
  you can populate the rest of the secrets.
  - On the newly created Cosmos Document DB account, create two new containers named ""BatchJob"" and ""Document"".
- Download and install [Visual Studio 2019](https://visualstudio.microsoft.com/downloads) with the ""ASP.NET and web development"" workload.
- In `/BrowseCloud.Service/BrowseCloud.Service/appsettings.json`, configure your development environment using the information from the services you just created.
- You can then build and run using Visual Studio's built in build and run feature.

This can be built and deployed to the Azure App Service generated in the steps above for everyday use.
The easiest method is to right click on the BrowseCloud.Service project and ""Publish"", but we should recommend a CI/CD pipeline of some type.
We have our Azure DevOps build pipelines checked in as yaml files which you are welcomed to use.

There are currently no tests on the Service, but we welcome contribution on this front.

## Trainer Jobs
This is the machine learning backend that powers BrowseCloud. It has many Azure dependencies.

- Visit the Azure Portal and create a new resource of type ""Template Deployment"".
On the next page, select ""Build your own template in the editor"", and upload the template file `/deployment/az-ml-backend-template.json`.
On the next page, fill in the resource and resource group names. Purchase this resource group.

Next, we will setup our VM. The work to setup dependencies on a machine in the cloud like this is automatable, but it hasn't been done. 
- Visit the Azure Portal and choose to create a new resource of type ""Windows Server 2016 Datacenter"". In this initial setup, make sure you have RDP enabled to setup the VM. 
- RDP into the non-production VM and [follow the setup instructions to get the CountingGridsPy library running on the VM](https://github.com/microsoft/browsecloud/wiki/Environment-Setup-&-Dependencies-to-run-CountingGridsPy-Locally). In your production instance of the VM, we recommend that you have RDP turned off. 
- Save your VM as an image within the new virtual machine resource on the Azure Portal. This will destabilize the VM, so you should delete the VM.

- Next, we'll take a look at the Batch resource you generated from the template. The purpose of Batch is to manage and scale computational power with the machine learning work to do. 

Create two jobs and two pools within this Batch resource, one for your dev environment and another for your production environment. You can do this by using the Azure portal or by using `\Batch\Batch\src\deployBrowseCloudBatchPool.py`. In our design, jobs are permenant, and each training request is a task underneath each job.

We recommend that you scale the number of VMs elastically with the number of tasks running on your queue, so work can be done in parallel. You can even have multiple tasks running on the same machine using Batch. Lastly, recommend that you always have one Windows VM running and ready to go due to in the autoScale Formula.

An example scaling configuration could be:

```json
""scaleSettings"": {
    ""autoScale"": {
        ""formula"": ""maxNumberofVMs = 5;sample =$PendingTasks.GetSample(10);pendingTaskSamplePercent = avg(sample);startingNumberOfVMs = 1; pendingTaskSamples = pendingTaskSamplePercent < 2 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));$TargetDedicatedNodes=min(maxNumberofVMs, pendingTaskSamples);"",
        ""evaluationInterval"": ""PT5M""
    }
}
```

We also recommend that you use a more powerful VM in your production instance than in your development instance. We use ""vmSize"" of ""STANDARD_D16_V3"" on our production site for training new models. We use a ""vmSize"" of ""STANDARD_A1"" in our development instance.

- In `/Batch/Batch/src/metadata.json` and `/Batch/Batch/src/keys.json` (which are not checked into this repo), configure your development environment using the information from the services you just created.


# Contributing
This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

## Feedback
Your pull request will now go through extensive checks by the subject matter experts on our team.
Please be patient; we have hundreds of pull requests across all of our repositories.
Update your pull request according to feedback until it is approved by one of the team members.

## Code of conduct
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Privacy Notice

There are also some features in the software that may enable you and Microsoft to collect data from users of your applications.
If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together
with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. 
You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.

# Reporting Security Issues
Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in
the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).
"
5,microsoft/FluidFramework,TypeScript,"# Fluid

The Fluid Framework is a TypeScript library for building distributed, real-time collaborative web
applications.

## Getting started using the Fluid Framework

You may be here because you want to...

* Learn more about the Fluid Framework
* Build a Fluid object

Documentation and guides can be found at <https://fluidframework.com/>.

Hello World repo can be found at <https://github.com/microsoft/FluidHelloWorld>.

Core Examples repo can be found at <https://github.com/microsoft/FluidExamples>.

Have questions? Engage with other Fluid Framework users and developers on
[StackOverflow](https://stackoverflow.com/questions/tagged/fluid-framework)

## Code structure

The core code for both the Fluid client packages _and_ the reference ordering service is contained within this repo.

The repo structure is somewhat unique because it contains two monorepos as well as several standalone packages. The
monorepos are managed using [Lerna](https://lerna.js.org/) and are versioned separately from one another, but internally
all packages in a monorepo are versioned together. Outside the monorepos there are plenty of packages which are
versioned independently.

Here's the breakdown of the repo:

* Fluid Framework Client Monorepo ([lerna.json](./lerna.json))
  * [Packages](./packages)
  * [Fluid Examples](./examples)
* Reference Fluid Ordering Service (""Routerlicious"") Monorepo ([dir](./server/routerlicious) | [lerna.json](server/routerlicious/lerna.json))
  * [Packages](./server/routerlicious/packages)
* Common Packages
  * [Common Definitions](./common/lib/common-definitions)
  * [Common Utils](./common/lib/common-utils)
* Auxiliary Microservice Packages (supporting Routerlicious)
  * [Server dir](./server) (excluding [Routerlicious](./server/routerlicious) itself)
* Internal/Misc Packages
  * [Build Common](./common/build/build-common)
  * [ESlint Config](./common/build/eslint-config-fluid)
  * [Docs](./docs)
  * [Tools](./tools)

Dependencies between packages in various layers of the system are enforced via a build step called
[layer-check](./tools/build-tools/src/layerCheck). You can view the full list of packages and layers in
[docs/PACKAGES.md](./docs/PACKAGES.md).

## Building
In order to build the Fluid Framework, ensure that you have installed [Git](https://git-scm.com/downloads) and the version of
[Node.js](https://nodejs.org/) noted in the [.nvmrc file](https://raw.githubusercontent.com/microsoft/FluidFramework/main/.nvmrc).

Note: we recommend using nvm (for [Windows](https://github.com/coreybutler/nvm-windows) or
[MacOS/Linux](https://github.com/nvm-sh/nvm)) to install Node.js, in case you find yourself needing to install different
versions of Node.js side-by-side.

Clone a copy of the repo and change to the repo root directory:

```shell
git clone https://github.com/microsoft/FluidFramework.git
cd FluidFramework
```

Run the following to build the client packages:

```shell
npm install
npm run build:fast
```

See also: [Contributing](#Contributing)

## Testing

You can run all of our tests from the root of the repo, or you can run a scoped set of tests by running the `test`
command from the package you're interested in.

Note: Some of the tests depend on test collateral that lives in a submodule here:
<https://github.com/microsoft/FluidFrameworkTestData>.  You may choose to fetch that collateral into your local
repository, which is required to run all the tests - otherwise some will be skipped.

First install Git LFS from <https://git-lfs.github.com/>. Then, from the repo root:

```shell
git lfs install
git submodule init
git submodule update
```

### Run the tests

```shell
npm run test
```

### Include code coverage

```shell
npm run test:coverage
```

### Mimic the official CI build

Our CI pipelines run on Linux machines, and the npm scripts all have the `ci` prefix.
To replicate the test steps from the CI pipeline locally, run the following commands for the packages or Lerna monorepos:

Run      | Non-Windows                | Windows                                               |
---------|----------------------------|-------------------------------------------------------|
PR       | `npm run ci:test`          | `npm run test:report && npm run test:copyresults`     |
Official | `npm run ci:test:coverage` | `npm run test:coverage && npm run test:copyresults`   |

### Run tests from within VS Code

We've checked in [VS Code configuration](https://github.com/microsoft/FluidFramework/blob/main/.vscode/launch.json)
enabling F5 from a `spec.ts` file to run those tests if you set the debug configuration to ""Debug Current Test"".

## Run it locally

### Single browser window, two panes

_This will use an in-memory implementation of the Fluid server to sync between the two panes in the browser window._

* Choose an example under `/examples`
* Navigate to the example's directory, e.g. `/examples/data-objects/clicker`
* `npm run start`
* Browse to <http://localhost:8080> to interact with two copies of the example side-by-side

### Multiple browser instances on the same device

_This will run the local Fluid server implementation we call ""Tinylicious"", so you can sync between multiple browser
instances._

First, start Tinylicious by running these commands from `/server/tinylicious`:

```shell
npm install
npm run build
npm run start
```

Then:

* Navigate to the example of your choice (same as above)
* `npm run start:tinylicious`
* Browse to <http://localhost:8080,> copy the full URL you're redirected to, and open in a second window to collaborate

## Contributing

There are many ways to [contribute](https://github.com/microsoft/FluidFramework/blob/main/CONTRIBUTING.md) to Fluid.

* Participate in Q&A on [StackOverflow](https://stackoverflow.com/questions/tagged/fluid-framework)
* [Submit bugs](https://github.com/microsoft/FluidFramework/issues) and help us verify fixes as they are checked in.
* Review the [source code changes](https://github.com/microsoft/FluidFramework/pulls).
* [Contribute bug fixes](https://github.com/microsoft/FluidFramework/blob/main/CONTRIBUTING.md).

Detailed instructions for working in the repo can be found in the
[Wiki](https://github.com/microsoft/FluidFramework/wiki).

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

This project may contain Microsoft trademarks or logos for Microsoft projects, products, or services. Use of these
trademarks or logos must follow Microsoft’s [Trademark & Brand Guidelines](https://www.microsoft.com/trademarks). Use of
Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft
sponsorship.
"
6,microsoft/PowerBI-visuals-CardBrowser,TypeScript,"
[![Node.js CI](https://github.com/microsoft/PowerBI-visuals-CardBrowser/workflows/Node.js%20CI/badge.svg)](https://github.com/microsoft/PowerBI-visuals-CardBrowser/actions)

# Card Browser
Browse documents using double-sided cards, and click to view in place.

Card Browser is a document set viewer featuring flippable, double-sided cards for natural navigation of media collections. 

The Preview face of each card renders the headline image, title, and origin of the story with a text sample, enabling rapid discovery of documents of interest.  Flipping the cards reveals the MetaData face, which lists document properties. Clicking on a card expands it in place for detailed reading.

![Alt text](assets/2-reader.png?raw=true ""Card Browser Reader"")

![Alt text](assets/3-metadata.png?raw=true ""Card Browser Metadata"")
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Debugging

* Install ssl certificate by running `yarn run install-certificate` and following the steps from: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md)
* Enable Developer Tools in PowerBI: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md)
* Run `yarn start` to start development.

## Building

* Run `yarn run package` to package the visual.
* `.pbiviz` file will be generated in the `dist` folder

## Testing

* Run `yarn test`
"
7,microsoft/Azure-Sphere-DevX,C,"# Azure Sphere DevX library

The DevX library can accelerate your development and improve your developer experience with Azure Sphere. The DevX library addresses many common Azure Sphere scenarios, it will help reduce the amount of code you write and improve readability and long-term application maintenance.

To learn more about Azure Sphere and Azure RTOS check out [Combining Azure Sphere IoT security with Azure RTOS real-time capabilities](https://techcommunity.microsoft.com/t5/internet-of-things/combining-azure-sphere-iot-security-with-azure-rtos-real-time/ba-p/1992869) article.

There are two Microsoft Learn modules which include hands-on labs you can download to start your Azure Sphere and Azure RTOS journey.

- [Develop secure IoT solutions for Azure Sphere, Azure RTOS and Azure IoT Central](https://docs.microsoft.com/en-us/learn/modules/develop-secure-iot-solutions-azure-sphere-iot-central?WT.mc_id=iot-10976-dglover)
- [Develop secure IoT Solutions for Azure Sphere, Azure RTOS and IoT Hub](https://docs.microsoft.com/en-us/learn/modules/develop-secure-iot-solutions-azure-sphere-iot-hub?WT.mc_id=iot-11691-dglover)

The DevX library is built from the [Azure Sphere samples](https://github.com/Azure/azure-sphere-samples), it's well tested, and aims to facilitate Azure Sphere best practices. The DevX library is lightweight, addresses common scenarios, and will sit alongside your existing code base.

The DevX library design is context-based, you declare a context and implement a context handler (or callback). See the [Encapsulate Pattern](https://accu.org/journals/overload/12/63/kelly_246/), it's a fair description of how this library works.

The library prefixes all file names, functions, structures, and enums with DX_ or dx_ to avoid clashes with existing code and file names.

The library supports the following contexts:

Note, you will find examples of each context in the [examples folder](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples) of this repo or you can click on the context type to navigate to an example in your web browser.

1. [Azure IoT messaging](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/send_message).
1. [Direct Methods](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/direct_methods).
1. [Device Twins](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/device_twins).
1. [GPIO](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/gpio_example). Note, GPIO is supported as it's use can be generalized. There are no plans to generalize the use of ADC, PWM, I2C and SPI peripherals given the varied nature of their use.
1. [Intercore communications](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/intercore_example).
1. [Termination](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/terminate_example).
1. [Timers](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples/timer_example).

---

## How to use the library

This example assumes you are using git, otherwise download the [Azure Sphere DevX library](https://github.com/microsoft/Azure-Sphere-DevX) and add to the directory to your project.

```bash
git submodule add https://github.com/microsoft/Azure-Sphere-DevX.git
```

### Update your CMakeLists.txt to include the DevX library

#### Add the library sub directory

```text
add_subdirectory(""Azure-Sphere-DevX"" out)
```

#### Add *azure_sphere_devx* to the link libraries

```text
target_link_libraries (${PROJECT_NAME} applibs pthread gcc_s c azure_sphere_devx)
```

#### Add to include directories

```text
target_include_directories(${PROJECT_NAME} PUBLIC Azure-Sphere-DevX/include )
```

---

## Example CMakeLists.txt file

The following is an example of what the completed CMakeLists.txt file could look like.

```text
cmake_minimum_required (VERSION 3.10)
project (AzureSphereAzureIoT C)

azsphere_configure_tools(TOOLS_REVISION ""21.01"")
azsphere_configure_api(TARGET_API_SET ""8"")

add_subdirectory(""Azure-Sphere-DevX"" out)

set(Source
    ""main.c""
)
source_group(""Source"" FILES ${Source})

set(ALL_FILES
    ${Source}
)

# Create executable
add_executable(${PROJECT_NAME} ${ALL_FILES})

target_compile_definitions(${PROJECT_NAME} PUBLIC AZURE_IOT_HUB_CONFIGURED)
target_link_libraries(${PROJECT_NAME} applibs pthread gcc_s c azure_sphere_devx )

target_include_directories(${PROJECT_NAME} PUBLIC Azure-Sphere-DevX/include . )

target_compile_options(${PROJECT_NAME} PRIVATE -Wno-unknown-pragmas)

azsphere_target_add_image_package(${PROJECT_NAME})
```

---

## Examples

You can find fully documented examples in the [examples folder](https://github.com/microsoft/Azure-Sphere-DevX/tree/main/examples).

The following is an example of how you can declare a number of device twins context objects.

```c
static DX_DEVICE_TWIN_BINDING dt_desired_sample_rate = {
	.twinProperty = ""DesiredSampleRate"",
	.twinType = DX_TYPE_INT,
	.handler = dt_desired_sample_rate_handler };
```

```c
static DX_DEVICE_TWIN_BINDING dt_reported_temperature = {
	.twinProperty = ""ReportedCurrentTime"",
	.twinType = DX_TYPE_STRING };
```

You will see that the declarations include the property name, the type, and the handler function to call when a device twin update for the property is received.

Next the each device twins needs to be added by reference to an array (or set) of device twins.

```c
DX_DEVICE_TWIN_BINDING* deviceTwinBindingSet[] = { &dt_desired_sample_rate, &dt_reported_temperature };
```

and finally the array of device twins needs to be initialized or started.

```c
dx_timerSetStart(timerSet, NELEMS(timerSet));
```

When a device twin message is received, this set of device twins is checked for a matching property name, when a match is found, the JSON payload is deserialized, the type is checked to ensure it is of the correct type as declared in the device twin context. Then the device twin context handler is called passing the context by reference to the handler function.

The following is an example of the device twin handler that is called.

```c
static void dt_desired_sample_rate_handler(DX_DEVICE_TWIN_BINDING* deviceTwinBinding) {
	// validate data is sensible range before applying
	if (deviceTwinBinding->twinType == DX_TYPE_INT && *(int*)deviceTwinBinding->twinState >= 0 && *(int*)deviceTwinBinding->twinState <= 120) {
		dx_timerChange(&report_now_timer, &(struct timespec){*(int*)deviceTwinBinding->twinState, 0});
		dx_deviceTwinAckDesiredState(deviceTwinBinding, deviceTwinBinding->twinState, DX_DEVICE_TWIN_COMPLETED);
	} else {
		dx_deviceTwinAckDesiredState(deviceTwinBinding, deviceTwinBinding->twinState, DX_DEVICE_TWIN_ERROR);
	}

	/*	Casting device twin state examples

		float value = *(float*)deviceTwinBinding->twinState;
		int value = *(int*)deviceTwinBinding->twinState;
		bool value = *(bool*)deviceTwinBinding->twinState;
		char* value = (char*)deviceTwinBinding->twinState;
	*/
}
```

This is how the context model works. You declare the object and you pass by reference to a function that understands the context object.  You don't have to deal with all the underlying code for managing the device twin callback, the JSON deserialisation, or the type checking. That is all done for you, the function is called, and you have access to the context to make further decisions in your code.
"
8,microsoft/ai.ed,Python,"# AI.Ed

This is the AI for (programming) EDucation project.  The goal is to provide a great set of tools to provide AI-powered
assistance to students taking programming classes and educators teaching them.

This project is open source and freely available for educational purposes.  All of the source code in this repository is
available under the MIT license.

**Please note:** this project depends on the Microsoft PROSE SDK, which is a separate, closed-source binary licensed
under separate, proprietary terms.  When the Microsoft PROSE SDK is included in the project at build time, the terms of
the Microsoft PROSE SDK may limit what you can do with your build of this project (e.g., non-commercial use only).  See
the [LICENSE](LICENSE) file for more information.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a Contributor License
Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate
the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only
need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks
or logos is subject to and must follow [Microsoft's Trademark & Brand
Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft
trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any
use of third-party trademarks or logos are subject to those third-party's policies.
"
9,microsoft/vscode-hydrate,TypeScript,"# Hydrate Extension for Visual Studio Code
[![Build Status](https://dev.azure.com/epicstuff/vscode-hydrate/_apis/build/status/microsoft.vscode-hydrate?branchName=master)](https://dev.azure.com/epicstuff/vscode-hydrate/_build/latest?definitionId=104&branchName=master)
## Overview
This is the Visual Studio Code Hydrate extension, which builds upon the [VSCode Kubernetes Extension](https://github.com/Azure/vscode-kubernetes-tools). It allows developers to use [Hydrate](https://github.com/microsoft/hydrate) within VSCode, which crawls a Kubernetes cluster and generates a high level description in a `component.yaml` file for its deployments.

Instead of running Hydrate from the command line and entering flags and options manually, this extension allows users to select a Kubernetes cluster and run Hydrate within VSCode.
 
## Install the Extension!
First, make sure that you have [Docker](https://www.docker.com/) set up (the extension runs Hydrate on Docker). Next, download the extension [here](https://marketplace.visualstudio.com/items?itemName=madelineliao.vscode-hydrate). If you do not have the [VSCode Kubernetes Extension](https://github.com/Azure/vscode-kubernetes-tools) installed, it will automatically be installed during the Hydrate extension installation. A window reload is required after installation.

Navigate to the Kubernetes view by clicking the Kubernetes icon in the sidebar. Right-click the cluster you would like to run Hydrate on, and select `Hydrate Cluster`. You will be prompted step-by-step through selecting options for Hydrate (e.g. output file path).

Note: all clusters displayed in the sidebar are associated with the same `kubeconfig` file. To test out a different kubeconfig, click the ""options"" icon (the three dots) in the Kubernetes extension cluster explorer and click `Set Kubeconfig` to change the current `kubeconfig` file used. Then, you can run Hydrate on the newly displayed clusters with the new kubeconfig. 

![](https://thumbs.gfycat.com/DifficultPiercingIndianjackal-size_restricted.gif)

For example, the results of running a verbose dry-run:

![alt text](https://thumbs.gfycat.com/CreativeSpectacularHound-size_restricted.gif)
## Testing the Extension
First, clone the repo locally by running the following command:
```
git clone https://github.com/microsoft/vscode-hydrate
```

There are two ways to run tests:
1. From the command line, within the `vscode-hydrate` directory, run:
```
npm test
```
2. Alternatively, tests can be run in VSCode. From wherever the cloned repo lives, run:
```
code ./vscode-hydrate
```
Then, navigate to the Debugger view in the sidebar. Click the dropdown next to the green 'play' button, and click `Extension Tests`. Then click the play button to run the tests. Output will be printed to the VSCode `Debug Console`.

## Dependencies
* [VSCode Kubernetes Tools and its dependencies](https://github.com/Azure/vscode-kubernetes-tools)
* [Hydrate](https://github.com/microsoft/hydrate) and its dependencies

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
10,microsoft/BuildXL,C#,"# Microsoft Build Accelerator

<img alt=""BuildXL Icon"" src=""Public/Src/Branding/BuildXL.png"" width=15%>

## Introduction

Build Accelerator, BuildXL for short, is a build engine originally developed for large internal teams at Microsoft, and owned by the [Tools for Software Engineers](https://www.microsoft.com/en-us/research/project/tools-for-software-engineers/) team, part of the Microsoft One Engineering System internal engineering group. Internally at Microsoft, BuildXL runs 30,000+ builds per day on [monorepo](https://en.wikipedia.org/wiki/Monorepo) codebases up to a half-terabyte in size with a half-million process executions per build, using distribution to thousands of data center machines and petabytes of source code, package, and build output caching. Thousands of developers use BuildXL on their desktops for faster builds even on mega-sized codebases.

BuildXL accelerates multiple build languages, including:

* MSBuild (using new features under development in MSBuild 16 which will ship in future versions of Visual Studio 2019 and the .NET Core SDK)
* CMake (under development)
* Its own internal scripting language, DScript, an experimental TypeScript based format used as an intermediate language by a small number of teams inside Microsoft

BuildXL has a command-line interface. There are currently no plans to integrate it into Visual Studio. The project is open source in the spirit of transparency of our engineering system. You may find our technology useful if you face similar issues of scale. Note that BuildXL is not intended as a replacement for MSBuild or to indicate any future direction of build languages from Microsoft.

## Documentation
The BuildXL documentation main page is [here](Documentation/INDEX.md).

## Examples and Demos
See the `Examples/` folder for basic project examples. See the [Demos](Public/Src/Demos/Demos.md) page for information about various technical demos like using the process sandboxing code.

# Building the Code

## Build Status - Azure DevOps Pipelines
[![Build status](https://dev.azure.com/mseng/Domino/_apis/build/status/8196?branchName=master)](https://dev.azure.com/mseng/Domino/_build/latest?definitionId=8196)

## Command Line Build and Test
See the [Developer Guide](Documentation/Wiki/DeveloperGuide.md) for instructions on compiling BuildXL.

# Contributing
See [CONTRIBUTING](CONTRIBUTING.md).
"
11,microsoft/vscode-css-languageservice,TypeScript,"# vscode-css-languageservice
Language services for CSS, LESS and SCSS

[![npm Package](https://img.shields.io/npm/v/vscode-css-languageservice.svg?style=flat-square)](https://www.npmjs.org/package/vscode-css-languageservice)
[![NPM Downloads](https://img.shields.io/npm/dm/vscode-css-languageservice.svg)](https://npmjs.org/package/vscode-css-languageservice)
[![Azure DevOps Build Status](https://img.shields.io/azure-devops/build/vscode/2377f926-a00b-46ed-9fb1-79465b3e998b/20.svg?label=Azure%20DevOps)](https://dev.azure.com/vscode/vscode-css-languageservice/_build?definitionId=20)
[![Travis Build Status](https://img.shields.io/travis/microsoft/vscode-css-languageservice.svg?label=Travis)](https://travis-ci.org/Microsoft/vscode-css-languageservice)

Why?
----
The _vscode-css-languageservice_ contains the language smarts behind the CSS, LESS and SCSS editing experience of Visual Studio Code
and the Monaco editor.
 - *doValidation* analyses an input string and returns syntax and lint errors.
 - *doComplete* provides completion proposals for a given location.
 - *doHover* provides a hover text for a given location.
 - *findDefinition* finds the definition of the symbol at the given location.
 - *findReferences* finds all references to the symbol at the given location.
 - *findDocumentHighlights* finds all symbols connected to the given location.
 - *findDocumentSymbols* provides all symbols in the given document
 - *doCodeActions* evaluates code actions for the given location, typically to fix a problem.
 - *findColorSymbols* evaluates all color symbols in the given document
 - *doRename* renames all symbols connected to the given location.
  - *getFoldingRanges* returns folding ranges in the given document.

Installation
------------

    npm install --save vscode-css-languageservice
    
    
API
---

For the complete API see [cssLanguageService.ts](./src/cssLanguageService.ts) and [cssLanguageTypes.ts](./src/cssLanguageTypes.ts) 


Development
-----------


- clone this repo, run yarn
- `yarn test` to compile and run tests

How can I run and debug the service?

- open the folder in VSCode.
- set breakpoints, e.g. in `cssCompletion.ts`
- run the Unit tests from the run viewlet and wait until a breakpoint is hit:
![image](https://user-images.githubusercontent.com/6461412/94239202-bdad4e80-ff11-11ea-99c3-cb9dbeb1c0b2.png)


How can I run and debug the service inside an instance of VSCode?

- run VSCode out of sources setup as described here: https://github.com/Microsoft/vscode/wiki/How-to-Contribute
- use `yarn link vscode-css-languageservice` in `vscode/extensions/css-language-features/server` to run VSCode with the latest changes from `vscode-css-languageservice`
- run VSCode out of source (`vscode/scripts/code.sh|bat`) and open a `.css` file
- in VSCode window that is open on the `vscode-css-languageservice` sources, run command `Debug: Attach to Node process` and pick the `code-oss` process with the `css-language-features` path
![image](https://user-images.githubusercontent.com/6461412/94242567-842b1200-ff16-11ea-8f85-3ebb72d06ba8.png)
- set breakpoints, e.g. in `cssCompletion.ts`
- in the instance run from sources, invoke code completion in the `.css` file



**Note: All CSS entities (properties, at-rules, etc) are sourced from https://github.com/microsoft/vscode-custom-data/tree/master/web-data and transpiled here. For adding new property or fixing existing properties' completion/hover description, please open PR there).**


License
-------

(MIT License)

Copyright 2016, 20 Microsoft

With the exceptions of `build/mdn-documentation.js`, which is built upon content from [Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web)
and distributed under CC BY-SA 2.5.
"
12,microsoft/BotFramework-Composer-Nightlies,,"
# Nightly Releases of [Bot Framework Composer](https://github.com/microsoft/BotFramework-Composer)

This repository is where nightly releases of [Bot Framework Composer](https://github.com/microsoft/BotFramework-Composer) are published.

**Note:** These releases aren't considered stable and contain the latest features in Composer.

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
13,microsoft/FHIR-Converter,Liquid,"# FHIR Converter

FHIR Converter is an open source project that enables conversion of health data from legacy formats to FHIR.

The first version of the FHIR Converter released to open source on Mar 6th, 2020. It used Handlebars template language and Javascript runtime. A new converter engine was released on Nov 13, 2020 that uses Liquid templating language and .Net runtime.

Both Handlebars and Liquid converters, and corresponding templates/filters, are supported by Microsoft. We recommend using Liquid converter for better alignment with [Azure API for FHIR](https://azure.microsoft.com/en-us/services/azure-api-for-fhir/), [FHIR Server for Azure](https://github.com/microsoft/fhir-server), and [Microsoft Logic Apps](https://azure.microsoft.com/en-us/services/logic-apps/).

The following table compares the two converter engines:

|  | Handlebars Engine | Liquid Engine | 
| ----- | ----- | ----- |
| **Template language** | [Handlebars](https://handlebarsjs.com/) | [Liquid](https://shopify.github.io/liquid/) |
| **Template authoring tool** | Self-hosted web-app | [VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-health-fhir-converter)|
| **Supported conversions** | 1. HL7v2 to FHIR <br> 2. C-CDA to FHIR | 1. HL7 v2 to FHIR <br> 2. *C-CDA to FHIR (to be released soon)*|
| **Available as** | 1. Self-deployed web service <br> (on-prem or on Azure)| 1. Command line tool <br> 2. $convert-data operation in  FHIR Server for Azure <br> 3. $convert-data operation in Azure API for FHIR.|


⚠ Rest of this document is about the Liquid converter. For the Handlebars converter, refer to the [Handlebars branch](https://github.com/microsoft/FHIR-Converter/tree/handlebars).

The Converter makes use of templates that define the mappings between different data formats.
The templates are written in [Liquid](https://shopify.github.io/liquid/) templating language and make use of custom [filters](docs/FiltersSummary.md), which make it easy with work with HL7 v2 messages.

The converter comes with ready to use templates for HL7v2 to FHIR conversion. These templates are based on the [spreadsheet](https://docs.google.com/spreadsheets/d/1PaFYPSSq4oplTvw_4OgOn6h2Bs_CMvCAU9CqC4tPBgk/edit#gid=0) created by the HL7 [2-To-FHIR project](https://confluence.hl7.org/display/OO/2-To-FHIR+Project). If needed, you can create new, or modify existing templates to meet your specific conversion requirements.

FHIR Converter with DotLiquid engine is integrated into the [Azure API for FHIR](https://azure.microsoft.com/en-us/services/azure-api-for-fhir/), and [FHIR Server for Azure](https://github.com/microsoft/fhir-server) as the [$convert-data](https://docs.microsoft.com/en-us/azure/healthcare-apis/convert-data) operation. In addition, it is also available as a command-line tool. The converter transforms the input data into FHIR bundles that can be persisted to a FHIR server.

This project consists of the following components:

1. A command-line tool) for converting data and managing templates.
2. [Templates](data/Templates) for HL7 v2 to FHIR conversion.
3. [Sample data](data/SampleData) for testing purpose.

## Using the FHIR Converter

### $convert-data operation in the Azure API for FHIR

FHIR Converter is integrated into Azure API for FHIR, and FHIR Server for Azure to run as part of the service. Refer to the [$convert-data](https://docs.microsoft.com/en-us/azure/healthcare-apis/convert-data) documentation for using the FHIR converter in the FHIR Server for Azure.

### Command-line tool

**Convert Data**

The command-line tool can be used to convert a folder containing HL7 v2 messages to FHIR resources.
Here are the parameters that the tool accepts:

| Option | Name | Optionality | Default | Description |
| ----- | ----- | ----- |----- |----- |
| -d | TemplateDirectory | Required | | Root directory of templates. |
| -r | RootTemplate | Required | | Name of root template. Valid values are ADT_A01, OML_O21, ORU_R01, VXU_V04. |
| -c | InputDataContent | Optional| | Input data content. Specify OutputDataFile to get the results. |
| -f | OutputDataFile | Optional | | Output data file. |
| -i | InputDataFolder | Optional | | Input data folder. Specify OutputDataFolder to get the results. |
| -o | OutputDataFolder | Optional | | Output data folder. |
| -t | IsTraceInfo | Optional | | Provide trace information in the output if ""-t"" is set. |
| --version | Version | Optional | | Display version information. |
| --help | Help | Optional | | Display usage information of this tool. |

Example usage to convert HL7 v2 messages to FHIR resources in a folder:
```
>.\Microsoft.Health.Fhir.Liquid.Converter.Tool.exe convert -d myTemplateDirectory -r ADT_A01 -i myInputDataFolder -o myOutputDataFolder
```

**Manage Templates**

The command-line tool also supports managing different versions of templates from Azure Container Registry (ACR). Users can customize templates and store them on ACR if default templates can not meet requirements. After [ACR authentication](docs/TemplateManagementCLI.md), users can pull and push templates from/to a remote ACR through our tool.

Example command to push a collection of templates to ACR image from a folder:
```
>.\Microsoft.Health.Fhir.Liquid.Converter.Tool.exe push testacr.azurecr.io/templatetest:default myInputFolder
```
Example usage of pulling an image of templates in a folder:

```
>.\Microsoft.Health.Fhir.Liquid.Converter.Tool.exe pull testacr.azurecr.io/templatetest@sha256:412ea84f1bb1a9d98345efb7b427ba89616ec29ac332d543eff9a2161ca12a58 myOutputFolder

```
More details of usage are given in [Template Management CLI tool](docs/TemplateManagementCLI.md).

Besides current version of [templates](data/Templates) given in our project, other versions that released by Microsoft are stored in a public ACR: healthplatformregistry.azurecr.io, users can directly pull templates from ``` healthplatformregistry.azurecr.io/hl7v2defaulttemplates:<version> ``` without authentication.
>Note!: Template version is aligned with the version of FHIR Converter. 

## Usage Notes

### Resource ID generation

The default templates provided with the Converter computes resource ids using the fields present in the input data. In order to preserve the generated resource ids, the converter created PUT requests, instead of POST requests in the generated bundles.

A set of [templates](data/Templates/Hl7v2/ID) help generate FHIR resource IDs from HL7 v2 messages. An ID generation template does 3 things: 1) extract identifiers from input segment or field; 2) combine the identifers with resource type and base ID (optional) as hash seed; 3) compute hash as output ID.

The Converter introduces a concept of ""base resource/base ID"". Base resources are independent entities, like Patient, Organization, Device, etc, whose IDs are defined as base ID. Base IDs could be used to generate IDs for other resources that relate to them. It helps enrich the input for hash and thus reduce ID collision.
For example, a Patient ID is used as part of hash input for an AllergyIntolerance ID, as this resource is closely related with a specific patient.

Below is an example where an AllergyIntolerance ID is generated, using ID/AllergyIntolerance template, AL1 segment and patient ID as its base ID.
The syntax is `{% evaluate [id] using [template] [variables] -%}`.

```liquid
{% evaluate allergyIntoleranceId using 'ID/AllergyIntolerance' AL1: al1Segment, baseId: patientId -%}
```

### Resource validation and post-processing

Real world HL7 messages vary in richness and level of conformance with the spec. The output of converter depends on the templates as well as the quality and richness of input messages. Therefore, it is important that you review and validate the Converter output before using those in production.

In general, you can use [HL7 FHIR validator](https://wiki.hl7.org/Using_the_FHIR_Validator) to validate a FHIR resource. You may be able to fix some of the conversion issues by appropriately changing the templates. For other issues, you may need to have a post-processing step in your pipeline.

In some cases, due to lack of field level data in the incoming messages, the Converter may produce resources without useful information or even without ID. You can use `Hl7.Fhir.R4` .NET library to filter such resources in your pipeline. Here is the sample code for such purpose.

```C#
using Hl7.Fhir.Model;
using Hl7.Fhir.Serialization;
using System;
using System.Collections.Generic;
using System.Linq;

public class PostProcessor
{
    private readonly FhirJsonParser _parser = new FhirJsonParser();

    public IEnumerable<Resource> FilterResources(IEnumerable<string> fhirResources)
    {
        return fhirResources
            .Select(fhirResource => _parser.Parse<Resource>(fhirResource))
            .Where(resource => !IsEmptyResource(resource))
            .Where(resource => !IsIdAbsentResource(resource));
    }

    public bool IsEmptyResource(Resource resource)
    {
        try
        {
            var fhirResource = resource.ToJObject();
            var properties = fhirResource.Properties().Select(property => property.Name);
            // an empty resource contains no properties other than ""resourceType"" and ""id""
            return !properties
                .Where(property => !property.Equals(""resourceType""))
                .Where(property => !property.Equals(""id""))
                .Any();
        }
        catch (Exception e)
        {
            Console.Error.WriteLine(e.Message);
            // deal with the exception...
        }

        return false;
    }

    public bool IsIdAbsentResource(Resource resource)
    {
        try
        {
            return string.IsNullOrWhiteSpace(resource.Id);
        }
        catch (Exception e)
        {
            Console.Error.WriteLine(e.Message);
            // deal with the exception...
        }
        return false;
    }
}
```



## Reference documentation
- [Filters summary](docs/FiltersSummary.md)
- [Snippet concept](docs/SnippetConcept.md)

## External resources
- [DotLiquid wiki](https://github.com/dotliquid/dotliquid/wiki)
- [HL7 Community 2-To-FHIR-Project](https://confluence.hl7.org/display/OO/2-To-FHIR+Project)
 
## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [the CLA site](https://cla.opensource.microsoft.com).

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
14,microsoft/CBL-MarinerCoreUI,Shell,"# CBL-MarinerCoreUI

CBL-MarinerCoreUI is an internal extension of the internal [CBL-Mariner](https://github.com/microsoft/CBL-Mariner) Linux distribution for Microsoft’s cloud infrastructure and edge products and services. CBL-Mariner is designed to provide a consistent platform for these devices and services and will enhance Microsoft’s ability to stay current on Linux updates. This initiative is part of Microsoft’s increasing investment in a wide range of Linux technologies.  CBL-MarinerCoreUI is being shared publicly as part of Microsoft’s commitment to Open Source and to contribute back to the Linux community. 

As with CBL-Mariner, CBL-MarinerCoreUI makes the latest security patches and fixes available for download with the goal of fast turn-around times.

CBL-MarinerCoreUI uses the same build system as CBL-Mariner.

# Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.

# Acknowledgments 

The CBL-MarinerCoreUI project, benefits from contributions by the open software community. We gratefully acknowledge all contributions made from the broader open source community, in particular:

1) [The Fedora Project](https://start.fedoraproject.org/) for SPEC files. 

2) [OpenSUSE](https://www.opensuse.org/) for SPEC files.

3) [GNU](https://www.gnu.org/) and the [Free Software Foundation](https://www.fsf.org/)
"
15,microsoft/mixed-reality-extension-sdk-samples,JavaScript,"# Mixed Reality Extension SDK Samples

<img width='200' height='200' src='https://github.com/Microsoft/mixed-reality-extension-sdk/blob/master/branding/MRe-RGB.png'/>

The Mixed Reality Extension SDK Samples is the easiest way to build and run
your first [AltspaceVR](https://altvr.com/) extension using the [Mixed Reality
Extension SDK](
https://github.com/Microsoft/mixed-reality-extension-sdk).

## Prerequisites
* Install [Node.js 8.12](https://nodejs.org/download/release/v8.12.0/) or
newer, which includes NPM 6.4.1 or newer, from nodejs.org

## How to Build and Run the Hello World sample
From command prompt:
* `git clone http://github.com/microsoft/mixed-reality-extension-sdk-samples`
* `cd mixed-reality-extension-sdk-samples\samples\hello-world`
* `npm install` This will install all dependent packages. (and will do very
little if there are no changes)
* `npm run build` This should not report any errors.
* `npm start` This should print ""INF: Multi-peer Adapter listening on...""

In AltspaceVR
* Go to your personal home
* Make sure you are signed in properly, not a guest
* Activate the Space Editor (only available if you indicate you want to participate in the Early Access Program in your AltspaceVR settings)
* Click Basics group
* Click on SDKApp
* For the URL field, enter `ws://localhost:3901`
* Enter a session ID (This step will eventually be optional. For now, put in
any random value)
* Click Confirm
* If the app doesn't seem to load, click on the gear icon next the MRE object
in to the present objects list, and make sure ""Is Playing"" is checked.
* After the app has been placed, you will see the MRE Anchor (the white box
with red/green/blue spikes on it), rendering on top of the MRE. You can use the
anchor to move the MRE around. To hide the anchor, uncheck ""Edit Mode"".

You should now see the words ""Hello World"" above a spinning cube.
Congratulations, you have now deployed a Node.js server with the MRE SDK onto
your local machine and connected to it from AltspaceVR.

### Hosting in the Cloud
In order for other AltspaceVR users to see your SDK app running, it must be hosted in a way they can connect to it. To learn about cloud hosting and other solutions, checkout [DEPLOYING.md](https://github.com/Microsoft/mixed-reality-extension-sdk/blob/master/DEPLOYING.md) in the SDK repo.

To learn more about the SDK, please read the [MRE SDK readme](
https://github.com/Microsoft/mixed-reality-extension-sdk/blob/master/README.md).

## Sample Descriptions
* Hello World - Shows text and a cube that animates when highlighted or clicked. Demonstrates basic scene creation and interaction.
* Solar System - Loads a 3d model for each planet and animates planetary motion. Demonstrates animation generation and more advanced scene creation.
* Tic-Tac-Toe - The classic game also known as ""Noughts & Crosses"". Demonstrates gameplay with win/lose conditions.
* Wear A Hat - Users can choose a hat from a menu and it will appear on their head. Demonstrates attachments.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
16,microsoft/vscode-node-debug,TypeScript,"# Node Debug (legacy)

[![build status](https://travis-ci.org/Microsoft/vscode-node-debug.svg?branch=master)](https://travis-ci.org/Microsoft/vscode-node-debug)
[![build status](https://ci.appveyor.com/api/projects/status/t74psolxi3k7bcjp/branch/master?svg=true)](https://ci.appveyor.com/project/weinand/vscode-node-debug)

This extension is bundled with Visual Studio Code and together with **Node Debug** forms the [Node.js](https://nodejs.org) debugging experience.

**Node debug (legacy)** is the debugger for Node.js versions < 8.0.

See a general overview of debugging in VS Code [here](https://code.visualstudio.com/docs/editor/debugging).

Documentation for Node.js specific debugging can be found [here](https://code.visualstudio.com/docs/nodejs/nodejs-debugging).

Please submit bugs and feature requests to the [VS Code repository](https://github.com/microsoft/vscode/issues).


## License

Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [MIT](LICENSE.txt) License.
"
17,microsoft/sarif-js-sdk,JavaScript,"# SARIF JS SDK

JavaScript code and supporting files for working with the 'Static Analysis Results Interchange Format' [SARIF][sarif].

| Package                                              | Version                                                                                                                     | Description                                          |
| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| [@microsoft/jest-sarif][@microsoft/jest-sarif]       | [![Version](https://img.shields.io/npm/v/@microsoft/jest-sarif.svg)](https://npmjs.org/package/@microsoft/jest-sarif)       | Custom SARIF matchers for [Jest][jest].              |
| [@microsoft/sarif-builder][@microsoft/sarif-builder] | [![Version](https://img.shields.io/npm/v/@microsoft/sarif-builder.svg)](https://npmjs.org/package/@microsoft/sarif-builder) | A builder library for authoring [SARIF][sarif] logs. |

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.

[@microsoft/jest-sarif]: https://github.com/microsoft/sarif-js-sdk/tree/main/packages/jest-sarif
[@microsoft/sarif-builder]: https://github.com/microsoft/sarif-js-sdk/tree/main/packages/sarif-builder
[sarif]: https://github.com/oasis-tcs/sarif-spec
[jest]: https://facebook.github.io/jest/
"
18,microsoft/react-native-tscodegen,TypeScript,"# react-native-tscodegen

TypeScript Code Generation for React Native Turbo Module

- Index
  - Contributing
  - Building this repo
  - Packages
  - Deploying
  - Development

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Building this repo

```cmd
yarn
yarn build
yarn test
```

## Packages

### tslint-shared

This is the shared tslint configuration for all other packages.

### RN-TSCodegen

This is the TypeScript code generation for TurboModule in react native.

There are two important exported functions:

- **typeScriptToCodeSchema** function:
  - **fileName** argument: Full path to a TypeScript source file, a module name.
  - **moduleName** argument: The module name. It is not reflected in generated files.
  - **targetName** argument (optional): It will be used in **the entry header file**, if this TypeScript source file registers a native module.
  - Output: A `SchemaType` data structure.
- **generator.generate** function
  - **options** argument:
    - **libraryName** property: A string that becomes part of type names in generated files.
    - **schema** property: Result from `typeScriptToCodeSchema`
    - **outputDirectory** property: Full path to a folder to write files. Multiple files will be generated and most of the file names are hard-coded.
    - **moduleSpecName** property: Name of **the entry header file**, no file extension.
  - **config** arguments:
    - **generators** property: An array that is or a subset of `['descriptors', 'events', 'props', 'tests', 'shadow-nodes', 'modules']` to control what files are generated.

### RN-TSCodegen-Test

This package contains all test cases for RN-TSCodegen, with unit test code.

### minimum-flow-parser

This is a Flow parser, just enough to convert necessary files to TypeScript for this repo.

### update-test-files

Get generated files sync to `facebook/react-native`

## Deploying

- [npm install react-native-tscodegen-types](https://www.npmjs.com/package/react-native-tscodegen-types)
- [npm install react-native-tscodegen](https://www.npmjs.com/package/react-native-tscodegen)
  - Follow the description to build your first Turbo Module program!
- [Demo project](https://github.com/ZihanChen-MSFT/react-native-tscodegen-demo) (not ready)

You are welcome to use cli tool `react-native-tscodegen` instead of calling functions in build scripts by yourself if possible.
Basically, just add `react-native-tscodegen ./react-native-tscodegen.json` to npm scripts, after getting `react-native-tscodegen.json` prepared.
The file name is not important.

```json
{
    ""libraryName"": ""PlaygroundModule"",
    ""outputDirectory"": ""./lib/cpp-generated"",
    ""moduleSpecName"": ""PlaygroundModuleSpec"",
    ""generators"": [
        ""descriptors"",
        ""events"",
        ""props"",
        ""tests"",
        ""shadow-nodes"",
        ""modules""
    ],
    ""inputFile"": ""./src/turboModule.ts""
}
```

`libraryName` and `moduleSpecName` control file names and some generated C++ class names.
`generators` controls what files get generated.
After the cli tool is successfully executed,
files will be created under `outputDirectory`.

## Development

### Sync react-native after pull

```cmd
git submodule update
```

### Sync react-native to a new version

```cmd
pushd react-native
git fetch
git merge origin/master
popd
git status
```

### Works to do after updating react-native

```cmd
yarn
yarn build
pushd update-test-files
npm run start
popd
git status
```

### Fixing test case codegen

```cmd
cls & pushd packages\update-test-files & npm run build & cd ..\RN-TSCodegen-Test & npm run build & popd
```

### Fixing compiler

```cmd
cls & pushd packages\RN-TSCodegen & npm run build & cd ..\RN-TSCodegen-Test & npm run build & npm run test & popd
```
"
19,microsoft/vscode-python,TypeScript,"# Python extension for Visual Studio Code

A [Visual Studio Code](https://code.visualstudio.com/) [extension](https://marketplace.visualstudio.com/VSCode) with rich support for the [Python language](https://www.python.org/) (for all [actively supported versions](https://devguide.python.org/#status-of-python-branches) of the language: >=3.6), including features such as IntelliSense (Pylance), linting, debugging, code navigation, code formatting, refactoring, variable explorer, test explorer, and more!

## Installed extensions

The Python extension will automatically install the [Pylance](https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance) and [Jupyter](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) extensions to give you the best experience when working with Python files and Jupyter notebooks. However, Pylance is an optional dependency, meaning the Python extension will remain fully functional if it fails to be installed. You can also [uninstall](https://code.visualstudio.com/docs/editor/extension-marketplace#_uninstall-an-extension) it at the expense of some features if you’re using a different language server.

Extensions installed through the marketplace are subject to the [Marketplace Terms of Use](https://cdn.vsassets.io/v/M146_20190123.39/_content/Microsoft-Visual-Studio-Marketplace-Terms-of-Use.pdf).

## Quick start

-   **Step 1.** [Install a supported version of Python on your system](https://code.visualstudio.com/docs/python/python-tutorial#_prerequisites) (note: that the system install of Python on macOS is not supported).
-   **Step 2.** Install the Python extension for Visual Studio Code.
-   **Step 3.** Open or create a Python file and start coding!

## Set up your environment

<!-- use less words -->

-   Select your Python interpreter by clicking on the status bar

     <img src=https://raw.githubusercontent.com/microsoft/vscode-python/main/images/InterpreterSelectionZoom.gif width=280 height=100>

-   Configure the debugger through the Debug Activity Bar

     <img src=https://raw.githubusercontent.com/microsoft/vscode-python/main/images/ConfigureDebugger.gif width=734 height=413>

-   Configure tests by running the `Configure Tests` command

     <img src=https://raw.githubusercontent.com/microsoft/vscode-python/main/images/ConfigureTests.gif width=734 height=413>

## Jupyter Notebook quick start

The Python extension and the [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) work together to give you a great Notebook experience in VS Code.

-   Open or create a Jupyter Notebook file (.ipynb) and start coding in our Notebook Editor!

     <img src=https://raw.githubusercontent.com/microsoft/vscode-python/main/images/OpenOrCreateNotebook.gif width=1029 height=602>

For more information you can:

-   [Follow our Python tutorial](https://code.visualstudio.com/docs/python/python-tutorial#_prerequisites) with step-by-step instructions for building a simple app.
-   Check out the [Python documentation on the VS Code site](https://code.visualstudio.com/docs/languages/python) for general information about using the extension.
-   Check out the [Jupyter Notebook documentation on the VS Code site](https://code.visualstudio.com/docs/python/jupyter-support) for information about using Jupyter Notebooks in VS Code.

## Useful commands

Open the Command Palette (Command+Shift+P on macOS and Ctrl+Shift+P on Windows/Linux) and type in one of the following commands:

| Command                               | Description                                                                                                                                                    |
| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Python: Select Interpreter`          | Switch between Python interpreters, versions, and environments.                                                                                                |
| `Python: Start REPL`                  | Start an interactive Python REPL using the selected interpreter in the VS Code terminal.                                                                       |
| `Python: Run Python File in Terminal` | Runs the active Python file in the VS Code terminal. You can also run a Python file by right-clicking on the file and selecting `Run Python File in Terminal`. |
| `Python: Select Linter`               | Switch from Pylint to Flake8 or other supported linters.                                                                                                       |
| `Format Document`                     | Formats code using the provided [formatter](https://code.visualstudio.com/docs/python/editing#_formatting) in the `settings.json` file.                        |
| `Python: Configure Tests`             | Select a test framework and configure it to display the Test Explorer.                                                                                         |

To see all available Python commands, open the Command Palette and type `Python`. For Jupyter extension commands, just type `Jupyter`.

## Feature details

Learn more about the rich features of the Python extension:

-   [IntelliSense](https://code.visualstudio.com/docs/python/editing#_autocomplete-and-intellisense): Edit your code with auto-completion, code navigation, syntax checking and more
-   [Linting](https://code.visualstudio.com/docs/python/linting): Get additional code analysis with Pylint, Flake8 and more
-   [Code formatting](https://code.visualstudio.com/docs/python/editing#_formatting): Format your code with black, autopep or yapf

-   [Debugging](https://code.visualstudio.com/docs/python/debugging): Debug your Python scripts, web apps, remote or multi-threaded processes

-   [Testing](https://code.visualstudio.com/docs/python/unit-testing): Run and debug tests through the Test Explorer with unittest, pytest or nose

-   [Jupyter Notebooks](https://code.visualstudio.com/docs/python/jupyter-support): Create and edit Jupyter Notebooks, add and run code cells, render plots, visualize variables through the variable explorer, visualize dataframes with the data viewer, and more

-   [Environments](https://code.visualstudio.com/docs/python/environments): Automatically activate and switch between virtualenv, venv, pipenv, conda and pyenv environments

-   [Refactoring](https://code.visualstudio.com/docs/python/editing#_refactoring): Restructure your Python code with variable extraction, method extraction and import sorting

## Supported locales

The extension is available in multiple languages: `de`, `en`, `es`, `fa`, `fr`, `it`, `ja`, `ko-kr`, `nl`, `pl`, `pt-br`, `ru`, `tr`, `zh-cn`, `zh-tw`

## Questions, issues, feature requests, and contributions

-   If you have a question about how to accomplish something with the extension, please [ask on Stack Overflow](https://stackoverflow.com/questions/tagged/visual-studio-code+python)
-   If you come across a problem with the extension, please [file an issue](https://github.com/microsoft/vscode-python)
-   Contributions are always welcome! Please see our [contributing guide](https://github.com/Microsoft/vscode-python/blob/main/CONTRIBUTING.md) for more details
-   Any and all feedback is appreciated and welcome!
    -   If someone has already [filed an issue](https://github.com/Microsoft/vscode-python) that encompasses your feedback, please leave a 👍/👎 reaction on the issue
    -   Otherwise please start a [new discussion](https://github.com/microsoft/vscode-python/discussions/categories/ideas)
-   If you're interested in the development of the extension, you can read about our [development process](https://github.com/Microsoft/vscode-python/blob/main/CONTRIBUTING.md#development-process)

## Data and telemetry

The Microsoft Python Extension for Visual Studio Code collects usage
data and sends it to Microsoft to help improve our products and
services. Read our
[privacy statement](https://privacy.microsoft.com/privacystatement) to
learn more. This extension respects the `telemetry.enableTelemetry`
setting which you can learn more about at
https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting.
"
20,microsoft/Dynamics-365-Fraud-Protection-ManualReview,TypeScript,"# Microsoft Dynamics 365 Fraud Protection - Manual review

## Main documents
* [FE README](./frontend/README.md)
* [FE Contribution guide](./frontend/CONTRIBUTION.md)
* [BE README](./backend/README.md)
* [BE Contribution guide](./backend/CONTRIBUTION.md)
* [Deployment README](./arm/README.md)

## Solution structure
![FunctionalSegregation](./documentation/pictures/MRStructureDiagrams-SolutionArchitecture.png)  

## Business description

### Terms
**Order/Purchase/Transaction** : 
The main object that describes a particular act of interaction between a Merchant and a User. It's stored in 
Dynamics 365 Fraud Protection (DFP) and sometime retrived by Manual Reviev tool (MR) for synchronization and local storing.

**Item** : 
One element in MR system that represents a particular purchase.

**Decision** : 
A reflection of the Purchase Status entity. Shows the decision about aparticular purchase. Could be generated on merchant side and in MR tool.

**Enrichment** : 
When purchase event is consumed by the MR application, it has no information about the purchase, just a reference to it via purchase ID. The process of filling the item with actual purchase data is called enrichment.

**Queue** : 
A logical container in the storage dynamically filled by items based on some filters.

**Filter** : 
A set of parameters that define a set of items in a queue. A filter is created alongside the queue.

**Escalation queue** : 
A queue that contains items with ESCALATE or HOLD labels. This is just specific view of the related main queue. Items in an escalated queue could be reviewed only by supervisors.

**Residual queue** : 
A queue that consists of orders which are not matching filters of any existing queue.

**Locked queue** : 
A queue that has sorting by one of the order fields. An analyst can review items only from the top of the sorted queue.

**Unlocked queue** : 
A queue where an analyst can pick items in random order for review.

**Label** : 
A mark for an order in the queue that is applied by an analyst or senior analyst as a result of a manual review. 
Labels are divided into two groups: final labels that forms decisions (GOOD, BAD, WATCH_INCONCLUSIVE, WATCH_NA) 
and intermediate labels for internal usage in MR (ESCALATE and HOLD). Final labels form a resolution object.

**Resolution** : 
A particular final decision that was made in the MR tool. Could be retrieved during resolution lifetime.

**Tag** : 
Tag is a short mark for specifying item specific. Tags can be applied by analysts and viewed in item/resolution surfing.

**Note** : 
Note is a comment left by an analyst in the order.


### Permissions
Manual Review has role-based access which means every user should have a particular role to use particular features. There are three main kinds of roles: 
* fraud analyst, 
* senior fraud analyst
* manager/administrator
All roles should be defined for the DFP Service principal in Azure AD. 
Role assignments can be done both by the Azure portal and by the DFP User Access tab (the second way is more preferable).
In addition to main roles, some privileges can be provided to users based on in-tool actions and assignments.

All frontend-intended APIs are protected with the OAuth2.0 Implicit flow grant. 
The frontend is responsible for routing the user on Azure Active Directory login page and for the token extracting. 
Once the token obtained the frontend attach this token to each call to the backend.  
The backend uses stateless token processing with role enrichment (in Azure AD, it uses caching).

Role permissions:

| The Analyst                                                                   | The Senior Analyst                                                            | The Fraud Manager                                                              |
| ----------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| view queues assigned to him                                                   | view any queue                                                                | view any queue                                                                 |
|                                                                               | create queues                                                                 | create queues                                                                  |
|                                                                               | assign people to any queue                                                    | assign people to any queue                                                     |
|                                                                               |                                                                               | update any queue (change name and deadline) where possible                     |
|                                                                               |                                                                               | delete any queue                                                               |
| view any order on queues visible to him                                       | view any item                                                                 | view any item                                                                  |
| lock items in queues assigned to him in accordance with sorting settings      | lock items in queues assigned to him in accordance with sorting settings      | lock any order in any queue                                                    |
| label, tag, comment, unlock items locked on him                               | label, tag, comment, unlock items locked on him                               | label, tag, comment, unlock items locked on him                                |
| apply bulk decisions on items that are visible for the analyst                | apply bulk decisions on any unlocked item (including already labeled)         | apply bulk decisions on any item                                               |
|                                                                               |                                                                               | search items among the queues                                                  |
|                                                                               |                                                                               | release any lock for any analyst (future feature)                              |
|                                                                               | view demand/supply dashboard                                                  | view demand/supply dashboard                                                   |
| view performance dashboard for themselves (including per-queue activity view) | view performance dashboard for themselves (including per-queue activity view) | view performance dashboard for any analyst (including per-queue activity view) |
|                                                                               |                                                                               | view performance dashboard for any queue (including per-analyst activity view) |
| view historical queue settings for participated queues                        | view historical queue settings for any queues                                 | view historical queue settings for any queues                                  |
|                                                                               | view historical analyst info                                                  | view historical analyst info                                                   |

Assignment-based permissions:

| Queue reviewer       | Queue supervisor                                                                          |
| -------------------- | ----------------------------------------------------------------------------------------- |
| lock items           | lock items                                                                                |
|                      | lock escalated items (in escalated queue)                                                 |
| process locked items | process locked items                                                                      |
|                      | receive notifications about orders being escalated in a supervised queue (future feature) |

## Microsoft Open Source code of conduct

For additional information, see the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)."
21,microsoft/MixedRealityToolkit-Unity,C#,"# The docs have moved!

**Starting from MRTK 2.6, we are publishing both conceptual docs and API references on docs.microsoft.com. For conceptual docs, please visit <a href=""https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/"">our new landing page</a>. For API references, please visit <a href=""https://docs.microsoft.com/dotnet/api/microsoft.mixedreality.toolkit"">the MRTK-Unity section of the dot net API explorer</a>. All links on this page have been updated.** 

**Existing content will remain here but will not be updated further.**

![Mixed Reality Toolkit](Documentation/Images/Logo_MRTK_Unity_Banner.png)

# What is the Mixed Reality Toolkit

MRTK-Unity is a Microsoft-driven project that provides a set of components and features, used to accelerate cross-platform MR app development in Unity. Here are some of its functions:

* Provides the **cross-platform input system and building blocks for spatial interactions and UI**.
* Enables **rapid prototyping** via in-editor simulation that allows you to see changes immediately.
* Operates as an **extensible framework** that provides developers the ability to swap out core components.
* **Supports a wide range of platforms**, including
  * OpenXR (Unity 2020.2 or newer)
    * Microsoft HoloLens 2
    * Windows Mixed Reality headsets
  * Windows Mixed Reality
    * Microsoft HoloLens
    * Microsoft HoloLens 2
    * Windows Mixed Reality headsets
  * Oculus (Unity 2019.3 or newer)
    * Oculus Quest
  * OpenVR
    * Windows Mixed Reality headsets
    * HTC Vive
    * Oculus Rift
  * Ultraleap Hand Tracking
  * Mobile devices such as iOS and Android

# Getting started with MRTK

If you're new to MRTK or Mixed Reality development in Unity, **we recommend you start at the beginning of our** [Unity development journey](https://docs.microsoft.com/windows/mixed-reality/unity-development-overview?tabs=mrtk%2Chl2) in the Microsoft Docs. The Unity development journey is specifically tailored to walk new developers through the installation, core concepts, and usage of MRTK.

| IMPORTANT: The Unity development journey currently uses **MRTK version 2.4.0** and **Unity 2019.4**. |
| --- |

If you're an experienced Mixed Reality or MRTK developer, check the links in the next section for the newest packages and release notes.

# Documentation

| [![Release notes](Documentation/Images/MRTK_Icon_ReleaseNotes.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/release-notes/mrtk-26-release-notes)<br/>[Release Notes](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/release-notes/mrtk-26-release-notes)| [![MRTK Overview](Documentation/Images/MRTK_Icon_ArchitectureOverview.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/architecture/overview)<br/>[MRTK Overview](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/architecture/overview)| [![Feature Guides](Documentation/Images/MRTK_Icon_FeatureGuides.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/button)<br/>[Feature Guides](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/button)| [![API Reference](Documentation/Images/MRTK_Icon_APIReference.png)](https://docs.microsoft.com/dotnet/api/Microsoft.MixedReality.Toolkit?view=mixed-reality-toolkit-unity-2020-dotnet-2.6.0)<br/>[API Reference](https://docs.microsoft.com/dotnet/api/Microsoft.MixedReality.Toolkit?view=mixed-reality-toolkit-unity-2020-dotnet-2.6.0)|
|:---|:---|:---|:---|

# Build status

| Branch | CI Status | Docs Status |
|---|---|---|
| `main` |[![CI Status](https://dev.azure.com/aipmr/MixedRealityToolkit-Unity-CI/_apis/build/status/public/mrtk_CI?branchName=main)](https://dev.azure.com/aipmr/MixedRealityToolkit-Unity-CI/_build/latest?definitionId=15)|[![Docs Status](https://dev.azure.com/aipmr/MixedRealityToolkit-Unity-CI/_apis/build/status/public/mrtk_docs?branchName=main)](https://dev.azure.com/aipmr/MixedRealityToolkit-Unity-CI/_build/latest?definitionId=7)

# Required software

 | [![Windows SDK 18362+](Documentation/Images/MRTK170802_Short_17.png)](https://developer.microsoft.com/windows/downloads/windows-10-sdk) [Windows SDK 18362+](https://developer.microsoft.com/windows/downloads/windows-10-sdk)| [![Unity](Documentation/Images/MRTK170802_Short_18.png)](https://unity3d.com/get-unity/download/archive) [Unity 2018.4.x](https://unity3d.com/get-unity/download/archive)| [![Visual Studio 2019](Documentation/Images/MRTK170802_Short_19.png)](http://dev.windows.com/downloads) [Visual Studio 2019](http://dev.windows.com/downloads)| [![Emulators (optional)](Documentation/Images/MRTK170802_Short_20.png)](https://docs.microsoft.com/windows/mixed-reality/using-the-hololens-emulator) [Emulators (optional)](https://docs.microsoft.com/windows/mixed-reality/using-the-hololens-emulator)|
| :--- | :--- | :--- | :--- |
| To build apps with MRTK v2, you need the Windows 10 May 2019 Update SDK. <br> To run apps for immersive headsets, you need the Windows 10 Fall Creators Update. | The Unity 3D engine provides support for building mixed reality projects in Windows 10 | Visual Studio is used for code editing, deploying and building UWP app packages | The Emulators allow you to test your app without the device in a simulated environment |

# Feature areas

| ![Input System](Documentation/Images/MRTK_Icon_InputSystem.png) [Input System](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/overview)<br/>&nbsp;  | ![Hand Tracking<br/> (HoloLens 2)](Documentation/Images/MRTK_Icon_HandTracking.png) [Hand Tracking<br/> (HoloLens 2)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/hand-tracking) | ![Eye Tracking<br/> (HoloLens 2)](Documentation/Images/MRTK_Icon_EyeTracking.png) [Eye Tracking<br/> (HoloLens 2)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/eye-tracking/eye-tracking-main) | ![Profiles](Documentation/Images/MRTK_Icon_Profiles.png) [Profiles](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/configuration/mixed-reality-configuration-guide)<br/>&nbsp; | ![Hand Tracking<br/> (Ultraleap)](Documentation/Images/MRTK_Icon_HandTracking.png) [Hand Tracking (Ultraleap)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/cross-platform/leap-motion-mrtk)|
| :--- | :--- | :--- | :--- | :--- |
| ![UI Controls](Documentation/Images/MRTK_Icon_UIControls.png) [UI Controls](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/#ux-building-blocks)<br/>&nbsp; | ![Solvers](Documentation/Images/MRTK_Icon_Solver.png) [Solvers](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/solvers/solver)<br/>&nbsp; | ![Multi-Scene<br/> Manager](Documentation/Images/MRTK_Icon_SceneSystem.png) [Multi-Scene<br/> Manager](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/scene-system/scene-system-getting-started) | ![Spatial<br/> Awareness](Documentation/Images/MRTK_Icon_SpatialUnderstanding.png) [Spatial<br/> Awareness](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/spatial-awareness/spatial-awareness-getting-started) | ![Diagnostic<br/> Tool](Documentation/Images/MRTK_Icon_Diagnostics.png) [Diagnostic<br/> Tool](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/diagnostics/diagnostics-system-getting-started) |
| ![MRTK Standard Shader](Documentation/Images/MRTK_Icon_StandardShader.png) [MRTK Standard Shader](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/rendering/mrtk-standard-shader) | ![Speech & Dictation](Documentation/Images/MRTK_Icon_VoiceCommand.png) [Speech](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/speech)<br/> & [Dictation](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/dictation) | ![Boundary<br/>System](Documentation/Images/MRTK_Icon_Boundary.png) [Boundary<br/>System](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/boundary/boundary-system-getting-started)| ![In-Editor<br/>Simulation](Documentation/Images/MRTK_Icon_InputSystem.png) [In-Editor<br/>Simulation](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input-simulation/input-simulation-service) | ![Experimental<br/>Features](Documentation/Images/MRTK_Icon_Experimental.png) [Experimental<br/>Features](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/contributing/experimental-features)|

# UX building blocks

|  [![Button](Documentation/Images/Button/MRTK_Button_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/button) [Button](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/button) | [![Bounds Control](Documentation/Images/BoundsControl/MRTK_BoundsControl_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/bounds-control) [Bounds Control](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/bounds-control) | [![Object Manipulator](Documentation/Images/ManipulationHandler/MRTK_Manipulation_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/object-manipulator) [Object Manipulator](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/object-manipulator) |
|:--- | :--- | :--- |
| A button control which supports various input methods, including HoloLens 2's articulated hand | Standard UI for manipulating objects in 3D space | Script for manipulating objects with one or two hands |
|  [![Slate](Documentation/Images/Slate/MRTK_Slate_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/slate) [Slate](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/slate) | [![System Keyboard](Documentation/Images/SystemKeyboard/MRTK_SystemKeyboard_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/system-keyboard) [System Keyboard](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/system-keyboard) | [![Interactable](Documentation/Images/Interactable/InteractableExamples.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/interactable) [Interactable](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/interactable) |
| 2D style plane which supports scrolling with articulated hand input | Example script of using the system keyboard in Unity  | A script for making objects interactable with visual states and theme support |
|  [![Solver](Documentation/Images/Solver/MRTK_Solver_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/solvers/solver) [Solver](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/solvers/solver) | [![Object Collection](Documentation/Images/ObjectCollection/MRTK_ObjectCollection_Main.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/object-collection) [Object Collection](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/object-collection) | [![Tooltip](Documentation/Images/Tooltip/MRTK_Tooltip_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/tooltip) [Tooltip](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/tooltip) |
| Various object positioning behaviors such as tag-along, body-lock, constant view size and surface magnetism | Script for laying out an array of objects in a three-dimensional shape | Annotation UI with a flexible anchor/pivot system, which can be used for labeling motion controllers and objects |
|  [![Slider](Documentation/Images/Slider/MRTK_UX_Slider_Main.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/sliders) [Slider](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/sliders) | [![MRTK Standard Shader](Documentation/Images/MRTKStandardShader/MRTK_StandardShader.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/rendering/mrtk-standard-shader) [MRTK Standard Shader](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/rendering/mrtk-standard-shader) | [![Hand Menu](Documentation/Images/Solver/MRTK_UX_HandMenu.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/hand-menu) [Hand Menu](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/hand-menu) |
| Slider UI for adjusting values supporting direct hand tracking interaction | MRTK's Standard shader supports various Fluent design elements with performance | Hand-locked UI for quick access, using the Hand Constraint Solver |
|  [![App Bar](Documentation/Images/AppBar/MRTK_AppBar_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/app-bar) [App Bar](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/app-bar) | [![Pointers](Documentation/Images/Pointers/MRTK_Pointer_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/pointers) [Pointers](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/pointers) | [![Fingertip Visualization](Documentation/Images/Fingertip/MRTK_FingertipVisualization_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/fingertip-visualization) [Fingertip Visualization](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/fingertip-visualization) |
| UI for Bounds Control's manual activation | Learn about various types of pointers | Visual affordance on the fingertip which improves the confidence for the direct interaction |
|  [![Near Menu](Documentation/Images/NearMenu/MRTK_UX_NearMenu.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/near-menu) [Near Menu](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/near-menu) | [![Spatial Awareness](Documentation/Images/SpatialAwareness/MRTK_SpatialAwareness_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/spatial-awareness/spatial-awareness-getting-started) [Spatial Awareness](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/spatial-awareness/spatial-awareness-getting-started) | [![Voice Command](Documentation/Images/Input/MRTK_Input_Speech.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/speech) [Voice Command](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/speech) / [Dictation](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/dictation) |
| Floating menu UI for the near interactions | Make your holographic objects interact with the physical environments | Scripts and examples for integrating speech input |
|  [![Progress Indicator](Documentation/Images/ProgressIndicator/MRTK_ProgressIndicator_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/progress-indicator) [Progress Indicator](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/progress-indicator) | [![Dialog](Documentation/Images/Dialog/MRTK_UX_Dialog_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/dialog) [Dialog [Experimental]](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/dialog) | [![Hand Coach](Documentation/Images/HandCoach/MRTK_UX_HandCoach_Main.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/hand-coach) [Hand Coach](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/hand-coach) |
| Visual indicator for communicating data process or operation | UI for asking for user's confirmation or acknowledgement  | Component that helps guide the user when the gesture has not been taught |
|  [![Hand Physics Service](Documentation/Images/HandPhysics/MRTK_UX_HandPhysics_Main.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/experimental/hand-physics-service) [Hand Physics Service [Experimental]](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/experimental/hand-physics-service) | [![Scrolling Collection](Documentation/Images/ScrollingCollection/ScrollingCollection_Main.jpg)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/scrolling-object-collection) [Scrolling Collection](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/ux-building-blocks/scrolling-object-collection) | [![Dock](Documentation/Images/Dock/MRTK_UX_Dock_Main.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/experimental/dock) [Dock [Experimental]](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/experimental/dock) |
| The hand physics service enables rigid body collision events and interactions with articulated hands | An Object Collection that natively scrolls 3D objects | The Dock allows objects to be moved in and out of predetermined positions |
|  [![Eye Tracking: Target Selection](Documentation/Images/EyeTracking/mrtk_et_targetselect.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/eye-tracking/eye-tracking-target-selection) [Eye Tracking: Target Selection](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/eye-tracking/eye-tracking-target-selection) | [![Eye Tracking: Navigation](Documentation/Images/EyeTracking/mrtk_et_navigation.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/eye-tracking/eye-tracking-navigation) [Eye Tracking: Navigation](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input/eye-tracking/eye-tracking-navigation) | [![Eye Tracking: Heat Map](Documentation/Images/EyeTracking/mrtk_et_heatmaps.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/example-scenes/eye-tracking-examples-overview#visualization-of-visual-attention) [Eye Tracking: Heat Map](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/example-scenes/eye-tracking-examples-overview#visualization-of-visual-attention) |
| Combine eyes, voice and hand input to quickly and effortlessly select holograms across your scene | Learn how to auto-scroll text or fluently zoom into focused content based on what you are looking at | Examples for logging, loading and visualizing what users have been looking at in your app |

# Tools

|  [![Optimize Window](Documentation/Images/MRTK_Icon_OptimizeWindow.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/tools/optimize-window) [Optimize Window](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/tools/optimize-window) | [![Dependency Window](Documentation/Images/MRTK_Icon_DependencyWindow.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/tools/dependency-window) [Dependency Window](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/tools/dependency-window) | ![Build Window](Documentation/Images/MRTK_Icon_BuildWindow.png) Build Window | [![Input recording](Documentation/Images/MRTK_Icon_InputRecording.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input-simulation/input-animation-recording) [Input recording](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/input-simulation/input-animation-recording) |
|:--- | :--- | :--- | :--- |
| Automate configuration of Mixed Reality projects for performance optimizations | Analyze dependencies between assets and identify unused assets |  Configure and execute an end-to-end build process for Mixed Reality applications | Record and playback head movement and hand tracking data in editor |

# Example scenes

Explore MRTK's various types of interactions and UI controls through the example scenes. You can find example scenes under [**Assets/MRTK/Examples/Demos**](/Assets/MixedRealityToolkit.Examples/Demos) folder.

[![Example Scene](Documentation/Images/MRTK_Examples.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/example-scenes/hand-interaction-examples)

# MRTK examples hub

With the MRTK Examples Hub, you can try various example scenes in MRTK. On HoloLens 2, you can download and install [MRTK Examples Hub through the Microsoft Store app](https://www.microsoft.com/p/mrtk-examples-hub/9mv8c39l2sj4).

See [Examples Hub README page](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/example-scenes/example-hub) to learn about the details on creating a multi-scene hub with MRTK's scene system and scene transition service.

[![Example Scene](Documentation/Images/MRTK_ExamplesHub.png)](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/features/example-scenes/hand-interaction-examples)

# Sample apps made with MRTK

| [![Periodic Table of the Elements](Documentation/Images/MRDL_PeriodicTable.jpg)](https://medium.com/@dongyoonpark/bringing-the-periodic-table-of-the-elements-app-to-hololens-2-with-mrtk-v2-a6e3d8362158)| [![Galaxy Explorer](Documentation/Images/MRTK_GalaxyExplorer.jpg)](https://docs.microsoft.com/windows/mixed-reality/galaxy-explorer-update)| [![Galaxy Explorer](Documentation/Images/MRDL_Surfaces.jpg)](https://docs.microsoft.com/windows/mixed-reality/galaxy-explorer-update)|
|:--- | :--- | :--- |
| [Periodic Table of the Elements](https://github.com/Microsoft/MRDL_Unity_PeriodicTable) is an open-source sample app which demonstrates how to use MRTK's input system and building blocks to create an app experience for HoloLens and Immersive headsets. Read the porting story: [Bringing the Periodic Table of the Elements app to HoloLens 2 with MRTK v2](https://medium.com/@dongyoonpark/bringing-the-periodic-table-of-the-elements-app-to-hololens-2-with-mrtk-v2-a6e3d8362158) |[Galaxy Explorer](https://github.com/Microsoft/GalaxyExplorer) is an open-source sample app that was originally developed in March 2016 as part of the HoloLens 'Share Your Idea' campaign. Galaxy Explorer has been updated with new features for HoloLens 2, using MRTK v2. Read the story: [The Making of Galaxy Explorer for HoloLens 2](https://docs.microsoft.com/windows/mixed-reality/galaxy-explorer-update) |[Surfaces](https://github.com/Microsoft/GalaxyExplorer) is an open-source sample app for HoloLens 2 which explores how we can create a tactile sensation with visual, audio, and fully articulated hand-tracking. Check out Microsoft MR Dev Days session [Learnings from the Surfaces app](https://channel9.msdn.com/Shows/Docs-Mixed-Reality/Learnings-from-the-MR-Surfaces-App) for the detailed design and development story. |

# Session videos from Mixed Reality Dev Days 2020

| [![MRDevDays](Documentation/Images/MRDevDays_Session1.png)](https://channel9.msdn.com/Shows/Docs-Mixed-Reality/Intro-to-MRTK-Unity)| [![MRDevDays](Documentation/Images/MRDevDays_Session2.png)](https://channel9.msdn.com/Shows/Docs-Mixed-Reality/MRTKs-UX-Building-Blocks)| [![MRDevDays](Documentation/Images/MRDevDays_Session3.png)](https://channel9.msdn.com/Shows/Docs-Mixed-Reality/MRTK-Performance-and-Shaders)|
|:--- | :--- | :--- |
| Tutorial on how to create a simple MRTK app from start to finish. Learn about interaction concepts and MRTK’s multi-platform capabilities. | Deep dive on the MRTK’s UX building blocks that help you build beautiful mixed reality experiences. | An introduction to performance tools, both in MRTK and external, as well as an overview of the MRTK Standard Shader.	 |

See [Mixed Reality Dev Days](https://docs.microsoft.com/windows/mixed-reality/mr-dev-days-sessions) to explore more session videos.

# Engage with the community

- Join the conversation around MRTK on [Slack](https://holodevelopers.slack.com/). You can join the Slack community via the [automatic invitation sender](https://holodevelopersslack.azurewebsites.net/).

- Ask questions about using MRTK on [Stack Overflow](https://stackoverflow.com/questions/tagged/mrtk) using the **MRTK** tag.

- Search for [known issues](https://github.com/Microsoft/MixedRealityToolkit-Unity/issues) or file a [new issue](https://github.com/Microsoft/MixedRealityToolkit-Unity/issues) if you find something broken in MRTK code.

- For questions about contributing to MRTK, go to the [mixed-reality-toolkit](https://holodevelopers.slack.com/messages/C2H4HT858) channel on slack.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Useful resources on the Mixed Reality Dev Center

| ![Discover](Documentation/Images/mrdevcenter/icon-discover.png) [Discover](https://docs.microsoft.com/windows/mixed-reality/)| ![Design](Documentation/Images/mrdevcenter/icon-design.png) [Design](https://docs.microsoft.com/windows/mixed-reality/design)| ![Develop](Documentation/Images/mrdevcenter/icon-develop.png) [Develop](https://docs.microsoft.com/windows/mixed-reality/development)| ![Distribute)](Documentation/Images/mrdevcenter/icon-distribute.png) [Distribute](https://docs.microsoft.com/windows/mixed-reality/implementing-3d-app-launchers)|
| :--------------------- | :----------------- | :------------------ | :------------------------ |
| Learn to build mixed reality experiences for HoloLens and immersive headsets (VR).          | Get design guides. Build user interface. Learn interactions and input.     | Get development guides. Learn the technology. Understand the science.       | Get your app ready for others and consider creating a 3D launcher. |

# Useful resources on Azure

| ![Spatial Anchors](Documentation/Images/mrdevcenter/icon-azurespatialanchors.png)<br> [Spatial Anchors](https://docs.microsoft.com/azure/spatial-anchors/)| ![Speech Services](Documentation/Images/mrdevcenter/icon-azurespeechservices.png) [Speech Services](https://docs.microsoft.com/azure/cognitive-services/speech-service/)| ![Vision Services](Documentation/Images/mrdevcenter/icon-azurevisionservices.png) [Vision Services](https://docs.microsoft.com/azure/cognitive-services/computer-vision/)|
| :------------------------| :--------------------- | :---------------------- |
| Spatial Anchors is a cross-platform service that allows you to create Mixed Reality experiences using objects that persist their location across devices over time.| Discover and integrate Azure powered speech capabilities like speech to text, speaker recognition or speech translation into your application.| Identify and analyze your image or video content using Vision Services like computer vision, face detection, emotion recognition or video indexer. |

# Learn more about the MRTK project

You can find our planning material on [our wiki](https://github.com/Microsoft/MixedRealityToolkit-Unity/wiki) under the Project Management Section. You can always see the items the team is actively working on in the Iteration Plan issue.

# How to contribute

Learn how you can contribute to MRTK at [Contributing](https://docs.microsoft.com/windows/mixed-reality/mrtk-unity/contributing/contributing).

**For details on the different branches used in the Mixed Reality Toolkit repositories, check this [Branch Guide here](https://github.com/Microsoft/MixedRealityToolkit-Unity/wiki/Branch-Guide).**
"
22,microsoft/vscode-jupyter,TypeScript,"# Jupyter Extension for Visual Studio Code

A [Visual Studio Code](https://code.visualstudio.com/) [extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) that provides basic notebook support for [language kernels](https://github.com/jupyter/jupyter/wiki/Jupyter-kernels) that are supported in [Jupyter Notebooks](https://jupyter.org/) today. Many language kernels will work with no modification. To enable advanced features, modifications may be needed in the VS Code language extensions.


## Working with Python

Whether you are on VS Code Stable or VS Code Insiders, if you would like to work with Python just make sure you're using the latest version of the [Python Extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python) to enjoy the joint partnership of the Python and Juypter Extensions.

Please follow the [Python Extension ReadMe](https://github.com/microsoft/vscode-python/blob/main/README.md) instructions to get started and visit the [Python Documentation](https://code.visualstudio.com/docs/python/jupyter-support) to learn more about how the Python and Jupyter Extension are working together to provide an optimum Python notebooks experience.

## Working with other Languages

The Jupyter Extension supports other languages in addition to Python such as Julia, R, and C# in VS Code Insiders with our latest Native VS Code Notebooks Experience!

### Quick Start

-   **Step 1.** Install [VS Code Insiders](https://code.visualstudio.com/insiders/)

-   **Step 2** If not working with Python, make sure to have a Jupyter kernelspec that corresponds to the language you would like to use installed on your machine.

-   **Step 3.** Install the [Jupyter Extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter)

-   **Step 4.** Open or create a notebook file and start coding!

- **Special Note:**  The Jupyter Extension in VS Code Insiders will include our Native Notebooks experience by default. Because we are running in VS Code Insiders and this build is updated every day, there may be times when our extension may fail to work at all. We do attempt to ensure that this doesn't happen frequently. If it does, we strive to provide an updated extension build by the next business day. However, if you'd like to opt out of the native experience while working in VS Code Insiders:
    - Open the command palette (Windows: Ctrl + Shift + P, iOS: Command + Shift + P) and select ""Preferences: Open Settings (JSON)""
    - Add the following code to your JSON settings:
     `""jupyter.experiments.optOutFrom"": [""NativeNotebookEditor""],`

## Notebooks Quick Start

- To create a new notebook open the command palette (Windows: Ctrl + Shift + P, iOS: Command + Shift + P) and select the command `""Jupyter: Create New Blank Notebook""`

     <img src=https://raw.githubusercontent.com/microsoft/vscode-jupyter/main/images/Jupyter%20README/CreateNewNotebook.png>

- Select your kernel by clicking on the kernel picker in the bottom right of the status bar or by invoking the `""Notebook: Select Notebook Kernel""` command.

     <img src=https://raw.githubusercontent.com/microsoft/vscode-jupyter/main/images/Jupyter%20README/KernelPicker.gif?>

- Change the cell language by clicking the language picker or by invoking the `""Notebook: Change Cell Language""` command.

     <img src=https://raw.githubusercontent.com/microsoft/vscode-jupyter/main/images/Jupyter%20README/LanguagePicker.gif?>



## Useful commands

Open the Command Palette (Command+Shift+P on macOS and Ctrl+Shift+P on Windows/Linux) and type in one of the following commands:

| Command                               | Description                                                                                                                                                    |
| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Jupyter: Create New Blank Notebook`| Create a new blank Jupyter Notebook   |
| `Notebook: Select Notebook Kernel`        | Select or switch kernels within your notebook|
| `Notebook: Change Cell Language`        | Change the language of the cell currently in focus |
| `Jupyter: Export to HTML Jupyter: Export to PDF` | Create a presentation-friendly version of your notebook in HTML or PDF

To see all available Jupyter Notebook commands, open the Command Palette and type `Jupyter` or `Notebook`.

## Feature details

Learn more about the rich features of the Jupyter extension:

-   [IntelliSense](https://code.visualstudio.com/docs/python/editing#_autocomplete-and-intellisense): Edit your code with auto-completion, code navigation, syntax checking and more!
     - *May be limited due to kernelspec of choice*

-   [Jupyter Notebooks](https://code.visualstudio.com/docs/python/jupyter-support): Create and edit Jupyter Notebooks, add and run code/markdown cells, render plots, create presentation-friendly versions of your notebook by exporting to HTML or PDF and more!


## Supported locales

The extension is available in multiple languages: `de`, `en`, `es`, `fa`, `fr`, `it`, `ja`, `ko-kr`, `nl`, `pl`, `pt-br`, `ru`, `tr`, `zh-cn`, `zh-tw`

## Questions, issues, feature requests, and contributions

-   If you have a question about how to accomplish something with the extension, please [ask on Stack Overflow](https://stackoverflow.com/questions/tagged/visual-studio-code+jupyter). Our [wiki](https://github.com/microsoft/vscode-jupyter/wiki) is also updated periodically with useful information.
-   Any and all feedback is appreciated and welcome! If you come across a problem with the extension, please [file an issue](https://github.com/microsoft/vscode-jupyter).
      - If someone has already [filed an issue](https://github.com/Microsoft/vscode-jupyter) that encompasses your feedback, please leave a 👍/👎 reaction on the issue.

- Contributions are always welcome! Please see our [contributing guide](https://github.com/Microsoft/vscode-jupyter/blob/main/CONTRIBUTING.md) for more details.

-   If you're interested in the development of the extension, you can read about our [development process](https://github.com/microsoft/vscode-jupyter/blob/main/CONTRIBUTING.md#development-process)

## Data and telemetry

The Microsoft Jupyter Extension for Visual Studio Code collects usage
data and sends it to Microsoft to help improve our products and
services. Read our
[privacy statement](https://privacy.microsoft.com/privacystatement) to
learn more. This extension respects the `telemetry.enableTelemetry`
setting which you can learn more about at
https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.
"
23,microsoft/ComplianceCxE,,"# Project

> This repo has been populated by an initial template to help get you started. Please
> make sure to update the content to build a great experience for community-building.

As the maintainer of this project, please make a few updates:

- Improving this README.MD file to provide a great experience
- Updating SUPPORT.MD with content about this project's support experience
- Understanding the security reporting process in SECURITY.MD
- Remove this section from the README

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
24,microsoft/vscode-react-native,TypeScript,"# React Native Tools

[![Build Status](https://dev.azure.com/vscode-webdiag-extensions/VS%20Code%20WebDiag%20extensions/_apis/build/status/%5BUnit%20tests%5D%20vscode-react-native%20%5Bmaster%5D?branchName=master)](https://dev.azure.com/vscode-webdiag-extensions/VS%20Code%20WebDiag%20extensions/_build/latest?definitionId=60&branchName=master)

Stable:
![Stable version](https://vsmarketplacebadge.apphb.com/version-short/msjsdiag.vscode-react-native.svg)
![VS Marketplace rating](https://vsmarketplacebadge.apphb.com/rating-star/msjsdiag.vscode-react-native.svg)

Preview:
![VS Marketplace version](https://vsmarketplacebadge.apphb.com/version-short/msjsdiag.vscode-react-native-preview.svg)
![VS Marketplace rating](https://vsmarketplacebadge.apphb.com/rating-star/msjsdiag.vscode-react-native-preview.svg)

## React Native Tools Preview

The extension has a [nightly version](https://marketplace.visualstudio.com/items?itemName=msjsdiag.vscode-react-native-preview) which is released on a daily basis at 9 PM PST on each day that changes occur.
To avoid conflicts, if both extensions are installed - the only stable version will be activated. So to use the preview version it is needed to disable or remove the stable version and reload VS Code.

## About the extension

This VS Code extension provides a development environment for React Native projects.
Using this extension, you can **debug your code and quickly run `react-native` commands** from the command palette.

![React Native features](images/react-features.gif)

<!-- TABLE OF CONTENTS -->

# Table of Contents

- [React Native Tools Preview](#react-native-tools-preview)
- [About the extension](#about-the-extension)
- [Getting started](#getting-started)
- [React Native commands in the Command Palette](#react-native-commands-in-the-command-palette)
- [Debugging React Native applications](#debugging-react-native-applications)
  - [Hermes engine](#hermes-engine)
  - [iOS applications](#ios-applications)
    - [iOS devices](#ios-devices)
    - [Custom scheme for iOS apps](#custom-scheme-for-ios-apps)
    - [iOS direct debugging](#iOS-direct-debugging)
    - [iOS Hermes debugging](#ios-hermes-debugging)
  - [Expo applications](#expo-applications)
    - [Configuring Expo](#configuring-expo)
  - [Windows applications](#react-native-for-windows)
  - [macOS applications](#react-native-for-macos)
    - [macOS Hermes debugging](#macos-hermes-debugging)
  - [TypeScript and Haul based applications](#typescript-and-haul)
  - [Debugger configuration properties](#debugger-configuration-properties)
- [Customization](#customization)
  - [Logging](#logging)
  - [Build APK and generate bundle](#build-apk-and-generate-bundle)
  - [Specifying custom arguments for `react-native run-*` command](#specifying-custom-arguments-for-react-native-run--command)
  - [Setting up the React Native packager](#setting-up-the-react-native-packager)
  - [Change project root](#change-project-root)
  - [Configure an Android LogCat Monitor](#configure-an-android-logcat-monitor)
- [Network Inspector](#network-inspector)
- [Developing inside a Docker Container](#developing-inside-a-docker-container)
- [Contributing](#contributing)
- [Known Issues](#known-issues)

# Getting started

Before going any further make sure that you:

- [have a working React Native environment](https://reactnative.dev/docs/environment-setup).
- are using [VS Code](https://code.visualstudio.com) and have [installed this extension from the Marketplace](https://marketplace.visualstudio.com/items?itemName=msjsdiag.vscode-react-native).
- have your React Native project root folder open in VS Code.

Please notice that the extension uses `.vscode/.react` directory at the project root to store intermediate files required for debugging. Although these files usually get removed after debug session ends, you may want to add this directory to your project's `.gitignore` file.

# React Native commands in the Command Palette

In the Command Palette, type `React Native` and choose a command.

![React Native commands](images/command-palette.png)

The **Run Android** command triggers `react-native run-android` and starts your app for Android.

The **Run iOS** command similarly triggers `react-native run-ios` and starts your app in the iOS simulator (e.g. iPhone 6).

The **Packager** commands allow you to start/stop the [**Metro Bundler**](https://github.com/facebook/metro-bundler) (formerly React Packager).

The full list of commands is:

| Name                             | Description                                                                                                                                                                                                                                |
| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Launch Android Emulator          | Prompts you to select the name of the available emulator and launch it. If only one emulator is installed in the system, it will be selected automatically                                                                                 |
| Run Android on Emulator          | Run an Android application on Emulator. Launch order: check target platform support, load run arguments, start Packager, run app in all connected emulators                                                                                |
| Run Android on Device            | Run an Android application on Device. Launch order: check target platform support, load run arguments, start Packager, run app in all connected devices                                                                                    |
| Run iOS on Simulator             | Run an iOS application on Simulator. Launch order: load run arguments, check target platform support, start Packager, run app in only one connected emulator                                                                               |
| Run iOS on Device                | Run an iOS application on Device. Launch order: load run arguments, check target platform support, start Packager, run app in only one connected device                                                                                    |
| Run Expo                         | Run Exponent application. Launch order: login to exponent, load run arguments, start Packager, run app                                                                                                                                     |
| Start Packager                   | Start Packager in context project workspace folder                                                                                                                                                                                         |
| Stop Packager                    | Stop Packager                                                                                                                                                                                                                              |
| Restart Packager                 | Restart Packager                                                                                                                                                                                                                           |
| Publish To Expo                  | Publish to Exponent Host. Launch order: login to exponent, execute `Run Expo` command, then publish app to host                                                                                                                            |
| Show Dev Menu                    | Show development menu for running aplication on iOS or Android device or emulator                                                                                                                                                          |
| ReloadApp                        | Reload an application                                                                                                                                                                                                                      |
| Run Element Inspector            | Load development tools for inspect application UI elements                                                                                                                                                                                 |
| Run React Native LogCat Monitor  | Creates a LogCat Monitor for the chosen online Android device to see the device LogCat logs. Default filtering arguments: [""*:S"", ""ReactNative:V"", ""ReactNativeJS:V""]. [How to configure filtering.](#configure-an-Android-LogCat-Monitor) |
| Stop React Native LogCat Monitor | Stops an existing LogCat Monitor and removes its output channel                                                                                                                                                                            |
| Run Network Inspector            | Run [Network inspector](#network-inspector)                                                                                                                                                                                                |
| Stop Network Inspector           | Stop [Network inspector](#network-inspector)                                                                                                                                                                                               |

# Debugging React Native applications

To start debugging create a new debug configuration for your ReactNative app in your `.vscode/launch.json`. Adding a new configuration can be done by opening your `launch.json` file and clicking on `Add Configuration...` button and choosing a relevant debug configuration. All available debug configurations for ReactNative can be accessed by typing in _ReactNative_ and picking one from the list populated by Intellisense as shown in the image below.

![Add React Native debug configuration](images/add-debug-configuration.gif)

In case you haven't created the `.vscode/launch.json` file yet, you can add a whole default debug configuration set. To do that click the debug icon ![Choose React Native debugger](images/debug-view-icon.png) in the View bar, and then click the configuration (gear) icon ![Configure-gear](images/configure-gear-icon.png), then choose the React Native debug environment.

![Choose React Native debugger](images/choose-debugger.png)

VS Code will generate a `launch.json` in your project with some default configuration settings as shown below. You can safely close this file, choose the appropriate configuration in the Configuration dropdown, and then press F5 (or click _Green Arrow_ ![Configure-gear](images/debug-icon.png) button) to start debugging your app in VS Code.

![React Native launch targets](images/debug-targets.png)

Once app is loaded and running, [open the developer menu](https://reactnative.dev/docs/debugging#accessing-the-in-app-developer-menu) inside your application and enable remote debugging by clicking on `Debug JS Remotely` button.

![React Native enable remote debug](images/enable-remote-debug.png)

The extension allows you to debug multiple devices and configurations, please read the following sections for more information for your particular use case.

## Hermes engine

The Hermes engine is an open source JavaScript engine created by Facebook to optimize building and running React Native applications. It improves app performance and decreases app size.

Click [here](https://reactnative.dev/docs/hermes) to learn more about Hermes and how to enable it for your application.

Debugging apps with Hermes enabled is currently experimental. Please, see [this issue](https://github.com/microsoft/vscode-react-native/issues/1073) for current known issues on Hermes support.

### Android Hermes

To debug while using Hermes engine use `Debug Android Hermes - Experimental` launch configuration:

```json
{
  ""name"": ""Debug Android Hermes - Experimental"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnativedirect"",
  ""request"": ""launch"",
  ""platform"": ""android""
}
```

### iOS Hermes

The extension provides experimental support of debugging iOS Hermes applications. See [iOS Hermes debugging](#ios-hermes-debugging) for more details.

### macOS Hermes

The extension provides experimental support of debugging macOS Hermes applications. See [macOS Hermes debugging](#macos-hermes-debugging) for more details.

### Attach to Hermes application

To attach to a running Hermes application use `Attach to Hermes application - Experimental` launch configuration:

```json
{
  ""name"": ""Attach to Hermes application - Experimental"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnativedirect"",
  ""request"": ""attach""
}
```

## iOS applications

### iOS devices

Debugging on an iOS device requires following manual steps:

- Install [ios-deploy](https://www.npmjs.com/package/ios-deploy) `npm install -g ios-deploy`.
- Install a valid iOS development certificate.
- In your project's `launch.json` file set `target` to `device`. If you need to specify the exact device to run, you can set `target` to `device=<iOS_device_name>`, or you can also use `runArguments` property to specify a particular device to run on in case multiple devices are connected (e.g. `""runArguments"": [ ""--device"", ""My iPhone"" ]`)
- Choose the **Debug iOS** option from the ""Configuration"" dropdown and press F5.
- Shake the device to open the development menu and select ""Debug JS Remotely"".

### Custom scheme for iOS apps

If you want to use a custom scheme for your application you can either pass it as part of the `runArguments` parameter arguments, or set the `scheme` configuration parameter as shown below:

```js
""runArguments"": [""--scheme"", ""customScheme"", ...]
// or
""runArguments"": [""--scheme=customScheme"", ...]
// or
""scheme"" : ""customScheme""
```

Please be aware, specifying the scheme value as a part of the `runArguments` parameter arguments will override the `scheme` configuration parameter value, if it set.

### iOS direct debugging

The extension provides experimental support of iOS direct debugging. See more info here: [react-native-community/discussions-and-proposals#40](https://github.com/react-native-community/discussions-and-proposals/issues/40), [react-native-community/discussions-and-proposals#206](https://github.com/react-native-community/discussions-and-proposals/issues/206)

For now the extension supports iOS direct debugging only on real iOS devices.

To be able to debug an iOS app directly, you need to instal [ios-webkit-debug-proxy](https://github.com/google/ios-webkit-debug-proxy):

- Install [HomeBrew](https://brew.sh) on your Mac.
- Open a Terminal and run `brew install ideviceinstaller ios-webkit-debug-proxy`

You can use the following debug scenarios to debug iOS apps directly:

- React Native Direct: Debug Direct iOS - Experimental

```json
    ""name"": ""Debug Direct iOS - Experimental"",
    ""cwd"": ""${workspaceFolder}"",
    ""type"": ""reactnativedirect"",
    ""request"": ""launch"",
    ""platform"": ""ios"",
    ""port"": 9221,
    ""target"": ""device""
```

- React Native Direct: Attach to the React Native iOS - Experimental

```json
    ""name"": ""Attach to the React Native iOS - Experimental"",
    ""cwd"": ""${workspaceFolder}"",
    ""type"": ""reactnativedirect"",
    ""request"": ""attach"",
    ""platform"": ""ios"",
    ""port"": 9221
```

### iOS Hermes debugging

You can enable Hermes engine for an iOS application by editing `ios/Podfile` file the following way:

```diff
-  use_react_native!(:path => config[:reactNativePath])
+  use_react_native!(:path => config[:reactNativePath], :hermes_enabled => true)
```

After this change you need to execute `pod install` command in `ios` folder. After that you can use `Debug iOS Hermes - Experimental` launch configuration to debug an iOS Hermes application:

```json
{
  ""name"": ""Debug iOS Hermes - Experimental"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnativedirect"",
  ""request"": ""launch"",
  ""platform"": ""ios""
}
```

## Expo applications

To debug a project created using Expo or the `create-react-native-app` task, you can use embedded support for Expo.

Prepare your environment by following the [Expo CLI Quickstart instruction](https://reactnative.dev/docs/environment-setup).
For correct work with Expo this extension **`requires Android SDK`**.
So also pay attention to the `React Native CLI Quickstart` tab, where you can find the Android SDK installation guide:

- Install the [Expo app](https://getexponent.com/) on the target device or emulator
- Ensure that the `Android SDK` is installed on your computer (You may install it using the [`React Native CLI Quickstart` guide](https://reactnative.dev/docs/environment-setup))
- Ensure that the `expo-cli` is installed globally (`npm install -g expo-cli`)

You can verify that everything is working correctly and that the environment is ready for use with the `npx react-native doctor` command.

To start debugging in Expo follow these steps:

1. Open your project in VS Code with this extension installed.
1. Create a debug configuration (as described in [Debugging React Native applications](#debugging-react-native-applications)), select `Debug in Exponent` in the debug drop-down menu, and start debugging
1. Wait while some dependencies are configured - the extension will install [`Expo Development Library(xdl)`](https://www.npmjs.com/package/xdl) when this feature is used for the first time.
1. If you have not used Exponent on this system before, you will be prompted for an Exponent username and password.
   Exponent account allows you to use Expo cloud services. More info about how it works is available [here](https://docs.expo.io/versions/latest/workflow/how-expo-works/).
   If you have not created an Exponent account, then specifying a new username and password will create one.
   Note that there is no e-mail associated with the account, and no way to recover a forgotten password.
   If you don't want to create an Exponent account, you can specify `expoHostType` parameter in your debug configuration to make Expo work locally (via LAN or on localhost).
1. Once the packager starts, the extension will open a separate tab with QR code to scan from the Exponent app. Once you do so, the Exponent app will connect to the packager and begin running your app.
1. Once the app is loaded and running, [open the developer menu](https://reactnative.dev/docs/debugging#accessing-the-in-app-developer-menu) and enable remote debugging by clicking on `Debug JS Remotely` button.

   ![React Native developer menu](./images/enable-remote-debug.png)

   From here you can run and debug the app as normal.

### Configuring Expo

The extension supports running through Exponent not just the applications created with Expo but even pure React Native applications (in that case you need to add `expo` package to `node_modules` in order to make it work with Expo: `npm install expo --save-dev`. In either cases it uses `app.json` configuration file in the root of the project.

If you are running `Debug in Exponent` configuration or any of pallette commands like `Run in Exponent`, `Publish to Exponent` then this file will be created automatically if absent or updated with the following basic configuration section:

```json
{
  ""expo"": {
    ""slug"": ""MyApp"", // Project slug
    ""name"": ""MyApp"", // Project name
    ""sdkVersion"": ""31.0.0"", // Expo SDK version
    ""entryPoint"": "".vscode\\exponentIndex.js"" // Entrypoint for the project
  },
  ""name"": ""MyApp"" // Project name
}
```

Full list of configuration parameters for `expo` section in `app.json` may be found on [official Expo documentation page](https://docs.expo.io/versions/latest/workflow/configuration).

For running **pure React Native app**, the extension, creates and uses `.vscode/exponentIndex.js` which points to the app entrypoint (`index.js` or `index.android.js` or `index.ios.js`) file.

If you want to change your app entrypoint (for example, from `index.js` to `index.android.js`), delete `.vscode/exponentIndex.js` and then restart your debugging session.

**NOTE**: The extension caches the version of the exponent SDK used by your project. This is helpful since we don't want to install the SDK each time you run exponent. If you want the extension to update the SDK version based on your React Native version, just restart VS Code and if it is supported it should work. If it does not please open an issue.

## React Native for Windows

### How to launch and debug a React Native for Windows application

Before launching and debugging a React Native for Windows application, please make sure that your development environment is configured properly in accordance with [the official system requirements](https://microsoft.github.io/react-native-windows/docs/rnw-dependencies).

You can debug UWP React Native for Windows applications by changing the `platform` in your `launch.json` configuration to `windows`:

```json
{
  ""name"": ""Debug Windows"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnative"",
  ""request"": ""launch"",
  ""platform"": ""windows""
}
```

### How to attach to a running React Native for Windows application

1. Add the `Attach to packager` configuration to `.vscode/launch.json` in your project

   ```json
   {
     ""name"": ""Attach to packager"",
     ""cwd"": ""${workspaceFolder}"",
     ""type"": ""reactnative"",
     ""request"": ""attach""
   }
   ```

1. (**Optional**) Start Metro packager by means of the `React Native: Start Packager` Command Palette command or run `npx react-native start` command in the terminal in the project root folder
1. Select the `Attach to packager` configuration and click the `play` button. If Metro packager isn't running yet, the extensnion will start it automatically.
1. Launch your React Native Windows application. Please make sure that the application is on remote debugging mode.

Then the extension should attach to the running application.

You can find more information on how to setup your application to work with Windows in [React Native for Windows Getting started instruction](https://microsoft.github.io/react-native-windows/docs/getting-started)

## React Native for macOS

You can debug React Native for macOS applications by changing the `platform` in your `launch.json` configuration to `macos`:

```json
{
  ""name"": ""Debug macOS"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnative"",
  ""request"": ""launch"",
  ""platform"": ""macos""
}
```

To attach to a running macOS application you can use the default `Attach to packager` debugging configuration. Please make sure that the application is on remote debugging mode.

```json
{
  ""name"": ""Attach to packager"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnative"",
  ""request"": ""attach""
}
```

You can find more information on how to setup your application to work with macOS in [React Native for macOS Getting started instruction](https://microsoft.github.io/react-native-windows/docs/rnm-getting-started)

### macOS Hermes debugging

Please follow [the official guide](https://microsoft.github.io/react-native-windows/docs/hermes#available-on-macos) to enable Hermes engine for a macOS application.

To debug a macOS Hermes application you can use `Debug macOS Hermes - Experimental` debugging scenario:

```json
{
  ""name"": ""Debug macOS Hermes - Experimental"",
  ""request"": ""launch"",
  ""type"": ""reactnativedirect"",
  ""cwd"": ""${workspaceFolder}"",
  ""platform"": ""macos""
}
```

## TypeScript and Haul

### Sourcemaps

The debugger uses sourcemaps to let you debug with your original sources, but sometimes the sourcemaps aren't generated properly and overrides are needed. In the config we support `sourceMapPathOverrides`, a mapping of source paths from the sourcemap, to the locations of these sources on disk. Useful when the sourcemap isn't accurate or can't be fixed in the build process.

The left hand side of the mapping is a pattern that can contain a wildcard, and will be tested against the `sourceRoot` + `sources` entry in the source map. If it matches, the source file will be resolved to the path on the right hand side, which should be an absolute path to the source file on disk.

Below there are some examples of how sourcemaps could be resolved in different scenarios:

```javascript
// webRoot = /Users/me/project
""sourceMapPathOverrides"": {
    ""webpack:///./~/*"": ""${webRoot}/node_modules/*"",       // Example: ""webpack:///./~/querystring/index.js"" -> ""/Users/me/project/node_modules/querystring/index.js""
    ""webpack:///./*"":   ""${webRoot}/*"",                    // Example: ""webpack:///./src/app.js"" -> ""/Users/me/project/src/app.js"",
    ""webpack:///*"":     ""*"",                               // Example: ""webpack:///project/app.ts"" -> ""/project/app.ts""
    ""webpack:///src/*"": ""${webRoot}/*""                     // Example: ""webpack:///src/app.js"" -> ""/Users/me/project/app.js""
}
```

### Haul debugging

The extension provides functional to attach to [Haul packager](https://callstack.github.io/haul/) based applications. You can use the `Attach to packager` scenario to attach to a Haul based app and debug it. For now launch scenarios aren't supported. You can find more info in [the issue](https://github.com/microsoft/vscode-react-native/issues/883).

You can prepare your React Native application to work with `Haul` by following the [`Haul Getting started` guide](https://github.com/callstack/haul#getting-started).

If you use the [legacy version](https://github.com/callstack/haul/tree/legacy) of `Haul` as your React Native bundler instead of the default [Metro](https://facebook.github.io/metro/), it could be required to add `sourceMapPathOverrides` to the `launch.json` file.

For example:

```json
{
  // Other configurations
  ""sourceMapPathOverrides"": {
    ""webpack:///./~/*"": ""${workspaceRoot}/node_modules/*"",
    ""webpack:///./*"": ""${workspaceRoot}/*"",
    ""webpack:///*"": ""*""
  }
}
```

## Debugger configuration properties

The following is a list of all the configuration properties the debugger accepts in `launch.json`:

| Name                               | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Type       | Defaults                                      |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- | --------------------------------------------- |
| `cwd`                              | The path to the project root folder                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `string`   | `${workspaceFolder}`                          |
| `sourceMaps`                       | Whether to use JavaScript source maps to map the generated bundled code back to its original sources                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `boolean`  | `true`                                        |
| `sourceMapPathOverrides`           | A set of mappings for rewriting the locations of source files from what the source map says, to their locations on disk. See [Sourcemaps](#sourcemaps) for details                                                                                                                                                                                                                                                                                                                                                                                                                                           | `object`   | n/a                                           |
| `enableDebug`                      | Whether to enable debug mode. If set to ""false"", an application will be launched without debugging                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `boolean`  | `true`                                        |
| `webkitRangeMin`, `webkitRangeMax` | Combines to specify the port range that you want the [ios-webkit-debug-proxy](https://github.com/google/ios-webkit-debug-proxy) to use to find the specific device described in the Direct iOS debug configuration                                                                                                                                                                                                                                                                                                                                                                                           | 9223, 9322 |
| `trace`                            | Logging level in debugger process. May be useful for diagnostics. If set to ""Trace"" all debugger process logs will be available in `Debug Console` output window                                                                                                                                                                                                                                                                                                                                                                                                                                             | `string`   | `log`                                         |
| `address`                          | TCP/IP address of packager to attach to for debugging                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | `string`   | `localhost`                                   |
| `port`                             | Port of packager to attach to for debugging                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `string`   | `8081`                                        |
| `remoteRoot`                       | The source root of the remote host                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `string`   | `null`                                        |
| `localRoot`                        | The local source root that corresponds to the 'remoteRoot'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `string`   | `${workspaceFolder}`                          |
| `skipFiles`                        | An array of file or folder names, or glob patterns, to skip when debugging                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | `array`    | `[]`                                          |
| `debuggerWorkerUrlPath`            | Path to the app debugger worker to override. For example, if debugger tries to attach to http://localhost:8081/debugger-ui/debuggerWorker.js and you get 404 error from packager output then you may want to change debuggerWorkerUrlPath to another value suitable for your packager (\""debugger-ui\"" will be replaced with the value you provide)                                                                                                                                                                                                                                                          | `string`   | `debugger-ui/`                                |
| `platform`                         | The platform to target. Possible values: `android`, `ios`, `exponent`, `windows`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `string`   | n/a                                           |
| `target`                           | Target to run on. Possible values: `simulator`, `device`, `device=<iOS device name>`, [`<Android emulator/device id>`](https://github.com/react-native-community/cli/blob/master/docs/commands.md#--deviceid-string), `<Android emulator name>`, `<iOS simulator name>`, `<iOS simulator id>`. If the value is `simulator` then the quick pick window will be expanded with the names of the available virtual devices, then the target value in `launch.json` will be changed to the name of the selected virtual device. If you have only one virtual device available, it will be selected automatically. | `string`   | `simulator`                                   |
| `logCatArguments`                  | Arguments to be used for LogCat (The LogCat output will appear on an Output Channel). It can be an array such as: `["":S"", ""ReactNative:V"", ""ReactNativeJS:V""]`                                                                                                                                                                                                                                                                                                                                                                                                                                               | `array`    | `[""*:S"", ""ReactNative:V"", ""ReactNativeJS:V""]` |
| `runArguments`                     | Run arguments to be passed to `react-native run-<platform>` command (override all other configuration params)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `array`    | n/a                                           |
| `launchActivity`                   | The Android activity to be launched for debugging, e.g. it specifies [`--main-activity`](https://github.com/react-native-community/cli/blob/master/docs/commands.md#--main-activity-string) parameter in `react-native` run arguments                                                                                                                                                                                                                                                                                                                                                                        | `string`   | `MainActivity`                                |
| `expoHostType`                     | The connection type to be used on Expo debugging to communicate with a device or an emulator. Possible values: <ul><li>`tunnel` - allows to deploy and debug an application by means of Expo cloud services</li><li>`lan` - allows to deploy and install an application via your LAN</li><li>`local` - allows to debug an application on an emulator or an Android device without network connection</li></ul>                                                                                                                                                                                               | `string`   | `lan`                                         |
| `env`                              | Environment variables passed to the debugger and `react-native run-<platform>` command                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `object`   | `{}`                                          |
| `envFile`                          | Absolute path to a file containing environment variable definitions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `string`   | `${workspaceFolder}/.env`                     |
| `variant`                          | A variant to be passed to `react-native run-android`, e.g. use `devDebug` to specify `--variant=devDebug`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `string`   | n/a                                           |
| `scheme`                           | A scheme name to be passed to `react-native run-ios`, e.g. `devDebug` to specify `--scheme=devDebug`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `string`   | n/a                                           |
| `productName`                      | iOS bundle display name e.g. `AwesomeProject` value means that the extension will search for `AwesomeProject.app` bundle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `string`   | n/a                                           |

# Customization

The extension can be further customized for other React Native scenarios. These are the most common:

## Logging

The extension logging is divided by several output channels:

- React Native - the main extension channel which collects outputs from React Native Packager and shows critical errors in the extension
- These channels are spawned only when the specific launch scenario is executed:
  - React Native: Run Android
    - LogCat monitor(to get LogCat output from Android device, can be filtered by debug configuration settings)
  - React Native: Run iOS
  - React Native: Run macOS
  - React Native: Run exponent
- Debug Console which is used to receive application logs and logs generated by the debug adapter (`console.log` and other `std` outputs from the app)
- Extension debugger verbose logs (these logs are shown up only if the `trace: ""verbose""` option is enabled in debug scenarios)
  - React Native Chrome Proxy - shows what runs in and out to the debugger and application
  - Debug Console becomes more informative and contains some debugging information from the debug adapter
  - Global extension errors are controlled by VS Code and printed in VS Code Developer Tools

There are also some global extension technical logs that might be exposed to the output. To see them set the following properties:

```json
{
  ""react-native-tools"": {
    ""logLevel"": ""Trace""
  }
}
```

`logLevel` can be `None` (no logs), `Error`, `Warning`, `Info`, `Debug`, `Trace` (all logs). Default is `Info`.

## Build APK and generate bundle

You can add VSCode tasks to build an `.apk` file and generate iOS/Android bundles.

The following is an example of a `tasks.json` for `react-native init` projects.
Place it in the `.vscode` folder in your project to use it:

```json
{
  ""version"": ""2.0.0"",
  ""presentation"": {
    ""reveal"": ""always"",
    ""panel"": ""new""
  },
  ""tasks"": [
    {
      ""taskName"": ""Build APK Debug"",
      ""group"": ""build"",
      ""type"": ""shell"",
      ""windows"": {
        ""command"": ""cd android; if($?) {./gradlew assembleDebug}""
      },
      ""linux"": {
        ""command"": ""cd android && ./gradlew assembleDebug""
      }
    },
    {
      ""taskName"": ""Build APK Release"",
      ""group"": ""build"",
      ""type"": ""shell"",
      ""windows"": {
        ""command"": ""cd android; if($?) {./gradlew assembleRelease}""
      },
      ""linux"": {
        ""command"": ""cd android && ./gradlew assembleRelease""
      }
    },
    {
      ""taskName"": ""Generate Android Bundle"",
      ""group"": ""build"",
      ""type"": ""shell"",
      ""command"": ""react-native bundle --platform android --dev false --entry-file index.js --bundle-output android/main.jsbundle""
    },
    {
      ""taskName"": ""Generate iOS Bundle"",
      ""group"": ""build"",
      ""type"": ""shell"",
      ""command"": ""react-native bundle --platform ios --dev false --entry-file index.js --bundle-output ios/main.jsbundle""
    }
  ]
}
```

To learn more about `tasks` in VSCode read [the official documentation](https://code.visualstudio.com/docs/editor/tasks).

Visit [generating Signed APK](https://reactnative.dev/docs/signed-apk-android.html) to learn more about this subject.

## Specifying custom arguments for `react-native run-*` command

Using custom run arguments for `react-native run-<platform>`:
**NOTE:** This overrides all other configuration parameters.

```json
{
  ""react-native.android.runArguments.simulator"": [
    ""--appFolder"",
    ""/Users/test/AwesomeProject/android/app"",
    ""--deviceId"",
    ""emulator-5555""
  ],
  ""react-native.ios.runArguments.device"": [
    ""--project-path"",
    ""ios"",
    ""--device"",
    ""Max's iPhone""
  ]
}
```

**NOTE:** You can get the list of installed simulator devices by:

iOS devices (macOS only):

```
xcrun simctl list --json devices
```

Android devices:

```
adb devices
```

**NOTE:** If you want to run the application on an iOS device, make sure you have `ios-deploy` installed globally.

`npm install -g ios-deploy`

## Setting up the React Native packager

To use a custom port for the `react-native` packager:

```json
{
  ""react-native"": {
    ""packager"": {
      ""port"": portNumber
    }
  }
}
```

If you change this port, then for iOS device and simulator scenarios you will have to modify the native code files. [Instructions here](https://blog.binoy.io/running-react-native-on-a-different-port-7deb43887cd4).

If you use Android, you need to change the debug server by:

1. `CTRL+M`(`CMD+M`) in the emulator
2. Go to `Dev Settings`
3. Debug server host for device => enter `localhost:<yourPortNumber>`.
4. Reload application (press `R` twice)
5. (Hermes only) Hermes engine listens port 8081 for debugging by default, to change it you might need to modify your [`metro.config.js` file adding `""port"": portNumber` argument in there to the server settings](https://facebook.github.io/metro/docs/configuration/#port).

```js
// Example of metro.config.js
module.exports = {
  server: {
    port: 9091,
  },
};
```

<details>
<summary>Port setup instruction</summary>

![image](images/select-dev-menu.png)

![image](images/dev-menu-setup-custom-host.png)

![image](images/custom-host-and-port.png)

</details>

**NOTE:** Some aspects of React Native hard-code the port to the default as specified in [this issue](https://github.com/facebook/react-native/issues/9145).

### Custom environment variables

Extension supports passing custom environment variables to the React Native Packager process context. To add custom variables you can create `.env` file in the root folder of your project and add needed environment variables in the following format:

```

Variable1_name=Variable1_value
Variable2_name=Variable2_value

```

Variables that are declared in this `.env` file can override the original environment variables from `process.env` of the Packager process.

It is possible to transfer environment variables (via `env` and `envFile` arguments in `launch.json`) from the `launch` or `attach` debug scenarios to the Packager. If these variables are defined, then they will be used, otherwise the `.env` file is used.

## Change project root

To specify a subfolder in which the react-native project is located, set `react-native-tools.projectRoot`. You can use either an absolute or relative path here:

```json
{
  ""react-native-tools"": {
    ""projectRoot"": ""./your/react-native/project""
  }
}
```

## Configure an Android LogCat Monitor

There are two ways to filter your LogCat Monitor output depending on how LogCat Monitor was launched:

1. Since LogCat Monitor is launched for all Android launch scenarios by default, you can add `logCatArguments` to your debug scenario in `launch.json` file like in the following example:

```json
{
  ""name"": ""Debug Android"",
  ""cwd"": ""${workspaceFolder}"",
  ""type"": ""reactnative"",
  ""request"": ""launch"",
  ""platform"": ""android"",
  ""logCatArguments"": [""ReactNativeJS:V""]
}
```

2. If you want to launch LogCat Monitor from the Command Pallette command `React Native: Run React Native LogCat Monitor` with filtering options set `react-native.android.logCatArguments` settings in your `settings.json`:

```json
{
  ""react-native.android.logCatArguments"": [
    ""*:S"",
    ""ReactNative:V"",
    ""ReactNativeJS:V""
  ]
}
```

To have better understanding on how LogCat filtering works take into account that the extension launches LogCat with flag `-s` and then adds user-provided filters as arguments. Please see the [official instruction on how does LogCat filtering works](https://developer.android.com/studio/command-line/logcat#filteringOutput).

# Network Inspector

The extension provides `Network inspector` feature to inspect outgoing network traffic in your apps. You can browse all requests being made and their responses in VS Code DevTools console.

![image](images/network-inspector.png)

### Network inspector requirements

Before using the Network inspector, please make sure that your system meets the following requirements:

- `OpenSSL` utility is installed and added to PATH. You can install `OpenSSL` the following way:
  - Windows: `choco install openssl`
  - macOS: `brew install openssl`
  - Linux: `sudo apt-get install openssl`
- (macOS only) [`idb`](https://fbidb.io/docs/installation/) utility is installed. It's required to interact with iOS physical devices

### Network inspector usage

- To run the Network inspector you can use `Run Network Inspector` Command Palette command</br>
  When the Network inspector detects a React Native application and connects to it, VS Code DevTools window will be opened automatically. But you can also open it manually, by opening `Help` menu and clicking `Toggle Developer Tools` option. After that you just need to open `Console` tab in DevTools, where network requests will be printed.
- To stop the Network inspector you can use `Stop Network Inspector` Command Palette command

For now the Network inspector doesn't support Expo applications.

# Developing inside a Docker Container

The extension supports [VS Code Remote Development](https://code.visualstudio.com/docs/remote/remote-overview) features on Linux. Please follow the [VS Code official documentation](https://code.visualstudio.com/docs/remote/containers) to setup your environment to use a remote development approach.

You can use [official React Native Docker image](https://hub.docker.com/r/reactnativecommunity/react-native-android) provided by the [react-native-community](https://github.com/react-native-community/docker-android).

Here are the steps to run React Native debugging inside a Docker Container on a real Android device:

1. Open Command Palette and run the following command
   ```
   Remote-Containers: Add Development Container Configuration Files...
   ```
   Then select `Existing Dockerfile` to create `.devcontainer/devcontainer.json` configuration file.
1. Сreate Dockerfile extending [reactnativecommunity/react-native-android image](https://hub.docker.com/r/reactnativecommunity/react-native-android). For example you can use the following Dockerfile:

   ```
   FROM reactnativecommunity/react-native-android:latest

   RUN npm install -g expo-cli react-native-cli
   ```

1. Configure your `devcontainer.json` file as needed. Below is a sample configuration:

   ```json
   {
     ""name"": ""React Native Android Container"",

     // Sets the run context to one level up instead of the .devcontainer folder.
     ""context"": "".."",

     // Update the 'dockerFile' property if you aren't using the standard 'Dockerfile' filename.
     ""dockerFile"": ""Dockerfile"",

     // The optional 'runArgs' property can be used to specify additional runtime arguments.
     ""runArgs"": [
       ""--privileged"", // give all capabilities to a container, in other words, the container can then do almost everything that the host can do
       ""--net"",
       ""host"", // forwarding all host machine ports
       ""-v"",
       ""/dev/bus/usb:/dev/bus/usb"" // mount connected USB devices to a container
     ],

     ""settings"": {
       // This will ignore your local shell user setting for Linux since shells like zsh are typically
       // not in base container images. You can also update this to an specific shell to ensure VS Code
       // uses the right one for terminals and tasks. For example, /bin/bash (or /bin/ash for Alpine).
       ""terminal.integrated.shell.linux"": null
     },

     // Add the IDs of extensions you want installed when the container is created in the array below.
     ""extensions"": [""msjsdiag.vscode-react-native""]
   }
   ```

1. Open Command Palette and run the following command `Remote-Containers: Open Folder in Container` to reopen your project in a container
1. Connect your device via USB and start debugging the same way as on local machine.

Currently the above scenario doesn't work on macOS and Windows. Docker Container implementation on these OS uses Virtual Machine tools which may have problems with USB forwarding for mobile devices.

# Contributing

Please see our [contributing guide](CONTRIBUTING.md) for more information.

# Known Issues

Here is the list of common known issues you may experience while using the extension:

| Issue                                                                                                      | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Debugger doesn't stop at breakpoints                                                                       | Breakpoints require sourcemaps to be correctly configured. If you are using TypeScript, then make sure to follow the `Getting started` section for how to ensure sourcemaps are correctly set up. Also, similar issues may occur on React Native version `0.58.*` in some special cases (see [#928](https://github.com/microsoft/vscode-react-native/issues/928), [#907](https://github.com/microsoft/vscode-react-native/issues/907)), bumping dependencies versions of `react` and `react-native` package to the more recent ones should resolve these. If you are on Linux, make sure that the project folder which is opened is not a symbolic link to the real folder, that might cause problems with sourcemaps (see [#1456](https://github.com/microsoft/vscode-react-native/issues/1456)) |
| 'adb: command not found'                                                                                   | If you receive an error `adb: command not found`, you need to update your system Path to include the location of your _ADB_ executable.The _ADB_ executable file is located in a subdirectory along with your other Android SDK files.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Targeting iPhone 6 doesn't work                                                                            | There was a known issue with React Native ([#5850](https://github.com/facebook/react-native/issues/5850)) but it was fixed. Please upgrade your version of React Native.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Can't communicate with socket pipe                                                                         | (Linux only) If you have two workspaces open that only differ in casing, the extension will fail to communicate effectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ""Add configuration"" button doesn't work when trying to add debug configuration to `launch.json`            | You have to add some json configuration to `launch.json` manually. Please, see ([#985](https://github.com/microsoft/vscode-react-native/issues/985)).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Error `None of these files exist: * .vscode/exponentIndex` appears when running React Native apps via Expo | On some project configurations (mostly on macOS) there could be problems with running RN app via Expo for the first time. You can resolve this by explicitly adding `module.exports.watchFolders = ['.vscode'];` to your Metro config. This will help Metro bundler to find the custom entry point generated by the extension needed to work with Expo. For details you can see the issue ([#1327](https://github.com/microsoft/vscode-react-native/issues/1327)).                                                                                                                                                                                                                                                                                                                                |

[Known-Issues](https://github.com/microsoft/vscode-react-native/issues?q=is%3Aissue+label%3Aknown-issues) provides a complete list of active and resolved issues.

# Telemetry reporting

VS Code React Native extension collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://www.visualstudio.com/en-us/dn948229) to learn more.

If you don’t wish to send usage data to Microsoft, edit `VSCodeTelemetrySettings.json` file at `~/.vscode-react-native` and add `optIn:false`.

# Code of conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
25,microsoft/helium-ui,TypeScript,"# Helium UI

Helium UI is a React application built in TypeScript created to test and display REST API endpoints. Helium UI's core functionality includes:
- Display all movies from endpoint (GET)
- Add a new movie to the list of movies (POST)
- Delete a movie from the list of movies (DELETE) - in progress

## Packages Used:
- Material UI - Styling of the application
- Axios - Performing CRUD requests and operations
- Formik - Handling state using React Forms and Dialogs
  

# Getting Started

## Run Locally with npm

1. Clone the repository
2. Open a terminal in the local respository directory
3. Run the application using
```
npm build && npm start
```

## Run Locally with Docker

1. Clone the repository
2. Open a terminal in the local repository directory
3. Build the application using
```
docker build -t helium-ui .
```

4. Run the application using
```
docker run -it -P helium-ui
```

   In another terminal, run:
```
docker ps
```

   Output will show the port number the image is running on:
```
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  
1dafc0296c23        helium-ui           ""npm start""              24 seconds ago      Up 23 seconds       0.0.0.0:32770->3000/tcp
```

5. In a browser, navigate to `http://localhost:<port number from previous step>` and the Helium UI should appear.


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
26,microsoft/DirectXShaderCompiler,C++,"# DirectX Shader Compiler

[![Build status](https://ci.appveyor.com/api/projects/status/6sx47j66g4dbyem9/branch/master?svg=true)](https://ci.appveyor.com/project/dnovillo/directxshadercompiler/branch/master)

The DirectX Shader Compiler project includes a compiler and related tools used to compile High-Level Shader Language (HLSL) programs into DirectX Intermediate Language (DXIL) representation. Applications that make use of DirectX for graphics, games, and computation can use it to generate shader programs.

For more information, see the [Wiki](https://github.com/microsoft/DirectXShaderCompiler/wiki).

## Downloads
You can download the latest successful build's artifacts (built by Appveyor) for the master branch:
| Downloads |        |
|-----------|--------|
| Windows   | [⬇](https://ci.appveyor.com/api/projects/dnovillo/directxshadercompiler/artifacts/build%2FRelease%2Fdxc-artifacts.zip?branch=master&pr=false&job=image%3A%20Visual%20Studio%202019) |
| Ubuntu    | [⬇](https://ci.appveyor.com/api/projects/dnovillo/directxshadercompiler/artifacts/build%2Fdxc-artifacts.tar.gz?branch=master&pr=false&job=image%3A%20Ubuntu) |

## Features and Goals

The starting point of the project is a fork of the [LLVM](http://llvm.org/) and [Clang](http://clang.llvm.org/) projects, modified to accept HLSL and emit a validated container that can be consumed by GPU drivers.

At the moment, the DirectX HLSL Compiler provides the following components:

- dxc.exe, a command-line tool that can compile HLSL programs for shader model 6.0 or higher

- dxcompiler.dll, a DLL providing a componentized compiler, assembler, disassembler, and validator

- dxilconv.dll, a DLL providing a converter from DXBC (older shader bytecode format)

- various other tools based on the above components

The Microsoft Windows SDK releases include a supported version of the compiler and validator.

The goal of the project is to allow the broader community of shader developers to contribute to the language and representation of shader programs, maintaining the principles of compatibility and supportability for the platform. It's currently in active development across two axes: language evolution (with no impact to DXIL representation), and surfacing hardware capabilities (with impact to DXIL, and thus requiring coordination with GPU implementations).

### Pre-built Releases

Binary packages containing the output of this project are available from appveyor. Development kits containing only the dxc.exe driver app, the dxcompiler.dll, and the dxil.dll signing binary are available [here](https://github.com/microsoft/DirectXShaderCompiler/wiki/Releases), or in the [releases tab](https://github.com/microsoft/DirectXShaderCompiler/releases).

### SPIR-V CodeGen

As an example of community contribution, this project can also target the [SPIR-V](https://www.khronos.org/registry/spir-v/) intermediate representation. Please see the [doc](docs/SPIR-V.rst) for how HLSL features are mapped to SPIR-V, and the [wiki](https://github.com/microsoft/DirectXShaderCompiler/wiki/SPIR%E2%80%90V-CodeGen) page for how to build, use, and contribute to the SPIR-V CodeGen.

## Building Sources
Note: If you intend to build from sources on Linux/macOS, follow [these instructions](docs/DxcOnUnix.rst).

Before you build, you will need to have some additional software installed. This is the most straightforward path - see [Building Sources](https://github.com/microsoft/DirectXShaderCompiler/wiki/Building-Sources) on the Wiki for more options, including Visual Studio 2015 and Ninja support.

* [Git](http://git-scm.com/downloads).
* [Python](https://www.python.org/downloads/) - version 3.x is required
* [Visual Studio 2017](https://www.visualstudio.com/downloads) - select the following workloads: 
    * Universal Windows Platform Development
    * Desktop Development with C++
* [Windows SDK](https://developer.microsoft.com/en-US/windows/downloads/windows-10-sdk) - version 10.0.18362.0 or newer
* [Windows Driver Kit](https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk) - same version as the SDK

After cloning the project, you can set up a build environment shortcut by double-clicking the `utils\hct\hctshortcut.js` file. This will create a shortcut on your desktop with a default configuration. If your system doesn't have the requisite association for .js files, this may not work. If so, open a cmd window and invoke: `wscript.exe utils\hct\hctshortcut.js`.

Tests are built using the TAEF framework which is included in the Windows Driver Kit.

To build, run this command on the HLSL Console.

    hctbuild

You can also clean, build and run tests with this command.

    hctcheckin


To see a list of additional commands available, run `hcthelp`

## Running Tests

To run tests, open the HLSL Console and run this command after a successful build.

    hcttest

Some tests will run shaders and verify their behavior. These tests also involve a driver that can run these execute these shaders. See the next section on how this should be currently set up.

## Running Shaders

To run shaders compiled as DXIL, you will need support from the operating system as well as from the driver for your graphics adapter. Windows 10 Creators Update is the first version to support DXIL shaders. See the [Wiki](https://github.com/microsoft/DirectXShaderCompiler/wiki/Running-Shaders) for information on using experimental support or the software adapter.

### Hardware Support

Hardware GPU support for DXIL is provided by the following vendors:

#### NVIDIA
NVIDIA's r396 drivers (r397.64 and later) provide release mode support for DXIL
1.1 and Shader Model 6.1 on Win10 1709 and later, and experimental mode support
for DXIL 1.2 and Shader Model 6.2 on Win10 1803 and later. These drivers also
support DXR in experimental mode.

Drivers can be downloaded from [geforce.com](https://www.geforce.com/drivers).

#### AMD
AMD’s driver (Radeon Software Adrenalin Edition 18.4.1 or later) provides release mode support for DXIL 1.1 and Shader Model 6.1. Drivers can be downloaded from [AMD's download site](https://support.amd.com/en-us/download).

### Intel
Intel's 15.60 drivers (15.60.0.4849 and later) support release mode for DXIL 1.0 and Shader Model 6.0 as well as
release mode for DXIL 1.1 and Shader Model 6.1 (View Instancing support only).

Drivers can be downloaded from the following link [Intel Graphics Drivers](https://downloadcenter.intel.com/product/80939/Graphics-Drivers)

Direct access to 15.60 driver (latest as of of this update) is provided below:

[Installer](https://downloadmirror.intel.com/27412/a08/win64_15.60.2.4901.exe)

[Release Notes related to DXIL](https://downloadmirror.intel.com/27266/eng/ReleaseNotes_GFX_15600.4849.pdf)

## Making Changes

To make contributions, see the [CONTRIBUTING.md](CONTRIBUTING.md) file in this project.

## Documentation

You can find documentation for this project in the `docs` directory. These contain the original LLVM documentation files, as well as two new files worth nothing:

* [HLSLChanges.rst](docs/HLSLChanges.rst): this is the starting point for how this fork diverges from the original llvm/clang sources
* [DXIL.rst](docs/DXIL.rst): this file contains the specification for the DXIL format
* [tools/clang/docs/UsingDxc.rst](tools/clang/docs/UsingDxc.rst): this file contains a user guide for dxc.exe

## License

DirectX Shader Compiler is distributed under the terms of the University of Illinois Open Source License.

See [LICENSE.txt](LICENSE.TXT) and [ThirdPartyNotices.txt](ThirdPartyNotices.txt) for details.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
27,microsoft/roosterjs,TypeScript,"[![Build Status](https://api.travis-ci.org/microsoft/roosterjs.svg?branch=master)](https://www.travis-ci.org/microsoft/roosterjs)

# Rooster

Rooster is a framework-independent JavaScript rich-text editor neatly nested
inside one HTML `<div>` element. Editing operations performed by end users are
handled in simple ways to generate the final HTML.

To view the sample site, please click the link below:

[RoosterJs Sample Site](https://microsoft.github.io/roosterjs/index.html).

## Upgrade from RoosterJs 7.\*

Please see [here](https://github.com/microsoft/roosterjs/wiki/RoosterJs-8).

## Features

### Packages

Rooster contains 6 packages.

1. [roosterjs](https://microsoft.github.io/roosterjs/docs/modules/roosterjs.html):
   A facade of all Rooster code for those who want a quick start. Use the
   `createEditor()` function in roosterjs to create an editor with default
   configurations.

2. [roosterjs-editor-core](https://microsoft.github.io/roosterjs/docs/modules/roosterjs_editor_core.html):
   Defines the core editor and plugin infrastructure. Use `roosterjs-editor-core`
   instead of `roosterjs` to build and customize your own editor.

3. [roosterjs-editor-api](https://microsoft.github.io/roosterjs/docs/modules/roosterjs_editor_api.html):
   Defines APIs for editor operations. Use these APIs to modify content and
   formatting in the editor you built using `roosterjs-editor-core`.

4. [roosterjs-editor-dom](https://microsoft.github.io/roosterjs/docs/modules/roosterjs_editor_dom.html):
   Defines APIs for DOM operations. Use `roosterjs-editor-api` instead unless
   you want to access DOM API directly.

5. [roosterjs-editor-plugins](https://microsoft.github.io/roosterjs/docs/modules/roosterjs_editor_plugins.html):
   Defines basic plugins for common features. Examples: making hyperlinks,
   pasting HTML content, inserting inline images.

6. [roosterjs-editor-types](https://microsoft.github.io/roosterjs/docs/modules/roosterjs_editor_types.html):
   Defines public interfaces and enumerations.

### APIs

Rooster provides DOM level APIs (in `roosterjs-editor-dom`), core APIs (in `roosterjs-editor-core`), and formatting APIs
(in `roosterjs-editor-api`) to perform editing operations.

`roosterjs-editor-dom` provides several levels of DOM operations:

-   Perform basic DOM operations such as `fromHtml()`, `wrap()`, `unwrap()`, ...
-   Wrap a given DOM node with `InlineElement` or `BlockElement` and perform
    operations with DOM Walker API.
-   Perform DOM operations on a given scope using scopers.
-   Travel among `InlineElements` and `BlockElements` with scope using
    ContentTraverser API.

`roosterjs-editor-core` provides APIs for editor core. Editor class will call such
APIs to perform basic editor operations. These APIs are overridable by specifying
API overrides in Editor options when creating the editor.

`roosterjs-editor-api` provides APIs for scenario-based operations triggered by
user interaction.

## Plugins

Rooster supports plugins. You can use built-in plugins or build your own.
Plugins call APIs to communicate with the editor. When an operation is
performed by the user or when content is changed by code, the editor will
trigger events for the plugins to handle.

Here's a sample plugin which will show a dialog containing ""Hello Rooster"" when
an ""a"" is typed in the editor:

```typescript
class HelloRooster implements EditorPlugin {
    getName() {
        return 'HelloRooster';
    }

    initialize(editor: IEditor) {}

    dispose() {}

    onPluginEvent(e: PluginEvent) {
        if (e.eventType == PluginEventType.KeyPress && e.rawEvent.which == 65) {
            alert('Hello Rooster');
        }
    }
}
```

## Installation

Install via NPM or Yarn:

`yarn add roosterjs`

or

`npm install roosterjs --save`

You can also install sub packages separately:

`yarn add roosterjs-editor-core`

`yarn add roosterjs-editor-api`

`...`

or

`npm install roosterjs-editor-core --save`

`npm install roosterjs-editor-api --save`

`...`

In order to run the code below, you may also need to install [webpack](https://webpack.js.org/):

`yarn add webpack -g`

or

`npm install webpack -g`

## Usage

### A quick start

1. Create `editor.htm` contains a DIV with some styles, and a reference to a
   .js file:

```html
<html>
    <body>
        <div
            id=""editorDiv""
            style=""width: 500px; height: 300px; overflow: auto;
        border: solid 1px black""
        ></div>
        <script src=""editor.js""></script>
    </body>
</html>
```

2. Create `source.js` to import roosterjs and create an editor:

```javascript
var roosterjs = require('roosterjs');
var editorDiv = document.getElementById('editorDiv');
var editor = roosterjs.createEditor(editorDiv);
editor.setContent('Welcome to <b>RoosterJs</b>!');
```

3. Compile the javascript file using webpack:

`webpack source.js editor.js`

4. Navigate to editor.htm, you will see a editor shown in the page.

### Add some format buttons

1. Add some buttons into `editor.htm`:

```html
<html>
    <body>
        <div
            id=""editorDiv""
            style=""width: 500px; height: 300px; overflow: auto;
        border: solid 1px black""
        ></div>
        <button id=""buttonB"">B</button> <button id=""buttonI"">I</button>
        <button id=""buttonU"">U</button>
        <script src=""editor.js""></script>
    </body>
</html>
```

2. Add code to `source.js` to handle click event of the buttons:

```javascript
var roosterjs = require('roosterjs');
var editorDiv = document.getElementById('editorDiv');
var editor = roosterjs.createEditor(editorDiv);
editor.setContent('Welcome to <b>RoosterJs</b>!');

document.getElementById('buttonB').addEventListener('click', function () {
    roosterjs.toggleBold(editor);
});
document.getElementById('buttonI').addEventListener('click', function () {
    roosterjs.toggleItalic(editor);
});
document.getElementById('buttonU').addEventListener('click', function () {
    roosterjs.toggleUnderline(editor);
});
```

3. Compile the javascript file using webpack:

`webpack source.js editor.js`

4. Navigate to editor.htm, you will see buttons with bold, italic, underline
   actions in the page.

## Sample code

To view the sample site, please click [here](https://microsoft.github.io/roosterjs/index.html).

To build the sample site code yourself, follow these instructions:

1. Get dependencies using [yarn](https://yarnpkg.com) or [npm](https://www.npmjs.com/):

    ```cmd
    yarn
    ```

    or

    ```cmd
    npm install
    ```

2. Build the source code, and start the sample editor:

    ```
    yarn start
    ```

    or

    ```
    npm start
    ```

3. Navigate to the sample editor at http://localhost:3000/

## Debugging

There are two options for debugging:

1. Debugging from VSCode

    - Ensure the sample editor is running
    - Set the breakpoints within VSCode
    - Select ""Debug app in Chrome"" from the VSCode debugging configuration dropdown
      <img src=""/assets/readme-images/debug-in-VSCode.png"" width=""411"" height=""278""><br>
    - Run the scenario that needs to be debugged

2. Debugging directly from the development tools within the web browser
    - The directions for how to do this are specific to each web browser. By opening the developer
      tools for the web browser that Rooster is running on, you will be able to set breakpoints in
      the code and debug accordingly.

## Running tests

There are two ways that tests can be run:

1. Run all tests or a single test from VSCode<br>
    - (Skip if running all tests) Ensure the file that you want to test is selected (ie: toggleBold.ts
      or toggleBoldTest.ts)
    - Select ""Test all files"" or ""Test current file"" from the VSCode debugging configuration dropdown
      <img src=""/assets/readme-images/test-in-VSCode.png"" width=""402"" height=""268"">
2. Run all tests from command line
    ```
    yarn test
    ```

## Dependencies

As a NodeJs package, RoosterJs has dependencies for runtime (specified in package.json under each sub
packages in ""dependencies"" section) and dependencies for build time (specified in package.json under
root path in ""devDependencies"" section).

For runtime dependencies, there are two parts:

-   Internal dependencies (a RoosterJs package depends on other RoosterJs packages)
-   External dependencies (RoosterJs depends on other NPM packages)

Currently we have very few external dependencies. Before adding any new dependency, we need to check:

1. What's the value of the new dependency and the code using the dependency bring into roosterjs?
   If we add a new dependency and create our new API to just call into the dependency, that new API
   doesn't actually bring too much value, and people who uses roosterjs in their project can do this
   themselves in their code, and we should not add such dependency to people who don't really need it.

2. What's the dependency tree of the dependency?
   If we introduce a new dependency which has a deep dependency tree, we need to be careful since it
   means we are actually adding a lot of new dependencies and our code size may be increased a lot.

3. How much functionalities do we need from the dependency?
   If the dependency provides a lot of functionalities but we actually only need a small piece of them,
   we may need to consider other solutions, such as find another smaller one, or do it ourselves.

4. What's the license of the dependency?
   A dependency package under MIT license is good to be used for RoosterJs. For other licenses, we need
   to review and see if we can take it as a dependency.

If you still feel a new dependency is required after checking these 3 questions, we can review it and
finally decide whether we should add the new dependency.

For build time dependencies, it is more flexable to add new dependencies since it won't increase runtime
code size or dependencies.

## More documentation

We are still working on more documentation in [roosterjs wiki](https://github.com/Microsoft/roosterjs/wiki) and [API reference](https://microsoft.github.io/roosterjs/docs/index.html).

## License - MIT

License
Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [MIT](LICENSE) License.
"
28,microsoft/fluentui-apple,Swift,"# Fluent UI Apple
Fluent UI Apple contains native UIKit and AppKit controls aligned with [Microsoft's Fluent UI design system](https://www.microsoft.com/design/fluent/#/). 

![Build Status](https://github.com/microsoft/fluentui-apple/workflows/CI/badge.svg?branch=main)
![Localization Status](https://github.com/microsoft/fluentui-apple/workflows/Localize/badge.svg)
![CocoaPod Publishing](https://github.com/microsoft/fluentui-apple/workflows/Pod-Publish/badge.svg)
[![Build Status](https://dev.azure.com/microsoftdesign/fluentui-native/_apis/build/status/microsoft.fluentui-apple?branchName=main)](https://dev.azure.com/microsoftdesign/fluentui-native/_build/latest?definitionId=144&branchName=main)
![License](https://img.shields.io/github/license/Microsoft/fluentui-apple)
[![Carthage Compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage)
[![CocoaPods Compatible](https://img.shields.io/cocoapods/v/MicrosoftFluentUI)](https://cocoapods.org/pods/MicrosoftFluentUI)
![Platform](https://img.shields.io/cocoapods/p/MicrosoftFluentUI.svg?style=flat)

## Getting Started
### Install and use FluentUI

#### Requirements

- iOS 13+ or macOS 10.14+
- Xcode 11+
- Swift 5.0+

#### Using Carthage

To integrate FluentUI using Carthage, specify it in your Cartfile:

```
github ""Microsoft/fluentui-apple"" ~> X.X.X
```

then follow the Carthage [integration steps](https://github.com/Carthage/Carthage#adding-frameworks-to-an-application) to add the `FluentUI.framework` into your Xcode project

#### Using CocoaPods

To get set up with CocoaPods visit their [getting started guide](https://guides.cocoapods.org/using/getting-started.html).

To integrate FluentUI into your Xcode project using CocoaPods, specify it in your Podfile:
```ruby
pod 'MicrosoftFluentUI', '~> X.X.X'
```

#### Manual installation

- Download the latest changes from the [FluentUI for Apple](https://github.com/microsoft/fluentui-apple) repository.
- Move the `fluentui-apple` folder into your project folder.
- Move the relevant `FluentUI.xcodeproj` into your Xcode project depending on which platform you want to support.
- In Xcode select your project -> your target -> General -> Embedded Binaries -> add `FluentUI.framework`.

#### Swift Package Manager
As of this writing, the version of Swift Package Manager shipped with the latest Xcode does not support packages that require resource bundles. As Fluent UI Apple does  require resource bundles, we do not currently support Swift Package Manager.

### Import and use FluentUI

After the framework has been added you can import the module to use it:

For Swift
```swift
import FluentUI
```
For Objective-C
```objective-c
#import <FluentUI/FluentUI.h>
```

## Contributing

Post bug reports, feature requests, and questions in [Issues](https://github.com/microsoft/fluentui-apple/issues).

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### Developing in the repo

Fluent UI Apple requires all [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) to come from forks of the repository. Please see [Fork a Repo - GitHub Help](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) for more details on how to set up a fork of Microsoft/fluentui-apple, keep it up-to-date, and submit pull requests back to this repository.

Fluent UI Apple doesn't have any external code dependencies, so developing in the repository is as easy as launching the appropriate Xcode project or workspace and building and running a test app.

For more platform-specific information, please see [the iOS readme file](ios/README.md) and the [the macOS readme file](macos/README.md).

#### Swift Lint
This project uses [SwiftLint](https://github.com/realm/SwiftLint) to automatically lint our Swift code for common errors. Please install it when developing in this repo by following the [SwiftLint Installation Instructions](https://realm.github.io/SwiftLint/).

## Changelog

We use [GitHub Releases](https://github.com/blog/1547-release-your-software) to manage our releases, including the changelog between every release. You'll find a complete list of additions, fixes, and changes on the [Releases page](https://github.com/microsoft/fluentui-apple/releases).

## License

All files on the FluentUI Apple GitHub repository are subject to the MIT license. Please read the [LICENSE](LICENSE) file at the root of the project.

Usage of the logos and icons referenced in FluentUI Apple is subject to the terms of the [assets license agreement](https://aka.ms/fabric-assets-license).
"
29,microsoft/FLAML,Jupyter Notebook,"[![PyPI version](https://badge.fury.io/py/FLAML.svg)](https://badge.fury.io/py/FLAML)
[![Build](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/FLAML/actions/workflows/python-package.yml)
![Python Version](https://img.shields.io/badge/3.6%20%7C%203.7%20%7C%203.8-blue)
[![Downloads](https://pepy.tech/badge/flaml/month)](https://pepy.tech/project/flaml)
[![Join the chat at https://gitter.im/FLAMLer/community](https://badges.gitter.im/FLAMLer/community.svg)](https://gitter.im/FLAMLer/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

# FLAML - Fast and Lightweight AutoML

<p align=""center"">
    <img src=""https://github.com/microsoft/FLAML/blob/main/docs/images/FLAML.png""  width=200>
    <br>
</p>

FLAML is a lightweight Python library that finds accurate machine
learning models automatically, efficiently and economically. It frees users from selecting
learners and hyperparameters for each learner. It is fast and economical. 
The simple and lightweight design makes it easy to extend, such as
adding customized learners or metrics. FLAML is powered by a new, [cost-effective
hyperparameter optimization](https://github.com/microsoft/FLAML/tree/main/flaml/tune)
and learner selection method invented by Microsoft Research.
FLAML leverages the structure of the search space to choose a search order optimized for both cost and error. For example, the system tends to propose cheap configurations at the beginning stage of the search,
but quickly moves to configurations with high model complexity and large sample size when needed in the later stage of the search. For another example, it favors cheap learners in the beginning but penalizes them later if the error improvement is slow. The cost-bounded search and cost-based prioritization make a big difference in the search efficiency under budget constraints.

## Installation

FLAML requires **Python version >= 3.6**. It can be installed from pip:

```bash
pip install flaml
```

To run the [`notebook example`](https://github.com/microsoft/FLAML/tree/main/notebook),
install flaml with the [notebook] option:

```bash
pip install flaml[notebook]
```

## Quickstart

* With three lines of code, you can start using this economical and fast
AutoML engine as a scikit-learn style estimator.
```python
from flaml import AutoML
automl = AutoML()
automl.fit(X_train, y_train, task=""classification"")
```

* You can restrict the learners and use FLAML as a fast hyperparameter tuning
tool for XGBoost, LightGBM, Random Forest etc. or a customized learner.
```python
automl.fit(X_train, y_train, task=""classification"", estimator_list=[""lgbm""])
```

* You can also run generic ray-tune style hyperparameter tuning for a custom function.
```python
from flaml import tune
tune.run(train_with_config, config={…}, low_cost_partial_config={…}, time_budget_s=3600)
```

## Advantages

* For classification and regression tasks, find quality models with lower computational resources.
* Users can choose their desired customizability: minimal customization (computational resource budget), medium customization (e.g., scikit-style learner, search space and metric), full customization (arbitrary training and evaluation code).
* Allow human guidance in hyperparameter tuning to respect prior on certain subspaces but also able to explore other subspaces.

## Examples

A basic classification example.

```python
from flaml import AutoML
from sklearn.datasets import load_iris
# Initialize an AutoML instance
automl = AutoML()
# Specify automl goal and constraint
automl_settings = {
    ""time_budget"": 10,  # in seconds
    ""metric"": 'accuracy',
    ""task"": 'classification',
    ""log_file_name"": ""test/iris.log"",
}
X_train, y_train = load_iris(return_X_y=True)
# Train with labeled input data
automl.fit(X_train=X_train, y_train=y_train,
                        **automl_settings)
# Predict
print(automl.predict_proba(X_train))
# Export the best model
print(automl.model)
```

A basic regression example.

```python
from flaml import AutoML
from sklearn.datasets import load_boston
# Initialize an AutoML instance
automl = AutoML()
# Specify automl goal and constraint
automl_settings = {
    ""time_budget"": 10,  # in seconds
    ""metric"": 'r2',
    ""task"": 'regression',
    ""log_file_name"": ""test/boston.log"",
}
X_train, y_train = load_boston(return_X_y=True)
# Train with labeled input data
automl.fit(X_train=X_train, y_train=y_train,
                        **automl_settings)
# Predict
print(automl.predict(X_train))
# Export the best model
print(automl.model)
```

More examples can be found in [notebooks](https://github.com/microsoft/FLAML/tree/main/notebook/).

## Documentation

The API documentation is [here](https://microsoft.github.io/FLAML/).

Read more about the 
hyperparameter optimization methods
in FLAML [here](https://github.com/microsoft/FLAML/tree/main/flaml/tune). They can be used beyond the AutoML context. 
And they can be used in distributed HPO frameworks such as ray tune or nni.

For more technical details, please check our papers.

* [FLAML: A Fast and Lightweight AutoML Library](https://www.microsoft.com/en-us/research/publication/flaml-a-fast-and-lightweight-automl-library/). Chi Wang, Qingyun Wu, Markus Weimer, Erkang Zhu. MLSys, 2021.
```
@inproceedings{wang2021flaml,
    title={FLAML: A Fast and Lightweight AutoML Library},
    author={Chi Wang and Qingyun Wu and Markus Weimer and Erkang Zhu},
    year={2021},
    booktitle={MLSys},
}
```
* [Frugal Optimization for Cost-related Hyperparameters](https://arxiv.org/abs/2005.01571). Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.
* [Economical Hyperparameter Optimization With Blended Search Strategy](https://www.microsoft.com/en-us/research/publication/economical-hyperparameter-optimization-with-blended-search-strategy/). Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.

If you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Developing

### Setup:

```
git clone https://github.com/microsoft/FLAML.git
pip install -e .[test,notebook]
```

### Coverage
Any code you commit should generally not significantly impact coverage. To run all unit tests:
```
coverage run -m pytest test
```

If all the tests are passed, please also test run notebook/flaml_automl to make sure your commit does not break the notebook example.

## Authors

* Chi Wang
* Qingyun Wu

Contributors (alphabetical order): Sebastien Bubeck, Surajit Chaudhuri, Nadiia Chepurko, Ofer Dekel, Alex Deng, Anshuman Dutt, Nicolo Fusi, Jianfeng Gao, Johannes Gehrke, Silu Huang, Dongwoo Kim, Christian Konig, John Langford, Amin Saied, Neil Tenenholtz, Markus Weimer, Haozhe Zhang, Erkang Zhu.

## License

[MIT License](LICENSE)
"
30,microsoft/winget-pkgs,PowerShell,"# Welcome to the Windows Package Manager Community repo
This repository contains the manifest files for the **Windows Package Manager**.  You are highly encouraged to submit manifests for your favorite application.

The **Windows Package Manager** is an open source client.  You will find the source code [here](https://github.com/microsoft/winget-cli).

# Submitting a Package
To submit a package to the repository, you should follow these steps:
1) Follow the **Contributing** guidelines below
2) Author a Manifest
3) Test your manifest
4) Submit your PR
5) Respond to any feedback

>Note: Please check that the package's manifest you intend to submit does not already exist in the manifests folder, and that there are no open PRs for it in order to avoid duplicates.

## Authoring a Manifest

The minimal manifest syntax is below. Additional information on writing manifests can be found on [Microsoft Docs](https://docs.microsoft.com/en-us/windows/package-manager/package/manifest) or on the [v1.0 manifest spec](https://github.com/microsoft/winget-cli/blob/master/doc/ManifestSpecv1.0.md).

Current limitations are:
* One manifest per PR

Be sure the manifest filenames match the `PackageIdentifier` manifest naming conventions and the manifest is located in the folder path matching `manifests\<first lower case letter of publisher>\<publisher>\<package>\<version>\.yaml`

### Using the YAMLCreate.ps1
To help author manifest files, we have provided a YAMLCreate.ps1 powershell script located in the Tools folder.  
The script will prompt you for the URL to the installer, then will prompt you to fill in metadata.

I recommend running the script in the location where you want to produce the manifest file.  For example: `manifests\<publisher>\<package>\`.  After successful completion, it will produce the YAML file.

### Using Windows Package Manager YAML Generator
If you prefer to use a GUI to generate YAML files, you can use the **Windows Package Manager YAML Generator**. It is available as an app [in the Microsoft Store](https://www.microsoft.com/en-us/p/windows-package-manager-yaml-generator/9p3n60fs22k5) and the code is also available [on GitHub](https://github.com/ptorr-msft/WinGetYamlGenerator).

Although the Windows Package Manager YAML Generator can create YAML files with multiple installers, winget does not support more than one installer for now.

## Test your manifest
Now that you have authored your manifest, you should make sure it works as expected.

### Locally
1) Verify the syntax.  You can do that by typing the following command: `winget validate <manifest>`
2) Test the install.  You can do that by installing the manifest: `winget install -m <manifest>`
For more details, see [packages](https://docs.microsoft.com/windows/package-manager/package).

### In Windows Sandbox
You can use the [`Tools\SandboxTest.ps1`](Tools/SandboxTest.ps1) script for testing a manifest installation in [Windows Sandbox](https://docs.microsoft.com/en-us/windows/security/threat-protection/windows-sandbox/windows-sandbox-overview). The manifest will be also validated.

Just provide the path to manifest as parameter:
```powershell
.\Tools\SandboxTest.ps1 <path-to-manifest>
```

## Submit your PR
With the manifest verified, you will need to submit a PR.  Your manifest should be located in the folder path matching `manifests\<first lower case letter of publisher>\<publisher>\<package>\<version>.yaml`

### Validation Process
The PR request will go through a validation process.  During the process, the PR request will get labels to help drive the validation.
In the event of a failure, the BOT will suggest where the problem is with the submission and assign the PR back to you.  

### Respond to PR feedback
If the PR has been assigned to you, a timer is triggered.  You will have 7 days to resolve the issue, or the PR will be closed automatically by the BOT.  

The installer may be identified as malware. If you believe it's a false positive you can submit the installer to the defender team for analysis from [here](https://www.microsoft.com/wdsi/filesubmission).

For a list of the BOT labels, see [packages](https://docs.microsoft.com/windows/package-manager/package/repository#pull-request-labels).

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For the avoidance of doubt, you may not make any Submissions linking to third party materials if such 
Submission is prohibited by the applicable third party and/or otherwise violates such third party's rights.
"
31,microsoft/azure-devops-extension-sample,TypeScript,"# Azure DevOps Web Sample Extension

[![Build Status](https://dev.azure.com/ms/azure-devops-extension-sample/_apis/build/status/Microsoft.azure-devops-extension-sample)](https://dev.azure.com/ms/azure-devops-extension-sample/_build/latest?definitionId=14)

This repository generates an [Azure DevOps extension](https://docs.microsoft.com/en-us/azure/devops/extend/overview?view=vsts) containing a number of different contributions of various types.

## Dependencies

The sample repository depends on a few Azure DevOps packages:

- [azure-devops-extension-sdk](https://github.com/Microsoft/azure-devops-extension-sdk): Required module for Azure DevOps extensions which allows communication between the host page and the extension iframe.
- [azure-devops-extension-api](https://github.com/Microsoft/azure-devops-extension-api): Contains REST client libraries for the various Azure DevOps feature areas.
- [azure-devops-ui](https://developer.microsoft.com/azure-devops): UI library containing the React components used in the Azure DevOps web UI.

Some external dependencies:

- `React` - Is used to render the UI in the samples, and is a dependency of `azure-devops-ui`.
- `TypeScript` - Samples are written in TypeScript and complied to JavaScript
- `SASS` - Extension samples are styled using SASS (which is compiled to CSS and delivered in webpack js bundles).
- `webpack` - Is used to gather dependencies into a single javascript bundle for each sample.

## Building the sample project

Just run:

    npm run build

This produces a .vsix file which can be uploaded to the [Visual Studio Marketplace](https://marketplace.visualstudio.com/azuredevops)

## Using the extension

The preferred way to get started is to use the `tfx extension init` command which will clone from this sample and prompt you for replacement information (like your publisher id). Just run:

    npm install -g tfx-cli
    tfx extension init

You can also clone the sample project and change the `publisher` property in `azure-devops-extension.json` to your own Marketplace publisher id. Refer to the online [documentation](https://docs.microsoft.com/en-us/azure/devops/extend/publish/overview?view=vsts) for setting up your own publisher and publishing an extension.

# Samples

Individual sample contributions are self-contained folders under `./src/Samples`. Within each sample you will find:

1. `{SampleName}.json` - describes the contribution objects being added to Azure DevOps
2. `{SampleName}.html` - page which is rendered within an iframe on the appropriate Azure DevOps page or pages. It may be visible UI (such as a Hub) or a background iframe (such as a Menu action handler). This will include a sample reference for `{SampleName}.js`, and for visible frames it will contain a single `<div>` element with an id of `root`.
3. `{SampleName}.ts(x)` - Root script that is run when the frame is loaded. A webpack entry is added for this file which will generate a single `js` file with this content and all its dependencies.
4. `{SampleName}.scss` - optional sass file containing the styles (CSS) for the UI
5. Additional ts/tsx files - For samples that are too big for one file, the code will be broken up appropriately

## BreadcrumbService

This sample adds a breadcrumb service which adds a ""Sample Breadcrumb Item"" global breadcrumb item to the sample hub.  Visit the ""Sample Hub"" in the `Pipelines` hub group to see this item.

## CodeEditorContribution

This sample adds a language definition and a JSON schema for the code editor.

To see the language definition in action, add a new file to git or TFVC called ""sample.mylog"", then copy the example log content from [the Monaco playground](https://microsoft.github.io/monaco-editor/playground.html#extending-language-services-custom-languages).

To see the JSON schema in action, add a new file to git or TFVC called ""myconfig.json"", then begin editing it.

## Feature

This sample shows how to hook into the Preview Features panel (under the user profile menu). It adds a simple hub that is only shown when an ""ABC"" feature is turned on. The feature can be toggled per-user or per-organization.

This also defines a second feature (ABC v2) which controls whether v1 or v2 of the ABC hub is used (when the ABC feature is turned on). When enabled, a ""property-provider"" contribution modifies the name and url of the hub contribution. Here we add a v2=true query parameter to our existing hub page, but you could also
specify a completely different html page here. This feature shows off a bit more advanced functionality provided by preview features. It can be toggled per-user, per-project, or per-organization (the ""null"" hostScopeValue). It is on by default (defaultState: true). And it has an override rule which causes the v2 feature to be OFF (and disabled in the preview features panel) whenever the ABC feature is off.

## Hub

This sample adds a hub named ""Sample Hub"" into the `Pipelines` hub group. If you visit a project-level page, you will find Sample Hub under the `Pipelines` navigation element in the vertical navigation menu on the left of the page.

The hub uses a Pivot component to draw 4 different tabs:

1. An `Overview` tab contains some simple details about the current user and project
2. A `Navigation` tab contains a few actions that allow you to integrate with the page's URL and title
3. An `Extension Data` tab demonstrates reading and writing to the extension data service
4. A `Messages` tab shows how to display global messages

There are also actions at the top-right of the hub which demonstrate opening dialogs and panels, including custom content within them (used in the `Panel` sample).

## Menu

This sample adds a ""Sample build definition menu item"" to the `Builds` hub in the dropdown actions menu in the top-right of the page. The menu handler gets the current build definition from the context that is passed to it, it makes a REST call, and shows the result in a message box.

## Panel

This sample is leveraged within the `Hub` sample. It is content that contains a toggle button along with OK/Cancel buttons. It can be used as custom panel or dialog content.

## Pivot

This sample adds a ""Sample Pivot"" pivot (tab) to the Organization (Project Collection) home page, next to ""Projects"", ""My work items"", and ""My pull requests"".

This pivot makes a REST call for all the projects in the organization and it displays them in a grid view.

## Pill

This sample adds pills to the title of the Pipeline definition (Runs) page.

## QueryParamsHandler

This sample adds a service that gets loaded on any page whenever a ""showMyPanel"" query parameter is present
in the URL when any page is loaded. The startup service shows the custom panel from the Panel sample, using
an optional ""myPanelTitle"" query parameter as the panel title.

## RepositoryActions

This sample adds a ""Sample repository action"" menu item to the repository picker in the header of code hub pages. If a ""href"" property is provided, clicking on the action will navigate to the given url. If a ""uri"" is provided, that code will be executed when the action is clicked.

## RepositoryServiceHub

This sample adds a ""Repository Information"" hub to the `Code` hub group. It demonstrates how to interact with the `IVersionControlRepositoryService` to obtain basic information about a user's currently selected Git repository.

## WorkItemFormGroup

This sample adds a ""Sample WorkItem Form Group"" extension to workitem form to show how to interact with the `IWorkItemFormService` service and `IWorkItemNotificationListener`. It gives UI to show case how to change field values using the form service and displaying workitem form notification events.

This sample also provides a unit testing example with minimal necessary mocks.

## WorkItemOpen

This sample adds a ""Sample WorkItem Open"" hub to the Boards hub group to show how to interact with the `IWorkItemFormNavigationService` service. It gives UI for you to open an existing work item (by id) or open the work item form for a new work item (by work item type). Either of these options open a dialog in the host frame.

# References

The full set of documentation for developing extensions can be found at [https://docs.microsoft.com/en-us/azure/devops/extend](https://docs.microsoft.com/en-us/azure/devops/extend/?view=vsts).

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.microsoft.com>.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
32,microsoft/BotFramework-WebChat,JavaScript,"# ![Bot Framework Web Chat](https://raw.githubusercontent.com/microsoft/BotFramework-WebChat/main/media/BotFrameworkWebChat_header.png)

### [Click here to find out what is new in Web Chat](https://github.com/microsoft/BotFramework-WebChat/blob/main/CHANGELOG.md)

# Bot Framework Web Chat

[![npm version](https://badge.fury.io/js/botframework-webchat.svg)](https://badge.fury.io/js/botframework-webchat)
[![Build Status](https://fuselabs.visualstudio.com/BotFramework-WebChat/_apis/build/status/BotFramework-WebChat-daily?branchName=main)](https://fuselabs.visualstudio.com/BotFramework-WebChat/_build/latest?definitionId=498&branchName=main)

This repository contains code for the Bot Framework Web Chat component. The Bot Framework Web Chat component is a highly-customizable web-based client for the Bot Framework V4 SDK. The Bot Framework SDK v4 enables developers to model conversation and build sophisticated bot applications.

This repo is part of the [Microsoft Bot Framework](https://github.com/microsoft/botframework) - a comprehensive framework for building enterprise-grade conversational AI experiences.

<hr />

# Version notes

> This section points out important version notes. For further information, please see the related links and check the [`CHANGELOG.md`](https://github.com/microsoft/BotFramework-WebChat/blob/main/CHANGELOG.md

### 4.12.1 patch: New style property `adaptiveCardsParserMaxVersion`

Web Chat 4.12.1 patch includes a new style property allowing developers to choose the max Adaptive Cards schema version. See [PR #3778](https://github.com/microsoft/BotFramework-WebChat/pull/3778) for code changes.

To specify a different max version, you can adjust the style options, shown below:

```js
window.WebChat.renderWebChat(
   {
      directLine,
      store,
      styleOptions: {
         adaptiveCardsParserMaxVersion: '1.2'
      }
   },
   document.getElementById('webchat')
);
```

-  Web Chat will apply the maximum schema available according to the Adaptive Cards version (as of this patch, schema 1.3) by default.
-  An invalid version will revert to Web Chat's default.

## Visual focus changes to transcript in Web Chat 4.12.0

A new accessibility update has been added to Web Chat from PR [#3703](https://github.com/microsoft/BotFramework-WebChat/pull/3703). This change creates visual focus for the transcript (bold black border) and `aria-activedescendent` focused activity (black dashed border) by default. Where applicable, `transcriptVisualKeyboardIndicator...` values will also be applied to carousel (`CarouselFilmStrip.js`) children. This is done in order to match current default focus styling for Adaptive Cards, which may be a child of a carousel.

To modify these styles, you can change the following props via `styleOptions`:

```
  transcriptActivityVisualKeyboardIndicatorColor: DEFAULT_SUBTLE,
  transcriptActivityVisualKeyboardIndicatorStyle: 'dashed',
  transcriptActivityVisualKeyboardIndicatorWidth: 1,
  transcriptVisualKeyboardIndicatorColor: 'Black',
  transcriptVisualKeyboardIndicatorStyle: 'solid',
  transcriptVisualKeyboardIndicatorWidth: 2,
```

The above code shows the default values you will see in Web Chat.

## API refactor into new package in Web Chat 4.11.0

The Web Chat API has been refactored into a separate package. To learn more, check out the [API refactor summary](https://github.com/microsoft/BotFramework-WebChat/pull/3543).

## Direct Line Speech support in Web Chat 4.7.0

Starting from Web Chat 4.7.0, Direct Line Speech is supported, and it is the preferred way to provide an integrated speech functionality in Web Chat. We are working on [closing feature gaps](https://github.com/microsoft/BotFramework-WebChat/labels/Direct%20Line%20Speech) between Direct Line Speech and Web Speech API (includes Cognitive Services and browser-provided speech functionality).

## Upgrading to 4.6.0

Starting from Web Chat 4.6.0, Web Chat requires React 16.8.6 or up.

Although we recommend that you upgrade your host app at your earliest convenience, we understand that host app may need some time before its React dependencies are updated, especially in regards to huge applications.

If your app is not ready for React 16.8.6 yet, you can follow the [hybrid React sample](https://github.com/microsoft/BotFramework-WebChat/tree/main/samples/01.getting-started/g.hybrid-react-npm) to dual-host React in your app.

## Speech changes in Web Chat 4.5.0

There is a breaking change on behavior expectations regarding speech and input hint in Web Chat. Please refer to the section on [input hint behavior before 4.5.0](https://github.com/microsoft/BotFramework-WebChat/blob/main/docs/SPEECH.md#input-hint-behavior-before-4-5-0) for details.

## Migrating from Web Chat v3 to v4

[View migration docs](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/MIGRATION.md) to learn about migrating from Web Chat v3.

<hr />

# How to use

First, create a bot using [Azure Bot Service](https://azure.microsoft.com/en-us/services/bot-service/).
Once the bot is created, you will need to [obtain the bot's Web Chat secret](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-channel-connect-webchat?view=azure-bot-service-3.0#step-1) in Azure Portal. Then use the secret to [generate a token](https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-direct-line-3-0-authentication?view=azure-bot-service-4.0) and pass it to your Web Chat.

## Connect a client app to bot

Web Chat provides UI on top of the Direct Line and Direct Line Speech Channels. There are two ways to connect to your bot through HTTP calls from the client: by sending the Bot secret or generating a token via the secret.

<!-- TODO: https://github.com/microsoft/BotFramework-WebChat/issues/2151 (ongoing) -->
<!-- Update the following paragraph and the API table (`directline`) with new documentation when updated docs are published  -->

We strongly recommend using the token API instead of providing the app with your secret. To learn more about why, see the [authentication documentation](https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-direct-line-3-0-authentication?view=azure-bot-service-4.0) on the [token API](https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-direct-line-3-0-api-reference?view=azure-bot-service-4.0) and client security.

For further reading, please see the following links:

-  Using Web Chat with [Azure Bot Services authentication](https://blog.botframework.com/2018/09/01/using-webchat-with-azure-bot-services-authentication/)

-  [Enhanced Direct Line authentication features](https://blog.botframework.com/2018/09/25/enhanced-direct-line-authentication-features/)

## Integrate with JavaScript

Web Chat is designed to integrate with your existing website using JavaScript or React. Integrating with JavaScript will give you moderate styling and customizability options.

You can use the full, typical Web Chat package (called full-feature bundle) that contains the most typically used features.

Here is how how you can add Web Chat control to your website:

<!-- prettier-ignore-start -->
```html
<!DOCTYPE html>
<html>
  <head>
    <script
      crossorigin=""anonymous""
      src=""https://cdn.botframework.com/botframework-webchat/latest/webchat.js""
    ></script>
    <style>
      html,
      body {
         height: 100%;
      }

      body {
        margin: 0;
      }

      #webchat {
        height: 100%;
        width: 100%;
      }
    </style>
  </head>
  <body>
    <div id=""webchat"" role=""main""></div>
    <script>
      window.WebChat.renderWebChat(
        {
          directLine: window.WebChat.createDirectLine({
            token: 'YOUR_DIRECT_LINE_TOKEN'
          }),
          userID: 'YOUR_USER_ID',
          username: 'Web Chat User',
          locale: 'en-US'
        },
        document.getElementById('webchat')
      );
    </script>
  </body>
</html>
```
<!-- prettier-ignore-end -->

> `userID`, `username`, and `locale` are all optional parameters to pass into the `renderWebChat` method. To learn more about Web Chat props, look at the [Web Chat API Reference](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/API.md) documentation.

> Assigning `userID` as a static value is not recommended since this will cause all users to share state. Please see the [`API userID entry`](https://github.com/microsoft/BotFramework-WebChat/blob/main/docs/API.md#userID) for more information.

More information on localization can be found in the [Localization](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/LOCALIZATION.md) documentation.

![Screenshot of Web Chat](https://raw.githubusercontent.com/microsoft/BotFramework-WebChat/main/media/weatherquery.png.jpg)

See the working sample of the [full Web Chat bundle](https://github.com/microsoft/BotFramework-WebChat/tree/main/samples/01.getting-started/a.full-bundle).

## Integrate with React

For full customizability, you can use React to recompose components of Web Chat.

To install the production build from NPM, run `npm install botframework-webchat`.

<!-- prettier-ignore-start -->
```js
import React, { useMemo } from 'react';
import ReactWebChat, { createDirectLine } from 'botframework-webchat';

export default () => {
  const directLine = useMemo(() => createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }), []);

  return <ReactWebChat directLine={directLine} userID=""YOUR_USER_ID"" />;
};
```
<!-- prettier-ignore-end -->

> You can also run `npm install botframework-webchat@main` to install a development build that is synced with Web Chat's GitHub `main` branch.

See the working sample of [Web Chat rendered via React](https://github.com/microsoft/BotFramework-WebChat/tree/main/samples/01.getting-started/e.host-with-react/).

### Experimental support for Redux DevTools

Web Chat internally use Redux for state management. [Redux DevTools](https://github.com/reduxjs/redux-devtools) is enabled in the NPM build as an opt-in feature.

This is for glancing into how Web Chat works. This is not an API explorer and is not an endorsement of using the Redux store to programmatically access the UI. The [hooks API](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/HOOKS.md) should be used instead.

To use Redux DevTools, use the `createStoreWithDevTools` function for creating a Redux DevTools-enabled store.

<!-- prettier-ignore-start -->
```diff
  import React, { useMemo } from 'react';
- import ReactWebChat, { createDirectLine, createStore } from 'botframework-webchat';
+ import ReactWebChat, { createDirectLine, createStoreWithDevTools } from 'botframework-webchat';

  export default () => {
    const directLine = useMemo(() => createDirectLine({ token: 'YOUR_DIRECT_LINE_TOKEN' }), []);
-   const store = useMemo(() => createStore(), []);
+   const store = useMemo(() => createStoreWithDevTools(), []);

    return <ReactWebChat directLine={directLine} store={store} userID=""YOUR_USER_ID"" />;
  };
```
<!-- prettier-ignore-end -->

There are some limitations when using the Redux DevTools:

-  The Redux store uses side-effects via [`redux-saga`](https://github.com/redux-saga/redux-saga). Time-traveling may break the UI.
-  Many UI states are stored in React context and state. They are not exposed in the Redux store.
-  Some time-sensitive UIs are based on real-time clock and not affected by time-traveling.
-  Dispatching actions are not officially supported. Please use [hooks API](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/HOOKS.md) instead.
-  Actions and reducers may move in and out of Redux store across versions. [Hooks API](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/HOOKS.md) is the official API for accessing the UI.

# Customizing the Web Chat UI

Web Chat is designed to be customizable without forking the source code. The table below outlines what kind of customizations you can achieve when you are importing Web Chat in different ways. This list is not exhaustive.

|                               | CDN bundle |  React   |
| ----------------------------- | :--------: | :------: |
| Change colors                 |  &#10004;  | &#10004; |
| Change sizes                  |  &#10004;  | &#10004; |
| Update/replace CSS styles     |  &#10004;  | &#10004; |
| Listen to events              |  &#10004;  | &#10004; |
| Interact with hosting webpage |  &#10004;  | &#10004; |
| Custom render activities      |            | &#10004; |
| Custom render attachments     |            | &#10004; |
| Add new UI components         |            | &#10004; |
| Recompose the whole UI        |            | &#10004; |

See more about [customizing Web Chat](https://github.com/microsoft/BotFramework-WebChat/blob/main/samples/README.md) to learn more on customization.

## Supported Activity Types on the Web Chat Client

Bot Framework has many activity types, but not all are supported in Web Chat. [View activity types docs](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/ACTIVITYTYPES.md) to learn more.

# Samples list

[View the complete list of Web Chat samples](https://github.com/microsoft/BotFramework-WebChat/tree/main/samples) for more ideas on customizing Web Chat.

# Further reading

## API Reference

View the [API documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/API.md) for implementing Web Chat.

## Browser compatibility

Web Chat supports the latest 2 versions of modern browsers like Chrome, Microsoft Edge, and FireFox.
If you need Web Chat in Internet Explorer 11, please see the [ES5 bundle demo](https://microsoft.github.io/BotFramework-WebChat/01.getting-started/c.es5-bundle).

Please note, however:

-  Web Chat does not support Internet Explorer older than version 11
-  Customization as shown in non-ES5 samples are not supported for Internet Explorer. Because IE11 is a non-modern browser, it does not support ES6, and many samples that use arrow functions and modern promises would need to be manually converted to ES5. If you are in need of heavy customization for your app, we strongly recommend developing your app for a modern browser like Google Chrome or Microsoft Edge.
-  Web Chat has no plan to support samples for IE11 (ES5).
   -  For customers who wish to manually rewrite our other samples to work in IE11, we recommend looking into converting code from ES6+ to ES5 using polyfills and transpilers like [`babel`](https://babeljs.io/docs/en/next/babel-standalone.html).

## Accessibility

View the [accessibility documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/ACCESSIBILITY.md).

## Localization

View the [localization documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/LOCALIZATION.md) for implementing in Web Chat.

## Notifications

View the [notification documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/NOTIFICATION.md) for implementing in Web Chat.

## Telemetry

View the [telemetry documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/TELEMETRY.md) for implementing in Web Chat.

## Technical Support Guide

View the [Technical Support Guide](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/TECHNICAL_SUPPORT_GUIDE.md) to get guidance and help on troubleshooting in the Web Chat repo for more information before filing a new issue.

## Speech

Web Chat supports a wide-range of speech engines for a natural chat experience with a bot. This section outlines the different engines that are supported:

-  [Direct Line Speech](#integrate-with-direct-line-speech)
-  [Cognitive Services Speech Services](#integrate-with-cognitive-services-speech-services)
-  [Browser-provided engine or other engines](#browser-provided-engine-or-other-engines)

### Integrate with Direct Line Speech

Direct Line Speech is the preferred way to add speech functionality in Web Chat. Please refer to the [Direct Line Speech](https://github.com/microsoft/BotFramework-WebChat/blob/main/docs/DIRECT_LINE_SPEECH.md) documentation for details.

### Integrate with Cognitive Services Speech Services

You can use Cognitive Services Speech Services to add speech functionality to Web Chat. Please refer to the [Cognitive Services Speech Services](https://github.com/microsoft/BotFramework-WebChat/blob/main/docs/SPEECH.md) documentation for details.

### Browser-provided engine or other engines

You can also use any speech engines which support [W3C Web Speech API standard](https://wicg.github.io/speech-api/). Some browsers support the [Speech Recognition API](https://caniuse.com/#feat=mdn-api_speechrecognition) and the [Speech Synthesis API](https://caniuse.com/#feat=mdn-api_speechsynthesis). You can mix-and-match different engines - including Cognitive Services Speech Services - to provide best user experience.

<hr />

# How to test with Web Chat's latest bits

Web Chat latest bits are available on the [Web Chat daily releases page](https://github.com/microsoft/BotFramework-WebChat/releases/daily).

Dailies will be released after 3:00AM Pacific Standard Time when changes have been committed to the main branch.

# Contributing

See our [Contributing page](https://github.com/microsoft/BotFramework-WebChat/tree/main/.github/CONTRIBUTING.md) for details on how to build the project and our repository guidelines for Pull Requests.

See our [CODE OF CONDUCT page](https://github.com/microsoft/BotFramework-WebChat/blob/main/.github/CODE_OF_CONDUCT.md) for details about the Microsoft Code of Conduct.

# Reporting Security Issues

[View the security documentation](https://github.com/microsoft/BotFramework-WebChat/tree/main/docs/SECURITY.md) to learn more about reporting security issues.
"
33,microsoft/cookie.gulp,JavaScript,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
34,microsoft/powerbi-visuals-linedotchart,TypeScript,"# powerbi-visuals-linedotchart
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-linedotchart.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-linedotchart) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-linedotchart/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-linedotchart?branch=master) [![Build Status](https://dev.azure.com/customvisuals/public/_apis/build/status/Microsoft.powerbi-visuals-linedotchart)](https://dev.azure.com/customvisuals/public/_build/latest?definitionId=1)

> The LineDot chart is an animated line chart with fun animated dots. Use the LineDot chart to engage your audience especially in a presentation context. The bubbles size can be dynamic based on data you provide. A counter is provided that you can use to show a running value as the chart animates. Format options are provided for Lines, Dots, and Animation.

![linedotchart screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680588/Asset_79376a97-0a89-48f3-9b3d-5396b4e5808b/LineDotChartscreenshot2.png)
# Overview
The LineDot chart is an animated line chart with fun animated dots.

Use the LineDot chart to engage your audience when presenting data. The size of the bubbles can be customized based on data.

Use the counter to show a running total as the chart animates. Format options are provided for Lines, Dots, and Animation.

See also [LineDot Chart at AppSource](https://appsource.microsoft.com/en-us/product/power-bi-visuals/WA104380766)
"
35,microsoft/CameraTraps,Jupyter Notebook,"# Overview

This repo contains the tools for training, running, and evaluating detectors and classifiers for images collected from motion-triggered camera traps.  The core functionality provided is:

- Data parsing from frequently-used camera trap metadata formats into a common format
- Training and evaluation of detectors, particularly our ""MegaDetector"", which does a pretty good job finding terrestrial animals in a variety of ecosystems
- Training and evaluation of species-level classifiers for specific data sets
- A Web-based demo that runs our models via a REST API that hosts them on a Web endpoint
- Miscellaneous useful tools for manipulating camera trap data
- Research experiments we're doing around camera trap data (i.e., some directories are highly experimental and you should take them with a grain of salt)

Classifiers and detectors are trained using TensorFlow.

This repo is maintained by folks in the [Microsoft AI for Earth](http://aka.ms/aiforearth) program who like looking at pictures of animals.  I mean, we want to use machine learning to support conservation too, but we also really like looking at pictures of animals.

# Who is using the AI for Earth camera trap tools?

We work with ecologists all over the world to help them spend less time annotating images and more time thinking about conservation.  You can read a little more about how this works on our [AI for Earth camera trap collaborations page](collaborations.md).

You can also read about what we do to support camera trap researchers in our recent [blog post](https://medium.com/microsoftazure/accelerating-biodiversity-surveys-with-azure-machine-learning-9be53f41e674).

Here are a few of the organizations that have used AI for Earth camera trap tools... we're only listing organizations who (a) we know about and (b) have generously allowed us to refer to them here, so if you're using MegaDetector or other tools from this repo and would like to be added to this list, <a href=""mailto:cameratraps@microsoft.com"">email us</a>!

* Idaho Department of Fish and Game
* San Diego Zoo Global
* University of Washington Quantitative Ecology Lab
* University of Idaho
* Borderlands Research Institute at Sul Ross State University
* Borneo Nature Foundation
* Parks Canada
* Australian Wildlife Conservancy
* Lab of Dr. Bilal Habib at the Wildlife Institute of India
* Royal Society for the Protection of Birds (RSPB)
* Wildlife Protection Solutions
* Island Conservation
* Synthetaic
* School of Natural Sciences, University of Tasmania
* Arizona Department of Environmental Quality
* Wildlife Research, Oregon Department of Fish and Wildlife
* National Wildlife Refuge System, Southwest Region, US Fish and Wildlife
* Mammal Spatial Ecology and Conservation Lab at Washington State University
* Point No Point Treaty Council
* SPEA (Portuguese Society for the Study of Birds)
* Ghost Cat Analytics
* EcoLogic Consultants Ltd.
* Smithsonian Northern Great Plains Program
* Federal University of Amapá, Ecology and Conservation of Amazonian Vertebrates Research Group
* Hamaarag, The Steinhardt Museum of Natural History, Tel Aviv University
* Czech University of Life Sciences Prague
* Ramat Hanadiv Nature Park, Israel
* TU Berlin, Department of Ecology
* DC Cat Count, led by the Humane Rescue Alliance
* Center for Biodiversity and Conservation at the American Museum of Natural History
* Camelot
* Graeme Shannon's Research Group at Bangor University 
* Snapshot USA
* University of British Columbia Wildlife Coexistence Lab


# Data

This repo does not directly host camera trap data, but we work with our collaborators to make data and annotations available whenever possible on [lila.science](http://lila.science).


# Models

This repo does not extensively host species classification models, though we will release models when they are at a level of generality that they might be useful to other people.  But...


## MegaDetector

Speaking of models that might be useful to other people, we have trained a one-class animal detector trained on several hundred thousand bounding boxes from a variety of ecosystems.  Lots more information &ndash; including download links &ndash; on the [MegaDetector page](megadetector.md).

Here's a ""teaser"" image of what detector output looks like:

![Red bounding box on fox](images/detector_example.jpg)

Image credit University of Washington.


# Contact

For questions about this repo, contact [cameratraps@microsoft.com](mailto:cameratraps@microsoft.com).


# Contents

This repo is organized into the following folders...


## api

Code for hosting our models as an API, either for synchronous operation (e.g. for real-time inference or for our Web-based demo) or as a batch process (for large biodiversity surveys).


## classification

Code for training species classifiers on new data sets, generally trained on crops generated via an existing detector.  We'll release some classifiers soon, but more importantly, here's a [tutorial](https://github.com/microsoft/CameraTraps/blob/master/archive/classification_marcel/TUTORIAL.md) on training your own classifier using our detector and our training pipeline.

Oh, and here's another ""teaser image"" of what you get at the end of training a classifier:

<img src=""images/warthog_classifications.jpg"" width=""700"">

## data_management

Code for:

- Converting frequently-used metadata formats to [COCO Camera Traps](https://github.com/Microsoft/CameraTraps/blob/master/data_management/README.md#coco-cameratraps-format) format
- Creating, visualizing, and  editing COCO Camera Traps .json databases
- Generating tfrecords

## demo

Source for the Web-based demo of our MegaDetector model (we'll release the demo soon!).


## detection

Code for training and evaluating detectors.


## research

Ongoing research projects that use this repository in one way or another; as of the time I'm editing this README, there are projects in this folder around active learning and the use of simulated environments for training data augmentation.


## sandbox

Random things that don't fit in any other directory.  Currently contains a single file, a not-super-useful but super-duper-satisfying and mostly-successful attempt to use OCR to pull metadata out of image pixels in a fairly generic way, to handle those pesky cases when image metadata is lost.


# Installation

We use [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) to manage our Python package dependencies. Conda is a package and environment management system. You can install a lightweight distribution of conda (Miniconda) for your OS via installers at [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html).

## Initial setup

### Utility and visualization scripts

The required Python packages for running utility and visualization scripts in this repo are listed in [environment.yml](environment.yml).  To set up your environment for these scripts, in your shell, navigate to the root directory of this repo and issue the following command to create a virtual environment via conda called `cameratraps` (specified in the environment file) and install the required packages:

```
conda env create --file environment.yml
```

For unix users, you need to have gcc installed in order to compile the pip packages. If you do not already have gcc installed, run the following command before creating the conda environment:

```bash
sudo apt update
sudo apt install build-essential
```

### Machine learning scripts

Scripts that execute machine learning code &ndash; specifically, scripts in the folders `api`, `detection`, and `classification` &ndash; require additional depdendencies.  In particular, the `detection/run_tf_detector*.py` scripts should use [environment-detector.yml](environment-detector.yml) to set up the environment, as follows:

```
conda env create --file environment-detector.yml
```

This environment file allows any TensorFlow version from 1.9 to 1.15 to be installed, but you may need to adjust that version for your environment.  Specifically, if you are running on an Azure Data Science Virtual Machine (which has CUDA 10.1 as of the time I'm writing this), you may receive a CUDA error, in which case you should change the line:

`- tensorflow-gpu>=1.9.0, <1.15.0`

...to:

`- tensorflow-gpu=1.13.1`

...before creating your environment.

### Troubleshooting

If you run into an error while creating either of the above environments, try updating conda to version 4.5.11 or above. Check the version of conda using `conda --version`.

## Usage

To enter the conda virtual environment at your current shell, run:

`conda activate cameratraps`

...or, if you used the environment-detector.yml file above:

`conda activate cameratraps-detector`

You should see `(cameratraps)` prepended to the command line prompt. Invoking `python` or `jupyter notebook` will now be using the interpreter and packages available in this virtual env.

To exit the virtual env, issue `conda deactivate`.

## Add additional packages

If you need to use additional packages, add them to the environment file and run

```bash
conda env update --name cameratraps --file environment.yml --prune
```
or
```bash
conda env update --name cameratraps-detector --file environment-detector.yml --prune
```

## Other notes

In some scripts, we also assume that you have the [AI for Earth utilities repo](https://github.com/Microsoft/ai4eutils) (`ai4eutils`) cloned and its path appended to `PYTHONPATH`. You can append a path to `PYTHONPATH` for the current shell session by executing the following on Windows:

```set PYTHONPATH=""%PYTHONPATH%;c:\wherever_you_put_the_ai4eutils_repo""```

You can do this with the following on Linux:

```export PYTHONPATH=""$PYTHONPATH:/absolute/path/to/repo/ai4eutils""```

Adding this line to your `~/.bashrc` (on Linux) modifies `PYTHONPATH` permanently.

We also do our best to follow [Google's Python Style Guide](http://google.github.io/styleguide/pyguide.html), and we have adopted their `pylintrc` file, with the following differences:
- indent code blocks with 4 spaces (instead of 2)

To lint a file, run `pylint` with the CameraTraps repo folder as the current working directory. This allows pylint to recognize the `pylintrc` file. For example,

```bash
pylint classification/train_classifier.py
```


# Gratuitous pretty camera trap picture

![Bird flying above water](images/nacti.jpg)

Image credit USDA, from the [NACTI](http://lila.science/datasets/nacti) data set.


# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [cla.microsoft.com](https://cla.microsoft.com).

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

This repository is licensed with the [MIT license](https://github.com/Microsoft/dotnet/blob/master/LICENSE).
"
36,microsoft/presidio,Python,"# Presidio - Data Protection and Anonymization API

**Context aware, pluggable and customizable PII anonymization service for text and images.**

---

[![Build Status](https://dev.azure.com/csedevil/Presidio/_apis/build/status/Presidio-CI%20V2?branchName=V2)](https://dev.azure.com/csedevil/Presidio/_build/latest?definitionId=207&branchName=V2)
[![MIT license](https://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT)
![Release](https://img.shields.io/github/release/Microsoft/presidio.svg)
[![Pypi Downloads](https://img.shields.io/pypi/dm/presidio-analyzer.svg)](https://img.shields.io/pypi/dm/presidio-analyzer.svg)
[![PyPI package version](https://badge.fury.io/py/presidio-analyzer.svg)](https://badge.fury.io/py/presidio-analyzer.svg)


Analyzer [![PyPI pyversions](https://img.shields.io/pypi/pyversions/presidio-analyzer.svg)](https://pypi.python.org/pypi/presidio-analyzer/)

Anonymizer [![PyPI pyversions](https://img.shields.io/pypi/pyversions/presidio-anonymizer.svg)](https://pypi.python.org/pypi/presidio-anonymizer/)

Image-Redactor [![PyPI pyversions](https://img.shields.io/pypi/pyversions/presidio-image-redactor.svg)](https://pypi.python.org/pypi/presidio-image-redactor/)

## What is Presidio

Presidio _(Origin from Latin praesidium ‘protection, garrison’)_ helps to ensure sensitive data is properly managed and governed. It provides fast **_identification_** and **_anonymization_** modules for private entities in text such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.

![Presidio demo gif](docs/assets/changing_text.gif)

### Goals

- Allow organizations to preserve privacy in a simpler way by democratizing de-identification technologies and introducing transparency in decisions.
- Embrace extensibility and customizability to a specific business need.
- Facilitate both fully automated and semi-automated PII de-identification flows on multiple platforms.

### Main features

1. **Predefined** or **custom PII recognizers** leveraging *Named Entity Recognition*, *regular expressions*, *rule based logic* and *checksum* with relevant context in multiple languages.
2. Options for connecting to external PII detection models.
3. Multiple usage options, **from Python or PySpark workloads through Docker to Kubernetes**.
4. **Customizability** in PII identification and anonymization.
5. Module for **redacting PII text in images**.

:warning: Presidio can help identify sensitive/PII data in un/structured text. However, because Presidio is using trained ML models, there is no guarantee that Presidio will find all sensitive information. Consequently, additional systems and protections should be employed.

### :notebook_with_decorative_cover: [Full documentation](https://microsoft.github.io/presidio)
### :thought_balloon: [Try Presidio with your own data](https://aka.ms/presidio-demo) 


## Installing Presidio

1. [Using pip](https://microsoft.github.io/presidio/installation/#using-pip)
2. [Using Docker](https://microsoft.github.io/presidio/installation/#using-docker)
3. [From source](https://microsoft.github.io/presidio/installation/#install-from-source)
4. [Migrating from V1 to V2](./docs/presidio_V2.md)

## Running Presidio
1. [Getting started](https://microsoft.github.io/presidio/getting_started)
2. [Setting up a development environment](https://microsoft.github.io/presidio/development)
3. [PII anonymization in text](https://microsoft.github.io/presidio/text_anonymization)
4. [PII anonymization in images](https://microsoft.github.io/presidio/image-redactor)
5. [Usage samples and example deployments](https://microsoft.github.io/presidio/samples)

---

## Support

- Before you submit an issue, please go over the [documentation](https://microsoft.github.io/presidio/). 
- For general discussions, please use the [Github repo's discussion board](https://github.com/microsoft/presidio/discussions).
- If you have a usage question, found a bug or have a suggestion for improvement, please file a [Github issue](https://github.com/microsoft/presidio/issues).
- For other matters, please email [presidio@microsoft.com](mailto:presidio@microsoft.com).

## Contributing

For details on contributing to this repository, see the [contributing guide](CONTRIBUTING.md).

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [https://cla.microsoft.com](https://cla.microsoft.com).

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
37,microsoft/playwright,TypeScript,"# 🎭 Playwright

[![npm version](https://img.shields.io/npm/v/playwright.svg?style=flat)](https://www.npmjs.com/package/playwright) [![Join Slack](https://img.shields.io/badge/join-slack-infomational)](https://aka.ms/playwright-slack) <!-- GEN:chromium-version-badge -->[![Chromium version](https://img.shields.io/badge/chromium-92.0.4498.0-blue.svg?logo=google-chrome)](https://www.chromium.org/Home)<!-- GEN:stop --> <!-- GEN:firefox-version-badge -->[![Firefox version](https://img.shields.io/badge/firefox-89.0b9-blue.svg?logo=mozilla-firefox)](https://www.mozilla.org/en-US/firefox/new/)<!-- GEN:stop --> <!-- GEN:webkit-version-badge -->[![WebKit version](https://img.shields.io/badge/webkit-14.2-blue.svg?logo=safari)](https://webkit.org/)<!-- GEN:stop -->

## [Documentation](https://playwright.dev) | [API reference](https://playwright.dev/docs/api/class-playwright/)

Playwright is a Node.js library to automate [Chromium](https://www.chromium.org/Home), [Firefox](https://www.mozilla.org/en-US/firefox/new/) and [WebKit](https://webkit.org/) with a single API. Playwright is built to enable cross-browser web automation that is **ever-green**, **capable**, **reliable** and **fast**.

|          | Linux | macOS | Windows |
|   :---   | :---: | :---: | :---:   |
| Chromium <!-- GEN:chromium-version -->92.0.4498.0<!-- GEN:stop --> | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| WebKit <!-- GEN:webkit-version -->14.2<!-- GEN:stop --> | :white_check_mark: | :white_check_mark: | :white_check_mark: |
| Firefox <!-- GEN:firefox-version -->89.0b9<!-- GEN:stop --> | :white_check_mark: | :white_check_mark: | :white_check_mark: |

Headless execution is supported for all the browsers on all platforms. Check out [system requirements](https://playwright.dev/docs/intro/#system-requirements) for details.

## Usage

```
npm i -D playwright
```

This installs Playwright and browser binaries for Chromium, Firefox and WebKit. Once installed, you can `require` Playwright in a Node.js script and automate web browser interactions.

* [Getting started](https://playwright.dev/docs/intro)
* [Installation configuration](https://playwright.dev/docs/installation)
* [API reference](https://playwright.dev/docs/api/class-playwright)

## Capabilities

Playwright is built to automate the broad and growing set of web browser capabilities used by Single Page Apps and Progressive Web Apps.

* Scenarios that span multiple page, domains and iframes
* Auto-wait for elements to be ready before executing actions (like click, fill)
* Intercept network activity for stubbing and mocking network requests
* Emulate mobile devices, geolocation, permissions
* Support for web components via shadow-piercing selectors
* Native input events for mouse and keyboard
* Upload and download files

## Examples

#### Page screenshot

This code snippet navigates to whatsmyuseragent.org in Chromium, Firefox and WebKit, and saves 3 screenshots.

```js
const playwright = require('playwright');

(async () => {
  for (const browserType of ['chromium', 'firefox', 'webkit']) {
    const browser = await playwright[browserType].launch();
    const context = await browser.newContext();
    const page = await context.newPage();
    await page.goto('http://whatsmyuseragent.org/');
    await page.screenshot({ path: `example-${browserType}.png` });
    await browser.close();
  }
})();
```

#### Mobile and geolocation

This snippet emulates Mobile Safari on a device at a given geolocation, navigates to maps.google.com, performs action and takes a screenshot.

```js
const { webkit, devices } = require('playwright');
const iPhone11 = devices['iPhone 11 Pro'];

(async () => {
  const browser = await webkit.launch();
  const context = await browser.newContext({
    ...iPhone11,
    locale: 'en-US',
    geolocation: { longitude: 12.492507, latitude: 41.889938 },
    permissions: ['geolocation']
  });
  const page = await context.newPage();
  await page.goto('https://maps.google.com');
  await page.click('text=""Your location""');
  await page.waitForRequest(/.*preview\/pwa/);
  await page.screenshot({ path: 'colosseum-iphone.png' });
  await browser.close();
})();
```

#### Evaluate in browser context

This code snippet navigates to example.com in Firefox, and executes a script in the page context.

```js
const { firefox } = require('playwright');

(async () => {
  const browser = await firefox.launch();
  const context = await browser.newContext();
  const page = await context.newPage();
  await page.goto('https://www.example.com/');
  const dimensions = await page.evaluate(() => {
    return {
      width: document.documentElement.clientWidth,
      height: document.documentElement.clientHeight,
      deviceScaleFactor: window.devicePixelRatio
    }
  });
  console.log(dimensions);

  await browser.close();
})();
```

#### Intercept network requests

This code snippet sets up request routing for a WebKit page to log all network requests.

```js
const { webkit } = require('playwright');

(async () => {
  const browser = await webkit.launch();
  const context = await browser.newContext();
  const page = await context.newPage();

  // Log and continue all network requests
  page.route('**', route => {
    console.log(route.request().url());
    route.continue();
  });

  await page.goto('http://todomvc.com');
  await browser.close();
})();
```

## Resources

* [Documentation](https://playwright.dev/docs/intro/)
* [API reference](https://playwright.dev/docs/api/class-playwright/)
* [Community showcase](https://playwright.dev/docs/showcase/)
* [Contribution guide](CONTRIBUTING.md)
* [Changelog](https://github.com/microsoft/playwright/releases)
"
38,microsoft/language-server-protocol,,"# Language Server Protocol

The Language Server Protocol is now available through its own [website](https://microsoft.github.io/language-server-protocol/). The website contains information about :

* How the protocol [works](https://microsoft.github.io/language-server-protocol/overview)
* A better readable [specification](https://microsoft.github.io/language-server-protocol/specifications/specification-current/)
* Documents about protocol [implementations](https://microsoft.github.io/language-server-protocol/implementors/servers/).

## Contributing

If you are interested in fixing issues like typos you can either file an issue or provide a pull request containing the changes to the relevant [specification file](https://github.com/microsoft/language-server-protocol/tree/gh-pages/_specifications).

When proposing an extension to the specification, then please refer to the [How to Contribute to the Language Server Protocol](contributing.md) document.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## The Language Server Protocol

See the [Web site](https://microsoft.github.io/language-server-protocol/specifications/specification-current/)

## License

[Creative Commons Attribution / MIT](License.txt)
"
39,microsoft/electionguard-ballot-box,TypeScript,"
![Microsoft Defending Democracy Program: ElectionGuard](images/electionguard-banner.svg)

# ElectionGuard Ballot Box

![build](https://github.com/microsoft/electionguard-ballot-box/workflows/Package/badge.svg)
[![license](https://img.shields.io/github/license/microsoft/electionguard-ballot-box)](LICENSE)

---------------------

### Note: This repository has been deprecated & transitioned

As of 06/15/2020, this repository is no longer being actively maintained. ElectionGuard development has transitioned to the [ElectionGuard-Python](https://github.com/microsoft/electionguard-python) Repo.

This repository will remain open sourced and available, but will no longer be actively maintained or updated. Development is underway for a replacement and will be posted on our [Main repository Page](https://github.com/microsoft/ElectionGuard). This URL will become archived and read-only in Summer of 2020.

--------------------------

The ElectionGuard Ballot Box is a fully functional
implementation built in ReactJS. It is a counter for the ballot box to count and store cast or spoil ballots.
This reference implementation is meant to demonstrate a possible use case of the ElectionGuard SDK.

## Contributing

Help defend democracy and [contribute to the project](CONTRIBUTING).

<!-- 
Guidelines on README format: https://review.docs.microsoft.com/help/onboard/admin/samples/concepts/readme-template?branch=master

Guidance on onboarding samples to docs.microsoft.com/samples: https://review.docs.microsoft.com/help/onboard/admin/samples/process/onboarding?branch=master

Taxonomies for products and languages: https://review.docs.microsoft.com/new-hope/information-architecture/metadata/taxonomies?branch=master
-->
"
40,microsoft/TypeScript,TypeScript,"
# TypeScript

[![GitHub Actions CI](https://github.com/microsoft/TypeScript/workflows/CI/badge.svg)](https://github.com/microsoft/TypeScript/actions?query=workflow%3ACI)
[![Devops Build Status](https://dev.azure.com/typescript/TypeScript/_apis/build/status/Typescript/node10)](https://dev.azure.com/typescript/TypeScript/_build?definitionId=7)
[![npm version](https://badge.fury.io/js/typescript.svg)](https://www.npmjs.com/package/typescript)
[![Downloads](https://img.shields.io/npm/dm/typescript.svg)](https://www.npmjs.com/package/typescript)

[TypeScript](https://www.typescriptlang.org/) is a language for application-scale JavaScript. TypeScript adds optional types to JavaScript that support tools for large-scale JavaScript applications for any browser, for any host, on any OS. TypeScript compiles to readable, standards-based JavaScript. Try it out at the [playground](https://www.typescriptlang.org/play/), and stay up to date via [our blog](https://blogs.msdn.microsoft.com/typescript) and [Twitter account](https://twitter.com/typescript).

Find others who are using TypeScript at [our community page](https://www.typescriptlang.org/community/).

## Installing

For the latest stable version:

```bash
npm install -g typescript
```

For our nightly builds:

```bash
npm install -g typescript@next
```

## Contribute

There are many ways to [contribute](https://github.com/microsoft/TypeScript/blob/master/CONTRIBUTING.md) to TypeScript.
* [Submit bugs](https://github.com/microsoft/TypeScript/issues) and help us verify fixes as they are checked in.
* Review the [source code changes](https://github.com/microsoft/TypeScript/pulls).
* Engage with other TypeScript users and developers on [StackOverflow](https://stackoverflow.com/questions/tagged/typescript).
* Help each other in the [TypeScript Community Discord](https://discord.gg/typescript).
* Join the [#typescript](https://twitter.com/search?q=%23TypeScript) discussion on Twitter.
* [Contribute bug fixes](https://github.com/microsoft/TypeScript/blob/master/CONTRIBUTING.md).
* Read the archived language specification ([docx](https://github.com/microsoft/TypeScript/blob/master/doc/TypeScript%20Language%20Specification%20-%20ARCHIVED.docx?raw=true),
 [pdf](https://github.com/microsoft/TypeScript/blob/master/doc/TypeScript%20Language%20Specification%20-%20ARCHIVED.pdf?raw=true), [md](https://github.com/microsoft/TypeScript/blob/master/doc/spec-ARCHIVED.md)).

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see
the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com)
with any additional questions or comments.

## Documentation

*  [TypeScript in 5 minutes](https://www.typescriptlang.org/docs/handbook/typescript-in-5-minutes.html)
*  [Programming handbook](https://www.typescriptlang.org/docs/handbook/intro.html)
*  [Homepage](https://www.typescriptlang.org/)

## Building

In order to build the TypeScript compiler, ensure that you have [Git](https://git-scm.com/downloads) and [Node.js](https://nodejs.org/) installed.

Clone a copy of the repo:

```bash
git clone https://github.com/microsoft/TypeScript.git
```

Change to the TypeScript directory:

```bash
cd TypeScript
```

Install [Gulp](https://gulpjs.com/) tools and dev dependencies:

```bash
npm install -g gulp
npm ci
```

Use one of the following to build and test:

```
gulp local             # Build the compiler into built/local.
gulp clean             # Delete the built compiler.
gulp LKG               # Replace the last known good with the built one.
                       # Bootstrapping step to be executed when the built compiler reaches a stable state.
gulp tests             # Build the test infrastructure using the built compiler.
gulp runtests          # Run tests using the built compiler and test infrastructure.
                       # You can override the specific suite runner used or specify a test for this command.
                       # Use --tests=<testPath> for a specific test and/or --runner=<runnerName> for a specific suite.
                       # Valid runners include conformance, compiler, fourslash, project, user, and docker
                       # The user and docker runners are extended test suite runners - the user runner
                       # works on disk in the tests/cases/user directory, while the docker runner works in containers.
                       # You'll need to have the docker executable in your system path for the docker runner to work.
gulp runtests-parallel # Like runtests, but split across multiple threads. Uses a number of threads equal to the system
                       # core count by default. Use --workers=<number> to adjust this.
gulp baseline-accept   # This replaces the baseline test results with the results obtained from gulp runtests.
gulp lint              # Runs eslint on the TypeScript source.
gulp help              # List the above commands.
```


## Usage

```bash
node built/local/tsc.js hello.ts
```


## Roadmap

For details on our planned features and future direction please refer to our [roadmap](https://github.com/microsoft/TypeScript/wiki/Roadmap).
"
41,microsoft/BotBuilder-V3,C#,"# V3 Deprecation Notification

Microsoft Bot Framework SDK V4 was released in September 2018, and since then we have shipped a few dot-release improvements. As announced previously, the V3  SDK is being retired with final long-term support ending on December 31st, 2019.
Accordingly, there will be no more development in this repo. **Existing V3 bot workloads will continue to run without interruption. We have no plans to disrupt any running workloads**.

We highly recommend that you start migrating your V3 bots to V4. In order to support this migration we have produced migration documentation and will provide extended support for migration initiatives (via standard channels such as Stack Overflow and Microsoft Customer Support).

For more information please refer to the following references:
* Migration Documentation: https://aka.ms/bf-migration-overview
* End of lifetime support announcement: https://aka.ms/bfmigfaq
* Primary V4 Repositories to develop Bot Framework bots
  * [Botbuilder for dotnet](https://github.com/microsoft/botbuilder-dotnet)
  * [Botbuilder for JS](https://github.com/microsoft/botbuilder-js) 
* QnA Maker Libraries were replaced with the following V4 libraries:
  * [Libraries for dotnet](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.AI.QnA)
  * [Libraries for JS](https://github.com/Microsoft/botbuilder-js/blob/master/libraries/botbuilder-ai/src/qnaMaker.ts)
* Azure Libraries were replaced with the following V4 libraries:
  * [Botbuilder for JS Azure](https://github.com/Microsoft/botbuilder-js/tree/master/libraries/botbuilder-azure)
  * [Botbuilder for dotnet Azure](https://github.com/Microsoft/botbuilder-dotnet/tree/master/libraries/Microsoft.Bot.Builder.Azure)


# Bot Builder SDK 

If you are new to the Bot Builder SDK, we strongly encourage you to build your bot using the [v4 SDK](https://github.com/Microsoft/botbuilder).  

This repo contains version 3.

The Bot Builder SDK enables you to build bots that support different types of interactions with users. You can design conversations in your bot to be freeform. Your bot can also have more guided interactions where it provides the user choices or actions. The conversation can use simple text or more complex rich cards that contain text, images, and action buttons. You can add natural language interactions and questions and answers, which let your users interact with your bots in a natural way.

![Bot Framework](https://botframework.blob.core.windows.net/web/images/bot-framework.png)

The Bot Builder includes a set of [command line tools](https://github.com/microsoft/botbuilder-tools) to streamline end-to-end conversation centric development experience, and an [emulator](https://github.com/microsoft/botframework-emulator) for debugging your bot locally or in the cloud. 

You can create a bot with Bot Builder v3 SDK using your favorite language: 
- [.NET](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-3.0)
- [JavaScript](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-3.0)

## Documentation
Visit azure.com for the primary [Azure Bot Service documentation page](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-3.0) to learn about building bots using Bot Builder. There is additional documentation on the SDK, oriented towards contributors. The v3 SDK currently supports two programing language: 
- [.NET](https://github.com/Microsoft/BotBuilder-v3/tree/master/CSharp)
- [JavaScript](https://github.com/Microsoft/BotBuilder-V3/tree/master/Node)

## Samples
Bot builder SDK v3 includes samples for all supported languages:
- [.NET](https://github.com/Microsoft/BotBuilder-Samples/tree/v3-sdk-samples/CSharp)
- [JavaScript](https://github.com/Microsoft/BotBuilder-Samples/tree/v3-sdk-samples/Node)

## Questions and Help 
If you have questions about Bot Builder SDK v3 or using Azure Bot Service, we encourage you to reach out to the community and Azure Bot Service dev team for help.
- For questions which fit the Stack Overflow format (""how does this work?""), we monitor the both [Azure-bot-service](https://stackoverflow.com/questions/tagged/azure-bot-service) and [bot framework](https://stackoverflow.com/questions/tagged/botframework) tags (search [both](https://stackoverflow.com/questions/tagged/azure-bot-service+or+botframework))
- You can also tweet/follow [@msbotframework](https://twitter.com/msbotframework) 

While we do our best to help out on a timely basis, we don't have any promise around the above resources. If you need an SLA on support from us, it's recommended you invest in an [Azure Support plan](https://azure.microsoft.com/en-us/support/options/).

## Issues and feature requests 
We track functional issues and features asks for and Bot Builder and Azure Bot Service in a variety of locations. If you have found an issue or have a feature request, please submit an issue to the below repositories.

|Item|Description|Link|
|----|-----|-----|
|SDK v3 (.NET and JS)| core bot runtime, abstractions, prompts, dialogs, FormFlow, etc. | [File an issue](https://github.com/Microsoft/BotBuilder-V3/issues) |
|Documentation | Docs for Bot Builder and Azure Bot Service | [File an issue](https://github.com/Microsoft/BotBuilder-V3/issues)|
|CLI tools| MSBot, chatdown, ludown, LUIS, LUISGen, QnA Maker, dispatch  | [File an issue](https://github.com/microsoft/botbuilder-tools/issues)|
|Emulator| view transcripts, connect to services, debug your bot | [File an issue](https://github.com/Microsoft/BotFramework-Emulator/issues)| 

## Helpful links
### GitHub repositories 
- [SDK v3 (.NET and node)](https://github.com/Microsoft/BotBuilder-V3/)
- [SDK v4 - .NET](https://github.com/Microsoft/botbuilder-dotnet)
- [SDK v4 - JavaScript](https://github.com/Microsoft/botbuilder-js)
- [SDK v4 - Python](https://github.com/Microsoft/botbuilder-python)
- [SDK v4 - Java](https://github.com/Microsoft/botbuilder-java)
- [Bot Builder tools](https://github.com/Microsoft/botbuilder-tools)
- [Bot Builder Emulator](https://github.com/Microsoft/BotFramework-Emulator) 

### Documentation 
- [SDK v3](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-3.0)
- [SDK v4](https://docs.microsoft.com/en-us/azure/bot-service/?view=azure-bot-service-4.0)

## Adding intelligence to your bot
Your bot can provide a great conversational experience without using any Azure Cognitive Services. You can increase your customers' delight with adding a more natural interaction using one or multiple Azure Cognitive Services.  The following are common services integrated to bots: 
- [LUIS](https://www.luis.ai)
- [QnA Maker](https://www.qnamaker.ai/)
- [Speech](https://azure.microsoft.com/services/cognitive-services/directory/speech/)
- [Personality Chat](https://github.com/Microsoft/BotBuilder-PersonalityChat) - Handles Small-Talk/Chitchat for any bot, in line with a distinct personality.
- all [Azure Cognitive Services](https://azure.microsoft.com/services/cognitive-services/)

Get started quickly with our samples:

* Bot Builder samples [GitHub repo](https://github.com/Microsoft/BotBuilder-Samples)
* More samples are available within the SDK [C#](https://github.com/Microsoft/BotBuilder/tree/master/CSharp/Samples), [Node.js](https://github.com/Microsoft/BotBuilder/tree/master/Node/examples)

Join the conversation on **[Gitter](https://gitter.im/Microsoft/BotBuilder)**.

See all the support options **[here](https://docs.microsoft.com/en-us/bot-framework/resources-support)**.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
42,microsoft/vscode-java-dependency,TypeScript,"# Project Manager for Java

> Manage Java projects in Visual Studio Code

[![GitHub Actions](https://img.shields.io/github/workflow/status/microsoft/vscode-java-dependency/CI/master?style=flat-square)](https://github.com/microsoft/vscode-java-dependency/actions?query=workflow%3ACI+branch%3Amaster)

## Overview

A lightweight extension to provide additional Java project explorer features. It works with [Language Support for Java by Red Hat](https://marketplace.visualstudio.com/items?itemName=redhat.java) to provide the following features:

* Project View

![project-view](https://raw.githubusercontent.com/Microsoft/vscode-java-dependency/master/images/project-explorer.png)

* Create Java Projects

![create project](https://raw.githubusercontent.com/Microsoft/vscode-java-dependency/master/images/create-project.png)

* Export Jar
> Note: For Spring Boot projects, please use the build tool to build the executable jar, for example: `mvn package`.

![export jar](https://raw.githubusercontent.com/Microsoft/vscode-java-dependency/master/images/export-jar.png)

## Requirements

- JDK (version 11 or later)
- VS Code (version 1.44.0 or later)
- [Language Support for Java by Red Hat](https://marketplace.visualstudio.com/items?itemName=redhat.java) (version 0.32.0 or later)


## Settings

| Setting Name | Description | Default Value |
|---|---|---|
| `java.dependency.showMembers` | Specify whether to show the members in the Java Projects explorer. | `false` |
| `java.dependency.syncWithFolderExplorer` | Specify whether to sync the folder with Java Projects explorer when browsing files.  | `true` |
| `java.dependency.autoRefresh` | Specify whether to automatically sync the change from editor to the Java Projects explorer. | `true` |
| `java.dependency.refreshDelay` | The delay time (ms) the auto refresh is invoked when changes are detected. | `2000ms` |
| `java.dependency.packagePresentation` | Specify how to display the package. Supported values are: `flat`, `hierarchical`.| `flat` |
| `java.project.exportJar.targetPath` | The output path of export jar. When this setting is **empty** or equals `askUser`, a file explorer will pop up to let the user select the output location.| `${workspaceFolder}/${workspaceFolderBasename}.jar` |

## Contribution

### Build
* Prerequirement
    - Node.js
    - Java SDK 11 or above

* Go to root folder:
```
npm install -g gulp
npm install
gulp build_server
```

## Telemetry
VS Code collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://go.microsoft.com/fwlink/?LinkID=528096&clcid=0x409) to learn more. If you don't wish to send usage data to Microsoft, you can set the `telemetry.enableTelemetry` setting to `false`. Learn more in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).


---

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
43,microsoft/SmartKG,C#,"# Contributing  

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.


# Usage

SmartKG是一款轻量级知识图谱可视化+智能对话框架。它能够根据用户输入的实体和关系数据自动生成知识图谱，并提供图谱可视化及基于图谱的智能对话机器人。

## 0. 更加详细的安装、编译和使用方法请见：https://github.com/microsoft/SmartKG/blob/master/SmartKG_Spec.pdf

## 1. 下载 SmartKG

### 1.1 下载安装下列软件： 

	(1) git: https://gitforwindows.org/
	(2) .NET Core 2.1 运行时环境	
	(3) Node.JS: https://nodejs.org/zh-cn/download/ (推荐 14.15.4)
	(4) 如果要改写或重新编译源代码，还需要安装 Visual Studio 2019: https://visualstudio.microsoft.com/zh-hans/downloads/ 

### 1.2 Clone Repositry

	(1) 进入 ${SourceCode_Base_Path}
	(2) 运行：
		git clone https://github.com/microsoft/SmartKG

### 1.3 目录结构

	在目录dockers里，是已经编译好前后端服务的二进制码、配置文件，以及对应的docker image。

	在目录Resources里，有用户上传数据的模板和用来做测试的图谱数据。其中，template子目录中是模板，用户如果要创建自己的知识图谱，就需要按照模板的格式要求，填入相应实体和实体关系。Input子目录内有包括西游记、红楼梦、中学物理课以及COVID19等数据。

	在目录SmartKGLocalBase里，是SmartKG后端服务会调用的一些Python文件和用于存储运行时数据的本地文件的目录。

	在目录SmartKGUI下面，是SmartKG UI的源代码。是基于Node.js, 用JavaScript开发的。

	在目录src下面，是SmartKG后端服务的源代码。这部分源代码是基于Asp.NET 框架，用C# 开发的。

## 2. 运行SmartKG

### 2.1 Windows 上运行 SmartKG

	
	(1) 在Windows环境里启动 SmartKG，首先应该新建一个目录，例如，我们创建一个名为 temp 的目录。然后，从本地 Repo 中dockers目录内的smartkg子目录中，将两个zip文件和local_config文件夹复制到temp文件夹中。还需要从dockers目录的ui 子目录中，将唯一一个zip文件和local_config文件夹复制到temp文件夹中。

	(2) 把temp目录中的三个压缩文件直接就地解压缩（Extraction Here）。

	(3) 并将temp/local_config中的config.js移动到解压缩后的 temp/smartkgui/public中。

	(4) 将local_confing的appsettings.File.json文件复制一份，并改名为appsettings.json，移动到temp/smartkg中。

	(5) 【启动后端】
	    命令行进入 temp/smartkg，运行命令：dotnet SmartKG.KGBot.dll
	    此命令用于启动 SmartKG 后端。启动后，会生成一个Now listening on地址，我们直接访问地址就可以。在浏览器中输入地址 http://localhost:5000/swagger/index.html，可以访问后，说明后台启动成功了。

	    *注意：后端启动成功后不要关闭，在使用SmartKG的整个过程中，前后端都要保持打开状态。*

	(6) 【启动前端】
	    进入temp/smartkgui目录下，输入命令：
		npm i
	    这个命令只需要第一次使用时运行，如果已经运行过，就可以跳过了。

	    此命令运行成功后，再输入命令：
		npm run serve

	    运行成功后，会给一个访问地址，例如：http://localhost:8080/
            用这个地址就可以直接在浏览器中访问 SmartKG 的主界面了，这个地址加上 ""/upload"" 是 SmartKG 的上传页面。


### 2.2 Linux 上运行 SmartKG

	(0) 在Linux环境部署前，需要提前安装好 docker 和 docker-compose。

	(1) 打开 Repo 中的 dockers 目录，将里面的 smartkg_services 目录整体压缩，并拷贝到Linux机器上，一般放在用户目录下。

	(2) 在 Linux 系统进入用户目录，并解压缩 smartkg_services.zip。
	
	(3) 进入 smartkg_services/ 目录，运行下列命令启动 SmartKG docker-compose OneBox:

		1) sudo docker-compose build --build-arg DOCKER_HOST=${docker_host_ip}
		2) sudo docker-compose up

	     访问后端：http://${docker_host_ip}:8082/swagger/index.html 能够获得 API 列表

             访问前端：http://${docker_host_ip}:8083 主页面
                       http://${docker_host_ip}:8083/upload 上传页面

## 3. 生成自己的图谱

### 3.1 填写模板

	(1) 模板位于 ${SourceCode_Base_Path}/SmartKG/Resources/_Template/SmartKG_KGDesc_Template.xlsx
	(2) 模板分为两页：顶点页和边页。前者为图谱中的实体，后者为实体间的关系。根据模板样例填写你自己的顶点和边数据。	
	(3) 做测试，建议使用 ${SourceCode_Base_Path}/SmartKG/Resources/Data/Excel/input/XYJ/SmartKG_Xiyouji_relations_new.xlsx

### 3.2 将填写好的 excel 文件通过 SmartKG 的前端 upload 页面上传

## 4. 编译 SmartKG

### 4.1 用 Visual Studio 2019 编译 SmartKG

	回到Repo目录，进入src文件夹，启动SmartKG.sln。
	进入到VS后，点击Build的Bulid Solution，编译源代码。

### 4.2 Publish SmartKG

	在 Visual Studio 中打开 SmartKG solution 后，右键点击 SmartKG.KGBot project，选择点击 Publish，接着点击 Folder，以及 Next。
	
	配置 Publish 选项：

		配置：Release
		目标框架：netcoreapp2.1.16
		目标运行时：可移植的

	保存设置后直接 Publish！



"
44,microsoft/electionguard-ballot-marking-device,TypeScript,"![Microsoft Defending Democracy Program: ElectionGuard](images/electionguard-banner.svg)

# 🗳 ElectionGuard Ballot Marking Device 
[![license](https://img.shields.io/github/license/microsoft/electionguard-admin-device)](License)

The ElectionGuard Reference Ballot Marking Device (BMD) is a fully functional
implementation of a BMD built in ReactJS. It uses the
[Gamepad API](https://developer.mozilla.org/en-US/docs/Web/API/Gamepad_API) to
support a physically-connected
[Xbox Adaptive Controller](https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller)
for navigating ballots with the A/B buttons, D-pad, or other devices plugged
into the XAC's connection ports.

The BMD was designed under the supervision of the
[Center for Civic Design](https://civicdesign.org) as an update of the
[Anywhere Ballot](https://civicdesign.org/projects/anywhere-ballot/) and built
by the team at [VotingWorks](https://voting.works). It runs locally in a web
browser (Chrome or Edge Beta). A sample set of ballots (in json format) is
available in both the /public/data/ and /src/data directories that showcases a
variety of contests and referenda use cases.

By default, when a ballot is printed, the BMD also generates a static tracking
ID. End-to-end verifiable elections create tracking IDs generated when the
ballot is encrypted. It's used by voters to check that their vote was included
in the final tally when all the artifacts of an end-to-end verifiable election
are published. When you have built working encryption capability (using the
[C Implementation](https://github.com/microsoft/ElectionGuard-SDK-C-Implementation)),
the election.json configuration file can be modified to retrieve the tracking ID
by updating
[this code block](https://github.com/microsoft/ElectionGuard-SDK-Ballot-Marking-Device-Reference-Implementation/blob/edee95d90fc5a4ce17a6cd9d537f9200b189e05d/src/endToEnd.ts#L14).

## Install and Run App Locally

This assumes you have `git` and `yarn` installed.

1. Clone the repo:

   ```
   git clone git@github.com:microsoft/ElectionGuard-SDK-Ballot-Marking-Device-Reference-Implementation.git
   ```

1. Navigate to the top level of the cloned directory

   ```
   cd ElectionGuard-SDK-Ballot-Marking-Device-Reference-Implementation
   ```

1. Install dependencies:

   ```
   yarn install
   ```

1. Run the app in your local browser:

   ```
   yarn start
   ```

## Local Development Scripts

- `yarn install` - Install the dependencies.
- `yarn start` - Run the app locally.
- `yarn test`- Run tests in interactive mode.
- `yarn test:coverage` - Run all tests and update test coverage report.

See `package.json` for all available scripts.

## Technical Implementation

This project was bootstrapped with
[Create React App](https://github.com/facebook/create-react-app) for TypeScript.
It uses [Styled Components](https://www.styled-components.com/docs/) for styles
(and some `css` files too). [ESLint](https://eslint.org/) is configured to lint
Javascript and TypeScript files, and format code using
[Prettier](https://prettier.io/). [stylelint](https://stylelint.io/) is used to
lint modern css. [Jest](https://jestjs.io/),
[dom-testing-library](https://testing-library.com),
[react-testing-library](https://github.com/kentcdodds/react-testing-library),
and [Cypress](https://www.cypress.io/) are used to test components and
end-to-end user flows.

## Contributing

Help defend democracy and [contribute to the project](CONTRIBUTING.md).
"
45,microsoft/SubscribableEvent,TypeScript,"# SubscribableEvent

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)](https://github.com/Microsoft/SubscribableEvent/blob/master/LICENSE) [![npm version](https://img.shields.io/npm/v/subscribableevent.svg?style=flat-square)](https://www.npmjs.com/package/subscribableevent) [![Build Status](https://img.shields.io/travis/Microsoft/SubscribableEvent/master.svg?style=flat-square)](https://travis-ci.org/Microsoft/SubscribableEvent) [![npm downloads](https://img.shields.io/npm/dm/subscribableevent.svg?style=flat-square)](https://www.npmjs.com/package/subscribableevent) ![npm bundle size (minified)](https://img.shields.io/bundlephobia/min/subscribableevent.svg?style=flat-square) ![npm bundle size (minified + gzip)](https://img.shields.io/bundlephobia/minzip/subscribableevent.svg?style=flat-square)

> A simple strongly-typed pub/sub/fire eventing system

## Installation

```shell
npm install --save subscribableevent
```

## Basic Example

```typescript
const event = new SubscribableEvent<(payload: string) => void>();

event.subscribe((payload: string) => {
    console.log(payload);
});

event.fire('Payload Params');
```

## Contributing

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

"
46,microsoft/reactxp,TypeScript,"# ReactXP

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)](https://github.com/Microsoft/reactxp/blob/master/LICENSE) [![npm version](https://img.shields.io/npm/v/reactxp.svg?style=flat-square)](https://www.npmjs.com/package/reactxp) [![Build Status](https://dev.azure.com/ms/reactxp/_apis/build/status/Microsoft.reactxp?)](https://dev.azure.com/ms/reactxp/_build/latest?definitionId=16) [![Build Status](https://img.shields.io/travis/Microsoft/reactxp/master.svg?style=flat-square)](https://travis-ci.org/Microsoft/reactxp) [![npm downloads](https://img.shields.io/npm/dm/reactxp.svg?style=flat-square)](https://www.npmjs.com/package/reactxp) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/Microsoft/reactxp#contributing) [![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg?style=flat-square)](https://gitter.im/msreactxp/Lobby)


ReactXP is a library for cross-platform app development using React and React Native.

## Why ReactXP
With React and React Native, your web app can share most of its logic with your iOS and Android apps, but the view layer needs to be implemented separately for each platform. We have taken this a step further and developed a thin cross-platform layer we call ReactXP. If you write your app to this abstraction, you can share your view definitions, styles and animations across multiple target platforms. Of course, you can still provide platform-specific UI variants, but this can be done selectively where desired.

## Getting Started
The [samples](/samples) directory contains a minimal “Hello World” app that demonstrates some basic ReactXP functionality. You can use this as a starting point. Just follow the build instructions in the README file.

Also included in the samples directory is the [RXPTest app](/samples/RXPTest) which attempts to exercise all of the functionality of ReactXP. It is a good source to consult for sample usage of APIs, components, and props.

You can read more about ReactXP and its APIs from the [ReactXP official Documentation](https://microsoft.github.io/reactxp/docs/getting-started.html).

Use the command-line tool called [create-rx-app](https://github.com/a-tarasyuk/create-rx-app) to create a starter project.

```sh
npm install create-rx-app -g
create-rx-app AppName
```

or

```sh
npx create-rx-app AppName
```

By default the project will be created in TypeScript. However if you prefer JavaScript instead, add `--javascript` when creating the project.

This will create a directory called **AppName** inside the current working directory. Inside **AppName**, this will generate the initial project structure and install all of its dependencies. Once this installation is done, there are some commands you can run in the project directory:

- `npm run start:web` - runs the Web version of the app in the development mode
- `npm run build:web` - builds the Web version of the app for production to the **dist-web** folder
- `npm run start:ios` - runs the iOS version of the app and attempts to open in the iOS Simulator if you're on a Mac and have it installed
- `npm run start:android` - runs the Android version of the app and attempts to open your app on a connected Android device or emulator
- `npm run start:windows` - runs the Windows version of the app
- `npm start:rn-dev-server` - runs react native (RN) development server

### Prerequisites
* [Node.Js](https://nodejs.org/) ([Setup Instructions](https://nodejs.org/en/download/package-manager/))
* [React Native](https://facebook.github.io/react-native/) ([Setup Instructions](https://facebook.github.io/react-native/docs/getting-started))

## ESLint rules

> [TSLint will be deprecated some time in 2019](https://github.com/palantir/tslint)

If you plan to migrate your projects from TSLint to ESlint and want to continue using the [_rules_](https://github.com/microsoft/reactxp/tree/master/src/tslint) to automate search common problems in *ReactXP* usage, you can use [eslint-plugin-reactxp](https://github.com/a-tarasyuk/eslint-plugin-reactxp).

## Contributing

We welcome contributions to ReactXP. See the [CONTRIBUTING](./CONTRIBUTING.md) file for how to help out.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details
"
47,microsoft/onnxruntime,C++,"<p align=""center""><img width=""50%"" src=""docs/images/ONNX_Runtime_logo_dark.png"" /></p>

**ONNX Runtime** is a cross-platform **inference and training machine-learning accelerator** compatible with deep learning frameworks, PyTorch and TensorFlow/Keras, as well as classical machine learning libraries such as scikit-learn, and more.

ONNX Runtime uses the portable [ONNX](https://onnx.ai) computation graph format, backed by execution providers optimized for operating systems, drivers and hardware.

Common use cases for ONNX Runtime:

* Improve inference performance for a wide variety of ML models
* Reduce time and cost of training large models
* Train in Python but deploy into a C#/C++/Java app
* Run with optimized performance on different hardware and operating systems
* Support models created in several different frameworks

[ONNX Runtime inference](https://www.onnxruntime.ai/docs/get-started/inference.html) APIs are stable and production-ready since the [1.0 release](https://github.com/microsoft/onnxruntime/releases/tag/v1.0.0) in October 2019 and can enable faster customer experiences and lower costs.

[ONNX Runtime training](https://www.onnxruntime.ai/docs/get-started/training.html) feature was introduced in May 2020 in preview. This feature supports acceleration of PyTorch training on multi-node NVIDIA GPUs for transformer models. Additional updates for this feature are coming soon.


## Get Started

**http://onnxruntime.ai/**
* [Install](https://www.onnxruntime.ai/docs/get-started/install.html)
* [Inference](https://www.onnxruntime.ai/docs/get-started/inference.html)
* [Training](https://www.onnxruntime.ai/docs/get-started/training.html)
* [Documentation](https://www.onnxruntime.ai/docs/)
* [Samples and Tutorials](https://www.onnxruntime.ai/docs/tutorials/)
* [Build Instructions](https://www.onnxruntime.ai/docs/how-to/build.html)
* [Frequently Asked Questions](./docs/FAQ.md)

## Build Pipeline Status
|System|CPU|GPU|EPs|
|---|---|---|---|
|Windows|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20CPU%20CI%20Pipeline?label=Windows+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=9)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20CI%20Pipeline?label=Windows+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=10)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20GPU%20TensorRT%20CI%20Pipeline?label=Windows+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=47)|
|Linux|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20CI%20Pipeline?label=Linux+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=11)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20Minimal%20Build%20E2E%20CI%20Pipeline?label=Linux+CPU+Minimal+Build)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=64)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20CPU%20x64%20NoContribops%20CI%20Pipeline?label=Linux+CPU+x64+No+Contrib+Ops)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/centos7_cpu?label=Linux+CentOS7)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=78)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-ci-pipeline?label=Linux+CPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=86)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20CI%20Pipeline?label=Linux+GPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=12)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20GPU%20TensorRT%20CI%20Pipeline?label=Linux+GPU+TensorRT)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=45)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-distributed?label=Distributed+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=140)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/orttraining-linux-gpu-ci-pipeline?label=Linux+GPU+Training)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=84)|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20NUPHAR%20CI%20Pipeline?label=Linux+NUPHAR)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=110)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Linux%20OpenVINO%20CI%20Pipeline%20v2?label=Linux+OpenVINO)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=108)|
|Mac|[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20CI%20Pipeline?label=MacOS+CPU)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=13)<br>[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/MacOS%20NoContribops%20CI%20Pipeline?label=MacOS+NoContribops)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=65)|||
|Android|||[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Android%20CI%20Pipeline?label=Android)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=53)|
|iOS|||[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/iOS%20CI%20Pipeline?label=iOS)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=134)|
|WebAssembly|||[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status/Windows%20WebAssembly%20CI%20Pipeline?label=WASM)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=161)|


## Data/Telemetry

This project may collect usage data and send it to Microsoft to help improve our products and services. See the [privacy statement](docs/Privacy.md) for more details.

## Contributions and Feedback

We welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).

For feature requests or bug reports, please file a [GitHub Issue](https://github.com/Microsoft/onnxruntime/issues).

For general discussion or questions, please use [Github Discussions](https://github.com/microsoft/onnxruntime/discussions).

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

This project is licensed under the [MIT License](LICENSE).
"
48,microsoft/charticulator-extensions,TypeScript,"# Charticulator Extensions

This repository contains a collection of charticulator extensions.

* [PowerBI Visual Builder](powerbi-visual-builder) - An extension that adds PowerBI visual export functionality to charticulator.

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
49,microsoft/vcpkg,CMake,"# Vcpkg: Overview

[中文总览](README_zh_CN.md)
[Español](README_es.md)
[한국어](README_ko_KR.md)
[Français](README_fr.md)

Vcpkg helps you manage C and C++ libraries on Windows, Linux and MacOS.
This tool and ecosystem are constantly evolving, and we always appreciate contributions!

If you've never used vcpkg before, or if you're trying to figure out how to use vcpkg,
check out our [Getting Started](#getting-started) section for how to start using vcpkg.

For short description of available commands, once you've installed vcpkg,
you can run `vcpkg help`, or `vcpkg help [command]` for command-specific help.

* Github: [https://github.com/microsoft/vcpkg](https://github.com/microsoft/vcpkg)
* Slack: [https://cppalliance.org/slack/](https://cppalliance.org/slack/), the #vcpkg channel
* Discord: [\#include \<C++\>](https://www.includecpp.org), the #🌏vcpkg channel
* Docs: [Documentation](docs/README.md)

[![Build Status](https://dev.azure.com/vcpkg/public/_apis/build/status/microsoft.vcpkg.ci?branchName=master)](https://dev.azure.com/vcpkg/public/_build/latest?definitionId=29&branchName=master)

# Table of Contents

- [Vcpkg: Overview](#vcpkg-overview)
- [Table of Contents](#table-of-contents)
- [Getting Started](#getting-started)
  - [Quick Start: Windows](#quick-start-windows)
  - [Quick Start: Unix](#quick-start-unix)
  - [Installing Linux Developer Tools](#installing-linux-developer-tools)
  - [Installing macOS Developer Tools](#installing-macos-developer-tools)
    - [Installing GCC for macOS before 10.15](#installing-gcc-for-macos-before-1015)
  - [Using vcpkg with CMake](#using-vcpkg-with-cmake)
    - [Visual Studio Code with CMake Tools](#visual-studio-code-with-cmake-tools)
    - [Vcpkg with Visual Studio CMake Projects](#vcpkg-with-visual-studio-cmake-projects)
    - [Vcpkg with CLion](#vcpkg-with-clion)
    - [Vcpkg as a Submodule](#vcpkg-as-a-submodule)
- [Tab-Completion/Auto-Completion](#tab-completionauto-completion)
- [Examples](#examples)
- [Contributing](#contributing)
- [License](#license)
- [Telemetry](#telemetry)

# Getting Started

First, follow the quick start guide for either
[Windows](#quick-start-windows), or [macOS and Linux](#quick-start-unix),
depending on what you're using.

For more information, see [Installing and Using Packages][getting-started:using-a-package].
If a library you need is not present in the vcpkg catalog,
you can [open an issue on the GitHub repo][contributing:submit-issue]
where the vcpkg team and community can see it,
and potentially add the port to vcpkg.

After you've gotten vcpkg installed and working,
you may wish to add [tab completion](#tab-completionauto-completion) to your shell.

Finally, if you're interested in the future of vcpkg,
check out the [manifest][getting-started:manifest-spec] guide!
This is an experimental feature and will likely have bugs,
so try it out and [open all the issues][contributing:submit-issue]!

## Quick Start: Windows

Prerequisites:
- Windows 7 or newer
- [Git][getting-started:git]
- [Visual Studio][getting-started:visual-studio] 2015 Update 3 or greater with the English language pack

First, download and bootstrap vcpkg itself; it can be installed anywhere,
but generally we recommend using vcpkg as a submodule for CMake projects,
and installing it globally for Visual Studio projects.
We recommend somewhere like `C:\src\vcpkg` or `C:\dev\vcpkg`,
since otherwise you may run into path issues for some port build systems.

```cmd
> git clone https://github.com/microsoft/vcpkg
> .\vcpkg\bootstrap-vcpkg.bat
```

To install the libraries for your project, run:

```cmd
> .\vcpkg\vcpkg install [packages to install]
```

Note: This will install x86 libraries by default. To install x64, run:

```cmd
> .\vcpkg\vcpkg install package:x64-windows
```

Or

```cmd
> .\vcpkg\vcpkg install [packages to install] --triplet=x64-windows
```

You can also search for the libraries you need with the `search` subcommand:

```cmd
> .\vcpkg\vcpkg search [search term]
```

In order to use vcpkg with Visual Studio,
run the following command (may require administrator elevation):

```cmd
> .\vcpkg\vcpkg integrate install
```

After this, you can now create a New non-CMake Project (or open an existing one).
All installed libraries are immediately ready to be `#include`'d and used
in your project without additional configuration.

If you're using CMake with Visual Studio,
continue [here](#vcpkg-with-visual-studio-cmake-projects).

In order to use vcpkg with CMake outside of an IDE,
you can use the toolchain file:

```cmd
> cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake
> cmake --build [build directory]
```

With CMake, you will still need to `find_package` and the like to use the libraries.
Check out the [CMake section](#using-vcpkg-with-cmake) for more information,
including on using CMake with an IDE.

For any other tools, including Visual Studio Code,
check out the [integration guide][getting-started:integration].

## Quick Start: Unix

Prerequisites for Linux:
- [Git][getting-started:git]
- [g++][getting-started:linux-gcc] >= 6

Prerequisites for macOS:
- [Apple Developer Tools][getting-started:macos-dev-tools]
- On macOS 10.14 or below, you will also need:
  - [Homebrew][getting-started:macos-brew]
  - [g++][getting-started:macos-gcc] >= 6 from Homebrew

First, download and bootstrap vcpkg itself; it can be installed anywhere,
but generally we recommend using vcpkg as a submodule for CMake projects.

```sh
$ git clone https://github.com/microsoft/vcpkg
$ ./vcpkg/bootstrap-vcpkg.sh
```

To install the libraries for your project, run:

```sh
$ ./vcpkg/vcpkg install [packages to install]
```

You can also search for the libraries you need with the `search` subcommand:

```sh
$ ./vcpkg/vcpkg search [search term]
```

In order to use vcpkg with CMake, you can use the toolchain file:

```sh
$ cmake -B [build directory] -S . -DCMAKE_TOOLCHAIN_FILE=[path to vcpkg]/scripts/buildsystems/vcpkg.cmake
$ cmake --build [build directory]
```

With CMake, you will still need to `find_package` and the like to use the libraries.
Check out the [CMake section](#using-vcpkg-with-cmake)
for more information on how best to use vcpkg with CMake,
and CMake Tools for VSCode.

For any other tools, check out the [integration guide][getting-started:integration].

## Installing Linux Developer Tools

Across the different distros of Linux, there are different packages you'll
need to install:

- Debian, Ubuntu, popOS, and other Debian-based distributions:

```sh
$ sudo apt-get update
$ sudo apt-get install build-essential tar curl zip unzip
```

- CentOS

```sh
$ sudo yum install centos-release-scl
$ sudo yum install devtoolset-7
$ scl enable devtoolset-7 bash
```

For any other distributions, make sure you're installing g++ 6 or above.
If you want to add instructions for your specific distro,
[please open a PR][contributing:submit-pr]!

## Installing macOS Developer Tools

On macOS 10.15, the only thing you should need to do is run the following in your terminal:

```sh
$ xcode-select --install
```

Then follow along with the prompts in the windows that comes up.

On macOS 10.14 and previous, you'll also need to install g++ from homebrew;
follow the instructions in the following section.

### Installing GCC for macOS before 10.15

This will _only_ be necessary if you're using a macOS version from before 10.15.
Installing homebrew should be very easy; check out <brew.sh> for more information,
but at its simplest, run the following command:

```sh
$ /bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)""
```

Then, in order to grab an up-to-date version of gcc, run the following:

```sh
$ brew install gcc
```

You'll then be able to bootstrap vcpkg along with the [quick start guide](#quick-start-unix)

## Using vcpkg with CMake

If you're using vcpkg with CMake, the following may help!

### Visual Studio Code with CMake Tools

Adding the following to your workspace `settings.json` will make
CMake Tools automatically use vcpkg for libraries:

```json
{
  ""cmake.configureSettings"": {
    ""CMAKE_TOOLCHAIN_FILE"": ""[vcpkg root]/scripts/buildsystems/vcpkg.cmake""
  }
}
```

### Vcpkg with Visual Studio CMake Projects

Open the CMake Settings Editor, and under `CMake toolchain file`,
add the path to the vcpkg toolchain file:

```
[vcpkg root]/scripts/buildsystems/vcpkg.cmake
```

### Vcpkg with CLion

Open the Toolchains settings
(File > Settings on Windows and Linux, CLion > Preferences on macOS),
and go to the CMake settings (Build, Execution, Deployment > CMake).
Finally, in `CMake options`, add the following line:

```
-DCMAKE_TOOLCHAIN_FILE=[vcpkg root]/scripts/buildsystems/vcpkg.cmake
```

Unfortunately, you'll have to add this to each profile.

### Vcpkg as a Submodule

When using vcpkg as a submodule of your project,
you can add the following to your CMakeLists.txt before the first `project()` call,
instead of passing `CMAKE_TOOLCHAIN_FILE` to the cmake invocation.

```cmake
set(CMAKE_TOOLCHAIN_FILE ${CMAKE_CURRENT_SOURCE_DIR}/vcpkg/scripts/buildsystems/vcpkg.cmake
  CACHE STRING ""Vcpkg toolchain file"")
```

This will still allow people to not use vcpkg,
by passing the `CMAKE_TOOLCHAIN_FILE` directly,
but it will make the configure-build step slightly easier.

[getting-started:using-a-package]: docs/examples/installing-and-using-packages.md
[getting-started:integration]: docs/users/integration.md
[getting-started:git]: https://git-scm.com/downloads
[getting-started:cmake-tools]: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cmake-tools
[getting-started:linux-gcc]: #installing-linux-developer-tools
[getting-started:macos-dev-tools]: #installing-macos-developer-tools
[getting-started:macos-brew]: #installing-gcc-on-macos
[getting-started:macos-gcc]: #installing-gcc-on-macos
[getting-started:visual-studio]: https://visualstudio.microsoft.com/
[getting-started:manifest-spec]: docs/specifications/manifests.md

# Tab-Completion/Auto-Completion

`vcpkg` supports auto-completion of commands, package names,
and options in both powershell and bash.
To enable tab-completion in the shell of your choice, run:

```pwsh
> .\vcpkg integrate powershell
```

or

```sh
$ ./vcpkg integrate bash
```

depending on the shell you use, then restart your console.

# Examples

See the [documentation](docs/README.md) for specific walkthroughs,
including [installing and using a package](docs/examples/installing-and-using-packages.md),
[adding a new package from a zipfile](docs/examples/packaging-zipfiles.md),
and [adding a new package from a GitHub repo](docs/examples/packaging-github-repos.md).

Our docs are now also available online at ReadTheDocs: <https://vcpkg.readthedocs.io/>!

See a 4 minute [video demo](https://www.youtube.com/watch?v=y41WFKbQFTw).

# Contributing

Vcpkg is an open source project, and is thus built with your contributions.
Here are some ways you can contribute:

* [Submit Issues][contributing:submit-issue] in vcpkg or existing packages
* [Submit Fixes and New Packages][contributing:submit-pr]

Please refer to our [Contributing Guide](CONTRIBUTING.md) for more details.

This project has adopted the [Microsoft Open Source Code of Conduct][contributing:coc].
For more information see the [Code of Conduct FAQ][contributing:coc-faq]
or email [opencode@microsoft.com](mailto:opencode@microsoft.com)
with any additional questions or comments.

[contributing:submit-issue]: https://github.com/microsoft/vcpkg/issues/new/choose
[contributing:submit-pr]: https://github.com/microsoft/vcpkg/pulls
[contributing:coc]: https://opensource.microsoft.com/codeofconduct/
[contributing:coc-faq]: https://opensource.microsoft.com/codeofconduct/

# License

The code in this repository is licensed under the [MIT License](LICENSE.txt).

# Telemetry

vcpkg collects usage data in order to help us improve your experience.
The data collected by Microsoft is anonymous.
You can opt-out of telemetry by re-running the bootstrap-vcpkg script with -disableMetrics,
passing --disable-metrics to vcpkg on the command line,
or by setting the VCPKG_DISABLE_METRICS environment variable.

Read more about vcpkg telemetry at docs/about/privacy.md
"
50,microsoft/vscode,TypeScript,"# Visual Studio Code - Open Source (""Code - OSS"")
[![Feature Requests](https://img.shields.io/github/issues/microsoft/vscode/feature-request.svg)](https://github.com/microsoft/vscode/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)
[![Bugs](https://img.shields.io/github/issues/microsoft/vscode/bug.svg)](https://github.com/microsoft/vscode/issues?utf8=✓&q=is%3Aissue+is%3Aopen+label%3Abug)
[![Gitter](https://img.shields.io/badge/chat-on%20gitter-yellow.svg)](https://gitter.im/Microsoft/vscode)

## The Repository

This repository (""`Code - OSS`"") is where we (Microsoft) develop the [Visual Studio Code](https://code.visualstudio.com) product together with the community. Not only do we work on code and issues here, we also publish our [roadmap](https://github.com/microsoft/vscode/wiki/Roadmap), [monthly iteration plans](https://github.com/microsoft/vscode/wiki/Iteration-Plans), and our [endgame plans](https://github.com/microsoft/vscode/wiki/Running-the-Endgame). This source code is available to everyone under the standard [MIT license](https://github.com/microsoft/vscode/blob/main/LICENSE.txt).

## Visual Studio Code

<p align=""center"">
  <img alt=""VS Code in action"" src=""https://user-images.githubusercontent.com/1487073/58344409-70473b80-7e0a-11e9-8570-b2efc6f8fa44.png"">
</p>

[Visual Studio Code](https://code.visualstudio.com) is a distribution of the `Code - OSS` repository with Microsoft specific customizations released under a traditional [Microsoft product license](https://code.visualstudio.com/License/).

[Visual Studio Code](https://code.visualstudio.com) combines the simplicity of a code editor with what developers need for their core edit-build-debug cycle. It provides comprehensive code editing, navigation, and understanding support along with lightweight debugging, a rich extensibility model, and lightweight integration with existing tools.

Visual Studio Code is updated monthly with new features and bug fixes. You can download it for Windows, macOS, and Linux on [Visual Studio Code's website](https://code.visualstudio.com/Download). To get the latest releases every day, install the [Insiders build](https://code.visualstudio.com/insiders).

## Contributing

There are many ways in which you can participate in the project, for example:

* [Submit bugs and feature requests](https://github.com/microsoft/vscode/issues), and help us verify as they are checked in
* Review [source code changes](https://github.com/microsoft/vscode/pulls)
* Review the [documentation](https://github.com/microsoft/vscode-docs) and make pull requests for anything from typos to new content

If you are interested in fixing issues and contributing directly to the code base,
please see the document [How to Contribute](https://github.com/microsoft/vscode/wiki/How-to-Contribute), which covers the following:

* [How to build and run from source](https://github.com/microsoft/vscode/wiki/How-to-Contribute)
* [The development workflow, including debugging and running tests](https://github.com/microsoft/vscode/wiki/How-to-Contribute#debugging)
* [Coding guidelines](https://github.com/microsoft/vscode/wiki/Coding-Guidelines)
* [Submitting pull requests](https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests)
* [Finding an issue to work on](https://github.com/microsoft/vscode/wiki/How-to-Contribute#where-to-contribute)
* [Contributing to translations](https://aka.ms/vscodeloc)

## Feedback

* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/vscode)
* [Request a new feature](CONTRIBUTING.md)
* Upvote [popular feature requests](https://github.com/microsoft/vscode/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)
* [File an issue](https://github.com/microsoft/vscode/issues)
* Follow [@code](https://twitter.com/code) and let us know what you think!

See our [wiki](https://github.com/microsoft/vscode/wiki/Feedback-Channels) for a description of each of these channels and information on some other available community-driven channels.

## Related Projects

Many of the core components and extensions to VS Code live in their own repositories on GitHub. For example, the [node debug adapter](https://github.com/microsoft/vscode-node-debug) and the [mono debug adapter](https://github.com/microsoft/vscode-mono-debug) have their own repositories. For a complete list, please visit the [Related Projects](https://github.com/microsoft/vscode/wiki/Related-Projects) page on our [wiki](https://github.com/microsoft/vscode/wiki).

## Bundled Extensions

VS Code includes a set of built-in extensions located in the [extensions](extensions) folder, including grammars and snippets for many languages. Extensions that provide rich language support (code completion, Go to Definition) for a language have the suffix `language-features`. For example, the `json` extension provides coloring for `JSON` and the `json-language-features` provides rich language support for `JSON`.

## Development Container

This repository includes a Visual Studio Code Remote - Containers / Codespaces development container.

- For [Remote - Containers](https://aka.ms/vscode-remote/download/containers), use the **Remote-Containers: Open Repository in Container...** command which creates a Docker volume for better disk I/O on macOS and Windows.
- For Codespaces, install the [Visual Studio Codespaces](https://aka.ms/vscs-ext-vscode) extension in VS Code, and use the **Codespaces: Create New Codespace** command.

Docker / the Codespace should have at least **4 Cores and 6 GB of RAM (8 GB recommended)** to run full build. See the [development container README](.devcontainer/README.md) for more information.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [MIT](LICENSE.txt) license.
"
51,microsoft/spektate,TypeScript,"[![Build Status](https://dev.azure.com/epicstuff/bedrock/_apis/build/status/microsoft.spektate?branchName=master)](https://dev.azure.com/epicstuff/bedrock/_build/latest?definitionId=124&branchName=master)
![Azure DevOps coverage](https://img.shields.io/azure-devops/coverage/epicstuff/bedrock/124/master)

# Spektate

This is an initiative to visualize [Project Bedrock](https://github.com/microsoft/bedrock). Spektate ties in information from the repositories API, the pipelines API and information stored in an Azure Table to display the dashboard with the following components:

![](./images/spektate-pieces-diagram.png)

Here's a detailed diagram describing the Spektate workflow. Each pipeline is responsible for sending a unique set of data to the storage, which is used to connect all the pieces together:

![](./images/spektate-workflow.png)

Currently, Spektate consists of a command line interface and a simple dashboard prototype. The instructions to use both are below.

Note: Spektate dashboard will delete deployments when their corresponding builds/releases have expired in Azure DevOps.

Official docker images for this dashboard are located at `mcr.microsoft.com/k8s/bedrock/spektate`.

## Onboard a Bedrock project to use Spektate

Follow the steps in this [guide](https://github.com/microsoft/bedrock-cli/blob/master/guides/service-introspection-onboarding.md) to onboard a project to use Spektate.

## Install on your cluster

The helm chart to use this dashboard is located [here](./chart). There's a Load Balancer provided in the helm chart but it's turned off in `values.yaml` with the setting `externalIP`. Set this to true if you would like to expose the dashboard via a public endpoint.

Substitute values for your configuration and install this dashboard via the command below:

```bash
cd ./chart
helm install . --name spektate --set storageAccessKey=<storageAccessKey> --set storageTableName=<storageTableName> --set storagePartitionKey=<storagePartitionKey> --set storageAccountName=<storageAccountName> --set pipelineProject=<pipelineProjectName> --set pipelineOrg=<pipelineOrg> --set pipelineAccessToken=<PipelinePAT> --set manifest=<manifestRepoName> --set manifestAccessToken=<manifestAccessToken> --set githubManifestUsername=<gitHubUserName>  --set sourceRepoAccessToken=<sourceRepoAccessToken>
```

- `storageAccessKey`: Access key for the storage account
- `storageTableName`: Table name for the storage account
- `storagePartitionKey`: Partition key for your configuration, you may want to use project name or some identifier that helps separate unrelated configurations for the purpose of introspection.
- `storageAccountName`: Storage account name
- `pipelineProject`: Project name for the pipelines in Azure DevOps
- `pipelineOrg`: Org name for the pipelines in Azure DevOps
- `pipelineAccessToken`: Access token for pipelines in Azure DevOps
- `manifestRepoName`: Manifest repository name
- `manifestAccessToken`: Access token for the manifest repository
- `sourceRepoAccessToken`: Access token for the source repository
- **Note**: If you're using GitHub, add `githubManifestUsername`: Account name or organization name under which the manifest repository resides.

If you're not using an external IP, use port-forwarding to access the dashboard:

1. Copy pod name from `kubectl get pods`
2. `kubectl port-forward pod/<pod-name> 2200:5000` or change `2200` to a port of your choice
3. Navigate to http://localhost:2200 or change `2200` to a port of your choice

## Dashboard dev mode

1. Clone this repository.
2. There is frontend and backend folders and each of them are separate yarn projects. `cd frontend` in one window and `cd backend` in another.
3. Add the following env variables to your shell where you have changed directory to `backend`:

   ```bash
   export REACT_APP_STORAGE_ACCESS_KEY=
   export REACT_APP_STORAGE_TABLE_NAME=
   export REACT_APP_STORAGE_PARTITION_KEY=
   export REACT_APP_STORAGE_ACCOUNT_NAME=
   export REACT_APP_PIPELINE_PROJECT=
   export REACT_APP_PIPELINE_ORG=
   export REACT_APP_PIPELINE_ACCESS_TOKEN=
   export REACT_APP_MANIFEST=
   export REACT_APP_MANIFEST_ACCESS_TOKEN=
   export REACT_APP_SOURCE_REPO_ACCESS_TOKEN=
   ```

   - `REACT_APP_STORAGE_ACCESS_KEY`: Access key for the storage account
   - `REACT_APP_STORAGE_TABLE_NAME`: Table name for the storage account
   - `REACT_APP_STORAGE_PARTITION_KEY`: Partition key for your configuration, you may want to use project name or some identifier that helps separate unrelated configurations for the purpose of introspection.
   - `REACT_APP_STORAGE_ACCOUNT_NAME`: Storage account name
   - `REACT_APP_PIPELINE_PROJECT`: Project name for the pipelines in Azure DevOps
   - `REACT_APP_PIPELINE_ORG`: Org name for the pipelines in Azure DevOps
   - `REACT_APP_PIPELINE_ACCESS_TOKEN`: Access token for pipelines in Azure DevOps
   - `REACT_APP_MANIFEST`: Manifest repository name
   - `REACT_APP_MANIFEST_ACCESS_TOKEN`: Access token for the manifest repository
   - `REACT_APP_SOURCE_REPO_ACCESS_TOKEN`: Access token for the source repository
   - **Note**: If you're using GitHub, add `REACT_APP_GITHUB_MANIFEST_USERNAME`: Account name or organization name under which the manifest repository resides.

4. Run `yarn` in both to install dependencies
5. Run `yarn start` in both to start the applications. You should be able to see the dashboard launch in one, and a Node.js server start in another! It should navigate you to the browser where dashboard is running.

## Publish Docker image

In order to publish images to this repository, you will need access to `devcrewsacr.azurecr.io`

1. Run `az acr login --name devcrewsacr`
2. If you do not know credentials you will need to login, grab them from portal.azure.com or run `az acr credential show --name devcrewsacr`.
3. Run `docker login devcrewsacr.azurecr.io` and you will be prompted to enter the credentials
4. Run `docker push devcrewsacr.azurecr.io/public/k8s/bedrock/spektate:<tag>`

## Azure Web App Hosting

You can provide an Azure Active Directory layer of authetication on top of Spektate. Follow instructions [here](./WebAppHosting.md).

## Command Line Interface

To use the CLI for Spektate, head over to https://github.com/microsoft/bedrock-cli.

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
52,microsoft/BotFramework-Emulator,TypeScript,"# ![Bot Framework Emulator](./docs/media/BotFrameworkEmulator_header.png)

### [Find out what's new with Bot Framework](https://github.com/Microsoft/botframework/blob/main/whats-new.md#whats-new)

# Bot Framework Emulator

[![Build Status](https://fuselabs.visualstudio.com/BotFramework-Emulator/_apis/build/status/%5BV4-Nightly%5D-Main-Build?branchName=main)](https://fuselabs.visualstudio.com/BotFramework-Emulator/_build/latest?definitionId=419&branchName=main) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/BotFramework-Emulator/badge.svg?branch=main)](https://coveralls.io/github/Microsoft/BotFramework-Emulator?branch=main)

The Bot Framework Emulator is a desktop application that allows bot developers to test and debug bots built using the [Bot Framework SDK](https://github.com/microsoft/botbuilder). You can use the Bot Framework Emulator to test bots running either locally on your machine or connect to bots running remotely through a tunnel.

This repo is part the [Microsoft Bot Framework](https://github.com/microsoft/botframework-sdk) - a comprehensive framework for building enterprise-grade conversational AI experiences.

## Download

* Download the Bot Framework V4 Emulator for your platform from the [GitHub releases](https://github.com/Microsoft/BotFramework-Emulator/releases/latest) page.

### Supported platforms

* Windows
* OS X
* Linux

  **Note for Linux users:**

  The Emulator leverages a library that uses `libsecret` so you may need to install it before running `npm install`.

  Depending on your distribution, you will need to run the following command:

  Debian/Ubuntu: `sudo apt-get install libsecret-1-dev`

  Red Hat-based: `sudo yum install libsecret-devel`

  Arch Linux: `sudo pacman -S libsecret`

## Documentation

Checkout the [Wiki](https://github.com/Microsoft/BotFramework-Emulator/wiki) for docs.

## Feedback

* File a bug or suggestion in [GitHub Issues](https://github.com/Microsoft/BotFramework-Emulator/blob/v4/CONTRIBUTING.md#submitting-issues)
* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/botframework)

## Related

* [Microsoft Bot Framework](https://dev.botframework.com/)
* [Bot Framework SDK](https://github.com/Microsoft/BotBuilder)
* [Bot Framework CLI](https://github.com/microsoft/botframework-cli)
* [Bot Framework Web Chat](https://github.com/Microsoft/BotFramework-WebChat)

## Nightly builds

Nightly builds are generated using the latest code. Therefore, they may not be stable, and most likely lack up to date documentation. These builds are better suited for more experienced users, although everyone is welcome to use them and provide feedback. Nightly builds of the V4 Emulator are available [here](https://github.com/Microsoft/botframework-emulator-nightlies/releases).

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Reporting Security Issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the [MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

Copyright (c) Microsoft Corporation. All rights reserved.
"
53,microsoft/typed-rest-client,TypeScript,"
<a href=""https://github.com/microsoft/typed-rest-client""><img alt=""GitHub Actions status"" src=""https://github.com/microsoft/typed-rest-client/workflows/all-tests/badge.svg""></a>

[![Build Status](https://dev.azure.com/ms/typed-rest-client/_apis/build/status/Microsoft.typed-rest-client?branchName=master)](https://dev.azure.com/ms/typed-rest-client/_build/latest?definitionId=42&branchName=master)


# Typed REST and HTTP Client with TypeScript Typings

A lightweight REST and HTTP client optimized for use with TypeScript with generics and async await.

## Features

  - REST and HTTP client with TypeScript generics and async/await/Promises
  - Typings included so no need to acquire separately (great for intellisense and no versioning drift)
  - Basic, Bearer and NTLM Support out of the box.  Extensible handlers for others.
  - Proxy support
  - Certificate support (Self-signed server and client cert)
  - Redirects supported

Intellisense and compile support:

![intellisense](./docs/intellisense.png)

## Install

```
npm install typed-rest-client --save
```

Or to install the latest preview:
```
npm install typed-rest-client@preview --save
```

## Samples

See the [samples](./samples) for complete coding examples. Also see the [REST](./test/tests/resttests.ts) and [HTTP](./test/tests/httptests.ts) tests for detailed examples.

## Errors

### HTTP

The HTTP client does not throw unless truly exceptional.

* A request that successfully executes resulting in a 404, 500 etc... will return a response object with a status code and a body.
* Redirects (3xx) will be followed by default.


See [HTTP tests](./test/tests/httptests.ts) for detailed examples.

### REST

The REST client is a high-level client which uses the HTTP client.  Its responsibility is to turn a body into a typed resource object.  

* A 200 will be success.  
* Redirects (3xx) will be followed.  
* A 404 will not throw but the result object will be null and the result statusCode will be set.
* Other 4xx and 5xx errors will throw.  The status code will be attached to the error object.  If a RESTful error object is returned (`{ message: xxx}`), then the error message will be that.  Otherwise, it will be a generic, `Failed Request: (xxx)`.

See [REST tests](./test/tests/resttests.ts) for detailed examples.

## Debugging

To enable detailed console logging of all HTTP requests and responses, set the NODE_DEBUG environment varible:

```
export NODE_DEBUG=http
```

or

```
set NODE_DEBUG=http
```



## Node support

The typed-rest-client is built using the latest LTS version of Node 8. We also support the latest LTS for Node 4 and Node 6.

## Contributing

To contribute to this repository, see the [contribution guide](./CONTRIBUTING.md)

To build:

```bash
$ npm run build
```

To run all tests:
```bash
$ npm test
```

To just run unit tests:
```bash
$ npm run units
```

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Security Issues

Do you think there might be a security issue?
Have you been phished or identified a security vulnerability?
Please don't report it here - let us know by sending an email to secure@microsoft.com.
"
54,microsoft/azuredatastudio,TypeScript,"# Azure Data Studio

[![Join the chat at https://gitter.im/Microsoft/sqlopsstudio](https://badges.gitter.im/Microsoft/sqlopsstudio.svg)](https://gitter.im/Microsoft/sqlopsstudio?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Build Status](https://dev.azure.com/azuredatastudio/azuredatastudio/_apis/build/status/Azure%20Data%20Studio%20CI?branchName=main)](https://dev.azure.com/azuredatastudio/azuredatastudio/_build/latest?definitionId=4&branchName=main)
[![Twitter Follow](https://img.shields.io/twitter/follow/azuredatastudio?style=social)](https://twitter.com/azuredatastudio)

Azure Data Studio is a data management tool that enables you to work with SQL Server, Azure SQL DB and SQL DW from Windows, macOS and Linux.

## **Download the latest Azure Data Studio release**

| Platform																|
| ---------------------------------------	|
| [Windows User Installer][win-user]			|
| [Windows System Installer][win-system]	|
| [Windows ZIP][win-zip]									|
| [macOS ZIP][osx-zip]										|
| [Linux TAR.GZ][linux-zip]								|
| [Linux RPM][linux-rpm]									|
| [Linux DEB][linux-deb]									|


Go to our [download page](https://aka.ms/getazuredatastudio) for more specific instructions.

## Try out the latest insiders build from `main`:
- [Windows User Installer - **Insiders build**](https://azuredatastudio-update.azurewebsites.net/latest/win32-x64-user/insider)
- [Windows System Installer - **Insiders build**](https://azuredatastudio-update.azurewebsites.net/latest/win32-x64/insider)
- [Windows ZIP - **Insiders build**](https://azuredatastudio-update.azurewebsites.net/latest/win32-x64-archive/insider)
- [macOS ZIP - **Insiders build**](https://azuredatastudio-update.azurewebsites.net/latest/darwin/insider)
- [Linux TAR.GZ - **Insiders build**](https://azuredatastudio-update.azurewebsites.net/latest/linux-x64/insider)

See the [change log](https://github.com/Microsoft/azuredatastudio/blob/main/CHANGELOG.md) for additional details of what's in this release.
Go to our [download page](https://aka.ms/getazuredatastudio) for more specific instructions.


## **Feature Highlights**

- Cross-Platform DB management for Windows, macOS and Linux with simple XCopy deployment
- SQL Server Connection Management with Connection Dialog, Server Groups, Azure Integration and Registered Servers
- Object Explorer supporting schema browsing and contextual command execution
- T-SQL Query Editor with advanced coding features such as autosuggestions, error diagnostics, tooltips, formatting and peek definition
- Query Results Viewer with advanced data grid supporting large result sets, export to JSON\CSV\Excel, query plan and charting
- Management Dashboard supporting customizable widgets with drill-through actionable insights
- Visual Data Editor that enables direct row insertion, update and deletion into tables
- Backup and Restore dialogs that enables advanced customization and remote filesystem browsing, configured tasks can be executed or scripted
- Task History window to view current task execution status, completion results with error messages and task T-SQL scripting
- Scripting support to generate CREATE, SELECT, ALTER and DROP statements for database objects
- Workspaces with full Git integration and Find In Files support to managing T-SQL script libraries
- Modern light-weight shell with theming, user settings, full-screen support, integrated terminal and numerous other features

Here are some of these features in action.

<img src='https://github.com/Microsoft/azuredatastudio/blob/main/docs/overview_screen.jpg' width='800px'>

## Contributing
If you are interested in fixing issues and contributing directly to the code base,
please see the document [How to Contribute](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute), which covers the following:

* [How to build and run from source](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#Build-and-Run-From-Source)
* [The development workflow, including debugging and running tests](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#development-workflow)
* [Submitting pull requests](https://github.com/Microsoft/azuredatastudio/wiki/How-to-Contribute#pull-requests)

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Localization
Azure Data Studio is localized into 10 languages: French, Italian, German, Spanish, Simplified Chinese, Traditional Chinese, Japanese, Korean, Russian, and Portuguese (Brazil). The language packs are available in the Extension Manager marketplace. Simply, search for the specific language using the extension marketplace and install. Once you install the selected language, Azure Data Studio will prompt you to restart with the new language.

## Privacy Statement
The [Microsoft Enterprise and Developer Privacy Statement](https://privacy.microsoft.com/en-us/privacystatement) describes the privacy statement of this software.

## Contributions and ""Thank You""
We would like to thank all our users who raised issues, and in particular the following users who helped contribute fixes:

* eulercamposbarros for `Prevent connections from moving on click (#7528)`
* AlexFsmn for `Fixed issue where task icons got hidden if text was too long`
* jamesrod817 for `Tempdb (#7022)`
* dzsquared for `fix(snippets): ads parenthesis to sqlcreateindex snippet #7020`
* devmattrick for `Update row count as updates are received #6642`
* mottykohn for `In Message panel onclick scroll to line #6417`
* Stevoni for `Corrected Keyboard Shortcut Execution Issue #5480`
* yamatoya for `fix the format #4899`
* GeoffYoung for `Fix sqlDropColumn description #4422`
* AlexFsmn for `Added context menu for DBs in explorer view to backup & restore db. #2277`
* sadedil for `Missing feature request: Save as XML #3729`
* gbritton1 for `Removed reference to object explorer #3463`
* Tarig0  for `Add Routine_Type to CreateStoredProc fixes #3257 (#3286)`
* oltruong  for `typo fix #3025'`
* Thomas-S-B for `Removed unnecessary IErrorDetectionStrategy #749`
* Thomas-S-B for `Simplified code #750`
* rdaniels6813  for `Add query plan theme support #3031`
* Ruturaj123 for `Fixed some typos and grammatical errors #3027`
* PromoFaux for `Use emoji shortcodes in CONTRIBUTING.md instead of � #3009`
* ckaczor for `Fix: DATETIMEOFFSET data types should be ISO formatted #714`
* hi-im-T0dd for `Fixed sync issue with my forked master so this commit is correct #2948`
* hi-im-T0dd for `Fixed when right clicking and selecting Manage-correct name displays #2794`
* philoushka  for `center the icon #2760`
* anthonypants for `Typo #2775`
* kstolte for `Fix Invalid Configuration in Launch.json #2789`
* kstolte for `Fixing a reference to SQL Ops Studio #2788`
* AlexFsmn `Feature: Ability to add connection name #2332`
* AlexFsmn `Disabled connection name input when connecting to a server. #2566`
* SebastianPfliegel `Added more saveAsCsv options #2099`
* ianychoi `Fixes a typo: Mimunum -> Minimum #1994`
* AlexFsmn `Fixed bug where proper file extension wasn't appended to the filename. #2151`
* AlexFsmn `Added functionality for adding any file to import wizard #2329`
* AlexFsmn `Fixed background issue when copying a chart to clipboard #2215`
* AlexFsmn `Fixed problem where vertical charts didn't display labels correctly. #2263`
* AlexFsmn `Fixed Initial values for charts to match visuals #2266`
* AlexFsmn `Renamed chart option labels #2264`
* AlexFsmn `Added feature for the opening file after exporting to CSV/XLS/JSON & query files #2216`
* AlexFsmm `Get Connection String should copy to clipboard #2175`
* lanceklinger `Fix for double-clicking column handle in results table #1504`
* westerncj for `Removed duplicate contribution from README.md (#753)`
* ntovas for `Fix for duplicate extensions shown in ""Save File"" dialog. (#779)`
* SebastianPfliegel for `Add cursor snippet (#475)`
* mikaoelitiana for the fix: `revert README and CONTRIBUTING after last VSCode merge (#574)`
* alextercete for `Reinstate menu item to install from VSIX (#682)`
* alextercete for `Fix ""No extension gallery service configured"" error (#427)`
* mwiedemeyer for `Fix #58: Default sort order for DB size widget (#111)`
* AlexTroshkin for `Show disconnect in context menu only when connectionProfile connected (#150)`
* AlexTroshkin for `Fix #138: Invalid syntax color highlighting (identity not highlighting) (#140))`
* stebet for `Fix #153: Fixing sql snippets that failed on a DB with a case-sensitive collation. (#152)`
* SebastianPfliegel `Remove sqlExtensionHelp (#312)`
* olljanat for `Implemented npm version check (#314)`
* Adam Machanic for helping with the `whoisactive` extension

And of course, we'd like to thank the authors of all upstream dependencies.  Please see a full list in the [ThirdPartyNotices.txt](https://raw.githubusercontent.com/Microsoft/azuredatastudio/main/ThirdPartyNotices.txt)

## License

Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [Source EULA](LICENSE.txt).

[win-user]: https://go.microsoft.com/fwlink/?linkid=2160781
[win-system]: https://go.microsoft.com/fwlink/?linkid=2160780
[win-zip]: https://go.microsoft.com/fwlink/?linkid=2160923
[osx-zip]: https://go.microsoft.com/fwlink/?linkid=2160874
[linux-zip]: https://go.microsoft.com/fwlink/?linkid=2160782
[linux-rpm]: https://go.microsoft.com/fwlink/?linkid=2160875
[linux-deb]: https://go.microsoft.com/fwlink/?linkid=2160876
"
55,microsoft/jacdac-docs,TypeScript,"# Jacdac Documentation

**Jacdac** is a bus-based plug-and-play hardware and software stack for microcontrollers and their peripherals such as sensors and actuators. Jacdac is primarily designed for “modular electronics” scenarios that support rapid prototyping, creative exploration, making and learning through physical computing. Jacdac is designed to be cheap, flexible and extensible.

This repository contains sources of [Jacdac](https://aka.ms/jacdac).

* [User Documentation](https://aka.ms/jacdac/)
* Discussions at https://github.com/microsoft/jacdac/discussions
* Issues are tracked on https://github.com/microsoft/jacdac/issues

The rest of this page is for developers of the jacdac-ts library.

## Developer setup
### Codespaces

Edit this project directly from your browser using GitHub Codespaces. If you have access to them,

* open project in a new codespace
* launch the docs server

```
yarn docs
```

* click on the generated URL in the terminal output and voila!

### Local Setup

* install node.js
* install yarn

```
npm install -g yarn
```

* setup repo

```
yarn setup
```
### VS Code

You are welcome to use any editor you want! Visual Studio Code
provides seamless support for git sub-modules and is our preferred editor.

* open [Visual Studio Code](https://code.visualstudio.com/)

```
code .
```

* install the recommended extensions (**MDX**, **ESLint** and **Prettier** extensions)
* in the Git view, click on the ``jacdac`` branch and select ``main`` so that changes are automatically synched

### Specs build

To regenerate the service definition JSON files from the ``.md`` files in jacdac-spec,
run

```
yarn buildspecs
```
### Docs build

* run the docs web site locally

```
yarn develop
```

* browse to the local server

```
http://localhost:8000?dbg=1
```

To analyze the webpack bundle size,

```
cd docs
gatsby build
gatsby serve
nav to http://127.0.0.1:3001
```

If the build fails after pulling, try

```
yarn clean
```

### Jacdac + MakeCode

### Local build

Run this command to rebuild the makecode packages

```
yarn buildpxt
```

#### Local debugging

Open the multi editor to test MakeCode devices with the Jacdac view. You can select to run Jacdac and/or MakeCode on localhost/web from the drop downs.

https://makecode.com/multi?jacdac=1&localhost=1&beta=1

### Adding a new MakeCode client

Create a new issue in https://github.com/microsoft/jacdac and select the ``MakeCode client`` template.

### HTML tools

You can do ``yarn watch`` to watch/build bundles. Bundles are placed under the ``dist`` folder.

```
yarn watch
```

On another terminal, launch a small web server and 
try all the tools under ``docs/static/tools/*`` at http://localhost:8080/docs/static/tools/js/console.html . These tools load the files under ``dist`` so you'll want 
to also run ``yarn watch`` on the side.

```
yarn tools
```

* console http://localhost:8080/docs/static/tools/js/console.html
* devices http://localhost:8080/docs/static/tools/js/devices.html
* flashing http://localhost:8080/docs/static/tools/js/flashing.html
* namer http://localhost:8080/docs/static/tools/js/namer.html
* tfite http://localhost:8080/docs/static/tools/js/tflite.html
* streaming http://localhost:8080/docs/static/tools/js/streaming.html
* streaming-rickshaw: http://localhost:8080/docs/static/tools/js/streaming-rickshaw.html

### Commits create releases

The releases are automatically created by the build system based on the title of the commit:

* ``patch|fix:...``  patch
* ``minor:feature:...`` minor

### NPM scripts

 - `yarn watch`: Run `yarn build` in watch mode
 - `yarn lint`: Lints code
 - `yarn docs`: Launch docs web service

## Microsoft Open Source Code of Conduct

This project is hosted at https://github.com/microsoft/jacdac-ts. 
This project has adopted the 
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns
"
56,microsoft/charticulator,TypeScript,"
[![Build Status](https://travis-ci.org/Microsoft/charticulator.svg?branch=master)](https://travis-ci.org/Microsoft/charticulator)

Charticulator
====

Charticulator is a new charting tool that allows you to design charts by interactively specifying
constraints.

Project Team
----

- [Donghao Ren](https://donghaoren.org/)
- [Bongshin Lee](http://research.microsoft.com/en-us/um/people/bongshin/)
- [Matthew Brehmer](https://www.microsoft.com/en-us/research/people/mabrehme/)
- [Nathan Evans](https://github.com/natoverse)
- [Kate Lytvynets](https://github.com/katua)
- [David Tittsworth](https://github.com/stopyoukid)
- [Chris Trevino](https://github.com/darthtrevino)

Build
----

Follow the following steps to prepare a development environment:

- Install nodejs 8.0+: <https://nodejs.org/>
- Install yarnjs 1.7+: <https://yarnpkg.com/>

Install node modules:

```bash
yarn
```

Copy the template configuration file and edit its contents:

```bash
cp config.template.yml config.yml
# (on windows, use copy instead of cp)
```

Run the following command to build Charticulator, which will create a self contained bundle in the `dist` folder:

```bash
yarn build
```

Run a local web server to test Charticulator:

```bash
# Serve Charticulator at http://localhost:4000
yarn server

# Serve Charticulator publicly at http://0.0.0.0:4000
# Use this if you want to enable access from another computer
yarn public_server
```

Development
----

For a live development environment, keep the following command running:

```bash
yarn start
```

This command watches for any change in `src/` and `sass/`, and recompiles Charticulator automatically.
Once this up, open <http://localhost:4000/>
to launch Charticulator. Now when you change the source code, the app can be updated by simply
refreshing the browser page (you may need to disable browser cache).

In development mode, there is a test application for UI components, which can be accessed at <http://localhost:4000/test.html>.

The watch mode won't update when you change the following:

- config.yml
- THIRD_PARTY.yml
- webpack.config.js

When you update these, please do `yarn build` again.

### Sample Datasets
You can add custom sample datasets that can be used with Charticulator.  To do so, create a `datasets` folder at the root of the repository(if it doesn't exist), add your `.csv` (or `.tsv`) to that folder, and finally create a `files.json` file in the folder with the following contents:

```
[
    {
        ""name"": ""<Your dataset display name>"",
        ""description"": ""<Your dataset desription>"",
        ""tables"": [
            {
                ""name"": ""<Your dataset file name without extension>"",
                ""type"": ""<csv || tsv>"",
                ""url"": ""<Your dataset file name with extension>""
            }
        ]
    }
]
```

Testing
----

Charticulator currently include a rudimentary test code:

```bash
yarn test
```

More test cases are needed.


# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Documentation

Run `yarn typedoc` to generate documentation pages.
The page will be awailable in [`./docs/charticulator`](./docs/charticulator/index.html)

Start point of documentation is index page {@link ""index""}"
57,microsoft/jacdac-ts,TypeScript,"# Jacdac TypeScript

**Jacdac** is a bus-based plug-and-play hardware/software stack 
for **microcontrollers** and their peripherals (sensors/actuators), 
with applications to rapid prototyping, making, and physical computing. 

This repository contains a **TypeScript/JavaScript** client library for the [Jacdac](https://aka.ms/jacdac) protocol.

* **[Jacdac Documentation](https://aka.ms/jacdac/)**
* Discussions at https://github.com/microsoft/jacdac/discussions
* Issues are tracked on https://github.com/microsoft/jacdac/issues

The rest of this page is for developers of the jacdac-ts library.

## Developer setup

* clone this repository and pull all submodules
```
git clone https://github.com/microsoft/jacdac-ts
git submodule update --init --recursive
git pull
```
* install node.js
* install yarn
```
npm install -g yarn
```
* install dependencies
```
yarn install --frozen-lockfile
```

### Visual Studio Code

You are welcome to use any editor you want! Visual Studio Code
provides seamless support for git sub-modules and is our preferred editor.

* open [Visual Studio Code](https://code.visualstudio.com/)
```
code .
```
* install the recommended extensions (**MDX**, **ESLint** and **Prettier** extensions)
* in the Git view, click on the ``jacdac`` branch and select ``main`` so that changes are automatically synched

### Build

To have a watch developement,

```
yarn watch
```

otherwise to build all libraries

```
yarn dist
```

### Specs build

To regenerate the service definition JSON files from the ``.md`` files in jacdac-spec,
run

```
yarn buildspecs
```

## Unit tests

We use [Mocha](https://mochajs.org/) to run the unit test suite from ``/tests``. To execute the tests,

```
yarn test
```

## Linting

Run the following command to detect linting issues

```
yarn lint
```

### Jacdac + MakeCode

### Local build

Run this command to rebuild the makecode packages

```
yarn buildpxt
```

### HTML Tools

Launch a small web server and 
try all the tools under ``/tools/*`` at [http://localhost:8080/tools](http://localhost:8080/tools) . These tools load the files under ``dist`` so you'll want 
to also run ``yarn watch`` on the side.

```
yarn tools
```

These tools are also available on the [GitHub pages](https://microsoft.github.io/jacdac-ts/) of this repository:

* [console](https://microsoft.github.io/jacdac-ts/tools/console.html)
* [devices](https://microsoft.github.io/jacdac-ts/tools/devices.html)
* [flashing](https://microsoft.github.io/jacdac-ts/tools/flashing.html)

Experimental...

* [namer](https://microsoft.github.io/jacdac-ts/tools/namer.html)
* [tfite](https://microsoft.github.io/jacdac-ts/tools/tflite.html)
* [streaming](https://microsoft.github.io/jacdac-ts/tools/streaming.html)
* [streaming-rickshaw](https://microsoft.github.io/jacdac-ts/tools/streaming-rickshaw.html)
* [peer.js](https://microsoft.github.io/jacdac-ts/tools/peerjs.html)

### Commits create releases

The releases are automatically created by the build system based on the title of the commit:

* ``patch:...`` or ``fix:...``  patch
* ``minor:...`` or ``feature:...`` minor

## Microsoft Open Source Code of Conduct

This project is hosted at https://github.com/microsoft/jacdac-ts. 
This project has adopted the 
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns
"
58,microsoft/google-play-vsts-extension,TypeScript,"<table style=""width: 100%; border-style: none;""><tr>
<td width=""140px"" style=""text-align: center;""><img src=""android_default.png"" style=""max-width:100%"" /></td>
<td><strong>Visual Studio Team Services Extension for Google Play</strong><br />
<i>Provides build/release tasks that enable performing continuous delivery to the Google Play store from an automated VSTS build or release definition</i><br />
<a href=""https://marketplace.visualstudio.com/items/ms-vsclient.google-play"">Install now!</a>
</td>
</tr></table>

# Visual Studio Team Services Extension for Google Play

[![Build status](https://dev.azure.com/mseng/AzureDevOps/_apis/build/status/CrossPlatform.google-play-vsts-extension.GitHub.CI)](https://dev.azure.com/mseng/AzureDevOps/_build/latest?definitionId=5350)

This extension contains a set of deployment tasks which allow you to automate the release, promotion and rollout of app updates to the Google Play store from your CI environment. This can reduce the effort needed to keep your internal test, alpha, beta, rollout and production deployments up-to-date, since you can simply push changes to the configured source control branches, and let your automated build take care of the rest.

## Prerequisites

This extension supports Visual Studio Team Services (VSTS) and Team Foundation Server (TFS) 2017 and later.

In order to automate the release of app updates to the Google Play store, you need to have manually released at least one version through the [Google Play Developer Console](https://play.google.com/apps/publish/). Additionally, you need to create a service account that is authorized to manage your app(s) releases on your behalf and can be used to authenticate ""headlessly"" from your VSTS build/release definitions. If you haven't already done so, then perform the following steps to create a service account:
> For a more in depth guide [click this link](https://docs.microsoft.com/en-us/appcenter/distribution/stores/googleplay).

1. Login to the [Google Play Developer Console](https://play.google.com/apps/publish/) and select **Settings** in the left-hand navigation menu (the gear icon)

2. Select the **API access** setting and click the **Create Service Account** button underneath the **Service Accounts** section

3. Follow the provided **Google Developers Console** hyperlink

4. Click the **Create credentials** button in the displayed modal dialog, and select **Service account key** (with the role ""Owner"")

5. Select **JSON** as the **Key type** and click the **Create** button

6. Save the provided JSON file somewhere safe and memorable. You'll be using it later.

7. Go to the **IAM** page and click on the **Add** button.

8. Select the newly created service account in the **New members** box and assign it the  **Service Account User Role**, then click **Save**.

9. Back in the **Google Play Developer Console**, click the **Done** button to close the modal

10. Click the **Grant access** button in the row associated with the service account you just created.
 
11. Ensure that the **Role** is set to **Release Manager** and then click the **Add user** button

To take advantage of the metadata updating capabilities, files need to be organized using fastlane’s [supply tool](https://github.com/fastlane/fastlane/tree/master/supply#readme) format:

1. Install the supply tool
```
sudo gem install supply
```
2. Navigate to your root folder 
```
cd [your_project_folder]
```
3. Download metadata for an existing app to the  project folder
```
supply init
```

## Quick Start

Once you have created or retrieved credentials for you Google Play service account, then perform the following steps to automate releasing updates from a VSTS build or release definition:

1. Install the Google Play extension from the [VSTS Marketplace](https://marketplace.visualstudio.com/items/ms-vsclient.google-play)

2. Go to your Visual Studio Team Services or TFS project, click on the **Build** tab, and create a new build definition (the ""+"" icon) that is hooked up to your project's appropriate source repo

3. Click **Add build step...** and select the neccessary tasks to generate your release assets (e.g. **Gulp**, **Cordova Build**)

4. Click **Add build step...** and select **Google Play - Release** from the **Deploy** category

5. Configure the **Google Play - Release** task with the JSON private key file created above, the generated APK file, and the desired release track.

6. Click the **Queue Build** button or push a change to your configured repo in order to run the newly defined build pipeline

7. Your app changes will now be automatically published to the Google Play store!

## Configuring Your Google Play Publisher Credentials

In addition to specifying your publisher credentials file directly within each build task, you can also configure your credentials globally and refer to them within each build or release definition as needed. To do this, perform the following steps:

1. Setup a publishing manager (https://play.google.com/apps/publish/) and get the JSON key file from the [Google Developer API console](https://console.developers.google.com/apis)

2. Go into your Visual Studio Team Services or TFS project and click on the gear icon in the lower left corner

3. Click on the **Service Connections** tab

4. Click on **New service connection** and select **Google Play**

5. Give the new endpoint a name and enter the credentials for the publishing manager you generated in step#1. The credentials you need can be found in the JSON file and are the Email and the private key.

6. Select this endpoint via the name you chose in #5 whenever you add either the **Google Play - Release** or **Google Play - Promote** tasks to a build or release definition

## Task Reference

In addition to the custom service endpoint, this extension also contributes the following three build and release tasks:

* [Google Play - Release](#google-play---release) - Allows automating the release of a new Android app version to the Google Play store.

* [Google Play - Promote](#google-play---promote) - Allows automating the promotion of a previously released Android app update from one track to another (e.g. `alpha` -> `beta`).

* [Google Play - Increase Rollout](#google-play---increase-rollout) - Allows automating increasing the rollout percentage of a previous release app update.

* [Google Play - Release Bundle](#google-play---release-bundle) - Allows automating the release of a new Android bundle to the Google Play store.

### Google Play - Release

Allows you to release an update to your app on Google Play, and includes the following options:

1. **JSON Key Path** *(File path)* or **Service Endpoint** - The credentials used to authenticate with Google Play. This can be acquired from the [Google Developer API console](https://console.developers.google.com/apis) and provided either directly to the task (via the `JSON Auth File` authentication method), 

    ![JSON Auth File](images/auth-with-json-file.png)

    or configured within a service endpoint that you reference from the task (via the `Service Endpoint` authentication method). 

    ![Service Endpoint](images/auth-with-endpoint.png)

    Note that in order to use the JSON Auth File method, the JSON file you get from the developer console needs to be checked into your source repo.


2. **APK Path** *(File path, Required)* - Path to the APK file you want to publish to the specified track.

    ![APK Path](images/apk-path.png)

3. **Track** *(String, Required)* - Release track to publish the APK to.

    ![Track](images/track.png)

4. **Rollout Fraction** *(String, Required if visible)* - The percentage of users to roll the specified APK out to, specified as a number between 0 and 1 (e.g. `0.5` == `50%` of users). This option is only available when the **Track** input is set to **Rollout**.

    ![Rollout Fraction](images/rollout-fraction.png)

5. **Release Notes** *(File path)* - Path to the file specifying the release notes for the APK you are publishing.

    ![Release Notes](images/release-notes.png)

6. **Language Code** *(String, Optional)* - An IETF language tag identifying the language of the release notes as specified in the BCP-47 document. Default value is _en-US_.

7. **Update Metadata** *(Boolean, Optional)* - Allows automating metadata updates to the Google Play store by leveraging the contents of the `Metadata Root Directory`.

    ![Update Metadata](images/update-metadata.png)

8. **Metadata Root Directory** *(String, Required if visible)* - Root directory for metadata related files. Becomes available after enabling the `Update Metadata` option. Expects a format similar to fastlane’s [supply tool](https://github.com/fastlane/fastlane/tree/master/supply#readme) which is summarized below:
 
```
$(Specified Directory)
   └ $(languageCodes)
     ├ full_description.txt
     ├ short_description.txt
     ├ title.txt
     ├ video.txt
     ├ images
     |  ├ featureGraphic.png    || featureGraphic.jpg   || featureGraphic.jpeg
     |  ├ icon.png              || icon.jpg             || icon.jpeg
     |  ├ promoGraphic.png      || promoGraphic.jpg     || promoGraphic.jpeg
     |  ├ tvBanner.png          || tvBanner.jpg         || tvBanner.jpeg
     |  ├ phoneScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ sevenInchScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ tenInchScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ tvScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  └ wearScreenshots
     |     └ *.png || *.jpg || *.jpeg
     └ changelogs
       └ $(versioncodes).txt
```

9. **Update only store listing**  *(Boolean, Optional)* - By default, the task will update the specified track and selected APK file(s) will be assigned to the related track. By selecting this option you can update only store listing. Default value is _false_. 

    ![Advanced Options](images//update-store-listing.png)

10. **Update APK(s)** *(Boolean, Optional)* - By default, the task will update the specified binary APK file(s) on your app release. By unselecting this option you can update metadata keeping the APKs untouched. Default value is _true_.

    ![Update APKs](images/update-apks.png)

#### Advanced Options

1. **Additional APK Path(s)** *(Text box)* - Paths to additional APK files you want to publish to the specified track (e.g. an x86 build) separated by new lines. This option allows the usage of wildcards and/or minimatch patterns. For example, **/*.apk to match the first APK file, in any directory.

    ![Advanced Options](images/advanced-options.png)

2. **Replace version codes** *(String, Required)* - You may specify which APK version codes should be replaced in the track with this deployment. Available options are: *All*, *List* - comma separated list of version codes, *Regular expression* - a regular expression pattern to select a list of APK version codes to be removed from the track with this deployment, e.g. _.\\*12?(3|4)?5_ 

### Google Play - Promote

Allows you to promote a previously released APK from one track to another (e.g. `alpha` -> `beta`), and includes the following options:

![Promote task](images/promote-task.png)

1. **JSON Key Path** *(File path)* or **Service Endpoint** - The credentials used to authenticate with Google Play. This can be acquired from the [Google Developer API console](https://console.developers.google.com/apis) and provided either directly to the task (via the `JSON Auth File` authentication method), or configured within a service endpoint that you reference from the task (via the `Service Endpoint` authentication method). Note that in order to use the JSON Auth File method, the JSON file you get from the developer console needs to be checked into your source repo.

2. **Package Name** *(String, Required)* - The unique package identifier (e.g. `com.foo.myapp`) that you wish to promote.

3. **Source Track** *(Required, Required)* - The track you wish to promote your app from (e.g. `alpha`). This assumes that you previously released an update to this track, potentially using the [`Google Play - Release`](#google-play---release) task.

4. **Destination Track** *(Required, Required)* - The track you wish to promote your app to (e.g. `production`).

5. **Rollout Fraction** *(String, Required if visible)* - The percentage of users to roll the app out to, specified as a number between 0 and 1 (e.g. `0.5` == `50%` of users). This option is only available when the **Destination Track** option is set to `Rollout`. If you use rollout, and want to be able to automate the process of increasing the rollout over time, refer to the `Google Play - Increase Rollout` task.

### Google Play - Increase Rollout

Allows you to increase the rollout percentage of an app that was previously released to the **Rollout** track, and includes the following options:

![Increase task](images/increase-task.png)

1. **JSON Key Path** *(File path)* or **Service Endpoint** - The credentials used to authenticate with Google Play. This can be acquired from the [Google Developer API console](https://console.developers.google.com/apis) and provided either directly to the task (via the `JSON Auth File` authentication method), or configured within a service endpoint that you reference from the task (via the `Service Endpoint` authentication method). Note that in order to use the JSON Auth File method, the JSON file you get from the developer console needs to be checked into your source repo.

2. **Package Name** *(String, Required)* - The unique package identifier (e.g. com.foo.myapp) of the app you wish to increase the rollout percentage for.

3. **Rollout Fraction** *(String, Required)* - The new user fraction to increase the rollout to, specified as a number between 0 and 1 (e.g. `0.5` == `50%` of users)

### Google Play - Release Bundle

Allows you to release an app bundle to Google Play, and includes the following options:

1. **JSON Key Path** *(File path)* or **Service Endpoint** - The credentials used to authenticate with Google Play. This can be acquired from the [Google Developer API console](https://console.developers.google.com/apis) and provided either directly to the task (via the `JSON Auth File` authentication method), 

    ![JSON Auth File](images/auth-with-json-file.png)

    or configured within a service endpoint that you reference from the task (via the `Service Endpoint` authentication method). 

    ![Service Endpoint](images/auth-with-endpoint.png)

    Note that in order to use the JSON Auth File method, the JSON file you get from the developer console needs to be checked into your source repo.
    Please note that from the point of security it's preferrable to store it as [Secure file](https://docs.microsoft.com/azure/devops/pipelines/library/secure-files) and download using [Download Secure File task](https://docs.microsoft.com/azure/devops/pipelines/tasks/utility/download-secure-file).


2. **Application Id** *(String, Required)* - The application id of the bundle you want to release, e.g. com.company.MyApp.

    ![Application id](images/bundle-app-id.png)

3. **Bundle Path** *(File path, Required)* - Path to the bundle (.aab) file you want to publish to the specified track. Wildcards can be used. For example, **/*.aab to match the first APK file, in any directory.

4. **Track** *(String, Required)* - Track you want to publish the bundle to.

5. **Roll out Release** *(Boolean, Optional)* - Roll out the release to a percentage of users.

6. **Update Metadata** *(Boolean, Optional)* - Allows automating metadata updates to the Google Play store by leveraging the contents of the `Metadata Root Directory`.

7. **Release Notes (file)** *(File path, Required if visible)* - Path to the file specifying the release notes (change log) for the APK you are publishing.

    ![Release Notes](images/bundle-release-notes.png)

8. **Language Code** *(String, Required if visible)* - An IETF language tag identifying the language of the release notes as specified in the BCP-47 document. Default value is _en-US_.

9. **Deobfuscation Path** *(String, Optional)* - The path to the proguard mapping.txt file to upload.

10. **Rollout Fraction** *(String, Optional)* - The percentage of users the specified APK will be released to for the specified 'Track'. It can be increased later with the 'Google Play - Increase Rollout' task.

11. **Metadata Root Directory** *(String, Required)* - Root directory for metadata related files. Becomes available after enabling the `Update Metadata` option. Expects a format similar to fastlane’s [supply tool](https://github.com/fastlane/fastlane/tree/master/supply#readme) which is summarized below:
 
```
$(Specified Directory)
   └ $(languageCodes)
     ├ full_description.txt
     ├ short_description.txt
     ├ title.txt
     ├ video.txt
     ├ images
     |  ├ featureGraphic.png    || featureGraphic.jpg   || featureGraphic.jpeg
     |  ├ icon.png              || icon.jpg             || icon.jpeg
     |  ├ promoGraphic.png      || promoGraphic.jpg     || promoGraphic.jpeg
     |  ├ tvBanner.png          || tvBanner.jpg         || tvBanner.jpeg
     |  ├ phoneScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ sevenInchScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ tenInchScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  ├ tvScreenshots
     |  |  └ *.png || *.jpg || *.jpeg
     |  └ wearScreenshots
     |     └ *.png || *.jpg || *.jpeg
     └ changelogs
       └ $(versioncodes).txt
```

12. **Upload Deobfuscation File (mapping.txt)** *(Boolean, Optional)* - Select this option to attach your proguard mapping.txt file to the primary APK.

#### Advanced Options

1. **Replace Version Codes** *(String, Optional)* - Specify version codes to replace in the selected track with the new APKs: all, the comma separated list, or a regular expression pattern.

    ![Advanced Options](images/bundle-advanced-options.png)

2. **Version Code List** *(String, Required if visible)* - The comma separated list of APK version codes to be removed from the track with this deployment. Available options are: *All*, *List* - comma separated list of version codes, *Regular expression* - a regular expression pattern to select a list of APK version codes to be removed from the track with this deployment, e.g. _.\\*12?(3|4)?5_ 

3. **Version Code Pattern** *(String, Required if visible)* - The regular expression pattern to select a list of APK version codes to be removed from the track with this deployment, e.g. .*12?(3|4)?5


#### Advanced Options

1. **Replace version codes** *(String, Required)* - You may specify which APK version codes should be replaced in the track with this deployment. Available options are: *All*, *List* - comma separated list of version codes, *Regular expression* - a regular expression pattern to select a list of APK version codes to be removed from the track with this deployment, e.g. _.\\*12?(3|4)?5_ 

## Contact Us

* [Report an issue](https://github.com/Microsoft/google-play-vsts-extension/issues)

Google Play and the Google Play logo are trademarks of Google Inc.


This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
59,microsoft/Bing-Maps-Fleet-Tracker,C#,"# Bing Maps Fleet Tracker

Bing Maps Fleet Tracker is a fleet management solution based on the Bing Maps APIs. It offers functionalities that enable tracking and managing a fleet of vehicles in real time. For more details, read this [blog post](https://aka.ms/bingmapsfleettrackerblog).

## Getting Started

There are two parts to the set-up process:
1. Set up the back-end services and administration portal;
2. Set up the mobile client.

#### Step 1: 
The easiest way to set up the back-end services and administration portal is to use the [Bing Maps Fleet Tracker Deployment Portal](https://bingmapsfleettracker.azurewebsites.net), which one-click deploys the Bing Maps Fleet Tracker solution into your Azure subscription. For a step by step walk-through of how to deploy, use the deployment portal: see [out of the box deployment](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/DEPLOYMENT.md). **None** of the data entered on the deployment portal is stored by Bing Maps Fleet Tracker. It is only used to configure your deployment.

After deploying the back-end services and administration portal successfully, you can find the walk-through of using the Bing Maps Fleet Tracker solution [here](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/WALKTHROUGH.md).

You can also build and run the back-end services and administration portal from source as detailed [here](#build-and-run).

#### Step 2:
##### Android client:
The easiest way to set up Android client is to download the prebuilt apk from [here](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/releases). You can also build and run the Android client from source as detailed [here](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/MobileClient/README.md#android).

##### iOS client:
You will need to build and run the iOS client from source as detailed [here](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/MobileClient/README.md#ios).

## Build and Run

The Bing Maps Fleet Tracker solution consist of 3 major components:

* [The Backend Services](#backend-services)
* [The Administration Portal](#administration-portal)
* [The Mobile Client](#mobile-client)

Each of these components can be built and run separately, and each can be replaced with implementations of your own.

#### Backend Services

The backend services are responsible for the collection and processing of location information, the provisioning of tracking devices and assets, dispatching, and report generation. They are made up of an ASP.NET Core 2 service, and two Azure Functions. For more information on building an running the backend services see [Backend\README.md](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/Backend/README.md).

#### Administration Portal

The administration portal is an angular application that exposes the functionalities of the backend to the deployment administrators. It can be used to view reports, track and provision assets, and compare dispatching routes. For more information on building an running the administration portal see [Frontend\README.md](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/Frontend/README.md).

#### Mobile Client

The mobile client is an ionic mobile application responsible for collection of asset location information and the forwarding of this information to the backend. It is meant to provide an out of the box background tracking solution but it is not the only way to integrate with Bing Maps Fleet Tracker; any GPS device with an internet connection can be used to provide the background tracking (see [Using the Rest APIs](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/WALKTHROUGH.md#using-the-rest-apis)). For more information on building and running the mobile client see [MobileClient\README.md](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/blob/master/MobileClient/README.md).

## Telemetry collected by Bing Maps Fleet Tracker

After deployment, on your first use of the application, you will be prompted to allow us to collect
anonymous aggregate telemetry and error/warning log data. We use the telemetry data to get a feel for the
usage of this project. Error log data are used to focus our efforts on fixing the issues you face.

Here is a list of all the telemetry items we collect:

* Deployment Id: this is a random GUID that is unique for each deployment.
* Assets count: the total number of vehicles registered.
* Locations count: the total number of locations stored.
* Auto locations count: the number of automatically detected locations.
* Tracking devices count: the total number of tracking devices.
* Geo-fences count: the number of geo-fences setup.
* Tracking points count: the number of tracking points stored on the system.

Error/warning log data collected consist of:

* Deployment Id: this is a random GUID that is unique for each deployment.
* Software version: the version of the software causing the error/warning.
* Error/warning message: the error/warning message.

You can always enable/disable the data collection from the settings tab in your administration portal.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [https://cla.microsoft.com](https://cla.microsoft.com).

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## History

See [release history](https://github.com/Microsoft/Bing-Maps-Fleet-Tracker/releases).
"
60,microsoft/redux-dynamic-modules,TypeScript,"<div>
<img src=""docs/redux-dynamic-modules.png"" alt=""logo"" width=""100"">
</img>
<h1>Redux Dynamic Modules</h1<
</div>

[![Pipelines](https://dev.azure.com/redux-dynamic-modules/redux-dynamic-modules/_apis/build/status/Microsoft.redux-dynamic-modules?branchName=master)](https://dev.azure.com/redux-dynamic-modules/redux-dynamic-modules/redux-dynamic-modules%20Team/_build?definitionId=1&_a=summary) [![npm (scoped)](https://img.shields.io/npm/v/redux-dynamic-modules.svg)](https://npmjs.org/package/redux-dynamic-modules) [![Join the chat at https://gitter.im/redux-dynamic-modules/community](https://badges.gitter.im/redux-dynamic-modules/community.svg)](https://gitter.im/redux-dynamic-modules/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) ![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)

## What is it?

**redux-dynamic-modules** is a library that aims to make Redux Reducers and middleware easy to modular-ize and add/remove dynamically.

## Motivation

In large Javascript applications, it is often desired to perform some kind of code-splitting, so that the initial script size is small. However, in Redux, you are required to define your reducers and middleware up-front; there is no good way to dynamically add/remove these constructs at runtime.

**redux-dynamic-modules** is designed to make dynamically loading these constructs easier. You can define a **module**, which contains reducers and middleware, and add it to the Redux store at runtime. We also provide a React component `DynamicModuleLoader`, which can load/unload modules on mount/unmount.

Modules provide the following benefits:

-   Modules can be easily re-used across the application, or between multiple similar applications.
-   Components declare the modules needed by them and redux-dynamic-modules ensures that the module is loaded for the component.
-   Modules can be added/removed from the store dynamically, ex. when a component mounts or when a user performs an action

## Features

-   Group together reducers, middleware, and state into a single, re-usable module.
-   Add and remove modules from a Redux store at any time.
-   Use the included `<DynamicModuleLoader />` component to automatically add a module when a component is rendered
-   Extensions provide integration with popular libraries, including `redux-saga` and `redux-thunk`

## Example Scenarios

-   You don't want to load the code for all your reducers up front. Define a module for some reducers and use `DynamicModuleLoader` and a library like [react-loadable](https://github.com/jamiebuilds/react-loadable) to download and add your module at runtime.
-   You have some common reducers/middleware that need to be re-used in different areas of your application. Define a module and easily include it in those areas.
-   You have a mono-repo that contains multiple applications which share similar state. Create a package containing some modules and re-use them across your applications

## Install

Run

```
npm install redux-dynamic-modules
```

or

```
yarn add redux-dynamic-modules
```

## Usage

-   Create a module with the following format

```typescript
export function getUsersModule(): IModule<IUserState> {
    return {
        id: ""users"",
        reducerMap: {
            users: usersReducer,
        },
        // Actions to fire when this module is added/removed
        // initialActions: [],
        // finalActions: []
    };
}
```

-   Create a `ModuleStore`

```typescript
import { createStore, IModuleStore } from ""redux-dynamic-modules"";
import { getUsersModule } from ""./usersModule"";

const store: IModuleStore<IState> = createStore({
      initialState: { /** initial state */ },
      enhancers: [ /** enhancers to include */ ], 
      extensions: [/** extensions to include i.e. getSagaExtension(), getThunkExtension() */],
},
    getUsersModule()
    /* ...any additional modules */
);
```

-   Use like a standard Redux store
-   Use the `DynamicModuleLoader` React component to add/remove modules when components mount/unmount

```jsx
<DynamicModuleLoader modules={[getHelloWorldModule()]}>
    <div>Hello World!!</div>
</DynamicModuleLoader>
```

## Extensions

-   redux-saga - [redux-dynamic-modules-saga](https://www.npmjs.com/package/redux-dynamic-modules-saga)
-   redux-thunk - [redux-dynamic-modules-thunk](https://www.npmjs.com/package/redux-dynamic-modules-thunk)

## Examples

See the [Widgets](https://github.com/Microsoft/redux-dynamic-modules/tree/master/packages/widgets-example) for a quick of example of the library in practice.

Step by step walthrough of start consuming `redux-dynamic-modules` in the widget app. [Youtube](https://www.youtube.com/watch?v=SktRbSZ-4Tk)

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
61,microsoft/azure_arc,PowerShell,"# Azure Arc Jumpstart documentation

If you are looking to explore the Jumpstart documentation, please go to the documentation website:

https://azurearcjumpstart.io

This repository contains the markdown files which generate the above website. See below for guidance on running with a local environment to contribute to the docs.

## Want to help and contribute?

Before making your first contribution, make sure to review the [contributing](https://azurearcjumpstart.io/contributing/) section in the docs.

* Found a bug?! Use the [Bug Report](https://github.com/microsoft/azure_arc/issues/new?assignees=&labels=bug&template=bug_report.md&title=) issue template to let us know.

* To ask for a Jumpstart scenario, create one yourself or submit an Azure Arc core product feature request, use the [Feature Request](https://github.com/microsoft/azure_arc/issues/new?assignees=&labels=&template=feature_request.md&title=) issue template.

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Jumpstart Roadmap

Up-to-date roadmap for the Azure Arc Jumpstart scenarios can be found under [the repository GitHub Project](https://github.com/microsoft/azure_arc/projects/1).

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
62,microsoft/Cloud-Native-In-a-Day,CSS,"# Cloud Native In a Day

This workshop is designed for Microsoft partners to drive the adoption of cloud native technologies on Azure across customers. It includes whiteboard design session materials and hands-on lab. Content is based on Microsoft Cloud Workshops, that are widely used by Microsoft specialists and solution architects.

April 2020

## Before you start

- Take a look at a partner marketing materials for [Cloud Native In a Day](https://partner.microsoft.com/en-us/asset/collection/cloud-native-in-a-day)
- Review [How to deliver Cloud Native In a Day workshop](https://note.microsoft.com/US-NOGEP-WBNR-FY20-05May-14-CloudNativeApps-SRDEM17754_Registration.html) session recording

## Abstracts

### Workshop

Fabrikam Medical Conferences provides conference web site services, tailored to the medical community. Their business has grown and the management of many instances of the code base and change cycle per tenant has gotten out of control.

The goal of this workshop is to help them build a proof of concept (POC) that will migrate their code to a more manageable process that involves containerization of tenant code, a better DevOps workflow, and a simple lift-and-shift story for their database backend.

In this workshop, you will build a proof of concept (POC) that will transform an existing on-premises application to a container-based application. This POC will deliver a multi-tenant web app hosting solution leveraging Azure Kubernetes Service (AKS), Docker containers on Linux nodes, and a migration from MongoDB to Cosmos DB.

At the end of this workshop, you will be better able to improve the reliability of and increase the release cadence of your container-based applications through time-tested DevOps practices.

### Part I: Presentation
Start from presenting Cloud Native applications on Azure story using the [included deck.](Presentation) Presentation should take 60-90 minutes and introduce the aspects of cloud native apps and value that they bring.

### Part II: Whiteboard Design Session

During [whiteboard design session](Whiteboard%20design%20session), you will discuss the choices related to building and deploying containerized applications in Azure, critical decisions around this, and other aspects of the solution, including ways to lift-and-shift parts of the application to reduce applications changes.

By the end of this design session, you will design solutions that target Azure Kubernetes Service (AKS) and define a DevOps workflow for containerized applications.

Package includes presenter deck, presenter guide and student guide.

If you are delivering the session remotely, we suggest to use [Microsoft Teams](https://products.office.com/microsoft-teams) and [Microsoft Whiteboard.](https://whiteboard.microsoft.com)

### Part III: Hands-on Lab

[Hands-on lab](Hands-on%20lab) is designed to guide attendees through the process of building and deploying Docker images to the Kubernetes platform hosted on Azure Kubernetes Services (AKS), in addition to learning how to work with dynamic service discovery, service scale-out, and high-availability.

At the end of this lab, attendees will be better able to build and deploy containerized applications to Azure Kubernetes Service and perform common DevOps procedures.

## Azure services and related products

- Azure Kubernetes Service (AKS)
- Azure Container Registry
- Azure DevOps
- Docker
- Cosmos DB (including MongoDB API)

## Related references

- [MCW](https://github.com/Microsoft/MCW)

## Help & Support

We welcome feedback and comments from Microsoft SMEs & learning partners who deliver MCWs.  

***Having trouble?***
- First, verify you have followed all written lab instructions (including the Before the Hands-on lab document).
- Next, create a new Issue in the repo with detailed description of the issue and any troubleshooting steps performed.

If you are planning to present a workshop, *review and test the materials early*! We recommend at least two weeks prior.

Leave your comments by creating a new topic in Issues section or send an email to appinnovationinaday@microsoft.com with any questions.
"
63,microsoft/vscode-java-pack,TypeScript,"# Java Extension Pack

Java Extension Pack is a collection of popular extensions that can help write, test and debug Java applications in Visual Studio Code. Check out [Java in VS Code](https://code.visualstudio.com/docs/languages/java) to get started.

## Extensions Included

By installing Java Extension Pack, the following extensions are installed:

- [📦 Language Support for Java™ by Red Hat ](https://marketplace.visualstudio.com/items?itemName=redhat.java)
    - Code Navigation
    - Auto Completion
    - Refactoring
    - Code Snippets
- [📦 Debugger for Java](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-debug)
    - Debugging
- [📦 Java Test Runner](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-test)
    - Run & Debug JUnit/TestNG Test Cases
- [📦 Maven for Java](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-maven)
    - Project Scaffolding
    - Custom Goals
- [📦 Project Manager for Java](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-dependency)
    - Manage Java projects, referenced libraries, resource files, packages, classes, and class members
- [📦 Visual Studio IntelliCode](https://marketplace.visualstudio.com/items?itemName=VisualStudioExptTeam.vscodeintellicode)
    - AI-assisted development
    - Completion list ranked by AI

## Other Recommendations

You can do more with VS Code. Here are some more recommendations that could help.

### Spring

Spring Tools 4 (ST4) is also available in Visual Studio Code. It understands Spring so you can navigate Spring code at the level of beans, routes, etc. It can also show live information of the running Spring Boot applications. Check out [the ST4 website](https://spring.io/tools) to see a complete list of its features.

To use ST4, install [📦 Spring Boot Extension Pack](https://marketplace.visualstudio.com/items?itemName=Pivotal.vscode-boot-dev-pack). Please also check out the [User Guide](https://github.com/spring-projects/sts4/wiki) to make the most of it.

### Eclipse MicroProfile

The [📦 Extension Pack for MicroProfile](https://marketplace.visualstudio.com/items?itemName=MicroProfile-Community.vscode-microprofile-pack) is a collection of extensions that can help develop your Java microservices using [Eclipse MicroProfile](https://microprofile.io/). You can quickly generate a MicroProfile project and utilize development tools for runtimes such as [Open Liberty](https://openliberty.io/) and [Quarkus](https://quarkus.io/).

### Quarkus

[📦 Quarkus Tools for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-quarkus) is a feature-packed extension tailored for Quarkus application
development within Visual Studio Code. You can quickly get started by using the extension's
project generation and project debugging feature. The extension also provides amazing
language features (completion, hover, validation etc.) for your project's application.properties file.

### Containers and Microservices

You can use [📦 Docker](https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker) extension to build docker images and work with image registries.

[📦 Kubernetes](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-kubernetes-tools) extension provides an explorer view to manage clusters and the nodes inside. It also provides advanced syntax support for editing Kubernetes manifest files.

### Tomcat and Jetty

Both [📦 Tomcat](https://marketplace.visualstudio.com/items?itemName=adashen.vscode-tomcat) and [📦 Jetty](https://marketplace.visualstudio.com/items?itemName=SummerSun.vscode-jetty) extension are available. They provide dedicated views to help work with your favorite web servers.

### Linting
The [📦 SonarLint](https://marketplace.visualstudio.com/items?itemName=SonarSource.sonarlint-vscode) extension lets you detect bugs and vulnerabilities as you write code in VS Code. The extension will simply run in the background and highlight code that poses a quality or security concern.

At the same time, [📦 CheckStyle](https://marketplace.visualstudio.com/items?itemName=shengchen.vscode-checkstyle) is also available.

## Questions & Issues

Each extension mentioned above is a separate open-source project and has its own repository. To make things easier, simply [🙋 open an issue in this repository](https://github.com/Microsoft/vscode-java-pack/issues). The new issue will be triaged and redirected.

## Telemetry

This extension collects telemetry data to help us build a better experience for building Java applications with VS Code. We only collect data on which commands are executed. We do not collect any information about names, addresses, paths, etc. The extension respects the `telemetry.enableTelemetry` setting which you can learn more about in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).

## License

[MIT](https://github.com/Microsoft/vscode-java-pack/blob/master/LICENSE.txt)

Happy Coding!
"
64,microsoft/tfs-cli,TypeScript,"# Node CLI for Azure DevOps

> NOTE: If you are looking for the new Azure DevOps CLI, see [vsts-cli](https://github.com/microsoft/vsts-cli)

[![NPM version](https://badge.fury.io/js/tfx-cli.svg)](http://badge.fury.io/js/tfx-cli)

Command utility for interacting with Microsoft Team Foundation Server and Azure DevOps Services (formerly VSTS). It is cross platform and supported on Windows, OS X, and Linux.

## Setup

First, download and install [Node.js](http://nodejs.org) 4.0.x or later and NPM (included with the installer)

### Linux/OSX
```bash
sudo npm install -g tfx-cli
```

### Windows
```bash
npm install -g tfx-cli
```

## Commands

To see a list of commands:
```
tfx
```

For help with an individual command:
```
tfx <command> --help
```

> Help info is dynamically generated, so it should always be the most up-to-date authority.

### Command sets

* `tfx build` ([builds](docs/builds.md)): Queue, view, and get details for builds
* `tfx build tasks` ([build tasks](docs/buildtasks.md)): Create, list, upload and delete build tasks
* `tfx extension` ([extensions](docs/extensions.md)): Package, manage, publish Team Foundation Server / Azure DevOps extensions
* `tfx workitem` ([work items](docs/workitems.md)): Create, query and view work items.

### Login

To avoid providing credentials with every command, you can login once. Currently supported credential types: Personal Access Tokens and basic auth credentials.

> NTLM support is under consideration

> Warning! Using this feature will store your login credentials on disk in plain text.

#### Personal access token

Start by [creating a personal access token](http://roadtoalm.com/2015/07/22/using-personal-access-tokens-to-access-visual-studio-online) and paste it into the login command.

```bash
~$ tfx login
Copyright Microsoft Corporation

> Service URL: {url}
> Personal access token: xxxxxxxxxxxx
Logged in successfully
```

Examples of valid URLs are:

* `https://marketplace.visualstudio.com` 
* `https://youraccount.visualstudio.com/DefaultCollection`

#### Basic auth

You can alternatively use basic auth by passing `--auth-type basic` (see [Configuring Basic Auth](docs/configureBasicAuth.md)).

### Settings cache

To avoid providing other options in every command, you can save options out to a settings file by adding the `--save` flag.

```bash
~$ tfx build list --project MyProject --definition-name println --top 5 --save

...

id              : 1
definition name : TestDefinition
requested by    : Teddy Ward
status          : NotStarted
queue time      : Fri Aug 21 2015 15:07:49 GMT-0400 (Eastern Daylight Time)

~$ tfx build list
Copyright Microsoft Corporation

...

id              : 1
definition name : TestDefinition
requested by    : Teddy Ward
status          : NotStarted
queue time      : Fri Aug 21 2015 15:07:49 GMT-0400 (Eastern Daylight Time)
```

If you used `--save` to set a default value for an option, you may need to override it by explicitly providing the option with a different value. You can clear any saved settings by running `tfx reset`. 

### Troubleshooting

To see detailed tracing output, you can set a value for the `TFX_TRACE` environment value and then run commands.  That may offer a clue into the problem (and will certainly help if logging an issue).

### Linux/OSX
```bash
export TFX_TRACE=1
```

### Windows
```bash
set TFX_TRACE=1
```

### PowerShell
```bash
$env:TFX_TRACE=1
```

## Contributing

We take contributions and fixes via Pull Request. [Read here](docs/contributions.md) for the details.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
65,microsoft/vscode-edge-debug2,TypeScript,"<h1 align=""center"">
  <br>
  VS Code - Debugger for Microsoft Edge
  <br>
</h1>

<h4 align=""center"">Debug your JavaScript code running in Microsoft Edge from VS Code and Visual Studio.</h4>

A VS Code extension to debug your JavaScript code in the Microsoft Edge browser. This is also used to enable JavaScript debugging inside the Microsoft Edge browser when launched from ASP.Net Projects in Visual Studio.

**Note:** This extension currently supports both Microsoft Edge (Chromium) and Microsoft Edge (EdgeHTML). This extension can debug any version of Microsoft Edge (Chromium) but only some versions of Microsoft Edge (EdgeHTML). To see if your Windows version supports debugging Microsoft Edge (EdgeHTML) via Edge DevTools Protocol, please refer [here](https://docs.microsoft.com/en-us/microsoft-edge/devtools-protocol/).

**Supported features**
* Setting breakpoints, including in source files when source maps are enabled
* Stepping through the code
* The Locals pane
* Debugging eval scripts, script tags, and scripts that are added dynamically
* Watches

**Unsupported scenarios**
* Debugging web workers
* Any features that aren't script debugging.

## Getting Started

### For debugging inside VS Code
1. [Install the extension.](https://marketplace.visualstudio.com/items?itemName=msjsdiag.debugger-for-edge)
2. Open the folder containing the project you want to work on.

### For debugging Microsoft Edge (EdgeHTML or Chromium) inside Visual Studio
1. Install a supported version of Windows.
2. Install the latest version of Visual Studio. Debugging Microsoft Edge (EdgeHTML) is supported for VS versions >= 15.7. Debugging Microsoft Edge (Chromium) is supported for VS versions >= 15.9.19.
3. Create an ASP.Net/ASP.Net Core Web Application.
4. Set a breakpoint in your JavaScript/TypeScript file.
5. Select 'Microsoft Edge' from the 'Web Browser' submenu in the debug target dropdown, and then press F5.

### For enabling both Microsoft Edge (EdgeHTML) and Microsoft Edge (Chromium) in Visual Studio
By default, installing Microsoft Edge (Chromium) will overwrite Microsoft Edge (EdgeHTML). To enable both browsers:
1. [Download Microsoft Edge group policy templates.](https://www.microsoftedgeinsider.com/en-us/enterprise)
2. After extracting the template files above, copy the files as shown below:

Source | Destination
--- | ---
<zip&#x2011;extract&#x2011;location>\MicrosoftEdgePolicyTemplates\windows\admx\\*.admx | C:\Windows\PolicyDefinitions
<zip&#x2011;extract&#x2011;location>\MicrosoftEdgePolicyTemplates\windows\admx\\<your-locale\>\\*.adml | C:\Windows\PolicyDefinitions\\<your-locale\>

3. Follow [these instructions](https://docs.microsoft.com/en-us/deployedge/microsoft-edge-sysupdate-access-old-edge) to enable side by side installations.

## Using the debugger
When your launch config is set up, you can debug your project. Pick a launch config from the dropdown on the Debug pane in Code. Press the play button or F5 to start.

### Configuration
The extension operates in two modes - it can launch an instance of Microsoft Edge navigated to your app, or it can attach to a running instance of Edge. Both modes require you to be serving your web application from a local web server, which is started from either a VS Code task or from your command-line. Using the `url` parameter you simply tell VS Code which URL to either open or launch in Edge.

You can configure these modes with a `.vscode/launch.json` file in the root directory of your project. You can create this file manually, or Code will create one for you if you try to run your project, and it doesn't exist yet.

### Launch

Below are two example `launch.json` configs with `""request"": ""launch""`. You must specify either `file` or `url` to launch Microsoft Edge against a local file or a url. If you use a url, set `webRoot` to the directory that files are served from. This can be either an absolute path or a path using `${workspaceFolder}` (the folder open in Code). Note that `webRoot` is used to resolve urls (like ""http://localhost/app.js"") to a file on disk (like `/Users/me/project/app.js`), so be careful that it's set correctly.
```json
{
    ""version"": ""0.2.0"",
    ""configurations"": [
        {
            ""name"": ""Launch localhost in Microsoft Edge"",
            ""type"": ""edge"",
            ""request"": ""launch"",
            ""url"": ""http://localhost/mypage.html"",
            ""webRoot"": ""${workspaceFolder}/wwwroot""
        },
        {
            ""name"": ""Launch index.html in Microsoft Edge"",
            ""type"": ""edge"",
            ""request"": ""launch"",
            ""file"": ""${workspaceFolder}/index.html""
        },
    ]
}
```

#### Microsoft Edge (Chromium)
If the stable release of Microsoft Edge (Chromium) is on your machine, this debug adapter will launch it by default. If you'd like to launch a different channel of Microsoft Edge (Chromium), simply add a `version` attribute to your existing configuration with the version you want to launch (`dev`, `beta`, or `canary`). The example configuration below will launch the Canary version of Microsoft Edge (Chromium):
```json
{
    ""name"": ""Launch localhost in Microsoft Edge (Chromium) Canary"",
    ""type"": ""edge"",
    ""request"": ""launch"",
    ""version"": ""canary"",
    ""url"": ""http://localhost/mypage.html"",
    ""webRoot"": ""${workspaceFolder}/wwwroot""
}
```

If you want to use a different installation of a Chromium-based browser, you can also set the `runtimeExecutable` field with a path to the browser executable. Note that if you are using the `runtimeExecutable` flag, you should **not** be using `version`.

#### Microsoft Edge (EdgeHTML)
If you do **not** have the stable release of Microsoft Edge (Chromium) on your machine, the debug adapter will launch Microsoft Edge (EdgeHTML) by default. You will have the same default configuration as above.

### Attach
With `""request"": ""attach""`, you must launch Microsoft Edge with remote debugging enabled in order for the extension to attach to it. Here's how you can do that:

__Windows__
* Open the Command Prompt
* Run `msedge.exe --remote-debugging-port=2015` for Microsoft Edge (Chromium) or `microsoftedge.exe --devtools-server-port=2015` for Microsoft Edge (EdgeHTML)

The example `launch.json` config below will attach to either Microsoft Edge (Chromium) or Microsoft Edge (EdgeHTML) depending on which one you launched on port `2015`.
```json
{
    ""version"": ""0.2.0"",
    ""configurations"": [
        {
            ""type"": ""edge"",
            ""request"": ""attach"",
            ""name"": ""Attach to Microsoft Edge"",
            ""port"": 2015,
            ""webRoot"": ""${workspaceFolder}""
        }
    ]
}
```

### Other optional launch config fields
* `trace`: When true, the adapter logs its own diagnostic info to a file. The file path will be printed in the Debug Console. This is often useful info to include when filing an issue on GitHub. If you set it to ""verbose"", it will log to a file and also log to the console.
* `version`: When set to `canary`, `dev`, or `beta`, it will launch the matching version of Microsoft Edge (Chromium). If not specified, Microsoft Edge (EdgeHTML) will be launched.
* `runtimeExecutable`: Workspace relative or absolute path to the runtime executable to be used. If not specified, Microsoft Edge (EdgeHTML) will be used from the default install location.
* `runtimeArgs`: Optional arguments passed to the runtime executable.
* `env`: Optional dictionary of environment key/value pairs.
* `cwd`: Optional working directory for the runtime executable.
* `userDataDir`: Normally, if Microsoft Edge is already running when you start debugging with a launch config, then the new instance won't start in remote debugging mode. So by default, the extension launches Microsoft Edge with a separate user profile in a temp folder. Use this option to set a different path to use, or set to false to launch with your default user profile. Note that this is only applicable to Microsoft Edge (Chromium) and will not work with Microsoft Edge (EdgeHTML).
* `url`: On a 'launch' config, it will launch Microsoft Edge at this URL.
* `urlFilter`: On an 'attach' config, or a 'launch' config with no 'url' set, search for a page with this url and attach to it. It can also contain wildcards, for example, `""localhost:*/app""` will match either `""http://localhost:123/app""` or `""http://localhost:456/app""`, but not `""https://stackoverflow.com""`.
* `targetTypes`: On an 'attach' config, or a 'launch' config with no 'url' set, set a list of acceptable target types from the default `[""page""]`. For example, if you are attaching to an Electron app, you might want to set this to `[""page"", ""webview""]`. A value of `null` disables filtering by target type. Note that this is only applicable to Microsoft Edge (Chromium) and will not work with Microsoft Edge (EdgeHTML).
* `sourceMaps`: By default, the adapter will use sourcemaps and your original sources whenever possible. You can disable this by setting `sourceMaps` to false.
* `pathMapping`: This property takes a mapping of URL paths to local paths, to give you more flexibility in how URLs are resolved to local files. `""webRoot"": ""${workspaceFolder}""` is just shorthand for a pathMapping like `{ ""/"": ""${workspaceFolder}"" }`.
* `smartStep`: Automatically steps over code that doesn't map to source files. Especially useful for debugging with async/await.
* `disableNetworkCache`: If true, the network cache will be disabled.
* `showAsyncStacks`: If true, callstacks across async calls (like `setTimeout`, `fetch`, resolved Promises, etc) will be shown.
* `useWebView`: If true or `advanced`, the debugger will treat the `runtimeExecutable` as an application hosting a WebView. See: [Microsoft Edge (Chromium) WebView applications](#Microsoft-Edge-(Chromium)-WebView-applications)

### Microsoft Edge (Chromium) WebView applications
You can also use the debugger to launch applications that are using an embedded [Microsoft Edge (Chromium) WebView](https://docs.microsoft.com/en-us/microsoft-edge/hosting/webview2). With the correct `launch.json` properties, the debugger will launch your host application and attach to the WebView allowing you to debug the running script content.

To use the debugger against a WebView application use the following properties in your launch config:
* `runtimeExecutable`: Set this to the full path to your host application.
* `useWebView`: Set this to be `true` or `advanced` depending on how your host application is using WebViews

In basic scenarios, your host application is using a single WebView that is loaded on launch of your application. If this is the case, you should set `useWebView` to be `true`. This will treat the host application just like it was another browser, attaching to the WebView on launch and failing with a timeout if it cannot find a matching `url` or `urlFilter` within the timeout.

In more advanced scenarios, your host appliation may be using a single WebView that doesn't load until later in your workflow. It may also be using multiple WebViews within the same application, or have a dependency on a specific `userDataDir` setting. In these cases you should set `useWebView` to be `advanced`. This will cause the debugger to treat your host application differently. When launching, the debugger will wait until it gets notified of a WebView that matches the `urlFilter` value without timing out. It will also not override the `userDataDir` internally and may attach on a different `port` value than what is specified in the config if several WebViews created in the host application.

### Other targets
You can also theoretically attach to other targets that support the same [Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/tot) as the Microsoft Edge (Chromium) browser, such as Electron or Cordova. These aren't officially supported, but should work with basically the same steps. You can use a launch config by setting `""runtimeExecutable""` to a program or script to launch, or an attach config to attach to a process that's already running. If Code can't find the target, you can always verify that it is actually available by navigating to `http://localhost:<port>/json` in a browser. If you get a response with a bunch of JSON, and can find your target page in that JSON, then the target should be available to this extension.

## Skip files / Mark as Library code
You can use the `skipFiles` property to mark specific files as Library code while debugging. For example, if you set `""skipFiles"": [""jquery.js""]`, then you will skip any file named 'jquery.js' when stepping through your code. You also won't break on exceptions thrown from 'jquery.js'. This works the same as ""Mark as Library code"" in the Microsoft Edge DevTools.

The supported formats are:
  * The name of a file (like `jquery.js`)
  * The name of a folder, under which to skip all scripts (like `node_modules`)
  * A path glob, to skip all scripts that match (like `node_modules/react/*.min.js`)

## Sourcemaps
The debugger uses sourcemaps to let you debug with your original sources, but sometimes the sourcemaps aren't generated properly and overrides are needed. In the config we support `sourceMapPathOverrides`, a mapping of source paths from the sourcemap, to the locations of these sources on disk. Useful when the sourcemap isn't accurate or can't be fixed in the build process.

The left hand side of the mapping is a pattern that can contain a wildcard, and will be tested against the `sourceRoot` + `sources` entry in the source map. If it matches, the source file will be resolved to the path on the right hand side, which should be an absolute path to the source file on disk.

A few mappings are applied by default, corresponding to some common default configs for Webpack and Meteor:
```javascript
// Note: These are the mappings that are included by default out of the box, with examples of how they could be resolved in different scenarios. These are not mappings that would make sense together in one project.
// webRoot = /Users/me/project
""sourceMapPathOverrides"": {
    ""webpack:///./~/*"": ""${webRoot}/node_modules/*"",       // Example: ""webpack:///./~/querystring/index.js"" -> ""/Users/me/project/node_modules/querystring/index.js""
    ""webpack:///./*"":   ""${webRoot}/*"",                    // Example: ""webpack:///./src/app.js"" -> ""/Users/me/project/src/app.js"",
    ""webpack:///*"":     ""*"",                               // Example: ""webpack:///project/app.ts"" -> ""/project/app.ts""
    ""webpack:///src/*"": ""${webRoot}/*"",                    // Example: ""webpack:///src/app.js"" -> ""/Users/me/project/app.js""
    ""meteor://💻app/*"": ""${webRoot}/*""                    // Example: ""meteor://💻app/main.ts"" -> ""/Users/me/project/main.ts""
}
```
If you set `sourceMapPathOverrides` in your launch config, that will override these defaults. `${workspaceFolder}` and `${webRoot}` can be used here. If you aren't sure what the left side should be, you can use the `trace` option to see the contents of the sourcemap, or look at the paths of the sources in the Microsoft Edge DevTools, or open your `.js.map` file and check the values manually.

### Ionic/gulp-sourcemaps note
Ionic and gulp-sourcemaps output a sourceRoot of `""/source/""` by default. If you can't fix this via your build config, we suggest this setting:
```json
""sourceMapPathOverrides"": {
    ""/source/*"": ""${workspaceFolder}/*""
}
```

## Troubleshooting

### My breakpoints aren't hit when debugging Microsoft Edge(EdgeHTML). What's wrong?
If your breakpoints weren't hit, it's most likely a sourcemapping issue or because you set breakpoints before launching Microsoft Edge (EdgeHTML) and were expecting them to hit while the browser loads. If that's the case, you will have to refresh the page in Microsoft Edge (EdgeHTML) after we have attached from VS Code/Visual Studio to hit your breakpoint.

If you are using sourcemaps, make sure they are configured right.

### Cannot connect to the target: connect ECONNREFUSED 127.0.0.1:2015
This message means that the extension can't attach to Microsoft Edge, probably because Microsoft Edge wasn't launched in debug mode. Here are some things to try:
* Ensure that the `port` property matches the port on which Microsoft Edge is listening for remote debugging connections. This is `2015` by default. Ensure nothing else is using this port, including your web server. If something else on your computer responds at `http://localhost:2015`, then set a different port.
* If all else fails, try to navigate to `http://localhost:<port>/json/list` in a browser when you see this message - if there is no response, then something is wrong upstream of the extension. If there is a page of JSON returned, then ensure that the `port` in the launch config matches the port in that url.
* If the above steps do not work, try closing all windows of Microsoft Edge and then relaunch.

### General things to try if you're having issues:
* Ensure `webRoot` is set correctly if needed
* Look at your sourcemap config carefully. A sourcemap has a path to the source files, and this extension uses that path to find the original source files on disk. Check the `sourceRoot` and `sources` properties in your sourcemap and make sure that they can be combined with the `webRoot` property in your launch config to build the correct path to the original source files.
* Check the console for warnings that this extension prints in some cases when it can't attach.
* Ensure the code in your browser matches the code in Code. The browser may cache an old version of your code.
* If your breakpoints bind, but aren't hit in Microsoft Edge (EdgeHTML), try refreshing the page. If you set a breakpoint in code that runs immediately when the page loads in Microsoft Edge (EdgeHTML), you won't hit that breakpoint until you refresh the page.

## Feedback
Send us your feedback by [filing an issue](https://github.com/Microsoft/vscode-edge-debug2/issues/new) against this extension's [GitHub repo](https://github.com/Microsoft/vscode-edge-debug2). Please include the debug adapter log file, which is created for each run in the %temp% directory with the name `vscode-edge-debug2.txt`. You can drag this file into an issue comment to upload it to GitHub.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
66,microsoft/cordova-plugin-code-push,Objective-C,"[![appcenterbanner](https://user-images.githubusercontent.com/31293287/32969262-3cc5d48a-cb99-11e7-91bf-fa57c67a371c.png)](http://microsoft.github.io/code-push/)

#### [Sign up With App Center](https://appcenter.ms/signup?utm_source=CodePush&utm_medium=Azure) to use CodePush

# Apache Cordova Plugin for CodePush

This plugin provides client-side integration for the [CodePush service](https://microsoft.github.io/code-push/), allowing you to easily add a dynamic update experience to your Cordova app(s).

<!-- Cordova Catalog -->

* [How does it work?](#how-does-it-work)
* [Supported Cordova Platforms](#supported-cordova-platforms)
* [Deprecating old versions](#deprecating-old-versions)
* [Getting Started](#getting-started)
* [Plugin Usage](#plugin-usage)
* [Releasing Updates](#releasing-updates)
* [API Reference](#api-reference)
* [PhoneGap Build](#phonegap-build)
* [Example Apps](#example-apps)

<!-- Cordova Catalog -->

## How does it work?

A Cordova app is composed of HTML, CSS and JavaScript files and any accompanying images, which are bundled together by the Cordova CLI and distributed as part of a platform-specific binary (i.e. an .ipa or .apk file). Once the app is released, updating either the code (e.g. making bug fixes, adding new features) or image assets, requires you to recompile and redistribute the entire binary, which of course, includes any review time associated with the store(s) you are publishing to.

The CodePush plugin helps get product improvements in front of your end users instantly, by keeping your code and images synchronized with updates you release to the CodePush server. This way, your app gets the benefits of an offline mobile experience, as well as the ""web-like"" agility of side-loading updates as soon as they are available. It's a win-win!

In order to ensure that your end users always have a functioning version of your app, the CodePush plugin maintains a copy of the previous update, so that in the event that you accidentally push an update which includes a crash, it can automatically roll back. This way, you can rest assured that your newfound release agility won't result in users becoming blocked before you have a chance to roll back on the server. It's a win-win-win!

*Note: Any product changes which touch native code (e.g. upgrading Cordova versions, adding a new plugin) cannot be distributed via CodePush, and therefore, must be updated via the appropriate store(s).*

## Supported Cordova Platforms

Cordova 5.0.0+ is fully supported, along with the following associated platforms:

* Android ([cordova-android](https://github.com/apache/cordova-android) 4.0.0+) - *Including CrossWalk!* *Note: Only on TLS 1.2 compatible devices*
* iOS ([cordova-ios](https://github.com/apache/cordova-ios) 3.9.0+) - please see notes below.

> Note: Starting with v2.0.0 `cordova-plugin-code-push` doesn't support apps using UIWebView due to [Apple officially deprecated it and discourage developers from using it](https://developer.apple.com/news/?id=12232019b). Prior versions of the plugin still support UIWebView but be aware that the App Store will no longer accept new apps using UIWebView as of April 2020 and app updates using UIWebView as of December 2020.

> Note: In order to use CodePush along with the [`cordova-plugin-wkwebview-engine`](https://github.com/apache/cordova-plugin-wkwebview-engine) plugin, you need to install `v1.5.1-beta+` version of `cordova-plugin-code-push`, which includes full support for apps using either WebView. Please see [Using WKWebView](#using-wkwebview) section for more information of how to confiure your app to use `cordova-plugin-wkwebview-engine`.

To check which versions of each Cordova platform you are currently using, you can run the following command and inspect the `Installed platforms` list:

```shell
cordova platform ls
```

If you're running an older Android and/or iOS platform than is mentioned above, and would be open to upgrading, you can easily do so by running the following commands (omitting a platform if it isn't necessary):

```shell
cordova platform update android
cordova platform update ios
```

## Deprecating old versions

Since CodePush is migrating to a new service all versions of cordova-plugin-code-push lower than **[1.12.0](https://github.com/microsoft/cordova-plugin-code-push/releases/tag/v1.12.0)** will not work in the nearest future.

You can find more information in our [documentation](https://github.com/microsoft/code-push/blob/master/migration-notice.md).

## Getting Started

Once you've followed the general-purpose [""getting started""](https://docs.microsoft.com/en-us/appcenter/distribution/codepush/) instructions for setting up your CodePush account, you can start CodePush-ifying your Cordova app by running the following command from within your app's root directory:

```shell
cordova plugin add cordova-plugin-code-push@latest
```

With the CodePush plugin installed, configure your app to use it via the following steps:

1. Add your deployment keys to the `config.xml` file, making sure to include the right key for each Cordova platform:

    ```xml
    <platform name=""android"">
        <preference name=""CodePushDeploymentKey"" value=""YOUR-ANDROID-DEPLOYMENT-KEY"" />
    </platform>
    <platform name=""ios"">
        <preference name=""CodePushDeploymentKey"" value=""YOUR-IOS-DEPLOYMENT-KEY"" />
    </platform>
    ```

    As a reminder, these keys are generated for you when you created your CodePush app via the CLI. If you need to retrieve them, you can simply run `appcenter codepush deployment list <ownerName>/<appName> --displayKeys`, and grab the key for the specific deployment you want to use (e.g. `Staging`, `Production`).

    *NOTE: You [must](https://docs.microsoft.com/en-us/appcenter/distribution/codepush/cli#releasing-updates) create a separate CodePush app for iOS and Android, which is why the above sample illustrates declaring separate keys for Android and iOS. If you're only developing for a single platform, then you only need to specify the deployment key for either Android or iOS, so you don't need to add the additional `<platform>` element as illustrated above.*

    Beginning from version **1.10.0** you can sign your update bundles (for more information about code signing please refer to relevant documentation [section](https://github.com/Microsoft/code-push/blob/master/cli/README.md#code-signing)). In order to enable code signing for Cordova application you should setup public key to verify bundles signature by providing following `preference` setting in `config.xml`:

     ```xml
    <platform name=""android"">
        ...
        <preference name=""CodePushPublicKey"" value=""YOUR-PUBLIC-KEY"" />
    </platform>
    <platform name=""ios"">
        ...
        <preference name=""CodePushPublicKey"" value=""YOUR-PUBLIC-KEY"" />
    </platform>
    ```
    You can use the same private/public key pair for each platform.

2. If you're using an `<access origin=""*"" />` element in your `config.xml` file, then your app is already allowed to communicate with the CodePush servers and you can safely skip this step. Otherwise, add the following additional `<access />` elements:

    ```xml
    <access origin=""https://codepush.appcenter.ms"" />
    <access origin=""https://codepush.blob.core.windows.net"" />
    <access origin=""https://codepushupdates.azureedge.net"" />
    ```

3. To ensure that your app can access the CodePush server on [CSP](https://developer.mozilla.org/en-US/docs/Web/Security/CSP/Introducing_Content_Security_Policy)-compliant platforms, add `https://codepush.appcenter.ms` to the `Content-Security-Policy` `meta` tag in your `index.html` file:

    ```xml
    <meta http-equiv=""Content-Security-Policy"" content=""default-src https://codepush.appcenter.ms 'self' data: gap: https://ssl.gstatic.com 'unsafe-eval'; style-src 'self' 'unsafe-inline'; media-src *"" />
    ```

4. Finally, double-check that you already have the [`cordova-plugin-whitelist`](https://github.com/apache/cordova-plugin-whitelist) plugin installed (most apps will). To check this, simply run the following command:

    ```shell
    cordova plugin ls
    ```

    If `cordova-plugin-whitelist` is in the list, then you are good to go. Otherwise, simply run the following command to add it:

    ```shell
    cordova plugin add cordova-plugin-whitelist
    ```

You are now ready to use the plugin in the application code. See the [sample applications](/samples) for examples and the API documentation for more details.

### Using WKWebView

For cordova-ios v4-v5 there is a possibility to specify WebView engine on the plugin build phase. By default UIWebView engine is used. To use WKWebView engine please do the following:

* Install [cordova-plugin-wkwebview-engine](https://github.com/apache/cordova-plugin-wkwebview-engine#installation)
* [Configure your app](https://github.com/apache/cordova-plugin-wkwebview-engine#required-permissions) to use WKWebView

> Note: `cordova-plugin-wkwebview-engine` is just a workaround for cordova-ios v4-v5 users to be able to use WKWebView in their apps to avoid stop accepting updates via AppStore as of December 2020.
Cordova-ios v6+ has full support for native WKWebView and doesn't require `cordova-plugin-wkwebview-engine`.

## Plugin Usage

With the CodePush plugin installed and configured, the only thing left is to add the necessary code to your app to control the following policies:

1. When (and how often) to check for an update? (e.g. app start, in response to clicking a button in a settings page, periodically at some fixed interval)

2. When an update is available, how to present it to the end user?

The simplest way to do this is to perform the following in your app's `deviceready` event handler:

```javascript
codePush.sync();
```

If an update is available, it will be silently downloaded, and installed the next time the app is restarted (either explicitly by the end user or by the OS), which ensures the least invasive experience for your end users. If an available update is mandatory, then it will be installed immediately, ensuring that the end user gets it as soon as possible.

If you would like your app to discover updates more quickly, you can also choose to call `sync` every time the app resumes from the background, by adding the following code (or something equivalent) as part of your app's startup behavior. You can call `sync` as frequently as you would like, so when and where you call it just depends on your personal preference.

```javascript
document.addEventListener(""resume"", function () {
    codePush.sync();
});
```

Additionally, if you would like to display an update confirmation dialog (an ""active install""), configure when an available update is installed (e.g. force an immediate restart) or customize the update experience in any way, refer to the `sync` method's API reference for information on how to tweak this default behavior.

*NOTE: While [Apple's developer agreement](https://developer.apple.com/programs/ios/information) fully allows performing over-the-air updates of JavaScript and assets (which is what enables CodePush!), it is against their policy for an app to display an update prompt. Because of this, we recommend that App Store-distributed apps don't enable the `updateDialog` option when calling `sync`, whereas Google Play and internally distributed apps (e.g. Enterprise, Fabric, HockeyApp) can choose to enable/customize it.*

## Releasing Updates

Once your app has been configured and distributed to your users, and you've made some code and/or asset changes, it's time to instantly release them! The simplest (and recommended) way to do this is to use the `release-cordova` command in the App Center CLI, which will handle preparing and releasing your update to the CodePush server.

*NOTE: Before you can start releasing updates, please log into App Center by running the `appcenter login` command*

In it's the most basic form, this command only requires one parameter: your owner name + ""/"" + app name.

```shell
appcenter codepush release-cordova -a <ownerName>/<appName>

appcenter codepush release-cordova -a <ownerName>/MyApp-ios
appcenter codepush release-cordova -a <ownerName>/MyApp-Android
```

*NOTE: When releasing updates to CodePush, you do not need to bump your app's version in the `config.xml` file, since you aren't modifying the binary version at all. You only need to bump this version when you upgrade Cordova and/or one of your plugins, at which point, you need to release an update to the native store(s). CodePush will automatically generate a ""label"" for each release you make (e.g. `v3`) in order to help identify it within your release history.*

The `release-cordova` command enables such a simple workflow because it understands the standard layout of a Cordova app, and therefore, can generate your update and know exactly which files to upload. Additionally, in order to support flexible release strategies, the `release-cordova` command exposes numerous optional parameters that let you customize how the update should be distributed to your end users (e.g. Which binary versions are compatible with it? Should the release be viewed as mandatory?).

```shell
# Release a mandatory update with a changelog
appcenter codepush release-cordova -a <ownerName>/MyApp-ios -m --description ""Modified the header color""

# Release a dev Android build to just 1/4 of your end users
appcenter codepush release-cordova -a <ownerName>/MyApp-android --rollout 25

# Release an update that targets users running any 1.1.* binary, as opposed to
# limiting the update to exact version name in the config.xml file
appcenter codepush release-cordova -a <ownerName>/MyApp-android --target-binary-version ""~1.1.0""

# Release an update now but mark it as disabled
# so that no users can download it yet
appcenter codepush release-cordova -a <ownerName>/MyApp-ios -x

# Release an update signed by private key (public key should be configured for application)
appcenter codepush release-cordova -a <ownerName>/MyApp-android --private-key-path ~/rsa/private_key.pem
```

The CodePush client supports differential updates, so even though you are releasing your app code on every update, your end users will only actually download the files they need. The service handles this automatically so that you can focus on creating awesome apps and we can worry about optimizing end user downloads.

*NOTE: for **Ionic** apps you need to run `ionic build` before running `cordova-release` or `release` command in order to build web assets.*

For more details about how the `release-cordova` command works, as well as the various parameters it exposes, refer to the [CLI docs](https://docs.microsoft.com/en-us/appcenter/distribution/codepush/cli#releasing-updates-cordova). Additionally, if you would prefer to handle running the `cordova prepare` command yourself, and therefore, want an even more flexible solution than `release-cordova`, refer to the [`release` command](https://docs.microsoft.com/en-us/appcenter/distribution/codepush/cli#releasing-updates-general) for more details.

If you run into any issues, or have any questions/comments/feedback, you can [e-mail us](mailto:codepushfeed@microsoft.com) and/or open a new issue on this repo and we'll respond ASAP!

## API Reference

The CodePush API is exposed to your app via the global `codePush` object, which is available after the `deviceready` event fires. This API exposes the following top-level methods:

- __[checkForUpdate](#codepushcheckforupdate)__: Asks the CodePush service whether the configured app deployment has an update available.

- __[getCurrentPackage](#codepushgetcurrentpackage)__: Retrieves the metadata about the currently installed update (e.g. description, installation time, size).

- __[getPendingPackage](#codepushgetpendingpackage)__: Retrieves the metadata for an update (if one exists) that was downloaded and installed, but hasn't been applied yet via a restart.

- __[notifyApplicationReady](#codepushnotifyapplicationready)__: Notifies the CodePush runtime that an installed update is considered successful. If you are manually checking for and installing updates (i.e. not using the sync method to handle it all for you), then this method **MUST** be called; otherwise CodePush will treat the update as failed and rollback to the previous version when the app next restarts.

- __[restartApplication](#codepushrestartapplication)__: Immediately restarts the app. If there is an update pending, it will be immediately displayed to the end user.

- __[sync](#codepushsync)__: Allows checking for an update, downloading it and installing it, all with a single call. Unless you need custom UI and/or behavior, we recommend most developers to use this method when integrating CodePush into their apps.

Additionally, the following objects and enums are also exposed globally as part of the CodePush API:

- __[InstallMode](#installmode)__: Defines the available install modes for updates.

- __[LocalPackage](#localpackage)__: Contains information about a locally installed package.

- __[RemotePackage](#remotepackage)__: Contains information about an update package available for download.

- __[SyncStatus](#syncstatus)__: Defines the possible intermediate and result statuses of the [sync](#codepushsync) operation.

### codePush.checkForUpdate

```javascript
codePush.checkForUpdate(onSuccess, onError?, deploymentKey?: String);
```

Queries the CodePush service to see whether the configured app deployment has an update available. By default, it will use the deployment key that is configured in your `config.xml` file, but you can override that by specifying a value via the optional `deploymentKey` parameter. This can be useful when you want to dynamically ""redirect"" a user to a specific deployment, such as allowing ""Early access"" via an easter egg or a user setting switch.

When the update check completes, it will trigger the `onUpdateCheck` callback with one of two possible values:

1. `null` if there is no update available. This occurs in the following scenarios:

    1. The configured deployment doesn't contain any releases, and therefore, nothing to update.

    2. The latest release within the configured deployment is targeting a different binary version than what you're currently running (either older or newer).

    3. The currently running app already has the latest release from the configured deployment, and therefore, doesn't need it again.

2. A `RemotePackage` instance which represents an available update that can be inspected and/or subsequently downloaded.

Parameters:

- __onSuccess__: Callback that is invoked upon receiving a successful response from the server. The callback receives a single parameter, which is described above.

- __onError__: Optional callback that is invoked in the event of an error. The callback takes one error parameter, containing the details of the error.

- __deploymentKey__: Optional deployment key that overrides the `config.xml` setting.

Example usage:

```javascript
codePush.checkForUpdate(function (update) {
    if (!update) {
        console.log(""The app is up to date."");
    } else {
        console.log(""An update is available! Should we download it?"");
    }
});
```

### codePush.getCurrentPackage

```javascript
codePush.getCurrentPackage(onSuccess, onError?);
```

Retrieves the metadata about the currently installed ""package"" (e.g. description, installation time). This can be useful for scenarios such as displaying a ""what's new?"" dialog after an update has been applied or checking whether there is a pending update that is waiting to be applied via a resume or restart.

When the update retrieval completes, it will trigger the `onSuccess` callback with one of two possible values:

1. `null` if the app is currently running the HTML start page from the binary and not a CodePush update. This occurs in the following scenarios:

    1. The end-user installed the app binary and has yet to install a CodePush update

    2. The end-user installed an update of the binary (e.g. from the store), which cleared away the old CodePush updates, and gave precedence back to the binary.

2. A `LocalPackage` instance which represents the metadata for the currently running CodePush update.

Parameters:

- __onSuccess__: Callback that is invoked upon receiving the metadata about the currently running update. The callback receives a single parameter, which is described above.

- __onError__: Optional callback that is invoked in the event of an error. The callback takes one error parameter, containing the details of the error.

Example Usage:

```javascript
codePush.getCurrentPackage(function (update) {
    if (!update) {
        console.log(""No updates have been installed"");
        return;
    }

    // If the current app ""session"" represents the first time
    // this update has run, and it had a description provided
    // with it upon release, let's show it to the end user
    if (update.isFirstRun && update.description) {
        // Display a ""what's new?"" modal
    }
});
```

### codePush.getPendingPackage

```javascript
codePush.getPendingPackage(onSuccess, onError?);
```

Gets the metadata for the currently pending update (if one exists). An update is considered ""pending"" if it has been downloaded and installed, but hasn't been applied yet via an app restart. An update could only ever be in this state if   `InstallMode.ON_NEXT_RESTART` or `InstallMode.ON_NEXT_RESUME` were specified upon calling `sync` or `LocalPackage.install`, and the app hasn't yet been restarted or resumed (respectively). This method can be useful if you'd like to determine whether there is a pending update and then prompt the user if they would like to restart now (via `codePush.restartApplication`) in order to apply it.

When the update retrieval completes, it will trigger the `onSuccess` callback with one of two possible values:

1. `null` if the app doesn't currently have a pending update (e.g. the app is already running the latest available version).

2. A `LocalPackage` instance which represents the metadata for the currently pending CodePush update.

Parameters:

- __onSuccess__: Callback that is invoked upon receiving the metadata about the currently pending update. The callback receives a single parameter, which is described above.

- __onError__: Optional callback that is invoked in the event of an error. The callback takes one error parameter, containing the details of the error.

Example Usage:

```javascript
codePush.getPendingPackage(function (update) {
    if (update) {
        // An update is currently pending, ask the
        // user if they would like to restart
    }
});
```

### codePush.notifyApplicationReady

```javascript
codePush.notifyApplicationReady(notifySucceeded?, notifyFailed?);
```

Notifies the CodePush runtime that a freshly installed update should be considered successful, and therefore, an automatic client-side rollback isn't necessary. It is mandatory to call this function somewhere in the code of the updated bundle. Otherwise, when the app next restarts, the CodePush runtime will assume that the installed update has failed and roll back to the previous version. This behavior exists to help ensure that your end users aren't blocked by a broken update.

If you are using the `sync` function, and doing your update check on app start, then you don't need to manually call `notifyApplicationReady` since `sync` will call it for you. This behavior exists due to the assumption that the point at which `sync` is called in your app represents a good approximation of a successful startup.

Parameters:

- __notifySucceeded__: Optional callback invoked if the plugin was successfully notified.

- __notifyFailed__: Optional callback invoked in case of an error during notifying the plugin.

### codePush.restartApplication

```javascript
codePush.restartApplication();
```

Immediately restarts the app. This method is for advanced scenarios, and is primarily useful when the following conditions are true:

1. Your app is specifying an install mode value of `ON_NEXT_RESTART` or `ON_NEXT_RESUME` when calling the `sync` or `LocalPackage.install` methods. This has the effect of not applying your update until the app has been restarted (by either the end user or OS) or resumed, and therefore, the update won't be immediately displayed to the end user.

2. You have an app-specific user event (e.g. the end user navigated back to the app's home route) that allows you to apply the update in an unobtrusive way, and potentially gets the update in front of the end user sooner then waiting until the next restart or resume.

### codePush.sync

```javascript
codePush.sync(syncCallback?, syncOptions?, downloadProgress?, syncErrback?);
```

Synchronizes your app's code and images with the latest release to the configured deployment. Unlike the `checkForUpdate` method, which simply checks for the presence of an update, and let's you control what to do next, `sync` handles the update check, download and installation experience for you.

This method provides support for two different (but customizable) ""modes"" to easily enable apps with different requirements:

1. **Silent mode** *(the default behavior)*, which automatically downloads available updates, and applies them the next time the app restarts (e.g. the OS or end user killed it, or the device was restarted). This way, the entire update experience is ""silent"" to the end user, since they don't see any update prompt and/or ""synthetic"" app restarts.

2. **Active mode**, which when an update is available, prompts the end user for permission before downloading it, and then immediately applies the update. If an update was released using the mandatory flag, the end user would still be notified about the update, but they wouldn't have the choice to ignore it.

Example Usage:

```javascript
// Fully silent update which keeps the app in
// sync with the server, without ever
// interrupting the end user
codePush.sync();

// Active update, which lets the end user know
// about each update, and displays it to them
// immediately after downloading it
codePush.sync(null, { updateDialog: true, installMode: InstallMode.IMMEDIATE });
```

*Note: If you want to decide whether you check and/or download an available update based on the end user's device battery level, network conditions, etc. then simply wrap the call to sync in a condition that ensures you only call it when desired.*

While the sync method tries to make it easy to perform silent and active updates with little configuration, it accepts the following optional parameters which allow you to customize numerous aspects of the default behavior mentioned above:

- __syncCallback__: Called when the sync process moves from one stage to another in the overall update process. The method is called with a status code which represents the current state, and can be any of the [`SyncStatus`](#syncstatus) values.

- __syncOptions__: Optional [`SyncOptions`](#syncoptions) parameter configuring the behavior of the sync operation.

- __downloadProgress__: Called periodically when an available update is being downloaded from the CodePush server. The method is called with a `DownloadProgress` object, which contains the following two properties:

    - __totalBytes__ *(Number)* - The total number of bytes expected to be received for this update (i.e. the size of the set of files which changed from the previous release).

    - __receivedBytes__ *(Number)* - The number of bytes downloaded thus far, which can be used to track download progress.

#### SyncOptions

While the `sync` method tries to make it easy to perform silent and active updates with little configuration, it accepts an ""options"" object that allows you to customize numerous aspects of the default behavior mentioned above:

- __deploymentKey__ *(String)* - Specifies the deployment key you want to query for an update against. By default, this value is derived from the `config.xml` file, but this option allows you to override it from the script-side if you need to dynamically use a different deployment for a specific call to `sync`.

- __installMode__ *(InstallMode)* - Specifies when you would like to install optional updates (i.e. those that aren't marked as mandatory). Defaults to `InstallMode.ON_NEXT_RESTART`. Refer to the [`InstallMode`](#installmode) enum reference for a description of the available options and what they do.

- __mandatoryInstallMode__ *(InstallMode)* - Specifies when you would like to install updates which are marked as mandatory. Defaults to `InstallMode.IMMEDIATE`. Refer to the [`InstallMode`](#installmode) enum reference for a description of the available options and what they do.

- __minimumBackgroundDuration__ *(Number)* - Specifies the minimum number of seconds that the app needs to have been in the background before restarting the app. This property only applies to updates which are installed using `InstallMode.ON_NEXT_RESUME`, and can be useful for getting your update in front of end users sooner, without being too obtrusive. Defaults to `0`, which has the effect of applying the update immediately after a resume, regardless how long it was in the background.

- __ignoreFailedUpdates__ *(Boolean)* - Specifies whether an available update should be ignored if it had been previously installed and rolled back on the client (because `notifyApplicationReady` wasn't successfully called). Defaults to `true`.

- __updateDialog__ *(UpdateDialogOptions)* - An ""options"" object used to determine whether a confirmation dialog should be displayed to the end user when an update is available, and if so, what strings to use. Defaults to `null`, which has the effect of disabling the dialog completely. Setting this to any truthy value will enable the dialog with the default strings, and passing an object to this parameter allows enabling the dialog as well as overriding one or more of the default strings.

    The following list represents the available options and their defaults:

    - __appendReleaseDescription__ *(Boolean)* - Indicates whether you would like to append the description of an available release to the notification message which is displayed to the end user. Defaults to `false`.

    - __descriptionPrefix__ *(String)* - Indicates the string you would like to prefix the release description with, if any, when displaying the update notification to the end user. Defaults to `"" Description: ""`.

    - __mandatoryContinueButtonLabel__ *(String)*: The text to use for the button the end user must press in order to install a mandatory update. Defaults to `""Continue""`.

    - __mandatoryUpdateMessage__ *(String)* - The text used as the body of an update notification, when the update is specified as mandatory. Defaults to `""An update is available that must be installed.""`.

    - __optionalIgnoreButtonLabel__ *(String)* - The text to use for the button the end user can press in order to ignore an optional update that is available. Defaults to `""Ignore""`.

    - __optionalInstallButtonLabel__ *(String)* - The text to use for the button the end user can press in order to install an optional update. Defaults to `""Install""`.

    - __optionalUpdateMessage__ *(String)* - The text used as the body of an update notification, when the update is optional. Defaults to `""An update is available. Would you like to install it?""`.

    - __updateTitle__ *(String)* - The text used as the header of an update notification that is displayed to the end user. Defaults to `""Update available""`.

Example Usage:

```javascript
// Download the update silently, but install it on
// the next resume, as long as at least 5 minutes
// has passed since the app was put into the background.
codePush.sync(null, { installMode: InstallMode.ON_NEXT_RESUME, minimumBackgroundDuration: 60 * 5 });

// Download the update silently, and install optional updates
// on the next restart, but install mandatory updates on the next resume.
codePush.sync(null, { mandatoryInstallMode: InstallMode.ON_NEXT_RESUME });

// Changing the title displayed in the
// confirmation dialog of an ""active"" update
codePush.sync(null, { updateDialog: { updateTitle: ""An update is available!"" } });

// Displaying an update prompt which includes the
// description associated with the CodePush release
codePush.sync(null, {
   updateDialog: {
    appendReleaseDescription: true,
    descriptionPrefix: ""\n\nChange log:\n""
   },
   installMode: InstallMode.IMMEDIATE
});

// Silently check for the update, but
// display a custom downloading UI
// via the SyncStatus and DownloadProgress callbacks
codePush.sync(syncStatus, null, downloadProgress);

function syncStatus(status) {
    switch (status) {
        case SyncStatus.DOWNLOADING_PACKAGE:
            // Show ""downloading"" modal
            break;
        case SyncStatus.INSTALLING_UPDATE:
            // Hide ""downloading"" modal
            break;
    }
}

function downloadProgress(downloadProgress) {
    if (downloadProgress) {
    	// Update ""downloading"" modal with current download %
        //console.log(""Downloading "" + downloadProgress.receivedBytes + "" of "" + downloadProgress.totalBytes);
    }
}
```

The `sync` method can be called anywhere you'd like to check for an update. That could be in the `deviceready` event handler, the `click` event of a button, in the callback of a periodic timer, or whatever else makes sense for your needs. Just like the `checkForUpdate` method, it will perform the network request to check for an update in the background, so it won't impact your UI thread and/or JavaScript thread's responsiveness.

### Package objects

The `checkForUpdate` and `getCurrentPackage` methods invoke success callbacks, that when triggered, provide access to ""package"" objects. The package represents your code update as well as any extra metadata (e.g. description, mandatory?). The CodePush API has the distinction between the following types of packages:

1. `LocalPackage`: Represents a downloaded update that is either already running, or has been installed and is pending an app restart.

2. `RemotePackage`: Represents an available update on the CodePush server that hasn't been downloaded yet.

#### LocalPackage

Contains details about an update that has been downloaded locally or already installed. You can get a reference to an instance of this object either by calling the `codePush.getCurrentPackage` method, or as the value provided to the success callback of the `RemotePackage.download` method.

##### Properties

- __appVersion__: The native version of the application this package update is intended for. *(String)*
- __deploymentKey__: Deployment key of the package. *(String)*
- __description__: The description of the update. This is the same value that you specified in the CLI when you released the update. *(String)*
- __failedInstall__: Indicates whether this update has been previously installed but was rolled back. The `sync` method will automatically ignore updates which have previously failed, so you only need to worry about this property if using `checkForUpdate`. *(Boolean)*
- __isFirstRun__: Flag indicating if the current application run is the first one after the package was applied. *(Boolean)*
- __isMandatory__: Indicates whether the update is considered mandatory. This is the value that was specified in the CLI when the update was released. *(Boolean)*
- __label__: The internal label automatically given to the update by the CodePush server, such as `v5`. This value uniquely identifies the update within it's deployment. *(String)*
- __packageHash__: The SHA hash value of the update. *(String)*
- __packageSize__: The size of the code contained within the update, in bytes. *(Number)*

##### Methods

- __install(installSuccess, installError, installOptions)__: Installs this package to the application.
The install behavior is dependent on the provided `installOptions`. By default, the update package is silently installed and the application is reloaded with the new content on the next application start.
On the first run after the update, the application will wait for a `codePush.notifyApplicationReady()` call. Once this call is made, the install operation is considered a success.
Otherwise, the install operation will be marked as failed, and the application is reverted to its previous version on the next run.

    ###### InstallOptions

    Interface defining several options for customizing install operation behavior.

    - __installMode__: Used to specify the [InstallMode](#installmode) used for the install operation. Defaults to `InstallMode.ON_NEXT_RESTART`.

    - __mandatoryInstallMode__: Used to specify the [InstallMode](#installmode) used for the install operation if the package is mandatory. Defaults to `InstallMode.IMMEDIATE`.

    - __minimumBackgroundDuration__: If __installMode__ is `InstallMode.ON_NEXT_RESUME`, used to specify the amount of time the app must be in the background before the update is installed when it is resumed. Defaults to `0`.

Example Usage:

```javascript
// App version 1 (current version)

var onError = function (error) {
    console.log(""An error occurred. "" + error);
};

var onInstallSuccess = function () {
    console.log(""Installation succeeded."");
};

var onPackageDownloaded = function (localPackage) {
    // Install regular updates after someone navigates away from the app for more than 2 minutes
    // Install mandatory updates after someone restarts the app
    localPackage.install(onInstallSuccess, onError, { installMode: InstallMode.ON_NEXT_RESUME, minimumBackgroundDuration: 120, mandatoryInstallMode: InstallMode.ON_NEXT_RESTART });
};

var onUpdateCheck = function (remotePackage) {
    if (!remotePackage) {
        console.log(""The application is up to date."");
    } else {
        // The hash of each previously reverted package is stored for later use.
        // This way, we avoid going into an infinite bad update/revert loop.
        if (!remotePackage.failedInstall) {
            console.log(""A CodePush update is available. Package hash: "" + remotePackage.packageHash);
            remotePackage.download(onPackageDownloaded, onError);
        } else {
            console.log(""The available update was attempted before and failed."");
        }
    }
};

window.codePush.checkForUpdate(onUpdateCheck, onError);

//------------------------------------------------

// App version 2 (updated version)

var app = {
    onDeviceReady: function () {
        // Calling this function is required during the first application run after an update.
        // If not called, the application will be reverted to the previous version.
        window.codePush.notifyApplicationReady();
        // ...
    }
}
```

For an example on how you are protected against a bad update, see the [notifyApplicationReady() documentation](#codepushnotifyapplicationready).

#### RemotePackage

Contains details about an update that is available for download from the CodePush server. You get a reference to an instance of this object by calling the `codePush.checkForUpdate` method when an update is available. If you are using the sync API, you don't need to worry about the `RemotePackage`, since it will handle the download and installation process automatically for you.

##### Properties

The `RemotePackage` inherits all of the same properties as the `LocalPackage`, but includes one additional one:

- __downloadUrl__: The URL at which the package is available for download. This property is only needed for advanced usage, since the `download` method will automatically handle the acquisition of updates for you. *(String)*

##### Methods

- __abortDownload(abortSuccess, abortError)__: Aborts the current download session, if any.

- __download(downloadSuccess, downloadError, downloadProgress)__: Downloads the package update from the CodePush service. The ```downloadSuccess``` callback is invoked with a [LocalPackage](#localpackage) argument, representing the downloaded package.
The optional `downloadProgress` callback is invoked several times during the download progress with one `DownloadProgress` parameter.

    ###### DownloadProgress

    Defines the format of the DownloadProgress object, used to send periodical update notifications on the progress of the update download.

    Properties

    - __totalBytes__: The size of the downloading update package, in bytes. (Number)
    - __receivedBytes__: The number of bytes already downloaded. (Number)

Example Usage:

```javascript
var onError = function (error) {
    console.log(""An error occurred. "" + error);
};

var onPackageDownloaded = function (localPackage) {
    console.log(""Package downloaded at: "" + localPackage.localPath);
    // you can now update your application to the downloaded version by calling localPackage.install()
};

var onProgress = function (downloadProgress) {
    console.log(""Downloading "" + downloadProgress.receivedBytes + "" of "" + downloadProgress.totalBytes + "" bytes."");
};

var onUpdateCheck = function (remotePackage) {
    if (!remotePackage) {
        console.log(""The application is up to date."");
    } else {
        console.log(""A CodePush update is available. Package hash: "" + remotePackage.packageHash);
        remotePackage.download(onPackageDownloaded, onError, onProgress);
    }
};

window.codePush.checkForUpdate(onUpdateCheck, onError);
```

### Enums

The CodePush API includes the following ""enum"" objects which can be used to customize the update experience, and are available globally off of the `window` object:

#### InstallMode

This enum specified when you would like an installed update to actually be applied, and can be passed to either the `sync` or `LocalPackage.install` methods. It includes the following values:

- __IMMEDIATE__: The update will be applied to the running application immediately. The application will be reloaded with the new content immediately.

- __ON_NEXT_RESTART__: Indicates that you want to install the update, but not forcibly restart the app. When the app is ""naturally"" restarted (due the OS or end user killing it), the update will be seamlessly picked up. This value is appropriate when performing silent updates, since it would likely be disruptive to the end user if the app suddenly restarted out of nowhere, since they wouldn't have realized an update was even downloaded. This is the default mode used for both the `sync` and `LocalPackage.install` methods.

- __ON_NEXT_RESUME__: Indicates that you want to install the update, but don't want to restart the app until the next time the end user resumes it from the background. This way, you don't disrupt their current session, but you can get the update in front of them sooner then having to wait for the next natural restart. This value is appropriate for silent installs that can be applied on resume in a non-invasive way.

#### SyncStatus

Defines the possible statuses of the [sync](#codepushsync) operation. There are two categories of statuses: intermediate and result (final). The intermediate statuses represent progress statuses of the sync operation, and are not final. The result statuses represent final statuses of the sync operation. Every sync operation ends with only one result status, but can have zero or more intermediate statuses.

- __UP_TO_DATE__: The app is fully up-to-date with the configured deployment.

- __UPDATE_INSTALLED__: An available update has been installed and will be run either immediately after the callback function returns or the next time the app resumes/restarts, depending on the `InstallMode` specified in `SyncOptions`.

- __UPDATE_IGNORED__: The app has an optional update, which the end user chose to ignore. *(This is only applicable when the `updateDialog` is used)*

- __ERROR__: An error occurred during the `sync` operation. This might be an error while communicating with the server, downloading or unzipping the update. The console logs should contain more information about what happened. No update has been applied in this case.

- __IN_PROGRESS__: Another sync is already running, so this attempt to sync has been aborted.

- __CHECKING_FOR_UPDATE__: The CodePush server is being queried for an update.

- __AWAITING_USER_ACTION__: An update is available, and a confirmation dialog was shown to the end user. *(This is only applicable when the `updateDialog` is used)*

- __DOWNLOADING_PACKAGE__: An available update is being downloaded from the CodePush server.

- __INSTALLING_UPDATE__: An available update was downloaded and is about to be installed.

## PhoneGap Build

This plugin is compatible with [PhoneGap Build](https://build.phonegap.com), and supports creating Android and iOS builds out-of-the-box. However, in order for CodePush to calculate the hash of your binary contents on Android, PhoneGap Build needs to use Gradle to build your app, which isn't its default behavior (it uses Ant). To resolve this, simply add the following element to your app's `config.xml` file, as a child of the `<platform name=""android"">` element:

```xml
<preference name=""android-build-tool"" value=""gradle"" />
```

## Example Apps

The Cordova community has graciously created some awesome open source apps that can serve as examples for developers that are getting started. The following is a list of OSS Cordova apps that are also using CodePush, and can therefore be used to see how others are using the service:

* [PGDay CodePush Demo](https://github.com/rangle/pgdays-codepush-demo) - Demo app created by [Rangle.io](http://rangle.io) used for [PhoneGap Day Europe 2016](http://pgday.phonegap.com/eu2016/).

*Note: If you've developed a Cordova app using CodePush, that is also open-source, please let us know. We would love to add it to this list!*

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
67,microsoft/PowerBI-visuals-AsterPlot,HTML,"# PowerBI-visuals-AsterPlot
[![Build Status](https://travis-ci.org/Microsoft/PowerBI-visuals-AsterPlot.svg?branch=master)](https://travis-ci.org/Microsoft/PowerBI-visuals-AsterPlot) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/PowerBI-visuals-AsterPlot/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/PowerBI-visuals-AsterPlot?branch=master)

> An Aster plot is a twist on a standard donut chart, using a second value to drive sweep angle.

![Aster plot screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680537/Asset_a1b3a886-e716-453a-96d1-fc96890b4817/AsterPlotscreenshot1.png)
# Overview
The Aster Plot allows a category that drives the chart and up to 2 measures:

The first measure controls the depth of each section

The second measure controls the width of each section

See also [Aster Plot chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380759&sourcecorrid=dd768b2b-0dc9-44e7-8c0e-01a6f95349d6&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)"
68,microsoft/azure-maven-plugins,Java,"# Maven Plugins for Azure Services

This repository contains all Maven plugins for Microsoft Azure services. 

* [Plugins](#plugins)
* [Authentication](#Authentication)
* [Common Configurations](#Common-Configurations)
* [CI/CD in Azure DevOps](#CI-CD-in-Azure-DevOps)
* [Feedback and Questions](#Feedback-and-Questions)
* [Contributing](#Contributing)
* [Telemetry](#Telemetry)

For more information on authentication, common configurations, CI CD, and general plugin documentation, [see the Wiki](https://github.com/microsoft/azure-maven-plugins/wiki).

## Plugins

Maven Plugin | Maven Central Version | Build Status
---|---|---
[Maven Plugin for Azure Web Apps](./azure-webapp-maven-plugin/README.md) | [![Maven Central](https://img.shields.io/maven-central/v/com.microsoft.azure/azure-webapp-maven-plugin.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.microsoft.azure%22%20AND%20a%3A%22azure-webapp-maven-plugin%22) | [![AppVeyor Webapp Plugin](https://ci.appveyor.com/api/projects/status/0vr4svfgl9u3rcaw/branch/develop?svg=true)](https://ci.appveyor.com/project/xscript/azure-maven-plugins-xt3xm)
[Maven Plugin for Azure Functions](https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Functions) | [![Maven Central](https://img.shields.io/maven-central/v/com.microsoft.azure/azure-functions-maven-plugin.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.microsoft.azure%22%20AND%20a%3A%22azure-functions-maven-plugin%22) | [![AppVeyor Function Plugin](https://ci.appveyor.com/api/projects/status/5jti4qwh0j4ekh72/branch/develop?svg=true)](https://ci.appveyor.com/project/xscript/azure-maven-plugins-vvy0i)
[Maven Plugin for Azure Spring Cloud](https://github.com/microsoft/azure-maven-plugins/wiki/Azure-Spring-Cloud) | [![Maven Central](https://img.shields.io/maven-central/v/com.microsoft.azure/azure-spring-cloud-maven-plugin.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.microsoft.azure%22%20AND%20a%3A%22azure-spring-cloud-maven-plugin%22) | 


## Authentication

All the Azure Maven plugins share the same authentication logic. There are 4 authentication methods by priority order:

1. [Service Principles in plugin configuration](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#service-principles-in-plugin-configuration)
1. [Service Principles in settings.xml](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#service-principles-in-settings.xml) (Recommended for production use)
1. [Maven Plugin for Azure Account](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#maven-plugin-for-azure-account) (Default if no other method are used)
1. [Azure CLI](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#azure-cli)

For example, if you have not only Service Principles configured in your plugin configuration, but also Azure CLI installed and logged in, the Azure Maven plugins will use the Service Principles in your plugin configuration.

If no credential found, Azure Maven plugins will automatically log you in with the third method like OAuth or DeviceLogin provided by Maven Plugin for Azure Account.

### AuthType (since Web App 1.9.0)
You can specify which authentication method to use with <authType> in Maven configuration, the default value is auto, and here are all the valid values:

* service_principal
    * Will use credential specified in plugin configuration or Maven settings.xml, this is also the first priority authentication method in auto
* azure_auth_maven_plugin
    * Will use credential provided by Azure Auth Maven Plugin, it will first consume existing secret files, and will guide you auth with Oath or Device Login if you hadn't authenticated with Auth Maven Plugin before.
* azure_cli
    * Will use credential provided by Azure CLI, this could also be used in Azure Cloud Shell.
* auto
    * Will try all the auth methods in the following sequence: service_principal, azure_auth_maven_plugin(existing secret files), azure_cli, azure_auth_maven_plugin

> Maven plugin will only try the specific authentication method (except auto) if <AuthType> is set in configuration.

See the [Authentication](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication) section in our wiki for more information.

## Common Configurations
The three Maven Plugins for Azure Web App/Functions/Spring Cloud support below configuration properties.

| Property | Required | Description | Version |
| --- | --- | --- | --- | 
| \<subscriptionId> | false	| Specifies the target subscription.<br>Use this setting when you have multiple subscriptions in your authentication file. | WebApp: 0.1.0<br>Function: 0.1.0<br>Spring: 1.0.0 |
| \<allowTelemetry> | false | Specifies whether to allow this plugin to send telemetry data; default value is true. | 	WebApp: 0.1.0 <br> Function: 0.1.0 <br> Spring: 1.0.0 |
| \<auth> | false | Specifies auth configuration. For more info, please refer to [here.](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#authentication) | WebApp:0.1.0 <br> Function:0.1.0 | 
 | \<authType> | false | Specifies which authentication method to use, default value is auto. For more infos, please refer to [here.](https://github.com/microsoft/azure-maven-plugins/wiki/Authentication#authtype) | WebApp:1.9.0 | 
 | \<skip> | false | Specifies whether to skip execution. Default value is false. | WebApp: 0.1.4 <br> Function: 0.1.0 | 

## Feedback and Questions
To report bugs or request new features, file issues on [Issues](https://github.com/microsoft/azure-maven-plugins/issues). Or, ask questions on [Stack Overflow with tag azure-java-tools](https://stackoverflow.com/questions/tagged/azure-java-tools).

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once in all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Telemetry
This project collects usage data and sends it to Microsoft to help improve our products and services.
Read Microsoft's [privacy statement](https://privacy.microsoft.com/en-us/privacystatement) to learn more.
If you would like to opt out of sending telemetry data to Microsoft, you can set `allowTelemetry` to false in the plugin configuration.
Please read our [documents](https://aka.ms/azure-maven-config) to find more details."
69,microsoft/tsiclient,TypeScript,"# TSIClient: The Azure Time Series Insights JavaScript SDK

<a href=""https://tsiclientsample.azurewebsites.net""><img src=""https://insights.timeseries.azure.com/favicon.ico"" align=""left"" hspace=""10"" vspace=""6"" height=""100px""></a>

The Azure Time Series Insights JavaScript SDK (aka **tsiclient**) is a JavaScript library for Microsoft Azure Time Series Insights, featuring components for data visualization and analytics, utilities for making calls directly to the TSI Platform API, and more.  **tsiclient** also ships with an associated CSS file (which you must include using your preferred css linking method), which makes the components look great out of the box.


[![License: MIT](https://img.shields.io/badge/License-MIT-red.svg)](https://opensource.org/licenses/MIT) [![npm version](https://badge.fury.io/js/tsiclient.svg)](https://badge.fury.io/js/tsiclient) 

## Resources

* [API Reference documentation](docs/API.md)
* [Product documentation](https://docs.microsoft.com/azure/time-series-insights/)
* [Authorization and authentication](https://docs.microsoft.com/azure/time-series-insights/time-series-insights-authentication-and-authorization)
* [Hosted tsiclient samples](https://tsiclientsample.azurewebsites.net)

## Installing

If you use npm, `npm install tsiclient`. You can also load directly from [unpkg](https://unpkg.com/tsiclient/). For example:

```html
<script src=""https://unpkg.com/tsiclient@latest/tsiclient.js""></script>
<link rel=""stylesheet"" type=""text/css"" href=""https://unpkg.com/tsiclient@latest/tsiclient.css""></link>
```

To import all of **tsiclient** into an ES2015 application, import everything into a namespace, like so...

```js
import TsiClient from ""tsiclient"";

// later, when you want a line chart
let tsiClient = new TsiClient();
let lineChart = new tsiClient.ux.LineChart(document.getElementById('chart'));
```

You can also import components individually.  If you only need the LineChart, you can import it like so...

```js
import LineChart from 'tsiclient/LineChart'

// later when you want a line chart
let lineChart = new LineChart(document.getElementById('chart'));
```
Importing individual components can help significantly reduce your bundle size as they work better with tree shaking. This is the recommended approach if your app only consumes specific components.

To import the tsiclient stylesheet into an ES2015 application, import either `tsiclient.css` or `tsiclient.min.css`, like so...

```js
import 'tsiclient/tsiclient.css' // Standard styles
import 'tsiclient/tsiclient.min.css' // Minified styles
```

## Release Notes

Starting with version 1.3.0, discrete events and state transitions will be represented just like numeric time series in the LineChart component.  This may be a breaking change for users representing non-numeric series in the line chart using the ""events"" and ""states"" Chart Options.  For usage instructions, consult [this example](https://tsiclientsample.azurewebsites.net/noauth/multipleseriestypes.html) and the associated [documentation](https://github.com/microsoft/tsiclient/blob/master/docs/UX.md#line-chart).


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."
70,microsoft/kiota,C#,"# Project

[![Dotnet](https://github.com/microsoft/kiota/actions/workflows/dotnet.yml/badge.svg)](https://github.com/microsoft/kiota/actions/workflows/dotnet.yml) [![CodeQL](https://github.com/microsoft/kiota/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/microsoft/kiota/actions/workflows/codeql-analysis.yml) [![Coverage](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota&metric=coverage)](https://sonarcloud.io/dashboard?id=microsoft_kiota) [![Sonarcloud Status](https://sonarcloud.io/api/project_badges/measure?project=microsoft_kiota&metric=alert_status)](https://sonarcloud.io/dashboard?id=microsoft_kiota)

Kiota is a project to build an OpenAPI based code generator for creating SDKs for HTTP APIs. The goal is to produce a lightweight, low maintenance, code generator that is fast enough to run as part of the compile time tool-chain but scalable enough to handle the largest APIs. Kiota generates a lightweight set of strongly typed classes that layer over a core HTTP library and produce an intuitive and discoverable way of creating HTTP requests. A set of abstractions decouple the generated service library from the core allowing a variety of core libraries to be supported.

This library builds on top of the [Microsoft.OpenAPI.NET](https://github.com/microsoft/openapi.net) library to ensure comprehensive support for APIs that use OpenAPI descriptions. One of the goals of the project is to provide the best code generator support possible for OpenAPI and JSON Schema features.

## Getting started

### Required tools

- [.NET SDK 5.0](https://dotnet.microsoft.com/download) *
- [Visual Studio Code](https://code.visualstudio.com/)
- [Microsoft Graph PowerShell SDK](https://github.com/microsoftgraph/msgraph-sdk-powershell), cloned into the same parent folder of this repository. This dependency is only required if you want to generate SDKs for Microsoft Graph.

#### TypeScript tools

- [NodeJS 14](https://nodejs.org/en/) *
- [TypeScript](https://www.typescriptlang.org/) `npm i -g typescript` *

#### Java tools

- [JDK 16](https://adoptopenjdk.net/) *
- [Gradle 7](https://gradle.org/install/) *

#### Dotnet tools

No additional tools are required for dotnet projects.

> Note: tools marked with * are required.

### Generating SDKs

You can either clone the repository and build Kiota locally, download and run binaries or run the docker image.

#### Running Kiota with Docker

1. Navigate to [New personal access token](https://github.com/settings/tokens/new) and generate a new token. (permissions: read:package).
1. Copy the token, you will need it later.
1. Enable the SSO on the token if you are a Microsoft employee.
1. Execute the following command to login to the registry.

    ```Shell
    echo ""<the personal access token>"" | docker login https://docker.pkg.github.com/microsoft/kiota/generator -u baywet --password-stdin
    ```

1. Execute the following command to start generating SDKs

    ```Shell
    docker run -v /some/output/path:/app/output -v /some/input/description.yml:/app/openapi.yml docker.pkg.github.com/microsoft/kiota/generator --language csharp -n samespaceprefix
    ```

    > Note: you can alternatively use the --openapi parameter with a URI instead of volume mapping.

> Note: steps 1-4 only need to be done once per machine.

#### Building Kiota

First, clone the current repository. You can either use Visual Studio Code or Visual Studio or execute the following commands:

```Shell
dotnet publish ./src/kiota/kiota.csproj -c Release -p:PublishSingleFile=true -r win-x64
```

> Note: refer to [.NET runtime identifier catalog](https://docs.microsoft.com/en-us/dotnet/core/rid-catalog) so select the appropriate runtime for your platform.

Navigate to the output directory (usually under `src/kiota/bin/Release/net5.0`) and start generating SDKs by running Kiota.

#### Running Kiota from binaries

If you haven't built kiota locally, select the appropriate version from the [releases page](https://github.com/microsoft/kiota/releases).

```Shell
kiota.exe --openapi ../msgraph-sdk-powershell/openApiDocs/v1.0/mail.yml --language csharp -o ../somepath -n namespaceprefix
```

> Note: once your SDK is generated in your target project, you will need to add references to kiota abstractions and kiota core in your project. Refer to [Initializing target projects](#initializing-target-projects)

#### Parameters reference

Kiota accepts the following parameters during the generation:

| Name | Shorthand | Required | Description | Accepted values | Default Value |
| ---- | --------- | -------- | ----------- | --------------- | ------------- |
| class-name | c | no | The class name to use the for main entry point | A valid class name according to the target language specification. | GraphClient |
| language | l | no | The programming language to generate the SDK in. | csharp, java, or typescript | csharp |
| loglevel |  | no | The log level to use when logging events to the main output. | Microsoft.Extensions.Logging.LogLevel values | Warning |
| namespace-name | n | no | The namespace name to use the for main entry point. | Valid namespace/module name according to target language specifications. | GraphClient |
| openapi |  | no | URI or Path to the OpenAPI description (JSON or YAML) to use to generate the SDK. | A valid URI pointing to an HTTP document or a file on the local file-system. | ./openapi.yml |
| output | o | no | The output path of the folder the code will be generated in. The folders will be created during the generation if they don't already exist. | A valid path to a folder. | ./output |

### Debugging

If you are using Visual Studio Code as your IDE, the **launch.json** file already contains the configuration to run Kiota. By default this configuration will use the `openApiDocs/v1.0/Mail.yml` under the PowerShell repository as the OpenAPI to generate an SDK for. By default this configuration will output the generated files in a graphdotnetv4|graphjavav4|graphtypescriptv4 folder located in the parent folder this repository is cloned in.

Selecting the language you want to generate an SDK for in the Visual Studio Debug tab and hitting **F5** will automatically build, start, and attach the debugging process to Kiota.

### Initializing target projects

Before you can compile and run the target project, you will need to initialize it. After initializing the test project, you will need to add references to the [abstraction](./abstractions) and the [core](./core) package from the GitHub feed.

#### TypeScript initialization

Clone a NodeJS/front end TypeScript starter like [this one](https://github.com/FreekMencke/node-typescript-starter).

```Shell
npm i @azure/identity node-fetch
```

#### Java initialization

Execute the following command in the directory you want to initialize the project in.

```Shell
gradle init
# Select a console application
```

Edit `utilities/build.gradle` to add the following dependencies.

```Groovy
api 'com.google.code.findbugs:jsr305:3.0.2'
api 'com.azure:azure-identity:1.2.5'
api 'com.squareup.okhttp3:okhttp:4.9.1'
api 'com.google.code.gson:gson:2.8.6'
```

#### Dotnet initialization

Execute the following command in the directory you want to initialize the project in.

```Shell
dotnet new console
dotnet new gitignore
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
71,microsoft/botbuilder-tools,JavaScript,"
# ![Bot Framework Tools](./docs/media/BotFrameWorkTools-header.png)

### [Click here to find out what's new for //build2019!](https://github.com/Microsoft/botframework/blob/master/whats-new.md#whats-new)

## The new BF CLI replaces legacy standalone tools

The Bot Framework SDK team is happy to announce the General Availability of the consolidated bot framework CLI tool [bf-cli](https://aka.ms/bfcli). The new BF CLI tool will replace legacy standalone tools to manage Bot Framework bots and related services. The old tools will be ported over in phases and all new features, bug fixes, and further investments will focus on the new BF CLI.  Old tools will still work for the time being, but they are going to be deprecated in future releases.

Upon the release of Bot Framework SDK version 4.6 the following legacy tools have been ported: Chatdown, QnAMaker, LuisGen, and LuDown.  Dispatch CLI is on the path to be deprecated and replaced with [Orchestrator](https://aka.ms/bf-orchestrator).

To learn more about the BF CLI please visit the [BF CLI github repository](https://aka.ms/bfcli).

__The following page is about the legacy tools.__

# Bot Framework Tools 
[![Build Status](https://fuselabs.visualstudio.com/SDK_v4/_apis/build/status/Tools/Botbuilder-tools-js-daily?branchName=master)](https://fuselabs.visualstudio.com/SDK_v4/_build/latest?definitionId=467&branchName=master) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/botbuilder-tools/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/botbuilder-tools?branch=master) [![lerna](https://img.shields.io/badge/maintained%20with-lerna-cc00ff.svg)](https://lernajs.io/)

The Bot Framework tools are a collection of cross-platform command line tools designed to cover end-to-end bot development workflow. This repo is part of the [Microsoft Bot Framework](https://github.com/Microsoft/botframework) -  a comprehensive framework for building enterprise-grade conversational AI experiences.

| Stable release   | Tool | Description |
|-----------------|------|--------------|
| [![npm version](https://badge.fury.io/js/chatdown.svg)](https://badge.fury.io/js/chatdown) | [Chatdown](packages/Chatdown) | Prototype mock conversations in markdown and convert the markdown to transcripts you can load and view in the new V4 Bot Framework Emulator |
| [![npm version](https://badge.fury.io/js/msbot.svg)](https://badge.fury.io/js/msbot) |[MSBot](packages/MSBot)| Create and manage connected services in your bot configuration file|
| [![npm version](https://badge.fury.io/js/ludown.svg)](https://badge.fury.io/js/ludown) |[LUDown](packages/Ludown)| Build LUIS language understanding models using markdown files|
| [![npm version](https://badge.fury.io/js/luis-apis.svg)](https://badge.fury.io/js/luis-apis) |[LUIS](packages/LUIS)| Create and manage your [LUIS.ai](http://luis.ai) applications |
| [![npm version](https://badge.fury.io/js/qnamaker.svg)](https://badge.fury.io/js/qnamaker) |[QnAMaker](packages/QnAMaker) | Create and manage [QnAMaker.ai](http://qnamaker.ai) Knowledge Bases. |
| [![npm version](https://badge.fury.io/js/botdispatch.svg)](https://badge.fury.io/js/botdispatch) | [Dispatch](packages/Dispatch) | Build language models allowing you to dispatch between disparate components (such as QnA, LUIS and custom code)|
| [![npm version](https://badge.fury.io/js/luisgen.svg)](https://badge.fury.io/js/luisgen)| [LUISGen](packages/LUISGen) | Auto generate backing C#/Typescript classes for your LUIS intents and entities.|
## Install CLI tools:
Pre-requisite:
- [Node.js](https://nodejs.org/) version 10.14.1 or higher
- [.NET Core SDK](https://www.microsoft.com/net/download) version 2.1.403 or higher

```
npm install -g chatdown msbot ludown luis-apis qnamaker botdispatch luisgen
```

## Overview

- Please see [here](https://aka.ms/BotBuilderOverview) for an overview of the end-to-end bot development workflow.
- Please see [here](./tools-overview.md) for an overview of using Bot Builder tools throughout various phases of bot development.

Bot Builder tools are designed to work with
- Bot Builder V4 SDK - [C# SDK](https://github.com/microsoft/botbuilder-dotnet), [JS SDK](https://github.com/microsoft/botbuilder-js)
- [Bot Builder V3 SDK](https://github.com/microsoft/botbuilder-v3)
- [Bot Framework Emulator V4](https://github.com/Microsoft/BotFramework-Emulator/releases)

Before writing code, review the [bot design guidelines](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-design-principles) for best practices and identify the needs for your bot: will a basic bot be enough or whether it should have more sophisticated capabilities, such as speech, language understanding, QnA, or the ability to extract knowledge from different sources and provide intelligent answers. This is also the phase where you might want to create mockup of conversations between the user and the bot for the specific scenarios your bot will support. [Chatdown](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/Chatdown) is the tool built for this purpose. You can author .chat files that mockup the conversations and then use chatdown CLI to convert them into rich transcripts.

As you build your bot, you may also need to integrate AI services like [LUIS.ai](http://luis.ai) for language understanding, [QnAMaker.ai](http://qnamaker.ai) for your bot to respond to simple questions in a Q&A format, and more. You can bootstrap language understanding for your bot using [LUDown](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/Ludown).

The tools are designed to work together. You can then use [LUIS](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/LUIS) CLI and/or the [QnAMaker](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/QnAMaker) CLI tools to create your LUIS.ai models and QnAMaker knowledge base.

As your bot grows in sophistication, [Dispatch](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/Dispatch) CLI can help create and evaluate LUIS models used to dispatch intent across multiple bot modules such as LUIS models, QnA knowledge bases and others (added to dispatch as a file type).

Throughout the Build phase, you can use [MSBot](https://github.com/Microsoft/botbuilder-tools/tree/master/packages/MSBot) CLI to create and keep your bot configuration file updated with all relevant service references.

To test and refine your bot, you can use the new [V4 Bot Framework Emulator](https://github.com/Microsoft/BotFramework-Emulator/releases). The Bot Framework Emulator is a cross-platform [Electron](https://electronjs.org/) application that enables you to test and debug your bots on local machine or in the cloud. The new emulator includes features like faster load times, an improved dynamic layout model, support for multiple bot configurations, simple bot components management, and the ability to inspect responses from connected services such as LUIS and QnA. The Bot Framework Emulator also deepens links to different parts used by the bot. The Bot Framework Emulator new functionality enables you to debug bots based on transcript logs and to view previous chat in presentation mode. The Bot Framework Emulator is available as open source on [Github](https://github.com/Microsoft/BotFramework-Emulator).

With the [Azure CLI Bot extension](./AzureCli), you can create, download, publish, configure channels with the [Azure Bot Service](https://azure.microsoft.com/en-us/services/bot-service/). Azure CLI Bot extension requires [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) (version 2.0.45 or higher]

## Building the tools

In order to build the SDK, ensure that you have [Git](https://git-scm.com/downloads) and [Node.js](https://nodejs.org/en/) installed.

Run the following commands to build all tools.

```
npm install
npm run build
```

Run the following command to verify your installation.

```
npm run test
```

This repository uses [lerna](https://github.com/lerna/lerna) to manage the packages included. This allows you to execute scripts for all packages or only for some packages. For instance, `lerna run test` will run all tests in each package, but `lerna run test --scope chatdown` will run the tests of chatdown.

To use lerna, install it as a global package with `npm install lerna --global`.



## Nightly builds

Nightly builds are generated using the latest code. Therefore, they may not be stable, and most likely lack up to date documentation. These builds are better suited for more experienced users, although everyone is welcome to use them and provide feedback.

You can get the latest nightly build of MSBot from the [BotBuilder MyGet](https://botbuilder.myget.org/gallery) feed. To install the nightly -

```shell
npm config set registry https://botbuilder.myget.org/F/botbuilder-tools-daily/npm/
```

Install using npm:
```shell
npm i -g chatdown msbot ludown luis-apis qnamaker botdispatch luisgen
```

To reset registry:
```shell
npm config set registry https://registry.npmjs.org/
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Reporting Security Issues
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the [MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

Copyright (c) Microsoft Corporation. All rights reserved.
"
72,microsoft/azure-pipelines-extensions,PowerShell,"# Azure Pipeline extensions for Azure DevOps

This repository is a common place for all the extensions that Azure DevOps team publishes as **Microsoft** or **Microsoft DevLabs** publisher.

## How to Build 

Ensure you have installed Node.js. Clone the repository, and go to the root folder of the repository and run the following commands. 

- `npm install` will install all the node modules required to run gulp to package, build etc.
- `gulp build`  will copy each task to ""_build"" folder, and install it's dependencies locally (wrt to the task) and copies the common modules required to run the task.
- `gulp test` will run all pester or mocha tests written for each task, in the Tests folder. 

## How to package extensions

You'll have to run `gulp build` and `gulp test` before you start packaging.

- `gulp package` will package all the extensions and stores them in ""_package"" folder.
- `gulp package --publisher=<publisher_name>` will package all the extensions under a new publisher name that you specify in ""_package"" folder.
- `gulp package --extension=<extension_name>` will package the single extension you mention, and stores it in ""_package"" folder.
- PS: Tested the compatibility with node version 10.22.0 on a windows machine. 

## Updating Feed

Feed with various nugets to consume resides at [this location](https://1essharedassets.visualstudio.com/1esPkgs/_packaging?_a=feed&feed=vsts_rm_extensions)

Feed can be updated/republished by executing [this build definition](https://dev.azure.com/mseng/AzureDevOps/_build?definitionId=6226&_a=summary)
"
73,microsoft/ShortStack,C#,"# Introduction
ShortStack is a tool to transform the way you check in code so that you get more and better code reviews.  
 The typical developer will check in something large and painful at the end of a period of private feature
 development, but with ShortStack, it becomes easy to create small pull requests for each atomic piece of
 work you do to advance the feature.   This allows you to get feedback EARLY, because you won't have to wait for 
 code reviews, and it will make code reviews MORE EFFECTIVE because you can isolate logic changes from 
 each other and from trivial changes such as boiler plate code.

# Getting Started
To get started with the powershell script:
1. Clone short stack to you local drive:  ```git clone https://github.com/microsoft/ShortStack.git [local folder path]```
2. Enable custom scripts on your machine with this powershell command:  ```Set-ExecutionPolicy Unrestricted```
3. Install posh-git:  ```PowerShellGet\Install-Module posh-git -Scope CurrentUser```
4. Get a VSTS access token:
    1. Visit https://mscodehub.visualstudio.com/ShortStack/_git/ShortStack
    2. Click your user icon and click 'Security' 
    3. Click on 'Add' to add a new token (make it active for a year)
    4. Click 'Create Token', then immediately copy the value displayed.
5. Edit your powershell profile with this command: ```notepad $profile```. If prompted, create the file if it doesn't already exist and then add these lines:
```
Import-Module Posh-Git -Force
Import-Module c:\tools\shortstack\scripts\ShortStack.psm1 -Force
$VSTSPersonalAccessToken=""(your VSTS personal access key)""
```
6. Close and re-open powershell to make sure your profile works.  You should be able to type ```ss``` on the command line and see ShortStack Help.
7. In each repository, Create a default reviewers file:
    1. Create the file ""stackprefs.txt"" in your .git folder
    2. run ```Get-VSTSUserGuids``` to see the user id's of users in your repository
    3. for each reviewer you want to include, add this line: ```reviewerid=(vsts_guid_of_user)``` (lines starting with '#' are treated as comments)


# Build and Test
Building and running the cmdlets:

1. Open .sln in Visual Studio 2019 or later

# Contribute
Please feel free to fork your own repo and submit your work as a PR.  We will code review and bring in changes that meet a high quality bar.
"
74,microsoft/powerbi-visuals-heatmap,TypeScript,"# Contributing

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

![HeatMap screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680662/Asset_12af7ce5-f2f7-4aad-9da8-8b00cb225b0d/TableHeatmapscreenshot2.png)
# Overview
Use this custom visual to build a table heat map that can be used to visualise and compare data values in an easy and intuitive way.

You have a built-in option within this visual to specify the number of buckets used for splitting your data.

Additionally, you can also customise it by choosing a colour scheme in line with your brand colours

See also [Table Heatmap at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380818&sourcecorrid=5eb141b8-0a43-4e89-a987-ca286076d449&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)"
75,microsoft/dotnet,HTML,"# .NET Home

This repository is a starting point to learn about and engage in .NET and .NET open source projects.

This repository is not an official .NET Framework support location, however, we will respond to issues filed here as best we can. Please file .NET Core product issues at [dotnet/core](https://github.com/dotnet/core/issues) and ASP.NET Core product issues at [aspnet/home](https://github.com/aspnet/home/issues).

You can try out an early access release of the .NET Framework at the [.NET Framework Early Access](https://github.com/microsoft/dotnet-framework-early-access) website.

## In this repository

- [.NET Framework Release Notes](releases/README.md)
- [.NET Framework Documentation](Documentation/README.md)
- [.NET Open Source Developer Projects](dotnet-developer-projects.md)
- [.NET Open Source Consumer Projects](dotnet-consumer-projects.md)
- [Free Services & Tools for Open Source .NET Projects](dotnet-free-oss-services.md)

Please contribute to this repository via [pull requests](https://github.com/Microsoft/dotnet/pulls)

## Finding .NET Open Source Projects

Here are some excellent community-maintained lists of projects:

- [Awesome .NET!](https://github.com/quozd/awesome-dotnet)
- [ASP.NET Core Library and Framework Support](https://github.com/jpsingleton/ANCLAFS)

There are many projects that you can use and contribute to, some of which are listed below. Please do contribute to these projects!

### .NET Core

- [.NET Core (dotnet/core)](https://github.com/dotnet/core)
- [.NET Core docs (dotnet/docs)](https://github.com/dotnet/docs)
- [ASP.NET Core (dotnet/aspnetcore)](https://github.com/dotnet/aspnetcore)
- [ASP.NET Core docs (dotnet/AspNetCore.Docs)](https://github.com/dotnet/AspNetCore.Docs)
- [Roslyn Compiler Platform (dotnet/roslyn)](https://github.com/dotnet/roslyn)
- [EntityFramework (dotnet/efcore)](https://github.com/dotnet/efcore)
- [WPF (dotnet/wpf)](https://github.com/dotnet/wpf)
- [Windows Forms (dotnet/winforms)](https://github.com/dotnet/winforms)

### .NET Framework

- [.NET Framework docs (dotnet/docs)](https://github.com/dotnet/docs)
- [.NET Framework source code - read-only subset (microsoft/referencesource)](https://github.com/microsoft/referencesource)

### Xamarin

- [Xamarin iOS + macOS (xamarin-macios)](https://github.com/xamarin/xamarin-macios)
- [Xamarin Android (xamarin/xamarin-android)](https://github.com/xamarin/xamarin-android)
- [Xamarin Forms (xamarin/Xamarin.Forms)](https://github.com/xamarin/Xamarin.Forms)
- [Mono Project](https://github.com/mono/)

### Community

Here is a short list of projects to check out:

* [.NET for Apache Spark](https://github.com/dotnet/spark)
* [Orleans](https://github.com/dotnet/orleans)
* [Exceptionless](https://github.com/exceptionless/Exceptionless)
* [Glimpse](https://github.com/Glimpse/Glimpse)
* [JSON.NET](https://github.com/JamesNK/Newtonsoft.Json)
* [MonoGame](https://github.com/MonoGame/MonoGame)
* [MVVM Cross](https://github.com/MvvmCross/MvvmCross)
* [ReactiveUI](https://github.com/reactiveui/ReactiveUI)

There are additional templates available for `dotnet new`. For more information, see [Available templates for dotnet new](https://github.com/dotnet/templating/wiki/Available-templates-for-dotnet-new)

## .NET Foundation

Many .NET open source projects are part of the
[.NET Foundation](https://www.dotnetfoundation.org/projects). Microsoft has contributed many projects, including ASP.NET Core and
.NET Core. You may want to consider [joining the .NET Foundation](https://dotnetfoundation.org/community/).

Check out the [.NET Foundation Forums](https://forums.dotnetfoundation.org/) to see what others are talking about, or start a new discussion to ask a question or make a point. 

## License

This repository is licensed with the [MIT](LICENSE) license.
"
76,microsoft/regexp-i18n,TypeScript,"# RegExpI18n library

[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)](https://github.com/Microsoft/regexp-i18n/blob/master/LICENSE) [![npm version](https://img.shields.io/npm/v/regexp-i18n.svg?style=flat-square)](https://www.npmjs.com/package/regexp-i18n) [![npm downloads](https://img.shields.io/npm/dm/regexp-i18n.svg?style=flat-square)](https://www.npmjs.com/package/regexp-i18n) [![Build Status](https://img.shields.io/travis/Microsoft/regexp-i18n/master.svg?style=flat-square)](https://travis-ci.org/Microsoft/regexp-i18n) [![David](https://img.shields.io/david/Microsoft/regexp-i18n.svg?style=flat-square)](https://github.com/Microsoft/regexp-i18n) [![David](https://img.shields.io/david/dev/Microsoft/regexp-i18n.svg?style=flat-square)](https://github.com/Microsoft/regexp-i18n)

Library provides range of the all letters in Unicode.
This ranges could be used in the RegExp as a part of the range. As ranges include astral symbols from astral pages you need to pass ~u~ flag to the regexp.

Library tested on latest versons of Safari, Chrome, Firefox and Edge browsers.

## Overview

The library designed to provide a way to match any i18n character in any alphabet.

The library exports following building blocks:

### Constants / Ranges

Constants & Ranges represent range of the symbols. You could use any of the constants provided as a part of the range regexp expression. Ranges could be used as an argument for the trim function.

```typescript
import { Constants, Ranges, trim } from 'regexp-i18n';

const matchLetterPattern = '[' + Constants.LETTERS + ']';
const rx = new RegExp(matchLetterPattern, 'ug');

let data = '他走過城市的狗他的兄弟生氣了123';
console.log(data.replace(rx, '')); // 123
console.log(trim(data, Ranges.LETTERS)); // 123
```

1. `LETTERS` - all 18n letters
1. `LETTERS_AND_DIACRITICS` - all i18n letters and diacritics
1. `LETTERS_DIGITS_AND_DIACRITICS` - all i18n letters, digits and diacritics
1. `DIACRITICS` - Special class of characters. Modifies previous character. Can't be stripped out without changing the text meaning.
1. `DIGITS` - all i18n digits
1. `IGNORABLE_SYMBOLS` - all ignorable unicode symbols.

### Patterns

The patterns are regular expressions ranges well tested and reusable.

1. `MATCH_LETTER` - Matches all 18n characters with diacritics. This is a strict pattern. All outstanding diacritics won't be matched.
1. `MATCH_IGNORABLE_SYMBOLS` - Matches ignorable unicode symbols. These symbols are usually are not visible and could be ignored.

```typescript
import { Patterns } from 'regexp-i18n';

const rx = new RegExp(Patterns.MATCH_LETTER, 'ug');

let data = '$ಕನ್ನಡೈಈ123#';
console.log(data.replace(rx, '')); // 123#
```

### Functions

```typescript
replaceNotMatching(pattern: string, replaceValue: string, text: string): string;
```

Attempt to make a function replacing everything not matching to the pattern.
The motivation for it that it is impossible to make an inverse `MATCH_LETTER` pattern.
Not very reliable in the complex cases yet.

```typescript
trim(text: string, range: Range): string;
```

Removes all leading and trailing characters in the given range from the text.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
77,microsoft/powerbi-visuals-gantt,TypeScript,"# powerbi-visuals-gantt
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-gantt.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-gantt) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-gantt/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-gantt?branch=master)

> A Gantt chart is a type of bar chart which illustrates a project timeline or schedule. The Gantt Chart visual shows the Tasks, Start Dates, Durations, % Complete, and Resources for a project. The Gantt Chart visual can be used to show current schedule status using percent-complete shadings and a vertical ""TODAY"" line. The Legend may be used to group or filter tasks based upon data values.

![Gantt chart screenshot](https://github.com/microsoft/powerbi-visuals-gantt/blob/master/assets/screenshot.png?raw=true)

# Overview

Gantt chart is a type of bar chart to illustrate a schedule with time axis. When you put Gantt chart along with other insightful charts in an interactive canvas, you can manage your project in whole new way. In Power BI as visuals interact with each other, you can look at your resource allocation, task completion, remaining tasks in different perspective and get a firm handle on the future.
Gantt charts are indispensable part of project management portfolio. Project Managers and executives love Gantt charts, since they visually show in a very effective at-a-glance way, the overall time line of the project, the current status & progress (or lack thereof) along with the assignment at considerable details.
With this custom visual, you can specify the Tasks, Start Date, Duration and %Completion for rendering them as Gantt. Please note that the %Completion expects a decimal value ( for example 0.85 means 85%) and Start Date , a date field and not a date hierarchy.
You can also control the color of the bar with a Legend. You can use any relevant field from your project as Legend for example task type for this purpose. Additionally, you can also specify the resource field, which would be listed next the bar in the Gantt.

See also [Gantt chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380765&sourcecorrid=968c5e90-8711-48fe-b9b4-a15ad9fe8dc4&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)
"
78,microsoft/vscode-js-profile-visualizer,TypeScript,"# vscode-js-profile-visualizer

A custom editor for viewing `.cpuprofile` files in VS Code. Pairs well with out new [JavaScript debugger](https://github.com/microsoft/vscode-js-debug).

![](./table.png)
![](./flame.png)


## Contributing

This is a Lerna monorepo, with a core package that shares data models and some UI between extensions.

- You can use `npm run watch` to watch _everything_, or, for example, `npm run watch:flame`, to only watch changes to the flame graph extension.
- There's a launch config that runs all extensions.
- `npm run compile`, again with scopes like `compile:flame`, create static compilations of various packages.
- If you need to install a dependency in one package, you can use `lerna add`, or add it to the package.json and then run `lerna bootstrap`.
"
79,microsoft/PowerBI-visuals-Tornado,TypeScript,"# PowerBI-visuals-Tornado
![Node.js CI](https://github.com/microsoft/PowerBI-visuals-Tornado/workflows/Node.js%20CI/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/PowerBI-visuals-Tornado/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/PowerBI-visuals-Tornado?branch=master)


> A bar chart with category values listed vertically. Use for comparing the relative importance of a variable between two distinct groups.

![Tornado chart screenshot](https://github.com/microsoft/PowerBI-visuals-Tornado/blob/master/assets/screenshot.png?raw=true)
# Overview
Tornado charts, are a special type of Bar chart, where the data categories are listed vertically instead of the standard horizontal presentation, and the categories are ordered so that the largest bar appears at the top of the chart, the second largest appears second from the top, and so on. They are so named because the final chart visually resembles either one half of or a complete tornado.

A tornado chart is a common tool used to depict the sensitivity of a result to changes in selected variables. It shows the effect on the output of varying each input variable at a time, keeping all the other input variables at their initial (nominal) values. Typically, you choose a “low” and a “high” value for each input.

See also [Tornado chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380768&sourcecorrid=dff81fda-dee7-4787-a5f6-1203b993fe0c&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)
"
80,microsoft/PhoneticMatching,C#,"[![Build status](https://dev.azure.com/maluuba/PhoneticMatching/_apis/build/status/PhoneticMatching-Build-Binaries-GitHub)](https://dev.azure.com/maluuba/PhoneticMatching/_build/latest?definitionId=96)

# Introduction
A phonetic matching library. Includes text utilities to do string comparisons on phonemes (the sound of the string), as opposed to characters.

Docs can be found at: https://microsoft.github.io/PhoneticMatching/

Supported API:
* C++
* Node.js (>=8.11.2)
* C# .NET Core (>=2.1)

Supported Languages
* English

Current pre-built binaries offered to save the trouble of compiling the source locally.
* node-v{67,64,59,57}-{win32,linux,darwin}-{x64}

(Run `node -p ""process.versions.modules""` to see which Node-ABI in use.)
# Getting Started
This repository consists of TypeScript and native dependencies built with `node-gyp`. See `package.json` for various scripts for the development process.

For first time building remember to `npm install`

This repository uses git submodules. If paths are outdated or non-existent run `git submodule update --init --recursive`

## Install
To install from NPM
```
npm install phoneticmatching
```

## Usage
See the typings for more details. <br> Classes prefixed with `En` make certain assumptions that are specific to the English language.
```ts
import { EnPronouncer, EnPhoneticDistance, FuzzyMatcher, AcceleratedFuzzyMatcher, EnHybridDistance, StringDistance } from ""phoneticmatching"";
```
__Speech__ The namespace containing the type interfaces of the library objects.

__EnPronouncer__ Pronounces a string, as a General English speaker, into its IPA string or array of Phones format.

__matchers__ module:

* __FuzzyMatcher__ Main use case for this library. Returns matches against a list of targets for a given query. The comparisons are not remembered and therefore better for one-off use cases.

* __AcceleratedFuzzyMatcher__ Same interface as `FuzzyMatcher` but the list of targets are precomputed, so beneficial for multiple queries at the cost of a higher initialization time.

* __EnContactMatcher__ A domain specialization of using the `AcceleratedFuzzyMatcher` for English speakers searching over a list of names. Does additional preprocessing and setups up the distance function for you.

* __EnPlaceMatcher__ A domain specialization of using the `AcceleratedFuzzyMatcher` for English speakers searching over a list of places. Does additional preprocessing and setups up the distance function for you.

__distance__ module:

* __EnPhoneticDistance__ Returns a metric distance score between two English pronunciations.

* __StringDistance__ Returns a metric distance score between two strings (edit distance).

* __EnHybridDistance__ Returns a metric distance score based on a combination of the two above distance metrics (English pronunciations and strings).

* __DistanceInput__ Input object for EnHybridDistance. Hold the text and the pronunciation of that text

__nlp__ module:

* __EnPreProcessor__ English Pre-processor.

* __EnPlacesPreProcessor__ English Pre-processor with specific rules for places.

* __SplittingTokenizer__ Tokenizing base-class that will split on the given RegExp.

Here are some example of how to import modules and classes:

```ts
import { EnContactMatcher, EnPlaceMatcher } from ""phoneticmatching"";
```
```ts
import * as Matchers from ""phoneticmatching/lib/matchers"";
```

## Example
JavaScript
```js
// Import core functionality from the library.
const { EnPhoneticDistance, FuzzyMatcher } = require(""phoneticmatching"");

// A distance metric over pronunciations.
const metric = new EnPhoneticDistance();

// The target list to match against.
const targets = [
    ""Apple"",
    ""Banana"",
    ""Blackberry"",
    ""Blueberry"",
    ""Grapefruit"",
    ""Pineapple"",
    ""Raspberry"",
    ""Strawberry"",
];

// Create the fuzzy matcher.
const matcher = new FuzzyMatcher(targets, metric);
// Find the nearest match.
const result = matcher.nearest(""blu airy"");
/* The result should be:
 * {
 *     // The object from the targets list.
 *     element: 'Blueberry',
 *     // The distance score the from distance function.
 *     distance: 0.041666666666666664
 * }
 */
console.log(result);
```
C#
```csharp
using System;

// Import core functionality from the library.
using Microsoft.PhoneticMatching.Matchers.FuzzyMatcher.Normalized;

public class Program
{
    public static void Main(string[] args)
    {
        // The target list to match against.
        string[] targets = 
        {
            ""Apple"",
            ""Banana"",
            ""Blackberry"",
            ""Blueberry"",
            ""Grapefruit"",
            ""Pineapple"",
            ""Raspberry"",
            ""Strawberry"",
        };

        // Create the fuzzy matcher.
        var matcher = new EnPhoneticFuzzyMatcher<string>(targets);

        // Find the nearest match.
        var result = matcher.FindNearest(""blu airy"");

        /* The result should be:
         * {
         *     // The object from the targets list.
         *     element: 'Blueberry',
         *     // The distance score the from distance function.
         *     distance: 0.0416666666666667
         * }
         */
        Console.WriteLine(""element : [{0}] - distance : [{1}]"", result.Element, result.Distance);
    }
}
```

## Build
### TypeScript Transpiling
```
npm run tsc
```
### Native Compiling
```py
# X is the parallelization number, usually set to the number of cores of the machine.
# This cleans and rebuilds everything.
JOBS=X npm run rebuild
# For incremental builds.
JOBS=X npm run build
```

## Test
```py
# Requires native dependencies built, but TypeScript transpiling not required.
npm test
```

## Docs
```py
# Generate the doc files from the docstrings.
npm run build-docs
```

## Release
```py
# Builds everything, TypeScript & native & docs, as a release build.
npm run release
```

## Deployment/Upload
Note that the .js library code and native dependencies will be deployed separately. Npm registries will be used for the .js code, `node-pre-gyp` will be used for prebuilt dependencies while falling back to building on the client.
```py
# Pushes pack to npmjs.com or a private registry if a .npmrc exists.
npm publish
```
```py
# Packages a ./build/stage/{version}/maluubaspeech-{node_abi}-{platform}-{arch}.tar.gz.
# See package.json:binary.host on where to put it.
npm run package
```

## NuGet Publish
A .NET Core NuGet package is published for this project. The package is published by Microsoft. Hence, it must follow guidance at https://aka.ms/nuget and sign package content and package itself with an official Microsoft certificate. To ease signing and publishing process, we integrate ESRP signing to Azure DevOps build tasks.
To publish a new version of the package, create a release for the latest build (Pipelines->Releases->PublishNuget->Create a release).

# Contributors
This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the
instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Reporting Security Issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in
the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

# License
Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the MIT License.

See sources for licenses of dependencies.
"
81,microsoft/PowerBI-visuals-StrippetsBrowser,JavaScript,"[![CircleCI](https://circleci.com/gh/Microsoft/PowerBI-visuals-StrippetsBrowser/tree/master.svg?style=svg)](https://circleci.com/gh/Microsoft/PowerBI-visuals-StrippetsBrowser/tree/master)

# Strippet Browser Custom Visual
![Alt text](assets/screenshot.png?raw=true ""Strippets Browser"")

## Debugging

1. Install ssl certificate by running `npm run install-certificate` and following the steps from: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md)
2. Enable Developer Tools in PowerBI: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md)
3. Run `npm start` to start development.

## Building

1. Run `npm install` to download the dependencies.
2. Run `npm run package` to package the visual.

A `.pbiviz` file will be generated in the `dist` folder

## Testing

Run `npm test`
"
82,microsoft/powerbi-visuals-bulletchart,TypeScript,"# powerbi-visuals-bulletchart
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-bulletchart.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-bulletchart) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-bulletchart/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-bulletchart?branch=master)

> A bullet chart that includes four orientations and a few customization options. Use to feature a single measure against a qualitative range.

![Bullet chart screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680538/Asset_5af45ce2-9b52-4aca-8c72-17e2a47b1c0b/BulletChartscreenshot3.png)
# Overview

Bullet chart serves as a replacement for dashboard gauges and meters. Bullet charts were developed to overcome the fundamental issues of gauges and meters.

The bullet chart features a single, primary measure (for example, current year-to-date revenue), compares that measure to one or more other measures to enrich its meaning (for example, compared to a target), and displays it in the context of qualitative ranges of performance, such as poor, satisfactory, and good. The qualitative ranges are displayed as varying intensities of a single hue to make them discernible by those who are color blind and to restrict the use of colors on the dashboard to a minimum.

Bullet charts may be horizontal or vertical, and may be stacked to allow comparisons of several measures at once.

The Bullet chart consists of 5 primary components:
* Text label: Your chart caption which defines what your chart is about and the unit of measurement.
* Quantitative Scale: Measures the value of your metric on a linear axis.
* The Featured Measure: The bar that displays the primary performance measure (eg: Revenue YTD).
* Comparative Measure: The measure against which you want to compare your featured measure (eg: Target revenue).
* Qualitative Scale: The background fill that encodes qualitative ranges like bad, satisfactory, and good.

See also [Bullet chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380755&sourcecorrid=69216a8c-bd11-4cd0-9e5b-9c4e0469b74b&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)"
83,microsoft/pxt,TypeScript,"# Microsoft MakeCode

* [Try out the editors in your browser...](https://makecode.com)

Microsoft MakeCode is based on the open source project [Microsoft Programming Experience Toolkit (PXT)](https://github.com/microsoft/pxt). ``Microsoft MakeCode`` is the name in the user-facing editors, ``PXT`` is used in all the GitHub sources.

PXT is a framework for creating special-purpose programming experiences for
beginners, especially focused on computer science education. PXT's underlying
programming language is a subset of TypeScript (leaving out JavaScript dynamic
features).

The main features of PXT are:
* a Blockly-based code editor along with converter to the text format
* a Monaco code editor that powers [VS Code](https://github.com/microsoft/vscode), editor's features are listed [here](https://code.visualstudio.com/docs/editor/editingevolved).
* extensibility support to define new blocks in TypeScript
* an ARM Thumb machine code emitter
* a command-line package manager

More info:
* [About](https://makecode.com/about)
* [Documentation](https://makecode.com/docs)

Examples of Editors built with MakeCode:

* https://makecode.microbit.org
* https://arcade.makecode.com
* https://makecode.adafruit.com
* https://minecraft.makecode.com
* https://makecode.mindstorms.com
* https://makecode.chibitronics.com
* More editors at https://makecode.com/labs

## Branches

* ``master`` is the active development branch, currently ``v3.*`` builds
* ``v*`` is the servicing branch for ``v*.*`` builds

## Running a target from localhost

Please follow the [instructions here](https://makecode.com/cli).

## Linking a target to PXT

If you are modifying your own instance of PXT and want a target (such as pxt-microbit) to use your local version, cd to the directory of the target (pxt-microbit, in our example, which should be a directory sibling of pxt) and perform

```
pxt link ../pxt
```

If you have multiple checkouts of pxt, you can do the following:
* run `npm i` in pxt and the target
* in the target, run `pxt link ..\some-other-pxt` (you may need to update your CLI first by running `npm install -g pxt`)

If you run `npm i` afterwards (in either the target or pxt), you might need to repeat these steps.

## Build

First, install [Node](https://nodejs.org/en/): minimum version 8.

To build the PXT command line tools:

```
npm install
npm run build
```

Then install the `pxt` command line tool (only need to do it once):

```
npm install -g pxt
```

After this you can run `pxt` from anywhere within the build tree.

To start the local web server, run `pxt serve` from within the root
of an app target (e.g. pxt-microbit). PXT will open the editor in your default web browser.

If you are developing against pxt, you can run `gulp watch` from within the root of the
pxt repository to watch for changes and rebuild.

```
gulp watch
```

If you are working on the CLI exclusively,

```
gulp watchCli
```

### Icons

There are a number of custom icons (to use in addition
to http://semantic-ui.com/elements/icon.html) in the `svgicons/` directory.
These need to be `1000x1000px`. Best start with an existing one. To see available icons go to
http://localhost:3232/icons.html (this file, along with `icons.css` containing
the generated WOFF icon font, is created during build).

If you're having trouble with display of the icon you created, try:
```
npm install -g svgo
svgo svgicons/myicon.svg
```

### Documentation Highlighting

In the documentation, highlighting of code snippets uses highlight.js (hljs).
Currently, the following languages are included:

* TypeScript
* Python
* JavaScript
* HTML,XML
* Markdown

If you need to add other languages or update existing ones,
you can find the distribution at [https://highlightjs.org/download/](https://highlightjs.org/download/);
select all the languages you want to include (including the ones above!),
download and unzip,
and finally copy over `highlight.pack.js` into `webapp/public/highlight.js/`.

## Tests

The tests are located in the `tests/` subdirectory and are a combination of node and
browser tests. To execute them, run `npm run test:all` in the root directory.

## License

[MIT License](https://github.com/microsoft/pxt/blob/master/LICENSE)

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Contact Us

[Get in touch](https://makecode.com/contact)

## Trademarks

MICROSOFT, the Microsoft Logo, and MAKECODE are registered trademarks of Microsoft Corporation. They can only be used for the purposes described in and in accordance with Microsoft’s Trademark and Brand guidelines published at https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general.aspx. If the use is not covered in Microsoft’s published guidelines or you are not sure, please consult your legal counsel or MakeCode team (makecode@microsoft.com).
"
84,microsoft/arcade-machine-react,TypeScript,"# @mixer/arcade-machine-react

:video_game: Input abstraction layer for gamepads, keyboards, and UWP apps in React.

See [the docs](https://arcademachinedocs.z13.web.core.windows.net/) for more information and interactive examples!
"
85,microsoft/LightGBM,C++,"<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 />

Light Gradient Boosting Machine
===============================

[![Python-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/Python-package/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)
[![R-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/R-package/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)
[![CUDA Version GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/CUDA%20Version/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)
[![Static Analysis GitHub Actions Build Status](https://github.com/microsoft/LightGBM/workflows/Static%20Analysis/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions)
[![Azure Pipelines Build Status](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master)](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1)
[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true)](https://ci.appveyor.com/project/guolinke/lightgbm/branch/master)
[![Documentation Status](https://readthedocs.org/projects/lightgbm/badge/?version=latest)](https://lightgbm.readthedocs.io/)
[![Link checks](https://github.com/microsoft/LightGBM/workflows/Link%20checks/badge.svg)](https://github.com/microsoft/LightGBM/actions?query=workflow%3A%22Link+checks%22)
[![License](https://img.shields.io/github/license/microsoft/lightgbm.svg)](https://github.com/microsoft/LightGBM/blob/master/LICENSE)
[![Python Versions](https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&logoColor=white)](https://pypi.org/project/lightgbm)
[![PyPI Version](https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&logoColor=white)](https://pypi.org/project/lightgbm)
[![CRAN Version](https://www.r-pkg.org/badges/version/lightgbm)](https://cran.r-project.org/package=lightgbm)

LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:

- Faster training speed and higher efficiency.
- Lower memory usage.
- Better accuracy.
- Support of parallel, distributed, and GPU learning.
- Capable of handling large-scale data.

For further details, please refer to [Features](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst).

Benefiting from these advantages, LightGBM is being widely-used in many [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.

[Comparison experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What's more, [distributed learning experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.

Get Started and Documentation
-----------------------------

Our primary documentation is at https://lightgbm.readthedocs.io/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) on that site.

Next you may want to read:

- [**Examples**](https://github.com/microsoft/LightGBM/tree/master/examples) showing command line usage of common tasks.
- [**Features**](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) and algorithms supported by LightGBM.
- [**Parameters**](https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst) is an exhaustive list of customization you can make.
- [**Distributed Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst) and [**GPU Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst) can speed up computation.
- [**Laurae++ interactive documentation**](https://sites.google.com/view/lauraepp/parameters) is a detailed guide for hyperparameters.
- [**FLAML**](https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/) provides automated tuning for LightGBM ([code examples](https://github.com/microsoft/FLAML/blob/main/notebook/flaml_lightgbm.ipynb)).
- [**Optuna Hyperparameter Tuner**](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters ([code examples](https://github.com/optuna/optuna/tree/master/examples/lightgbm)).

Documentation for contributors:

- [**How we update readthedocs.io**](https://github.com/microsoft/LightGBM/blob/master/docs/README.rst).
- Check out the [**Development Guide**](https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst).

News
----

Please refer to changelogs at [GitHub releases](https://github.com/microsoft/LightGBM/releases) page.

Some old update logs are available at [Key Events](https://github.com/microsoft/LightGBM/blob/master/docs/Key-Events.md) page.

External (Unofficial) Repositories
----------------------------------

FLAML (AutoML library for hyperparameter optimization): https://github.com/microsoft/FLAML

Optuna (hyperparameter optimization framework): https://github.com/optuna/optuna

Julia-package: https://github.com/IQVIA-ML/LightGBM.jl

JPMML (Java PMML converter): https://github.com/jpmml/jpmml-lightgbm

Treelite (model compiler for efficient deployment): https://github.com/dmlc/treelite

Hummingbird (model compiler into tensor computations): https://github.com/microsoft/hummingbird

cuML Forest Inference Library (GPU-accelerated inference): https://github.com/rapidsai/cuml

daal4py (Intel CPU-accelerated inference): https://github.com/IntelPython/daal4py

m2cgen (model appliers for various languages): https://github.com/BayesWitnesses/m2cgen

leaves (Go model applier): https://github.com/dmitryikh/leaves

ONNXMLTools (ONNX converter): https://github.com/onnx/onnxmltools

SHAP (model output explainer): https://github.com/slundberg/shap

dtreeviz (decision tree visualization and model interpretation): https://github.com/parrt/dtreeviz

MMLSpark (LightGBM on Spark): https://github.com/Azure/mmlspark

Kubeflow Fairing (LightGBM on Kubernetes): https://github.com/kubeflow/fairing

Kubeflow Operator (LightGBM on Kubernetes): https://github.com/kubeflow/xgboost-operator

ML.NET (.NET/C#-package): https://github.com/dotnet/machinelearning

LightGBM.NET (.NET/C#-package): https://github.com/rca22/LightGBM.Net

Ruby gem: https://github.com/ankane/lightgbm

LightGBM4j (Java high-level binding): https://github.com/metarank/lightgbm4j

lightgbm-rs (Rust binding): https://github.com/vaaaaanquish/lightgbm-rs

MLflow (experiment tracking, model monitoring framework): https://github.com/mlflow/mlflow

`{treesnip}` (R `{parsnip}`-compliant interface): https://github.com/curso-r/treesnip

`{mlr3learners.lightgbm}` (R `{mlr3}`-compliant interface): https://github.com/mlr3learners/mlr3learners.lightgbm

Support
-------

- Ask a question [on Stack Overflow with the `lightgbm` tag](https://stackoverflow.com/questions/ask?tags=lightgbm), we monitor this for new questions.
- Open **bug reports** and **feature requests** (not questions) on [GitHub issues](https://github.com/microsoft/LightGBM/issues).

How to Contribute
-----------------

Check [CONTRIBUTING](https://github.com/microsoft/LightGBM/blob/master/CONTRIBUTING.md) page.

Microsoft Open Source Code of Conduct
-------------------------------------

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

Reference Papers
----------------

Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. ""[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)"". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. ""[A Communication-Efficient Parallel Algorithm for Decision Tree](http://papers.nips.cc/paper/6380-a-communication-efficient-parallel-algorithm-for-decision-tree)"". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.

Huan Zhang, Si Si and Cho-Jui Hsieh. ""[GPU Acceleration for Large-scale Tree Boosting](https://arxiv.org/abs/1706.08359)"". SysML Conference, 2018.

**Note**: If you use LightGBM in your GitHub projects, please add `lightgbm` in the `requirements.txt`.

License
-------

This project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/LightGBM/blob/master/LICENSE) for additional details.
"
86,microsoft/react-native-windows,C++,"<h1 align=""center""> React Native for Windows </h1>

<p align=""center"">
  Build native Windows apps with React.
</p>

<p align=""center"">
  <a href=""https://github.com/microsoft/react-native-windows/blob/master/LICENSE"">
    <img src=""https://img.shields.io/badge/license-MIT-blue.svg"" alt=""React Native for Windows is released under the MIT license."" />
  </a>
  <a href=""https://www.npmjs.org/package/react-native-windows"">
    <img src=""https://img.shields.io/npm/v/react-native-windows?color=e80441&label=react-native-windows"" alt=""Current npm package version."" />
  </a>
  <a href=""https://github.com/microsoft/react-native-windows#contributing"">
    <img src=""https://img.shields.io/badge/PRs-welcome-brightgreen.svg"" alt=""PRs welcome!"" />
  </a>
</p>

![Hero Image with Logo](https://github.com/microsoft/react-native-windows/raw/master/.github/hero2.png)

> See the official [React Native website](https://reactnative.dev/) for an introduction to React Native.

[React Native](https://reactnative.dev) is a framework developed by Facebook that enables you to build world-class application experiences on native platforms using a consistent developer experience based on JavaScript and [React](https://reactjs.org/). The focus of React Native is on developer efficiency across all the platforms you care about - learn once, write anywhere.

This repository adds support for the [Windows 10 SDK](https://developer.microsoft.com/en-us/windows/downloads), which allows you to build apps for [all devices supported by Windows 10](https://developer.microsoft.com/en-us/windows/get-started-windows-10) including PCs, tablets, 2-in-1s, Xbox, Mixed reality devices etc.

Visit the official [React Native for Windows + macOS website](https://microsoft.github.io/react-native-windows) to learn more.

## Contents

- [Requirements](#requirements)
- [Getting Started](#getting-started)
- [Contributing](#contributing)
- [Documentation](#documentation)
- [License](#license)
- [Code of Conduct](#code-of-conduct)

### Status and roadmap
[Check out our blog](https://microsoft.github.io/react-native-windows/blog/) if you'd like to stay up to date on the status of React Native for Windows and check out current and past roadmaps. We will post all new releases, updates and general news about the project there.

## Requirements
You can run React Native Windows apps only on devices supported by the [Windows 10 SDK](https://developer.microsoft.com/en-us/windows/downloads).

For a full and detailed list of the system requirements and how to set up your development platform, see our [System Requirements](https://microsoft.github.io/react-native-windows/docs/rnw-dependencies) documentation on our website.

## Getting Started
See the [Getting Started Guide](https://microsoft.github.io/react-native-windows/docs/getting-started) on our React Native for Windows + macOS website to build your first React Native for Windows app.

### Logging Issues
Search the [existing issues](https://github.com/microsoft/react-native-windows/issues) and try to make sure your problem doesn’t already exist before opening a new issue. If your issue doesn't exist yet, try to make sure you provide as much information as possible to us so we can help you sooner. It’s helpful if you include information like:

- The version of Windows, React Native, React Native Windows extension, and device family (i.e., mobile, desktop, Xbox, etc.) where you ran into the issue.
- A stack trace and reduced repro case when possible.
- Ensure the [appropriate template](https://github.com/microsoft/react-native-windows/issues/new/choose) is used when filing your issue(s).

## Contributing
See [Contributing guidelines](https://github.com/microsoft/react-native-windows/blob/master/docs/contributing.md) for how to setup your fork of the repo and start a PR to contribute to React Native for Windows.

[good first issue](https://github.com/microsoft/react-native-windows/labels/good%20first%20issue) and [help wanted](https://github.com/microsoft/react-native-windows/labels/help%20wanted) are great starting points for PRs.

## Documentation
[React Native already has great documentation](https://reactnative.dev/docs/getting-started) and we're working to ensure the React Native Windows is part of that documentation story.

[React Native for Windows](https://microsoft.github.io/react-native-windows/) has it's own separate documentation site where Windows and macOS specific information, like API docs and blog updates live.

### Examples
- Using the CLI in the [Getting Started](https://microsoft.github.io/react-native-windows/docs/getting-started) guide will set you up with a sample React Native for Windows app that you can begin editing right away.
- If you're looking for sample code, just browse the RNTester folder in the GitHub web UI

## License
The React Native Windows extension, including modifications to the original Facebook source code, and all newly contributed code is provided under the [MIT License](LICENSE). Portions of the React Native Windows extension derived from React Native are copyright Facebook.

## Code of Conduct
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
87,microsoft/PSRule-vscode,PowerShell,"# PSRule

Validate infrastructure as code (IaC) and DevOps repositories using the PSRule PowerShell module.
PSRule is powerful, feature rich, and highly customizable to meet your needs.

![ext-stable-version-badge] ![ext-stable-installs-badge] ![module-version-badge]

This extension is available in two release channels:

Channel | Description | Version/ downloads
------- | ----------- | ---
[Preview][ext-preview] | More frequent releases but more likely to contain bugs. | [![Preview][ext-preview-version-badge]][ext-preview] ![ext-preview-installs-badge]
[Stable][ext-stable] | Less frequent releases, with more user testing, experimental features are disabled. | [![Stable][ext-stable-version-badge]][ext-stable] ![ext-stable-installs-badge]

## Features

### IntelliSense

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/microsoft/PSRule-vscode/main/docs/images/options-schema-flyout.png"" alt=""Options suggestion context menu"" />
</p>

- Adds IntelliSense and validation support for configuring options and resources.
  - **Workspace options** &mdash; use IntelliSense to configure options for the workspace.
    - Type or trigger IntelliSense with `Ctrl+Space` from `ps-rule.yaml`.
  - **Create resources** &mdash; define _baselines_ and _selectors_ by using pre-built snippets and IntelliSense.

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/microsoft/PSRule-vscode/main/docs/images/snippet-rule-type.png"" alt=""Rule definition snippet"" />
</p>

- Adds snippets for defining new rules.
  - **Define rules** with snippets and IntelliSense support.
    - Trigger IntelliSense by typing `rule` in a `.Rule.ps1` file.
    IntelliSense can also be triggered by using the shortcut `Ctrl+Space`.

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/microsoft/PSRule-vscode/main/docs/images/snippet-markdown.png"" alt=""Rule markdown documentation snippet"" />
</p>

- Adds snippets for creating markdown documentation.
  - **Quick documentation**  &mdash; create rule documentation to provide rule recommendations and examples.
    - Trigger IntelliSense by typing `rule` in a `.md` file.
    IntelliSense can also be triggered by using the shortcut `Ctrl+Space`.

## Support

This project uses GitHub Issues to track bugs and feature requests.
Please search the existing issues before filing new issues to avoid duplicates.

- For new issues, file your bug or feature request as a new [issue].
- For help, discussion, and support questions about using this project, join or start a [discussion].

Support for this project/ product is limited to the resources listed above.

## Installing PSRule module

PSRule is available from the PowerShell Gallery and is required for this extension to work.

To install the module use the following command from a PowerShell prompt.

```powershell
Install-Module -Name PSRule -Scope CurrentUser;
```

## Installing the extension

You can install the latest release of the extension by following the steps in the [Visual Studio Code documentation][vscode-ext-gallery].
In the Extensions pane, search for _PSRule_ extension and install it there.
You will get notified automatically about any future extension updates.

```text
code --install-extension bewhite.psrule-vscode-preview
```

> NOTE: If you are using VS Code Insiders, the command will be `code-insiders`.

## Contributing

This project welcomes contributions and suggestions.
If you are ready to contribute, please visit the [contribution guide].

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Maintainers

- [Bernie White](https://github.com/BernieWhite)

## License

This project is [licensed under the MIT License][license].

[issue]: https://github.com/Microsoft/PSRule-vscode/issues
[discussion]: https://github.com/microsoft/PSRule-vscode/discussions
[ci-badge]: https://dev.azure.com/bewhite/PSRule-vscode/_apis/build/status/PSRule-vscode-CI?branchName=main
[vscode-ext-gallery]: https://code.visualstudio.com/docs/editor/extension-gallery
[ext-preview]: https://marketplace.visualstudio.com/items?itemName=bewhite.psrule-vscode-preview
[ext-preview-version-badge]: https://vsmarketplacebadge.apphb.com/version/bewhite.psrule-vscode-preview.svg
[ext-preview-installs-badge]: https://vsmarketplacebadge.apphb.com/installs-short/bewhite.psrule-vscode-preview.svg
[ext-stable]: https://marketplace.visualstudio.com/items?itemName=bewhite.psrule-vscode
[ext-stable-version-badge]: https://vsmarketplacebadge.apphb.com/version/bewhite.psrule-vscode.svg
[ext-stable-installs-badge]: https://vsmarketplacebadge.apphb.com/installs-short/bewhite.psrule-vscode.svg
[module-version-badge]: https://img.shields.io/powershellgallery/v/PSRule.svg?label=PowerShell%20Gallery&color=brightgreen
[contribution guide]: https://github.com/Microsoft/PSRule-vscode/blob/main/CONTRIBUTING.md
[change log]: https://github.com/Microsoft/PSRule-vscode/blob/main/CHANGELOG.md
[license]: https://github.com/Microsoft/PSRule-vscode/blob/main/LICENSE
[chat]: https://gitter.im/PSRule/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
[chat-badge]: https://img.shields.io/static/v1.svg?label=chat&message=on%20gitter&color=informational&logo=gitter
"
88,microsoft/powerbi-visuals-chord,TypeScript,"# powerbi-visuals-chord
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-chord.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-chord) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-chord/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-chord?branch=master)
[![Build Status](https://dev.azure.com/customvisuals/public/_apis/build/status/Microsoft.powerbi-visuals-chord)](https://dev.azure.com/customvisuals/public/_build/latest?definitionId=2) [![Known Vulnerabilities](https://snyk.io/test/github/Microsoft/powerbi-visuals-chord/badge.svg)](https://snyk.io/test/github/Microsoft/powerbi-visuals-chord)

> A chord diagram is a graphical method of displaying the inter-relationships between data in a matrix.

![Chord diagramm screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680579/Asset_75a4e3dc-78d5-4a5d-b14b-d11cc0d3b730/Chordscreenshot1.png)
# Overview
This type of diagram visualizes the inter-relationships between entities. The connections between entities are used to display that they share something in common. This makes Chord Diagrams ideal for comparing the similarities within a dataset or between different groups of data.

Nodes are arranged around a circle, with the relationships between points connected to each other either through the use of arcs or Bézier curves. Values are assigned to each connection, which is represented proportionally by the size of each arc.

Customize data colors, axis, labels and more.

See also [Chord chart at the Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380761&sourcecorrid=6ef257f3-686c-4b70-8b23-0dafd318e298&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)
"
89,microsoft/PowerBI-visuals-WordCloud,TypeScript,"# PowerBI-visuals-WordCloud
[![Build Status](https://travis-ci.org/Microsoft/PowerBI-visuals-WordCloud.svg?branch=master)](https://travis-ci.org/Microsoft/PowerBI-visuals-WordCloud)
[![Coverage Status](https://coveralls.io/repos/github/Microsoft/PowerBI-visuals-WordCloud/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/PowerBI-visuals-WordCloud?branch=master)
> Word Cloud is a visual representation of word frequency and value. Use it to get instant insight into the most important terms in a set.

![Wordcloud chart screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680547/Asset_82e6791c-3970-4cd9-b687-1e9100c8ef5a/WordCloudscreenshot1.png)
# Overview
Word Cloud is a visual representation of word frequency and value. Use it to get instant insight into the most important terms in your data.

With the interactive experience of Word Cloud in Power BI, you no longer have to tediously dig through large volumes of text to find out which terms are prominent or prevalent. You can simply visualize them as Word Cloud and get the big picture instantly and user Power BI’s interactivity to slice and dice further to uncover the themes behind the text content.

This visual also puts you in control on the appearance of the work cloud, be it the size or usage of space and how to treat the data. You can choose to break the words in the text to look for the frequency word or keep word break off to project a measure as a value of the text. You can also enable stop words to remove the common terms from the word cloud to avoid the clutter. By enabling rotation and playing with the angles allowed, you can become very creative with this visual.

Optionally you can also use a measure to provide weightage to the text. If none provided, it will simply use the frequency. Check out the formatting pane for more options.

See also [Word Cloud at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380752&sourcecorrid=037b6fba-5738-4e90-a8ff-c4f1575a0b05&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)"
90,microsoft/azure-devops-extension-tasks,TypeScript,"# Azure DevOps Extension Tasks
Build: [![Build Status](https://dev.azure.com/jessehouwing/azure-devops-extensions/_apis/build/status/azure-devops-extension-tasks/microsoft.azure-devops-extension-tasks?branchName=main&stageName=Build)](https://dev.azure.com/jessehouwing/azure-devops-extensions/_build/latest?definitionId=77&branchName=main) 
Release: [![Build Status](https://dev.azure.com/jessehouwing/azure-devops-extensions/_apis/build/status/azure-devops-extension-tasks/microsoft.azure-devops-extension-tasks?branchName=main&stageName=Publish%20publicly%20to%20MsDevLabs)](https://dev.azure.com/jessehouwing/azure-devops-extensions/_build/latest?definitionId=77&branchName=main)

This extension provides build and release tasks for packaging and publishing Azure Devops Extensions to the [Visual Studio Marketplace](https://marketplace.visualstudio.com). There are also tasks to share and install your extension to your Azure Devops organization or Team Foundation Server.

## To use

[Learn more](https://marketplace.visualstudio.com/items?itemName=ms-devlabs.vsts-developer-tools-build-tasks) about this extension about and install the extension into your Azure DevOps Organisation via the Visual Studio Marketplace.

## Available tasks

Azure DevOps

* **Package**: package an Azure DevOps extension into an extension package (.VSIX) file
* **Publish**: (optionally) package and publish an extension (either privately or publicly) to the Visual Studio Marketplace
* **Unpublish**: unpublish an extension from the Visual Studio Marketplace
* **Share**: share an extension with an Azure DevOps organisation
* **Install**: install an extension to an Azure DevOps organisation
* **Query version**: query an extension's version (to make it easy to increment on your next package or publish)
* **Wait for validation**: waits for the Visual Studio Marketplace validation to come through.

Visual Studio

* **Publish**: Publish a Visual Studio extension to the Visual Studio Marketplace

### Required scopes

 When creating a personal access token for use by your pipeline, make sure the token has at least the following scopes for the task(s) you are using:

* **Publish**: `All accessible organisations`, `Marketplace (publish)`
* **Unpublish**: `All accessible organisations`, `Marketplace (manage)`
* **Share**: `All accessible organisations`, `Marketplace (publish)`
* **Install**: `All accessible organisations` or a specific Organisation, `Extensions (read and manage)`, `Marketplace (acquire)`
* **Query Version**: `All accessible organisations`, `Marketplace (read)`
* **Is Valid**: `All accessible organisations`, `Marketplace (read)`

![Permissions](permissions.png)

## Contribute

1. From the root of the repo run `npm run initdev`. This will pull down the necessary modules and TypeScript declare files.
2. Run `npm run build` to compile the build tasks.
3. Run `npm run package` to create a .vsix extension package that includes the build tasks.
"
91,microsoft/powerbi-visuals-utils-dataviewutils,TypeScript,"# Microsoft Power BI visuals DataViewUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-dataviewutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-dataviewutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-dataviewutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-dataviewutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-dataviewutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-dataviewutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-dataviewutils)

> DataViewUtils is a set of functions and classes in order to simplify parsing of the DataView object for Power BI custom visuals

## Usage
Learn how to install and use the DataViewUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-dataview)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-dataviewutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
92,microsoft/powerbi-visuals-utils-tooltiputils,TypeScript,"# Microsoft Power BI visuals TooltipUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-tooltiputils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-tooltiputils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-tooltiputils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-tooltiputils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-tooltiputils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-tooltiputils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-tooltiputils)

> TooltipUtils is a set of functions and classes in order to simplify usage of the Tooltip API for Power BI custom visuals

## Usage
Learn how to install and use the TooltipUtils in your custom visuals:
* [Installation Guide](./docs/usage/installation-guide.md)
* [Usage Guide](./docs/usage/usage-guide.md)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-tooltiputils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
93,microsoft/license-checker-webpack-plugin,JavaScript,"# license-checker-webpack-plugin

Webpack plugin that verifies licenses of all external dependencies in a compilation, and outputs all that information to a file.

## Installation

### npm

```
npm install license-checker-webpack-plugin --save-dev
```

### yarn

```
yarn add license-checker-webpack-plugin --dev
```

## Usage

Require the plugin into your Webpack configuration, and pass it to the `plugins` array.

```js
const LicenseCheckerWebpackPlugin = require(""license-checker-webpack-plugin"");

module.exports = {
  // ...
  plugins: [new LicenseCheckerWebpackPlugin({ outputFilename: ""ThirdPartyNotices.txt"" })]
};
```

## Options

| Property         | Type                   | Default                                                    | Description                                                                                                                                                       |
| ---------------- | ---------------------- | ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `allow`          | `string`               | `""(Apache-2.0 OR BSD-2-Clause OR BSD-3-Clause OR MIT)""`    | SPDX expression with allowed licenses.                                                                                                                            |
| `ignore`         | `array`                | `[]`                                                       | Array of dependencies to ignore, in the format `[""<dependency name>@<version range>""]`. For example, `[""assignment@^2.0.0""]`.                                     |
| `override`       | `object`               | `{}`                                                       | Object of dependencies to override, in the format `{""<dependency name>@<version range>"": { ... }}`. For example, `{""assignment@^2.0.0"": { licenseName: ""MIT"" }}`. |
| `emitError`      | `boolean`              | `false`                                                    | Whether to emit errors instead of warnings.                                                                                                                       |
| `outputWriter`   | `string` or `function` | See [`defaultOutputWriter`](./src/defaultOutputWriter.js). | Path to a `.ejs` template, or function that will generate the contents of the third-party notices file.                                                           |
| `outputFilename` | `string`               | `""ThirdPartyNotices.txt""`                                  | Name of the third-party notices file with all licensing information.                                                                                              |

The data that gets passed to the `outputWriter` function looks like this:

```json
[
  {
    ""name"": ""react"",
    ""version"": ""16.3.2"",
    ""repository"": ""git+https://github.com/facebook/react.git"",
    ""licenseName"": ""MIT"",
    ""licenseText"": ""MIT License\n\nCopyright (c) 2013-present, Facebook, Inc. [...]""
  },
  {
    ""name"": ""webpack"",
    ""version"": ""4.8.3"",
    ""author"": ""Tobias Koppers @sokra"",
    ""repository"": ""git+https://github.com/webpack/webpack.git"",
    ""licenseName"": ""MIT"",
    ""licenseText"": ""Copyright JS Foundation and other contributors [...]""
  },
  {
    ""name"": ""whatwg-fetch"",
    ""version"": ""2.0.4"",
    ""repository"": ""git+https://github.com/github/fetch.git"",
    ""licenseName"": ""MIT"",
    ""licenseText"": ""Copyright (c) 2014-2016 GitHub, Inc. [...]""
  }
]
```

Here's an example `webpack.config.js` file that uses all options:

```js
const path = require(""path"");
const LicenseCheckerWebpackPlugin = require(""license-checker-webpack-plugin"");
const template = require(""lodash.template"");

module.exports = {
  // ...
  plugins: [
    new LicenseCheckerWebpackPlugin({
      allow: ""(Apache-2.0 OR BSD-2-Clause OR BSD-3-Clause OR MIT)"",
      ignore: [""@microsoft/*""],
      override: {
        ""assignment@2.0.0"": { licenseName: ""MIT"" },
        ""intersection-observer@0.5.0"": { licenseName: ""MIT"" },
        ""querystring-es3@0.2.1"": { licenseName: ""MIT"" }
      },
      emitError: true,
      outputWriter: path.resolve(__dirname, ""customTemplate.ejs""),
      outputFilename: ""ThirdPartyNotices.txt""
    })
  ]
};
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit <https://cla.microsoft.com>.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Licensing

All files on this repository are subject to the MIT license. Please read the `LICENSE` file at the root of the project.
"
94,microsoft/pxt-event-emitter,TypeScript,"# Project

> This repo has been populated by an initial template to help get you started. Please
> make sure to update the content to build a great experience for community-building.

As the maintainer of this project, please make a few updates:

- Improving this README.MD file to provide a great experience
- Updating SUPPORT.MD with content about this project's support experience
- Understanding the security reporting process in SECURITY.MD
- Remove this section from the README

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
95,microsoft/powerbi-visuals-utils-testutils,TypeScript,"# Microsoft Power BI visuals TestUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-testutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-testutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-testutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-testutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-testutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-testutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-testutils)

> TestUtils is a set of mocks and fakes in order to simplify unit testing for Power BI custom visuals

## Usage
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-test)

## 2.3.0 Migration note

From version 2.3.0 `testDom` function returns `HTMLElement` instead of `JQuery` object. If you are using JQuery in tests, wrap the `testDom` calls with `$(...)` for compatibility:

```typescript
    // 2.2.1 and below
    let element: JQuery = testDom(""100"", ""100"");
    // 2.3.0 and above
    let element: JQuery = $(testDom(""100"", ""100""));
```

The motivation is not to force JQuery usage. It might be not necessary in tests. In lots of cases `element.get(0)` is the next operation after receiving an element with `testDom`. Now JQuery is not required to use powerbi-visuals-utils-testutils, so you can drop this dependency. If you keep it, you can easily migrate your code to 2.3.* version using the example above.


## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-testutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
96,microsoft/powerbi-visuals-utils-svgutils,TypeScript,"# Microsoft Power BI visuals SVGUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-svgutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-svgutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-svgutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-svgutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-svgutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-svgutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-svgutils)

> SVGUtils is a set of functions and classes in order to simplify SVG manipulations for Power BI custom visuals

## Usage
Learn how to install and use the SVGUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-svg)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-svgutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).

"
97,microsoft/powerbi-visuals-utils-colorutils,TypeScript,"# Microsoft Power BI visuals ColorUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-colorutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-colorutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-colorutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-colorutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-colorutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-colorutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-colorutils)

> ColorUtils is a set of functions and classes in order to simplify color manipulations for Power BI custom visuals

## Usage
Learn how to install and use the ColorUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-color)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-colorutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
98,microsoft/powerbi-visuals-utils-interactivityutils,TypeScript,"# Microsoft Power BI visuals InteractivityUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-interactivityutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-interactivityutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-interactivityutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-interactivityutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-interactivityutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-interactivityutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-interactivityutils)

> InteractivityUtils is a set of functions and classes in order to simplify implementation of cross-selection and cross-filtering for Power BI custom visuals

## Usage
Learn how to install and use the InteractivityUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-interactivity-selections)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-interactivityutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
99,microsoft/powerbi-visuals-utils-typeutils,TypeScript,"# Microsoft Power BI visuals TypeUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-typeutils/workflows/Build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-typeutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-typeutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-typeutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-typeutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-typeutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-typeutils)

> TypeUtils is a set of functions and classes in order to extend the basic types for Power BI custom visuals

## Usage
Learn how to install and use the TypeUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-type)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-typeutils/issues)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
100,microsoft/powerbi-visuals-histogram,TypeScript,"# powerbi-visuals-histogram
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-histogram.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-histogram) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-histogram/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-histogram?branch=master)

> A histogram chart plots data ranges into intervals. Useful for estimating density.

![Histogram chart screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680587/Asset_941ab72a-2c82-405a-b258-6c8c01d13e68/Histogramscreenshot1.png)
# Overview
A Histogram shows history representation of the distribution of numerical data. It is an estimate of the probability distribution of a continuous variable (quantitative variable).

The data is grouped into bins, that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.

Customize the number of bins, whether to use frequency or density, and the data colors.

See also [Histogram chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380776&sourcecorrid=e26ecce9-0b9e-47f4-9c7a-a023465dafdc&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)
"
101,microsoft/nni,Python,"<p align=""center"">
<img src=""docs/img/nni_logo.png"" width=""300""/>
</p>

-----------

[![MIT licensed](https://img.shields.io/badge/license-MIT-brightgreen.svg)](LICENSE)
[![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=62&branchName=master)
[![Issues](https://img.shields.io/github/issues-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen)
[![Bugs](https://img.shields.io/github/issues/Microsoft/nni/bug.svg)](https://github.com/Microsoft/nni/issues?q=is%3Aissue+is%3Aopen+label%3Abug)
[![Pull Requests](https://img.shields.io/github/issues-pr-raw/Microsoft/nni.svg)](https://github.com/Microsoft/nni/pulls?q=is%3Apr+is%3Aopen)
[![Version](https://img.shields.io/github/release/Microsoft/nni.svg)](https://github.com/Microsoft/nni/releases) [![Join the chat at https://gitter.im/Microsoft/nni](https://badges.gitter.im/Microsoft/nni.svg)](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Documentation Status](https://readthedocs.org/projects/nni/badge/?version=stable)](https://nni.readthedocs.io/en/stable/?badge=stable)

[NNI Doc](https://nni.readthedocs.io/) | [简体中文](README_zh_CN.md)

**NNI (Neural Network Intelligence)** is a lightweight but powerful toolkit to help users **automate** <a href=""https://nni.readthedocs.io/en/stable/FeatureEngineering/Overview.html"">Feature Engineering</a>, <a href=""https://nni.readthedocs.io/en/stable/NAS/Overview.html"">Neural Architecture Search</a>, <a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html"">Hyperparameter Tuning</a> and <a href=""https://nni.readthedocs.io/en/stable/Compression/Overview.html"">Model Compression</a>.

The tool manages automated machine learning (AutoML) experiments, **dispatches and runs** experiments' trial jobs generated by tuning algorithms to search the best neural architecture and/or hyper-parameters in **different training environments** like <a href=""https://nni.readthedocs.io/en/stable/TrainingService/LocalMode.html"">Local Machine</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/RemoteMachineMode.html"">Remote Servers</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/PaiMode.html"">OpenPAI</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/KubeflowMode.html"">Kubeflow</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/FrameworkControllerMode.html"">FrameworkController on K8S (AKS etc.)</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/DLTSMode.html"">DLWorkspace (aka. DLTS)</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/AMLMode.html"">AML (Azure Machine Learning)</a>, <a href=""https://nni.readthedocs.io/en/stable/TrainingService/AdaptDLMode.html"">AdaptDL (aka. ADL)</a> , other cloud options and even <a href=""https://nni.readthedocs.io/en/stable/TrainingService/HybridMode.html"">Hybrid mode</a>.

## **Who should consider using NNI**

* Those who want to **try different AutoML algorithms** in their training code/model.
* Those who want to run AutoML trial jobs **in different environments** to speed up search.
* Researchers and data scientists who want to easily **implement and experiment new AutoML algorithms**, may it be: hyperparameter tuning algorithm, neural architect search algorithm or model compression algorithm.
* ML Platform owners who want to **support AutoML in their platform**.

## **What's NEW!** &nbsp;<a href=""#nni-released-reminder""><img width=""48"" src=""docs/img/release_icon.png""></a>
* **New release**: [v2.2 is available](https://github.com/microsoft/nni/releases) - _released on April-26-2021_
* **New demo available**: [Youtube entry](https://www.youtube.com/channel/UCKcafm6861B2mnYhPbZHavw) | [Bilibili 入口](https://space.bilibili.com/1649051673) - _last updated on Apr-21-2021_

* **New use case sharing**: [Cost-effective Hyper-parameter Tuning using AdaptDL with NNI](https://medium.com/casl-project/cost-effective-hyper-parameter-tuning-using-adaptdl-with-nni-e55642888761) - _posted on Feb-23-2021_

## **NNI capabilities in a glance**

NNI provides CommandLine Tool as well as an user friendly WebUI to manage training experiments. With the extensible API, you can customize your own AutoML algorithms and training services. To make it easy for new users, NNI also provides a set of build-in state-of-the-art AutoML algorithms and out of box support for popular training platforms.

Within the following table, we summarized the current NNI capabilities, we are gradually adding new capabilities and we'd love to have your contribution.

<p align=""center"">
  <a href=""#nni-has-been-released""><img src=""docs/img/overview.svg"" /></a>
</p>

<table>
  <tbody>
    <tr align=""center"" valign=""bottom"">
    <td>
      </td>
      <td>
        <b>Frameworks & Libraries</b>
        <img src=""docs/img/bar.png""/>
      </td>
      <td>
        <b>Algorithms</b>
        <img src=""docs/img/bar.png""/>
      </td>
      <td>
        <b>Training Services</b>
        <img src=""docs/img/bar.png""/>
      </td>
    </tr>
    </tr>
    <tr valign=""top"">
    <td align=""center"" valign=""middle"">
    <b>Built-in</b>
      </td>
      <td>
      <ul><li><b>Supported Frameworks</b></li>
        <ul>
          <li>PyTorch</li>
          <li>Keras</li>
          <li>TensorFlow</li>
          <li>MXNet</li>
          <li>Caffe2</li>
          <a href=""https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html"">More...</a><br/>
        </ul>
        </ul>
      <ul>
        <li><b>Supported Libraries</b></li>
          <ul>
           <li>Scikit-learn</li>
           <li>XGBoost</li>
           <li>LightGBM</li>
           <a href=""https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html"">More...</a><br/>
          </ul>
      </ul>
        <ul>
        <li><b>Examples</b></li>
         <ul>
           <li><a href=""examples/trials/mnist-pytorch"">MNIST-pytorch</li></a>
           <li><a href=""examples/trials/mnist-tfv1"">MNIST-tensorflow</li></a>
           <li><a href=""examples/trials/mnist-keras"">MNIST-keras</li></a>
           <li><a href=""https://nni.readthedocs.io/en/stable/TrialExample/GbdtExample.html"">Auto-gbdt</a></li>
           <li><a href=""https://nni.readthedocs.io/en/stable/TrialExample/Cifar10Examples.html"">Cifar10-pytorch</li></a>
           <li><a href=""https://nni.readthedocs.io/en/stable/TrialExample/SklearnExamples.html"">Scikit-learn</a></li>
           <li><a href=""https://nni.readthedocs.io/en/stable/TrialExample/EfficientNet.html"">EfficientNet</a></li>
           <li><a href=""https://nni.readthedocs.io/en/stable/TrialExample/OpEvoExamples.html"">Kernel Tunning</li></a>
              <a href=""https://nni.readthedocs.io/en/stable/SupportedFramework_Library.html"">More...</a><br/>
          </ul>
        </ul>
      </td>
      <td align=""left"" >
        <a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html"">Hyperparameter Tuning</a>
        <ul>
          <b>Exhaustive search</b>
          <ul>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Random"">Random Search</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#GridSearch"">Grid Search</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Batch"">Batch</a></li>
            </ul>
          <b>Heuristic search</b>
          <ul>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Evolution"">Naïve Evolution</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Anneal"">Anneal</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#Hyperband"">Hyperband</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#PBTTuner"">PBT</a></li>
          </ul>
          <b>Bayesian optimization</b>
            <ul>
              <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#BOHB"">BOHB</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#TPE"">TPE</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#SMAC"">SMAC</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#MetisTuner"">Metis Tuner</a></li>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#GPTuner"">GP Tuner</a></li>
            </ul>
          <b>RL Based</b>
          <ul>
            <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#PPOTuner"">PPO Tuner</a> </li>
          </ul>
        </ul>
          <a href=""https://nni.readthedocs.io/en/stable/NAS/Overview.html"">Neural Architecture Search</a>
          <ul>
            <ul>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/ENAS.html"">ENAS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/DARTS.html"">DARTS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/PDARTS.html"">P-DARTS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/CDARTS.html"">CDARTS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/SPOS.html"">SPOS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/Proxylessnas.html"">ProxylessNAS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/BuiltinTuner.html#NetworkMorphism"">Network Morphism</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/TextNAS.html"">TextNAS</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/NAS/Cream.html"">Cream</a></li>
            </ul>
          </ul>
          <a href=""https://nni.readthedocs.io/en/stable/Compression/Overview.html"">Model Compression</a>
          <ul>
            <b>Pruning</b>
            <ul>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#agp-pruner"">AGP Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#slim-pruner"">Slim Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#fpgm-pruner"">FPGM Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#netadapt-pruner"">NetAdapt Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#simulatedannealing-pruner"">SimulatedAnnealing Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#admm-pruner"">ADMM Pruner</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Pruner.html#autocompress-pruner"">AutoCompress Pruner</a></li>
            </ul>
            <b>Quantization</b>
            <ul>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Quantizer.html#qat-quantizer"">QAT Quantizer</a></li>
              <li><a href=""https://nni.readthedocs.io/en/stable/Compression/Quantizer.html#dorefa-quantizer"">DoReFa Quantizer</a></li>
            </ul>
          </ul>
          <a href=""https://nni.readthedocs.io/en/stable/FeatureEngineering/Overview.html"">Feature Engineering (Beta)</a>
          <ul>
          <li><a href=""https://nni.readthedocs.io/en/stable/FeatureEngineering/GradientFeatureSelector.html"">GradientFeatureSelector</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/FeatureEngineering/GBDTSelector.html"">GBDTSelector</a></li>
          </ul>
          <a href=""https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html"">Early Stop Algorithms</a>
          <ul>
          <li><a href=""https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html#MedianStop"">Median Stop</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/Assessor/BuiltinAssessor.html#Curvefitting"">Curve Fitting</a></li>
          </ul>
      </td>
      <td>
      <ul>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/LocalMode.html"">Local Machine</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/RemoteMachineMode.html"">Remote Servers</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/HybridMode.html"">Hybrid mode</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/AMLMode.html"">AML(Azure Machine Learning)</a></li>
        <li><b>Kubernetes based services</b></li>
        <ul>
          <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/PaiMode.html"">OpenPAI</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/KubeflowMode.html"">Kubeflow</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/FrameworkControllerMode.html"">FrameworkController on K8S (AKS etc.)</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/DLTSMode.html"">DLWorkspace (aka. DLTS)</a></li>
          <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/AdaptDLMode.html"">AdaptDL (aka. ADL)</a></li>
        </ul>
      </ul>
      </td>
    </tr>
      <tr align=""center"" valign=""bottom"">
      </td>
      </tr>
      <tr valign=""top"">
       <td valign=""middle"">
    <b>References</b>
      </td>
     <td style=""border-top:#FF0000 solid 0px;"">
      <ul>
        <li><a href=""https://nni.readthedocs.io/en/stable/autotune_ref.html#trial"">Python API</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/Tutorial/AnnotationSpec.html"">NNI Annotation</a></li>
         <li><a href=""https://nni.readthedocs.io/en/stable/installation.html"">Supported OS</a></li>
      </ul>
      </td>
       <td style=""border-top:#FF0000 solid 0px;"">
      <ul>
        <li><a href=""https://nni.readthedocs.io/en/stable/Tuner/CustomizeTuner.html"">CustomizeTuner</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/Assessor/CustomizeAssessor.html"">CustomizeAssessor</a></li>
        <li><a href=""https://nni.readthedocs.io/en/stable/Tutorial/InstallCustomizedAlgos.html"">Install Customized Algorithms as Builtin Tuners/Assessors/Advisors</a></li>
      </ul>
      </td>
        <td style=""border-top:#FF0000 solid 0px;"">
      <ul>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/Overview.html"">Support TrainingService</li>
        <li><a href=""https://nni.readthedocs.io/en/stable/TrainingService/HowToImplementTrainingService.html"">Implement TrainingService</a></li>
      </ul>
      </td>
    </tr>
  </tbody>
</table>

## **Installation**

### **Install**

NNI supports and is tested on Ubuntu >= 16.04, macOS >= 10.14.1, and Windows 10 >= 1809. Simply run the following `pip install` in an environment that has `python 64-bit >= 3.6`.

Linux or macOS

```bash
python3 -m pip install --upgrade nni
```

Windows

```bash
python -m pip install --upgrade nni
```

If you want to try latest code, please [install NNI](https://nni.readthedocs.io/en/stable/installation.html) from source code.

For detail system requirements of NNI, please refer to [here](https://nni.readthedocs.io/en/stable/Tutorial/InstallationLinux.html#system-requirements) for Linux & macOS, and [here](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html#system-requirements) for Windows.

Note:

* If there is any privilege issue, add `--user` to install NNI in the user directory.
* Currently NNI on Windows supports local, remote and pai mode. Anaconda or Miniconda is highly recommended to install [NNI on Windows](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html).
* If there is any error like `Segmentation fault`, please refer to [FAQ](https://nni.readthedocs.io/en/stable/Tutorial/FAQ.html). For FAQ on Windows, please refer to [NNI on Windows](https://nni.readthedocs.io/en/stable/Tutorial/InstallationWin.html#faq).

### **Verify installation**

* Download the examples via clone the source code.

  ```bash
  git clone -b v2.2 https://github.com/Microsoft/nni.git
  ```

* Run the MNIST example.

  Linux or macOS

  ```bash
  nnictl create --config nni/examples/trials/mnist-pytorch/config.yml
  ```

  Windows

  ```powershell
  nnictl create --config nni\examples\trials\mnist-pytorch\config_windows.yml
  ```

* Wait for the message `INFO: Successfully started experiment!` in the command line. This message indicates that your experiment has been successfully started. You can explore the experiment using the `Web UI url`.

```text
INFO: Starting restful server...
INFO: Successfully started Restful server!
INFO: Setting local config...
INFO: Successfully set local config!
INFO: Starting experiment...
INFO: Successfully started experiment!
-----------------------------------------------------------------------
The experiment id is egchD4qy
The Web UI urls are: http://223.255.255.1:8080   http://127.0.0.1:8080
-----------------------------------------------------------------------

You can use these commands to get more information about the experiment
-----------------------------------------------------------------------
         commands                       description
1. nnictl experiment show        show the information of experiments
2. nnictl trial ls               list all of trial jobs
3. nnictl top                    monitor the status of running experiments
4. nnictl log stderr             show stderr log content
5. nnictl log stdout             show stdout log content
6. nnictl stop                   stop an experiment
7. nnictl trial kill             kill a trial job by id
8. nnictl --help                 get help information about nnictl
-----------------------------------------------------------------------
```

* Open the `Web UI url` in your browser, you can view detailed information of the experiment and all the submitted trial jobs as shown below. [Here](https://nni.readthedocs.io/en/stable/Tutorial/WebUI.html) are more Web UI pages.

<table style=""border: none"">
    <th><img src=""./docs/img/webui-img/full-oview.png"" alt=""drawing"" width=""395"" height=""300""/></th>
    <th><img src=""./docs/img/webui-img/full-detail.png"" alt=""drawing"" width=""410"" height=""300""/></th>
</table>

## **Releases and Contributing**
NNI has a monthly release cycle (major releases). Please let us know if you encounter a bug by [filling an issue](https://github.com/microsoft/nni/issues/new/choose).

We appreciate all contributions. If you are planning to contribute any bug-fixes, please do so without further discussions.

If you plan to contribute new features, new tuners, new training services, etc. please first open an issue or reuse an exisiting issue, and discuss the feature with us. We will discuss with you on the issue timely or set up conference calls if needed.

To learn more about making a contribution to NNI, please refer to our [How-to contribution page](https://nni.readthedocs.io/en/stable/contribution.html). 

We appreciate all contributions and thank all the contributors!

<a href=""https://github.com/microsoft/nni/graphs/contributors""><img src=""docs/img/contributors.png"" /></a>


## **Feedback**
* [File an issue](https://github.com/microsoft/nni/issues/new/choose) on GitHub.
* Discuss on the NNI [Gitter](https://gitter.im/Microsoft/nni?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) in NNI.

Join IM discussion groups:
|Gitter||WeChat|
|----|----|----|
|![image](https://user-images.githubusercontent.com/39592018/80665738-e0574a80-8acc-11ea-91bc-0836dc4cbf89.png)| OR |![image](https://github.com/scarlett2018/nniutil/raw/master/wechat.png)|


## Test status

### Essentials

| Type | Status |
| :---: | :---: |
| Fast test | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/fast%20test?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=54&branchName=master) |
| Full linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20linux?repoName=microsoft%2Fnni&branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=62&repoName=microsoft%2Fnni&branchName=master) |
| Full windows | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/full%20test%20-%20windows?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=63&branchName=master) |

### Training services

| Type | Status |
| :---: | :---: |
| Remote - linux to linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20linux%20to%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=64&branchName=master) |
| Remote - linux to windows | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20linux%20to%20windows?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=67&branchName=master) |
| Remote - windows to linux | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20remote%20-%20windows%20to%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=68&branchName=master) |
| OpenPAI | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20openpai%20-%20linux?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=65&branchName=master) |
| Frameworkcontroller | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20frameworkcontroller?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=70&branchName=master) |
| Kubeflow | [![Build Status](https://msrasrg.visualstudio.com/NNIOpenSource/_apis/build/status/integration%20test%20-%20kubeflow?branchName=master)](https://msrasrg.visualstudio.com/NNIOpenSource/_build/latest?definitionId=69&branchName=master) |

## Related Projects

Targeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) had also released few other open source projects.

* [OpenPAI](https://github.com/Microsoft/pai) : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale.
* [FrameworkController](https://github.com/Microsoft/frameworkcontroller) : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller.
* [MMdnn](https://github.com/Microsoft/MMdnn) : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The ""MM"" in MMdnn stands for model management and ""dnn"" is an acronym for deep neural network.
* [SPTAG](https://github.com/Microsoft/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.

We encourage researchers and students leverage these projects to accelerate the AI development and research.

## **License**

The entire codebase is under [MIT license](LICENSE)
"
102,microsoft/pxt-microbit,TypeScript,"# micro:bit target for PXT

[![Build Status](https://travis-ci.org/microsoft/pxt-microbit.svg?branch=master)](https://travis-ci.org/microsoft/pxt-microbit) ![pxt-testghpkgs](https://github.com/microsoft/pxt-microbit/workflows/pxt-testghpkgs/badge.svg)

pxt-microbit is a [Microsoft Programming Experience Toolkit (PXT)](https://github.com/Microsoft/pxt) target that allows you to program a [BBC micro:bit](https://microbit.org/). 

* pxt-microbit **beta**, ``v3.0.*`` requires 
  * [pxt-microbit#stable3.0](https://github.com/Microsoft/pxt-microbit/tree/stable3.0)
  * [pxt#stable6.0](https://github.com/Microsoft/pxt/tree/stable6.0).
  * [pxt-common-packages#stable6.0](https://github.com/Microsoft/pxt-common-packages/tree/stable7.0).
* pxt-microbit ``v2.0.*``, branch ``stable2.0``, requires [pxt v5.15.\*](https://github.com/microsoft/pxt/tree/stable5.15). It is the servicing branch for live editor.
* pxt-microbit ``v1.*`` requires pxt v4.4, which is currently in the [stable4.4 branch of pxt](https://github.com/Microsoft/pxt/tree/stable4.4).
* pxt-microbit ``v0.*`` is in the [v0 branch of this repository](https://github.com/microsoft/pxt-microbit/tree/v0)

* [Try it live](https://makecode.microbit.org/)

## Issue tracking

Please add an issue if you discover an (unreported) bug.

## Developing new extensions

Authoring and testing of new extensions can be done directly from the web editor. See [our documentation](https://makecode.com/blog/github-packages) on how to get started. If you want to run the editor locally, keep reading.

## Local server setup

The local server lets you to run the editor and serve the documentation from your own computer. It is meant for a single developer used and not designed to serve the editor to a large amount of users.

1. Install [Node.js](https://nodejs.org/) 8.9.4 or higher.
2. Clone this repository.
```
git clone https://github.com/microsoft/pxt-microbit
cd pxt-microbit
```
3. Install the PXT command line (add `sudo` for Mac/Linux shells).
```
npm install -g pxt
```
4. Install the pxt-microbit dependencies.
```
npm install
```

Go to the **Running** section.

### Developer Setup

This is the typical setup used by the MakeCode team to work on the microbit.

1. Install [Node.js](https://nodejs.org/) 8.9.4 or higher.
2. Install [Docker](https://www.docker.com/get-started) if you plan to build ``.cpp`` files.
3. Clone the pxt repository.
```
git clone https://github.com/microsoft/pxt
cd pxt
```
4. Install the dependencies of pxt and build it
```
npm install
npm run build
cd ..
```
5. Clone the pxt-common-packages repository
```
git clone https://github.com/microsoft/pxt-common-packages
cd pxt-common-packages
npm install
cd ..
```
6. Clone this repository.
```
git clone https://github.com/microsoft/pxt-microbit
cd pxt-microbit
```
7. Install the PXT command line (add `sudo` for Mac/Linux shells).
```
npm install -g pxt
```
8. Install the pxt-microbit dependencies.
```
npm install
```
8. Link pxt-microbit back to base pxt repo (add `sudo` for Mac/Linux shells). 
This step is only required if you intend to make changes to pxt and/or 
pxt-common-packages repos. If all you want is serve a local Makecode, you can skip
this step.
```
pxt link ../pxt
pxt link ../pxt-common-packages
```
Note the above command assumes the folder structure of   
```
       makecode
          |
  ----------------------------------
  |       |                        |
 pxt      pxt-common-packages  pxt-microbit
 ```

### Running

Run this command from inside pxt-microbit to open a local web server
```
pxt serve
```
If the local server opens in the wrong browser, make sure to copy the URL containing the local token. 
Otherwise, the editor will not be able to load the projects.

If you need to modify the `.cpp` files (and have installed yotta), enable yotta compilation using the `--localbuild` flag:
```
pxt serve --local
```

If you want to speed up the build, you can use the ``rebundle`` option, which skips building and simply refreshes the target information
```
pxt serve --rebundle
```

### Cleaning

Sometimes, your built folder might be in a bad state, clean it and try again.
```
pxt clean
```


### Building with CODAL locally

The following commands force a local build using CODAL.

```
pxt buildtarget --local
```

To disable docker, run

```
export PXT_NODOCKER=1
```

If you are also modifiying CODAL, consider running ``pxt clean`` to ensure the proper branch is picked up.

### Modifying DAL/CODAL locally

* follow instructions above until `pxt serve`
* open editor on localhost and create a project
* do `export PXT_FORCE_LOCAL=1 PXT_RUNTIME_DEV=1 PXT_ASMDEBUG=1`; you can add `PXT_NODOCKER=1`; `pxt help` has help on these
* find project folder under `pxt-microbit/projects`, typically `pxt-microbit/projects/Untitled-42`
* if you're going to modify `.cpp` files in PXT, replace `""core"": ""*""` in `pxt.json` with `""core"": ""file:../../libs/core""`;
  similarly `""radio"": ""file:../../libs/radio""` and `""microphone"": ""file:../../libs/microphone""`
* you can edit `main.ts` to change the PXT side of the program; you can also edit it from the localhost editor;
  note that `Download` in the localhost editor will produce different binary than command line, as it builds in the cloud
  and uses tagged version of CODAL
* in that folder run `pxt build` - this will clone codal somewhere under `built/` (depends on build engine and docker)
* there can be an issue with exporting the variables i.e. PXT_FORCE, so including them in the build command can help solve issues `sudo PXT_NODOCKER=1 PXT_ASMDEBUG=1 PXT_RUNTIME_DEV=1 PXT_DEBUG=1 PXT_FORCE_LOCAL=1 PXT_COMPILE_SWITCHES=csv---mbcodal pxt build`
* if the target is not building, delete files in `hexcache` found in `pxt-microbit/built/hexcache` to force local build
* the built hex can be found in `pxt-microbit/projects/<your project name>/built` named `binary.hex`
* similarly, you can run `pxt deploy` (or just `pxt` which is the same) - it will build and copy to `MICROBIT` drive
* assuming the build folder is under `built/codal`, go to `built/codal/libraries` and run `code *`
* in git tab, checkout appropriate branches (they are all in detached head state to the way we tag releases)
* modify files, run `pxt`, see effects
* you can also run `pxt gdb` to debug; this requires `openocd`
* other commands using `openocd` are `pxt dmesg` which dumps `DMESG(...)` buffer and `pxt heap` which can be used to visualize PXT heap 
  (and CODAL's one to some extent)

### Updating dal.d.ts

```
cd libs/blocksprj
rm -rf built
PXT_FORCE_LOCAL=1 PXT_COMPILE_SWITCHES=csv---mbcodal pxt build
PXT_FORCE_LOCAL=1 PXT_COMPILE_SWITCHES=csv---mbcodal pxt builddaldts
mv dal.d.ts ../core
```

### Updates

Make sure to pull changes from all repos regularly. More instructions are at https://github.com/Microsoft/pxt#running-a-target-from-localhost

## Update playlists in markdown

To add a new playlist, add an entry in ``/playlists.json``, and regenerate the markdown (see paragraph below). You'll now have a new markdown gallery file listing the videos which you can reference in ``/targetconfig.json``.

Get a Google API key and store it in the ``GOOGLE_API_KEY`` environment variables (turn on data from the app).

```
pxt downloadplaylists
```

## Repos 

The pxt-microbit target depends on several other repos. The main ones are:
- https://github.com/Microsoft/pxt, the PXT framework
- https://github.com/Microsoft/pxt-common-packages, common APIs accross various MakeCode editors
- https://github.com/lancaster-university/microbit, basic wrapper around the DAL
- https://github.com/lancaster-university/microbit-dal

## History

See the [MakeCode blog](https://makecode.com/blog).

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

MICROSOFT, the Microsoft Logo, and MAKECODE are registered trademarks of Microsoft Corporation. They can only be used for the purposes described in and in accordance with Microsoft’s Trademark and Brand guidelines published at https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general.aspx. If the use is not covered in Microsoft’s published guidelines or you are not sure, please consult your legal counsel or MakeCode team (makecode@microsoft.com).
"
103,microsoft/pxt-common-packages,TypeScript,"# MakeCode Common packages

A set of packages used in various MakeCode editors such as https://makecode.adafruit.com, https://maker.makecode.com, https://makecode.microbit.org, https://makecode.mindstorms.com, etc...

## Contributing

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
104,microsoft/azure-tools-for-java,Java,"# Azure Toolkits for Java [![Travis CI](https://travis-ci.org/Microsoft/azure-tools-for-java.svg?branch=develop)](https://travis-ci.org/Microsoft/azure-tools-for-java)
Azure Toolkits for Java is an open-source project that helps Java developers easily create, develop, configure, test, and deploy highly available and scalable Java web apps to Azure from [Eclipse](https://docs.microsoft.com/en-us/java/azure/eclipse/azure-toolkit-for-eclipse) and [IntelliJ IDEA](https://docs.microsoft.com/en-us/java/azure/intellij/azure-toolkit-for-intellij) on all supported platforms. 
* [Azure Toolkit for IntelliJ IDEA](https://docs.microsoft.com/en-us/java/azure/intellij/azure-toolkit-for-intellij)
* [Azure Toolkit for Eclipse](https://docs.microsoft.com/en-us/java/azure/eclipse/azure-toolkit-for-eclipse)
* [Release Notes](https://github.com/Microsoft/azure-tools-for-java/releases)
* [Issues](https://github.com/Microsoft/azure-tools-for-java/issues)

## Azure Toolkit for IntelliJ IDEA

### Installation
* [Set up the toolkits for IntelliJ](https://docs.microsoft.com/en-us/java/azure/intellij/azure-toolkit-for-intellij-installation)
* [IntelliJ IDEA Plugin Repository](https://plugins.jetbrains.com/plugin/8053?pr=idea)

### Documentation 
* [Get Started Tutorial](https://docs.microsoft.com/en-us/azure/app-service-web/app-service-web-intellij-create-hello-world-web-app)
* [Home Page of Azure Toolkit for IntelliJ](https://docs.microsoft.com/en-us/java/azure/intellij/azure-toolkit-for-intellij)
* [Java Developer Center on Azure](https://docs.microsoft.com/en-us/java/azure/)
* [Get Started for HDInsight](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-intellij-tool-plugin)

## Azure Toolkit for Eclipse

### Installation

* [Set up the toolkits for Eclipse](https://docs.microsoft.com/en-us/java/azure/eclipse/azure-toolkit-for-eclipse-installation)  
* [Eclipse Marketplace](http://marketplace.eclipse.org/content/azure-toolkit-eclipse)
* Update site: `http://dl.microsoft.com/eclipse/` 

### Documentation
* [Get Started Tutorial](https://docs.microsoft.com/azure/app-service-web/app-service-web-eclipse-create-hello-world-web-app)
* [Home Page of Azure Toolkit for Eclipse](https://docs.microsoft.com/en-us/java/azure/eclipse/azure-toolkit-for-eclipse)
* [Java Developer Center on Azure](https://docs.microsoft.com/en-us/java/azure/)
* [Get Started for HDInsight](https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-eclipse-tool-plugin)

## Known issues
Follow this [link](https://github.com/Microsoft/azure-tools-for-java/issues?q=is%3Aissue+label%3Aknown-issue) to track the known issues.

## Data/Telemetry
Azure Toolkits for Java collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://go.microsoft.com/fwlink/?LinkID=620956) to learn more. If you don't wish to send usage data to Microsoft, you can turn off it in the following places:
* For **Eclipse**: Open `Window > Preferences > Azure`, and uncheck the checkbox.
* For **IntelliJ IDEA**: Open `Settings/Preferences > Microsoft Tools > Azure`, and uncheck the checkbox.

## Contribution
Please see the [contribution instructions](CONTRIBUTING.md) if you wish to build the plugins from source.

## Disclaimer
*azure-tools-for-java uses JxBrowser http://www.teamdev.com/jxbrowser, which is a proprietary software. The use of JxBrowser is governed by JxBrowser Product Licence Agreement http://www.teamdev.com/jxbrowser-licence-agreement. If you would like to use JxBrowser in your development, please contact TeamDev.*
"
105,microsoft/service-fabric-explorer,TypeScript,"# Service Fabric Explorer (SFX)

NOTE: Sfx is currently in the process of migrating from angularjs(Sfx v1) to angular 10(Sfx v2). The Sfx folder holds the previous version and SfxWeb is the new version. While this migration is happening, Sfx v1 will still be available but once Sfx v2 is considered safe and has 100% parity with V1 will be removed and official deprecated. All new development is being focused on V2 and unless its a critical bug with V1, V1 will not be getting any continued support.

Service Fabric Explorer is an application for inspecting and managing cloud applications and nodes in a Microsoft Azure Service Fabric cluster.

## Build Status
Windows | Linux / macOS
------------ | -------------
![Image of Windows Build Badge](https://ci.appveyor.com/api/projects/status/ejfk6b0c3dlunkws/branch/master) | ![Image of Linux/macOS Build Badge](https://travis-ci.org/Microsoft/service-fabric-explorer.svg?branch=master) 

## Installation

Microsoft publishes the following installer packages for SFX:

- Windows
  - https://aka.ms/sfx-windows

- Linux
  - https://aka.ms/sfx-linux-x86
  - https://aka.ms/sfx-linux-x64

- macOS
  - https://aka.ms/sfx-macos

For more information about the application and how to use it: https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-visualizing-your-cluster

## Developer Help and Documentation

Service Fabric Explorer consists of two main components, an AngularJS based application (Sfx) and an Electron part to host the AngularJS application (Sfx-Standalone).

### Preparing the development machine

To develop Service Fabric Explorer, the following components are required.

* Git: https://git-scm.com/
* Node.js (Latest is preferred): https://nodejs.org/

The recommended IDE for Service Fabric Explorer development is VSCode because VSCode is a cross-platform editor, which supports Windows, Linux and macOS. But you can use whatever editor to develop. 

Here's a list of common IDE used.
* VSCode: https://code.visualstudio.com/ 
* Visual Studio: https://www.visualstudio.com/

### Set up the development environment

1. Clone the master branch.
`git clone --recurse-submodules https://github.com/Microsoft/service-fabric-explorer.git <path to the local folder>`
2. Install project dependencies: *This can be done inside VSCode or use a console window.*
   1. [SFX] Navigate to `src/SfxWeb` and run the following scripts.
   ```Shell
   npm install   
   ```
   2. [SFX Standalone] Navigate to `src/Sfx-Standalone` and run the following scripts.
   ```Shell
   npm install
   ```
   3. [SFX Proxy] Navigate to `src/Sfx-Proxy` and run the following scripts.
   ```Shell
   npm install   
   ```

3. Build projects
   * VSCode
      1. Open `src/SfxWeb`, `src/Sfx-Standalone` and `src/Sfx-Proxy` in VSCode with multiple-root workspce.
      2. Run following tasks orderly.
         * `clean-build` for Sfx-Standalone
   * Console
      1. Install Gulp globally on the machine.
      ```Shell
      npm install gulp-cli -g
      ```
      2. [SFX] Navigate to `src/SfxWeb` and run the following scripts.
      For a develop/quick build
      ```Shell
      npm run build
      ```
      For a production build
      ```
      npm run build:prod
      ```
      4. [SFX Standalone] Navigate to `src/Sfx-Standalone` and run the following scripts.
      ```Shell
      gulp clean-build
      ```

### Develop Locally and Run Local Proxy
Navigate to `src/SfxWeb`
```Shell
npm start
```
Navigate to `src/Sfx-Proxy`
```Shell
npm start
```

There are 2 optional flags
-r which would record every request to a folder(by default called playbackRecordings) and overwriting if the same request is made again
-p every request will be checked for a saved response and if one exists get served instead
```Shell
npm start -- -r -p
```

If proxying requests to a secure cluster adding a file called localsettings.json to src/Sfx-Proxy can take a cert pfx location like below.
```
{
  ""TargetCluster"": {
    ""Url"": ""https://test.eastus.cloudapp.azure.com:19080"",
    ""PFXLocation"": ""C:/some_cert.pfx"",
    ""PFXPassPhrase"": ""password""
  },
  ""recordFileBase"": ""playbackRecordings/""
}
```


## Testing

### Run unit tests
Navigate to  `sfx/SfxWeb` folder and run 
```
npm test
```

### Run E2E tests
Navigate to sfx/SfxWeb folder and run
```
npm run cypress:local
```
This assumes that the angular local dev server is running

### CI overview
The CI will run the following

* production build
* unit tests
* E2E tests

## Issues and questions

For questions related to Azure Service Fabric clusters, take a look at the [tag on StackOverflow](https://stackoverflow.com/questions/tagged/azure-service-fabric)
and [official documentation](https://docs.microsoft.com/en-us/azure/service-fabric/).

### General Service Fabric issues

If your issue is not specific to the Service Fabric Explorer, please use the [Service Fabric issues repository](https://github.com/Azure/service-fabric-issues/issues) to report an issue.

### Service Fabric Explorer specific issues

If your issue is relevant to the Service Fabric Explorer, please use this repositories issue tracker.

Be sure to search for similar previously reported issues prior to creating a new one.
In addition, here are some good practices to follow when reporting issues:

- Add a `+1` reaction to existing issues that are affecting you
- Include detailed output or screenshots when reporting unexpected error messages
- Include the version of SFX installed
- Include the version of Service Fabric runtime for the cluster you have selected

## New ideas and improvements

We encourage everyone to contribute to this project, following the contribution guidelines below. If you have ideas and want to share these with the community before taking on implementing the change, feel free to suggest these using issues.

# Contribution guidelines

For general contribution guidelines, plese see here: https://github.com/Microsoft/service-fabric/blob/master/CONTRIBUTING.md
"
106,microsoft/pxt-jacdac,TypeScript,"# Jacdac Services for MakeCode

This project contains [Jacdac](https://aka.ms/jacdac) host and client services for MakeCode editors.

**This project is still under construction.**

## Using this extensions

* Open your MakeCode editor (see supported editors)
* Go to the extension dialog and search for https://github.com/microsoft/pxt-jacdac
* Import the extension.

### Supported editors

* Maker, https://maker.makecode.com
* Arcade BETA, https://arcade.makecoe.com/beta
* micro:bit Beta, https://makecode.microbit.org/beta

## Developer section

Issues are tracked at https://github.com/microsoft/jacdac/issues .

To build all projects
```
sh mk.sh
```

To refresh the ``constants.ts`` files, build jacdac-spec (``yarn buildspecs`` from jacdac-ts) from https://github.com/microsoft/jacdac-ts .

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
107,microsoft/SpareNet,Python,"# Style-based Point Generator with Adversarial Rendering for Point Cloud Completion (CVPR 2021)

An efficient PyTorch library for Point Cloud Completion. 

<!-- ![image](./teaser.png) -->
<div  align=""center"">    
<img src=""./teaser.png"" width = ""800""   align=center />
</div>

### [Project page](https://alphapav.github.io/SpareNet/) |   [Paper](https://arxiv.org/abs/2103.02535) | [Video]()

[Chulin Xie*](https://github.com/AlphaPav), [Chuxin Wang*](https://chuxwa.github.io/), [Bo Zhang](https://bo-zhang.me/), [Hao Yang](https://www.microsoft.com/en-us/research/people/haya/), [Dong Chen](https://www.microsoft.com/en-us/research/people/doch/), and [Fang Wen](https://www.microsoft.com/en-us/research/people/fangwen/). (\*Equal contribution)

## Abstract
>We proposed a novel Style-based Point Generator with Adversarial Rendering (SpareNet) for point cloud completion. Firstly, we present the channel-attentive EdgeConv to fully exploit the local structures as well as the global shape in point features. Secondly, we observe that the concatenation manner used by vanilla foldings limits its potential of generating a complex and faithful shape. Enlightened by the success of StyleGAN, we regard the shape feature as style code that modulates the normalization layers during the folding, which considerably enhances its capability. Thirdly, we realize that existing point supervisions, e.g., Chamfer Distance or Earth Mover’s Distance, cannot faithfully reﬂect the perceptual quality of the reconstructed points. To address this, we propose to project the completed points to depth maps with a differentiable renderer and apply adversarial training to advocate the perceptual realism under different viewpoints. Comprehensive experiments on ShapeNet and KITTI prove the effectiveness of our method, which achieves state-of-the-art quantitative performance while offering superior visual quality.


## Installation

1. Create a virtual environment via `conda`.

   ```shell
   conda create -n sparenet python=3.7
   conda activate sparenet
   ```

2. Install `torch` and `torchvision`.

   ```shell
   conda install pytorch cudatoolkit=10.1 torchvision -c pytorch
   ```

3. Install requirements.

   ```shell
   pip install -r requirements.txt
   ```

4. Install cuda
   ```shell
   sh setup_env.sh
   ```


## Dataset
* Download [the processed ShapeNet dataset](https://gateway.infinitescript.com/?fileName=ShapeNetCompletion) generated by [GRNet](https://github.com/hzxie/GRNet), and the [KITTI dataset](https://drive.google.com/drive/folders/1fSu0_huWhticAlzLh3Ejpg8zxzqO1z-F). 

* Update the file path of the datasets in `configs/base_config.py`:

   ```
   __C.DATASETS.shapenet.partial_points_path = ""/path/to/datasets/ShapeNetCompletion/%s/partial/%s/%s/%02d.pcd""
   __C.DATASETS.shapenet.complete_points_path = ""/path/to/datasets/ShapeNetCompletion/%s/complete/%s/%s.pcd""
   __C.DATASETS.kitti.partial_points_path = ""/path/to/datasets/KITTI/cars/%s.pcd""
   __C.DATASETS.kitti.bounding_box_file_path = ""/path/to/datasets/KITTI/bboxes/%s.txt""

   # Dataset Options: ShapeNet, ShapeNetCars, KITTI
   __C.DATASET.train_dataset = ""ShapeNet""
   __C.DATASET.test_dataset = ""ShapeNet""
   ```


## Get Started

### Inference Using Pretrained Model

The pretrained models:

- [SpareNet for ShapeNet](https://drive.google.com/file/d/15PiH-bRlSlK4AUUnVwREzuAlMVJ9TfQG) (316 MB)
- [PCN for ShapeNet](https://drive.google.com/drive/folders/1ruN16MlJm4OeRMd41C19HyWqYOIrNrNh)
- [GRNet for ShapeNet](https://gateway.infinitescript.com/?fileName=GRNet-ShapeNet.pth) (307 MB)
- [GRNet for KITTI](https://gateway.infinitescript.com/?fileName=GRNet-KITTI.pth) (307 MB)
- [MSN for ShapeNet](https://drive.google.com/drive/folders/14UZXKqXIZ0gL3hhrV2ySll_pH2eLGFL5) (8192 points)


-  run

   ```shell
   python   --gpu ${GPUS}\
            --work_dir ${WORK_DIR} \
            --model ${network} \
            --weights ${path to checkpoint} \
            --test_mode ${mode}
   ```

-  example
   ```shell
   python  test.py --gpu 0 --work_dir /path/to/logfiles --model sparenet --weights /path/to/cheakpoint --test_mode default
   ```

### Train

All log files in the training process, such as log message, checkpoints, etc, will be saved to the work directory.

-  run

   ```shell
   python   --gpu ${GPUS}\
            --work_dir ${WORK_DIR} \
            --model ${network} \
            --weights ${path to checkpoint}
   ```
-  example
   ```shell
   python  train.py --gpu 0,1,2,3 --work_dir /path/to/logfiles --model sparenet --weights /path/to/cheakpoint
   ```

<!-- ## Contributors -->

## Differentiable Renderer
A fully differentiable point renderer that enables end-to-end rendering from 3D point cloud to 2D depth maps. See the paper for details.

<!-- ![image](./renderer.png) -->
<div  align=""center"">    
<img src=""./renderer.png"" width = ""600""   align=center />
</div>

### Test FPD on ShapeNet Dataset
* Run your model and save your results of test dataset

* Update the file path of the results in `test_fpd.py` and run it:
   ```
   parser.add_argument('--log_dir', default='/path/to/save/logs')
   parser.add_argument('--data_dir', default='/path/to/test/dataset/pcds')
   parser.add_argument('--fake_dir', default='/path/to/methods/pcds',
                              help='/path/to/results/shapenet_fc/pcds/')
   ```

### Usage of Renderer

The inputs of renderer are pcd, views and radius, and the outputs of renderer are depth_maps.
-  example
   ```shell
   # `projection_mode`: a str with value ""perspective"" or ""orthorgonal""
   # `eyepos_scale`: a float that defines the distance of eyes to (0, 0, 0)
   # `image_size`: an int defining the output image size
   renderer = ComputeDepthMaps(projection_mode, eyepos_scale, image_size)

   # `data`: a tensor with shape [batch_size, num_points, 3]
   # `view_id`: the index of selected view satisfying 0 <= view_id < 8
   # `radius_list`: a list of floats, defining the kernel radius to render each point
   depthmaps = renderer(data, view_id, radius_list)
   ```

## License

The codes and the pretrained model in this repository are under the MIT license as specified by the LICENSE file. 

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.


## BibTex

If you like our work and use the codebase or models for your research, please cite our work as follows.

```bibtex
@inproceedings{xie2021stylebased,
      title={Style-based Point Generator with Adversarial Rendering for Point Cloud Completion}, 
      author={Chulin Xie and Chuxin Wang and Bo Zhang and Hao Yang and Dong Chen and Fang Wen},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2021},
}
``` 

<!-- ## Acknowledgement
We thank for the inspiration from [MSN]() and [GRNet]() -->
"
108,microsoft/ads-kerberos,C++,"Kerberos
========
[![Build Status](https://travis-ci.org/mongodb-js/kerberos.svg?branch=master)](https://travis-ci.org/mongodb-js/kerberos)

The `kerberos` package is a C++ extension for Node.js that provides cross-platform support for kerberos authentication using GSSAPI on linux/osx, and SSPI on windows. Much of the code in this module is adapted from [ccs-kerberos](https://github.com/apple/ccs-pykerberos) and [winkerberos](https://github.com/mongodb-labs/winkerberos).

### Requirements

**Linux**
- `python` v2.7
- `make`
- A proper C/C++ compiler toolchain, like [GCC](https://gcc.gnu.org/)
- Distribution-specific kerberos packages (e.g. `krb5-dev` on Ubuntu)

**macOS**
- `Xcode Command Line Tools`: Can be installed with `xcode-select --install`
- Distribution-specific kerberos packages (e.g. `krb5` on Homebrew)

**Windows**
- **Option 1:** Install all the required tools and configurations using Microsoft's [windows-build-tools](https://github.com/felixrieseberg/windows-build-tools) by running `npm install -g windows-build-tools` from an elevated PowerShell (run as Administrator).
- **Option 2:** Install dependencies and configuration manually
   1. Visual C++ Build Environment:
     * **Option 1:** Install [Visual C++ Build Tools](http://go.microsoft.com/fwlink/?LinkId=691126) using the *Default Install* option.
     * **Option 2:** Install [Visual Studio 2015](https://www.visualstudio.com/products/visual-studio-community-vs) (or modify an existing installation) and select *Common Tools for Visual C++* during setup.

  > :bulb: [Windows Vista / 7 only] requires [.NET Framework 4.5.1](http://www.microsoft.com/en-us/download/details.aspx?id=40773)

  2. Install [Python 2.7](https://www.python.org/downloads/) or [Miniconda 2.7](http://conda.pydata.org/miniconda.html) (`v3.x.x` is not supported), and run `npm config set python python2.7`
  3. Launch cmd, `npm config set msvs_version 2015`

### Installation

Now you can install `kerberos` with the following:

```bash
npm install kerberos
```

### Testing

Run the test suite using:

```bash
npm test
```

NOTE: The test suite requires an active kerberos deployment, see `test/scripts/travis.sh` to better understand these requirements.

### Releasing

Release a new version of the extension by:

1. Run `npm run release`
2. Run `git push --follow-tags origin main`
3. The release will be created in Github automatically by the CD pipeline, go to it and download the package artifact (tgz)
4. Run `npm publish <path to tarball>`

# Documentation

## Classes

<dl>
<dt><a href=""#KerberosClient"">KerberosClient</a></dt>
<dd></dd>
<dt><a href=""#KerberosServer"">KerberosServer</a></dt>
<dd></dd>
</dl>

## Functions

<dl>
<dt><a href=""#checkPassword"">checkPassword(username, password, service, [defaultRealm], [callback])</a> ⇒ <code>Promise</code></dt>
<dd><p>This function provides a simple way to verify that a user name and password
match those normally used for Kerberos authentication.
It does this by checking that the supplied user name and password can be
used to get a ticket for the supplied service.
If the user name does not contain a realm, then the default realm supplied
is used.</p>
<p>For this to work properly the Kerberos must be configured properly on this
machine.
That will likely mean ensuring that the edu.mit.Kerberos preference file
has the correct realms and KDCs listed.</p>
<p>IMPORTANT: This method is vulnerable to KDC spoofing attacks and it should
only be used for testing. Do not use this in any production system - your
security could be compromised if you do.</p>
</dd>
<dt><a href=""#principalDetails"">principalDetails(service, hostname, [callback])</a> ⇒ <code>Promise</code></dt>
<dd><p>This function returns the service principal for the server given a service type and hostname.</p>
<p>Details are looked up via the <code>/etc/keytab</code> file.</p>
</dd>
<dt><a href=""#initializeClient"">initializeClient(service, [options], [callback])</a> ⇒ <code>Promise</code></dt>
<dd><p>Initializes a context for client-side authentication with the given service principal.</p>
</dd>
<dt><a href=""#initializeServer"">initializeServer(service, [callback])</a> ⇒ <code>Promise</code></dt>
<dd><p>Initializes a context for server-side authentication with the given service principal.</p>
</dd>
</dl>

<a name=""KerberosClient""></a>

## KerberosClient
**Properties**

| Name | Type | Description |
| --- | --- | --- |
| username | <code>string</code> | The username used for authentication |
| response | <code>string</code> | The last response received during authentication steps |
| responseConf | <code>string</code> | Indicates whether confidentiality was applied or not (GSSAPI only) |
| contextComplete | <code>boolean</code> | Indicates that authentication has successfully completed or not |


* [KerberosClient](#KerberosClient)

    * [.step(challenge, [callback])](#KerberosClient+step)

    * [.wrap(challenge, [options], [callback])](#KerberosClient+wrap)

    * [.unwrap(challenge, [callback])](#KerberosClient+unwrap)


<a name=""KerberosClient+step""></a>

### *kerberosClient*.step(challenge, [callback])

| Param | Type | Description |
| --- | --- | --- |
| challenge | <code>string</code> | A string containing the base64-encoded server data (which may be empty for the first step) |
| [callback] | <code>function</code> |  |

Processes a single kerberos client-side step using the supplied server challenge.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""KerberosClient+wrap""></a>

### *kerberosClient*.wrap(challenge, [options], [callback])

| Param | Type | Description |
| --- | --- | --- |
| challenge | <code>string</code> | The response returned after calling `unwrap` |
| [options] | <code>object</code> | Optional settings |
| [options.user] | <code>string</code> | The user to authorize |
| [callback] | <code>function</code> |  |

Perform the client side kerberos wrap step.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""KerberosClient+unwrap""></a>

### *kerberosClient*.unwrap(challenge, [callback])

| Param | Type | Description |
| --- | --- | --- |
| challenge | <code>string</code> | A string containing the base64-encoded server data |
| [callback] | <code>function</code> |  |

Perform the client side kerberos unwrap step

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""KerberosServer""></a>

## KerberosServer
**Properties**

| Name | Type | Description |
| --- | --- | --- |
| username | <code>string</code> | The username used for authentication |
| response | <code>string</code> | The last response received during authentication steps |
| targetName | <code>string</code> | The target used for authentication |
| contextComplete | <code>boolean</code> | Indicates that authentication has successfully completed or not |

<a name=""KerberosServer+step""></a>

### *kerberosServer*.step(challenge, [callback])

| Param | Type | Description |
| --- | --- | --- |
| challenge | <code>string</code> | A string containing the base64-encoded client data |
| [callback] | <code>function</code> |  |

Processes a single kerberos server-side step using the supplied client data.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""checkPassword""></a>

## checkPassword(username, password, service, [defaultRealm], [callback])

| Param | Type | Description |
| --- | --- | --- |
| username | <code>string</code> | The Kerberos user name. If no realm is supplied, then the `defaultRealm` will be used. |
| password | <code>string</code> | The password for the user. |
| service | <code>string</code> | The Kerberos service to check access for. |
| [defaultRealm] | <code>string</code> | The default realm to use if one is not supplied in the user argument. |
| [callback] | <code>function</code> |  |

This function provides a simple way to verify that a user name and password
match those normally used for Kerberos authentication.
It does this by checking that the supplied user name and password can be
used to get a ticket for the supplied service.
If the user name does not contain a realm, then the default realm supplied
is used.

For this to work properly the Kerberos must be configured properly on this
machine.
That will likely mean ensuring that the edu.mit.Kerberos preference file
has the correct realms and KDCs listed.

IMPORTANT: This method is vulnerable to KDC spoofing attacks and it should
only be used for testing. Do not use this in any production system - your
security could be compromised if you do.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""principalDetails""></a>

## principalDetails(service, hostname, [callback])

| Param | Type | Description |
| --- | --- | --- |
| service | <code>string</code> | The Kerberos service type for the server. |
| hostname | <code>string</code> | The hostname of the server. |
| [callback] | <code>function</code> |  |

This function returns the service principal for the server given a service type and hostname.

Details are looked up via the `/etc/keytab` file.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""initializeClient""></a>

## initializeClient(service, [options], [callback])

| Param | Type | Description |
| --- | --- | --- |
| service | <code>string</code> | A string containing the service principal in the form 'type@fqdn' (e.g. 'imap@mail.apple.com'). |
| [options] | <code>object</code> | Optional settings |
| [options.principal] | <code>string</code> | Optional string containing the client principal in the form 'user@realm' (e.g. 'jdoe@example.com'). |
| [options.gssFlags] | <code>number</code> | Optional integer used to set GSS flags. (e.g.  GSS_C_DELEG_FLAG|GSS_C_MUTUAL_FLAG|GSS_C_SEQUENCE_FLAG will allow for forwarding credentials to the remote host) |
| [options.mechOID] | <code>number</code> | Optional GSS mech OID. Defaults to None (GSS_C_NO_OID). Other possible values are `GSS_MECH_OID_KRB5`, `GSS_MECH_OID_SPNEGO`. |
| [callback] | <code>function</code> |  |

Initializes a context for client-side authentication with the given service principal.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
<a name=""initializeServer""></a>

## initializeServer(service, [callback])

| Param | Type | Description |
| --- | --- | --- |
| service | <code>string</code> | A string containing the service principal in the form 'type@fqdn' (e.g. 'imap@mail.apple.com'). |
| [callback] | <code>function</code> |  |

Initializes a context for server-side authentication with the given service principal.

**Returns**: <code>Promise</code> - returns Promise if no callback passed
"
109,microsoft/coe-starter-kit,,"# NOTE: THIS IS A REPO IN TRANSITION.  WE ARE IN THE PROCESS OF MOVING FROM https://github.com/microsoft/powerapps-tools/tree/master/Administration/CoEStarterKit TO HERE.  PLEASE CONSIDER THE TRANSITION INCOMPLETE UNTIL THIS NOTE IS REMOVED.  THANK YOU FOR YOUR PATIENCE.

# Microsoft Power Platform Center of Excellence (CoE) Starter Kit
The Center of Excellence (CoE) Starter Kit is a set of templates that are designed to help develop a strategy for adopting, maintaining and supporting the Power Platform, with a focus on Power Apps and Power Automate. The kit includes multiple Power Apps and Power BI analytics reports to view and interact with the data collected.  The kit also provides several assets that provide templates and suggested patterns and practices for implementing CoE efforts. The assets part of the CoE Starter Kit should be seen as a template from which you inherit your individual solution or can serve as inspiration for implementing your own apps and flows.

## Setup Instructions and Documentation
Please find all information on how to install and use the kit on https://docs.microsoft.com/power-platform/guidance/coe/starter-kit

## Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

## Trademarks 
This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft's Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc).

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).

## Contributing
This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
110,microsoft/clarity,TypeScript,"# Clarity
Clarity is an open-source behavioral analytics library written in typescript, with two key goals: privacy & performance. 

It helps you understand how users view and use your website across all modern devices and browsers. Understanding how users navigate, interact and browse your website can provide new insights about your users. Empathizing with your users and seeing where features fail or succeed can help improve your product, grow revenue and improve user retention.

It's the same code that powers Microsoft's hosted behavioral analytics solution: <a href=""https://clarity.microsoft.com"">https://clarity.microsoft.com</a>. If you would like to see a demo of how it works, checkout <a href=""https://clarity.microsoft.com/demo/projects/view/3t0wlogvdz/impressions?date=Last%203%20days"">live demo</a>. 

We encourage the community to join us in building the best behavioral analytics library, that puts privacy first and prioritizes performance. 

## Project Structure
1. **[clarity-js](https://github.com/microsoft/clarity/tree/master/packages/clarity-js)**: Instrumentation code that goes on the website and tracks user interactions as well as layout changes.

2. **[clarity-decode](https://github.com/microsoft/clarity/tree/master/packages/clarity-decode)**: Code, which usually runs on the server, decodes incoming data back into its original format.

3. **[clarity-visualize](https://github.com/microsoft/clarity/tree/master/packages/clarity-visualize)**: It takes the decoded data from clarity-decode and turns it back into pixel-perfect session replay.

4. **[clarity-devtools](https://github.com/microsoft/clarity/tree/master/packages/clarity-devtools)**: Devtools extension for chromium based browsers to generate live captures against any website.

## Examples
Here are some example sessions on popular websites visualized to demonstrate the telemetry captured:
1. CNN (Web)
</br><a href=""https://thumbs.gfycat.com/AggressiveLankyAbyssiniangroundhornbill-size_restricted.gif""><img src=""https://thumbs.gfycat.com/AggressiveLankyAbyssiniangroundhornbill-size_restricted.gif"" title=""Clarity - CNN Example""/></a>

2. Cook with Manali (Mobile)
</br><a href=""https://thumbs.gfycat.com/CoolDependableAdamsstaghornedbeetle-size_restricted.gif""><img src=""https://thumbs.gfycat.com/CoolDependableAdamsstaghornedbeetle-size_restricted.gif"" title=""Clarity - Cook With Manali Example""/></a> 

## Privacy Notice
Clarity handles sensitive data with care. By default sensitive content on the page is masked before uploading to the server.

## Improving Clarity
If you haven't already done so, start contributing by following instructions **[here](https://github.com/microsoft/clarity/blob/master/CONTRIBUTING.md)**.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

Happy coding!
"
111,microsoft/AzureTrailblazerAcademy,CSS,"# Azure Trailblazer Academy

<!-- 
Guidelines on README format: https://review.docs.microsoft.com/help/onboard/admin/samples/concepts/readme-template?branch=master

Guidance on onboarding samples to docs.microsoft.com/samples: https://review.docs.microsoft.com/help/onboard/admin/samples/process/onboarding?branch=master

Taxonomies for products and languages: https://review.docs.microsoft.com/new-hope/information-architecture/metadata/taxonomies?branch=master
-->

Welcome Azure Trailblazers!

In this repository you will find the content and labs you will need to complete the Azure Trailblazers Academy.

## Sessions

### [**Month 1**: Azure Fundamentals, IaaS and WebApps](./month1/labs)

Learn the basics of Azure, the difference between Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) and build a web application from scratch using Azure native services.

### [**Month 2**: Azure Data Services](./month2/labs)

Learn how to use CosmosDB to store data for modern applications. Understand the basics of Azure Synapse (formerly known as Azure Data Warehouse) and how to transform and integrate data from other data stores using Synapse Pipelines.


### [**Month 3**: Azure Serverless Computing and Azure DevOps](./month3)

Learn about the different Serverless services available in Azure such as Funtions, LogicApps and EventGrid. This month you will build an end-to-end solution using only serverless components and learn how to automate your deployments using Azure DevOps.


<!---
### [**Month 4**: Azure Serverless Services](./month4)

Learn about the different Serverless services available in Azure such as Funtions, LogicApps and EventGrid. This month you will build an end-to-end solution using only serverless components.

Monitor your Azure Infrastructure and learn how to configure/query logs in Log Analytics Workspace, intall Microsoft Monitoring Agent Extension and setup alerting to be notified under different conditions.

--->
## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
112,microsoft/PowerBI-visuals-FacetKey,JavaScript,"[![CircleCI](https://circleci.com/gh/Microsoft/PowerBI-visuals-FacetKey/tree/master.svg?style=svg)](https://circleci.com/gh/Microsoft/PowerBI-visuals-FacetKey/tree/master)

# Facet Key Powerbi Custom Visual
![Alt text](assets/screenshot.png?raw=true ""Facet Key"")

## Install
* Run `npm install`

## Debugging

* Install ssl certificate by running `npm run install-certificate` and following the steps from: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/CertificateSetup.md)
* Enable Developer Tools in PowerBI: [https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md](https://github.com/Microsoft/PowerBI-visuals/blob/master/tools/DebugVisualSetup.md)
* Run `npm start` to start development.

## Building

* Run `npm run package` to package the visual.
* `.pbiviz` file will be generated in the `dist` folder

## Testing

* Run `npm test`"
113,microsoft/BotFramework-DirectLineJS,TypeScript,"![Bot Framework DirectLineJS](./docs/media/FrameWorkDirectLineJS@1x.png)

### [Click here to find out what's new in Bot Framework for //build2019!](https://github.com/Microsoft/botframework/blob/master/whats-new.md#whats-new)

# Microsoft Bot Framework Direct Line JS Client

[![Build Status](https://travis-ci.org/Microsoft/BotFramework-DirectLineJS.svg?branch=master)](https://travis-ci.org/Microsoft/BotFramework-DirectLineJS)

Client library for the [Microsoft Bot Framework](http://www.botframework.com) *[Direct Line](https://docs.botframework.com/en-us/restapi/directline3/)* protocol.

Used by [WebChat](https://github.com/Microsoft/BotFramework-WebChat) and thus (by extension) [Emulator](https://github.com/Microsoft/BotFramework-Emulator), WebChat channel, and [Azure Bot Service](https://azure.microsoft.com/en-us/services/bot-service/).

## FAQ

### *Who is this for?*

Anyone who is building a Bot Framework JavaScript client who does not want to use [WebChat](https://github.com/Microsoft/BotFramework-WebChat).

If you're currently using WebChat, you don't need to make any changes as it includes this package.

### *What is that funny `subscribe()` method in the samples below?*

Instead of callbacks or Promises, this library handles async operations using Observables. Try it, you'll like it! For more information, check out [RxJS](https://github.com/reactivex/rxjs/).

### *Can I use [TypeScript](http://www.typescriptlang.com)?*

You bet.

### How ready for prime time is this library?

This is an official Microsoft-supported library, and is considered largely complete. Future changes (aside from supporting future updates to the Direct Line protocol) will likely be limited to bug fixes, performance improvements, tutorials, and samples. The big missing piece here is unit tests.

That said, the public API is still subject to change.

## How to build from source

0. Clone this repo
1. `npm install`
2. `npm run build` (or `npm run watch` to rebuild on every change, or `npm run prepublishOnly` to build production)

## How to include in your app

There are several ways:

1. Build from scratch and include either `/directLine.js` (webpacked with rxjs) or `lib/directline.js` in your app
2. `npm install botframework-directlinejs`

## Using from within a Node environment

This library uses RxJs/AjaxObserverable which is meant for use in a DOM environment. That doesn't mean you can't also use it from Node though, you just need to do a couple of extra things:

1. `npm install --save ws xhr2`
2. Add the following towards the top of your main application file:

```typescript
global.XMLHttpRequest = require('xhr2');
global.WebSocket = require('ws');
```

## How to create and use a directLine object

### Obtain security credentials for your bot:

1. If you haven't already, [register your bot](https://azure.microsoft.com/en-us/services/bot-service/).
2. Add a DirectLine (**not WebChat**) channel, and generate a Direct Line Secret. Make sure Direct Line 3.0 is enabled.
3. For testing you can use your Direct Line Secret as a security token, but for production you will likely want to exchange that Secret for a Token as detailed in the Direct Line [documentation](https://docs.microsoft.com/en-us/azure/bot-service/bot-service-channel-directline?view=azure-bot-service-4.0).

### Create a DirectLine object:

```typescript
import { DirectLine } from 'botframework-directlinejs';
// For Node.js:
// const { DirectLine } = require('botframework-directlinejs');

var directLine = new DirectLine({
    secret: /* put your Direct Line secret here */,
    token: /* or put your Direct Line token here (supply secret OR token, not both) */,
    domain: /* optional: if you are not using the default Direct Line endpoint, e.g. if you are using a region-specific endpoint, put its full URL here */
    webSocket: /* optional: false if you want to use polling GET to receive messages. Defaults to true (use WebSocket). */,
    pollingInterval: /* optional: set polling interval in milliseconds. Defaults to 1000 */,
    timeout: /* optional: a timeout in milliseconds for requests to the bot. Defaults to 20000 */,
    conversationStartProperties: { /* optional: properties to send to the bot on conversation start */
        locale: 'en-US'
    }
});
```

### Post activities to the bot:

```typescript
directLine.postActivity({
    from: { id: 'myUserId', name: 'myUserName' }, // required (from.name is optional)
    type: 'message',
    text: 'a message for you, Rudy'
}).subscribe(
    id => console.log(""Posted activity, assigned ID "", id),
    error => console.log(""Error posting activity"", error)
);
```

You can also post messages with attachments, and non-message activities such as events, by supplying the appropriate fields in the activity.

### Listen to activities sent from the bot:

```typescript
directLine.activity$
.subscribe(
    activity => console.log(""received activity "", activity)
);
```

You can use RxJS operators on incoming activities. To see only message activities:

```typescript
directLine.activity$
.filter(activity => activity.type === 'message')
.subscribe(
    message => console.log(""received message "", message)
);
```

Direct Line will helpfully send your client a copy of every sent activity, so a common pattern is to filter incoming messages on `from`:

```typescript
directLine.activity$
.filter(activity => activity.type === 'message' && activity.from.id === 'yourBotHandle')
.subscribe(
    message => console.log(""received message "", message)
);
```

### Monitor connection status

Subscribing to either `postActivity` or `activity$` will start the process of connecting to the bot. Your app can listen to the connection status and react appropriately :

```typescript

import { ConnectionStatus } from 'botframework-directlinejs';

directLine.connectionStatus$
.subscribe(connectionStatus => {
    switch(connectionStatus) {
        case ConnectionStatus.Uninitialized:    // the status when the DirectLine object is first created/constructed
        case ConnectionStatus.Connecting:       // currently trying to connect to the conversation
        case ConnectionStatus.Online:           // successfully connected to the converstaion. Connection is healthy so far as we know.
        case ConnectionStatus.ExpiredToken:     // last operation errored out with an expired token. Your app should supply a new one.
        case ConnectionStatus.FailedToConnect:  // the initial attempt to connect to the conversation failed. No recovery possible.
        case ConnectionStatus.Ended:            // the bot ended the conversation
    }
});
```

### Reconnect to a conversation

If your app created your DirectLine object by passing a token, DirectLine will refresh that token every 15 minutes.
Should your client lose connectivity (e.g. close laptop, fail to pay Internet access bill, go under a tunnel), `connectionStatus$`
will change to `ConnectionStatus.ExpiredToken`. Your app can request a new token from its server, which should call
the [Reconnect](https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-direct-line-3-0-reconnect-to-conversation?view=azure-bot-service-4.0) API.
The resultant Conversation object can then be passed by the app to DirectLine.

```typescript
var conversation = /* a Conversation object obtained from your app's server */;
directLine.reconnect(conversation);
```

### Resume an existing conversation

When using DirectLine with WebChat, closing the current tab or refreshing the page will create a new conversation in most cases. You can resume an existing conversation to keep the user in the same context.

**When using a secret** you can resume a conversation by:
- Storing the conversationid (in a *permanent* place, like local storage)
- Giving this value back while creating the DirectLine object along with the secret

```typescript
import { DirectLine } from 'botframework-directlinejs';

const dl = new DirectLine({
    secret: /* SECRET */,
    conversationId: /* the conversationid you stored from previous conversation */
});
```

**When using a token** you can resume a conversation by:
- Storing the conversationid and your token (in a *permanent* place, like local storage)
- Calling the DirectLine reconnect API yourself to get a refreshed token and a streamurl
- Creating the DirectLine object using the ConversationId, Token, and StreamUrl

```typescript
import { DirectLine } from 'botframework-directlinejs';

const dl = new DirectLine({
    token: /* the token you retrieved while reconnecting */,
    streamUrl: /* the streamUrl you retrieved while reconnecting */,
    conversationId: /* the conversationid you stored from previous conversation */
});
```

**Getting any history that Direct Line has cached** : you can retrieve history using watermarks:
You can see the watermark as an *activity 'bookmark'*. The resuming scenario will replay all the conversation activities from the watermark you specify.

```typescript
import { DirectLine } from 'botframework-directlinejs';

const dl = new DirectLine({
    token: /* the token you retrieved while reconnecting */,
    streamUrl: /* the streamUrl you retrieved while reconnecting */,
    conversationId: /* the conversationid you stored from previous conversation */,
    watermark: /* a watermark you saved from a previous conversation */,
    webSocket: false
});
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Reporting Security Issues
Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the [MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

Copyright (c) Microsoft Corporation. All rights reserved.
"
114,microsoft/dicom-server,C#,"# Medical Imaging Server for DICOM

 [![Build Status](https://microsofthealthoss.visualstudio.com/DicomServer/_apis/build/status/CI-Build-OSS?branchName=main)](https://microsofthealthoss.visualstudio.com/DicomServer/_build/latest?definitionId=34&branchName=main)

The Medical Imaging Server for DICOM is an open source DICOM server that is easily deployed on Azure. It allows standards-based communication with any DICOMweb&trade; enabled systems, and injects DICOM metadata into a FHIR server to create a holistic view of patient data. The Medical Imaging Server for DICOM integrates tightly with the [FHIR Server for Azure](https://github.com/microsoft/fhir-server) enabling healthcare professionals, ISVs, and medical device vendors to create new and innovative solutions. FHIR is becoming an important standard for clinical data and provides extensibility to support integration of other types of data directly, or through references. By using the Medical Imaging Server for DICOM, organizations can store references to imaging data in FHIR and enable queries across clinical and imaging datasets.

![Architecture](docs/images/DICOM-arch.png)

The Medical Imaging Server for DICOM is a .NET Core implementation of DICOMweb&trade;. [DICOMweb&trade;](https://www.dicomstandard.org/dicomweb) is the DICOM Standard for web-based medical imaging. Details of our conformance to the standard can be found in our [Conformance Statement](docs/resources/conformance-statement.md).

## Deploy the Medical Imaging Server for DICOM

The Medical Imaging Server for DICOM is designed to run on Azure. However, for development and test environments it can be deployed locally as a set of Docker containers to speed up development.

### Deploy to Azure

If you already have an Azure subscription, deploy the Medical Imaging Server for DICOM directly to Azure: <br/>
    <a href=""https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fdicom-server%2Fmain%2Fsamples%2Ftemplates%2Fdefault-azuredeploy.json"" target=""_blank""><img src=""https://azuredeploy.net/deploybutton.png""/></a>

To sync your Medical Imaging Server for DICOM metadata directly into a FHIR server, deploy **DICOM Cast** (alongside a FHIR OSS Server and Medical Imaging Server for DICOM) via: <br/>
    <a href=""https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2Fdicom-server%2Fmain%2Fconverter%2Fdicom-cast%2Fsamples%2Ftemplates%2Fdefault-azuredeploy.json"" target=""_blank""><img src=""https://azuredeploy.net/deploybutton.png""/>
    </a> 

For a complete set of instructions for how to deploy the Medical Imaging Server for DICOM to Azure, refer to the Quickstart [Deploy to Azure ](docs/quickstarts/deploy-via-azure.md).

### Deploy locally

Follow the [Development Setup Instructions](docs/development/setup.md) to deploy a local copy of the Medical Imaging Server for DICOM. Be aware that this deployment leverages the [Azurite container](https://github.com/Azure/Azurite) which emulates the Azure Storage API, and should not be used in production.

Note that the webapp library, ARM templates and Web.Zip package are for testing purposes only, they are not recommended for production scenarios. These will not be versioned. You can find the artifact feed generated by the Medical Imaging Server for DICOM at the [Azure Devops Public Feed](https://microsofthealthoss.visualstudio.com/FhirServer/_packaging?_a=feed&feed=Public), including the versioned packages.

## Quickstarts

- [Deploy Medical Imaging Server for DICOM via Azure](docs/quickstarts/deploy-via-azure.md)
- [Deploy Medical Imaging Server for DICOM via Docker](docs/quickstarts/deploy-via-docker.md)
- [Set up DICOM Cast](docs/quickstarts/deploy-dicom-cast.md)

## Tutorials

- [Use the Medical Imaging Server for DICOM APIs](docs/tutorials/use-the-medical-imaging-server-apis.md)
- [Use DICOMweb&trade; Standard APIs with C#](docs/tutorials/use-dicom-web-standard-apis-with-c%23.md)
- [Use DICOMweb&trade; Standard APIs with Python](docs/tutorials/use-dicom-web-standard-apis-with-python.md)
- [Use DICOMweb&trade; Standard APIs with cURL](docs/tutorials/use-dicom-web-standard-apis-with-curl.md)

## How-to guides

- [Configure Medical Imaging Server for DICOM server settings](docs/how-to-guides/configure-dicom-server-settings.md)
- [Enable Authentication and retrieve an OAuth token](docs/how-to-guides/enable-authentication-with-tokens.md)
- [Enable Authorization](docs/how-to-guides/enable-authorization.md)
- [Pull Changes from Medical Imaging Server for DICOM with Change Feed](docs/how-to-guides/pull-changes-from-change-feed.md)
- [Sync DICOM metadata to FHIR](docs/how-to-guides/sync-dicom-metadata-to-fhir.md)
- [Extended Query Tags](docs/how-to-guides/extended-query-tags.md)

## Concepts

- [DICOM](docs/concepts/dicom.md)
- [Change Feed](docs/concepts/change-feed.md)
- [DICOM Cast](docs/concepts/dicom-cast.md)

## Resources

- [FAQ](docs/resources/faq.md)
- [Conformance Statement](docs/resources/conformance-statement.md)
- [Health Check API](docs/resources/health-check-api.md)
- [Performance Guidance](docs/resources/performance-guidance.md)

## Development

- [Setup](docs/development/setup.md)
- [Code Organization](docs/development/code-organization.md)
- [Naming Guidelines](docs/development/naming-guidelines.md)
- [Exception handling](docs/development/exception-handling.md)
- [Tests](docs/development/tests.md)
- [Identity Server Authentication](docs/development/identity-server-authentication.md)
- [Roles](docs/development/roles.md)

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repositories using our CLA.

There are many other ways to contribute to Medical Imaging Server for DICOM.
* [Submit bugs](https://github.com/Microsoft/dicom-server/issues) and help us verify fixes as they are checked in.
* Review the [source code changes](https://github.com/Microsoft/dicom-server/pulls).
* Engage with Medical Imaging Server for DICOM users and developers on [StackOverflow](https://stackoverflow.com/questions/tagged/medical-imaging-server-for-dicom).
* Join the [#dicomonazure](https://twitter.com/hashtag/dicomonazure?f=tweets&vertical=default) discussion on Twitter.
* [Contribute bug fixes](CONTRIBUTING.md).

See [Contributing to Medical Imaging Server for DICOM](CONTRIBUTING.md) for more information.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
115,microsoft/PowerBI-visuals-HeatStreams,TypeScript,"![Node.js CI](https://github.com/microsoft/PowerBI-visuals-HeatStreams/workflows/Node.js%20CI/badge.svg)

# Heat Streams PowerBI Visual

# About
The HeatStreams Visual is a categorical heat-map encoding of a metric over a time or numeric domain. Users can pick from a variety of color schemes (all provided by d3), and basic selection of categories is provided. Categories may be multi-selected with ctrl+click. 

# Development
This visualization is a Lerna monorepo split into two main components: a React-based data visualization, and a PowerBI wrapper around the visual. The Essex build system is used to construct the visual.

> yarn --ignore-engines # PowerBI insists on a Node 6.x engine, which isn't really necessary
> yarn build            # Transpiles react components, bundles the PowerBI visual under `packages/powerbi-heat-streams/dist`
> yarn start            # Starts up the local PowerBI visual development server

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
116,microsoft/exp-extension,TypeScript,
117,microsoft/vcpkg-tool,C++,"# Vcpkg: Overview

[中文总览](https://github.com/microsoft/vcpkg/blob/master/README_zh_CN.md)
[Español](https://github.com/microsoft/vcpkg/blob/master/README_es.md)
[한국어](https://github.com/microsoft/vcpkg/blob/master/README_ko_KR.md)
[Français](https://github.com/microsoft/vcpkg/blob/master/README_fr.md)

Vcpkg helps you manage C and C++ libraries on Windows, Linux and MacOS.
This tool and ecosystem are constantly evolving, and we always appreciate contributions!

Please see the main repository https://github.com/microsoft/vcpkg for all feature discussion, issue
tracking, and edits to which libraries are available.

# Vcpkg-tool: Overview

This repository contains the contents formerly at https://github.com/microsoft/vcpkg in the
""toolsrc"" tree, and build support.

# Contributing

Please refer to the ""contributing"" section of the
[main `README.md`](https://github.com/microsoft/vcpkg/blob/master/README.md).

This project has adopted the [Microsoft Open Source Code of Conduct][contributing:coc].
For more information see the [Code of Conduct FAQ][contributing:coc-faq]
or email [opencode@microsoft.com](mailto:opencode@microsoft.com)
with any additional questions or comments.

[contributing:submit-issue]: https://github.com/microsoft/vcpkg/issues/new/choose
[contributing:submit-pr]: https://github.com/microsoft/vcpkg/pulls
[contributing:coc]: https://opensource.microsoft.com/codeofconduct/
[contributing:coc-faq]: https://opensource.microsoft.com/codeofconduct/

# License

The product code in this repository is licensed under the [MIT License](LICENSE.txt). The tests
contain 3rd party code as documented in `NOTICE.txt`.

# Telemetry

vcpkg collects usage data in order to help us improve your experience.
The data collected by Microsoft is anonymous.
You can opt-out of telemetry by re-running the bootstrap-vcpkg script with -disableMetrics,
passing --disable-metrics to vcpkg on the command line,
or by setting the VCPKG_DISABLE_METRICS environment variable.

Read more about vcpkg telemetry at docs/about/privacy.md
"
118,microsoft/Application-Insights-Workbooks,JSON,"# Azure Monitor Workbook Templates [![Build Status](https://github.com/microsoft/Application-Insights-Workbooks/workflows/Template%20Validation/badge.svg)](https://travis-ci.org/microsoft/Application-Insights-Workbooks)



## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a 
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us 
the rights to use your contribution. For details, visit https://cla.microsoft.com.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments. 

## What is this repository for?
This repository contains the templates shown in the workbook galleries of [Azure Monitor Workbooks](https://docs.microsoft.com/azure/application-insights/app-insights-usage-workbooks). Templates added to this repository will show up in the various Workbook Galleries for users of Azure Monitor Workbooks. By contributing templates, you can help others solve interesting problems using the workbooks you've found helpful on your own team.

## Azure Monitor Workbooks

Workbooks allow Azure Monitor users to create customizable interactive reports and analytic narratives by providing a flexible canvas that allows them to:

1.	Create rich visual reports and analytics experiences within the Azure portal.
2.	Use metric, log and Azure Resource Graph data
3.	Build interactive experiences based on user input
4.	Customize their analysis and persist for later use.
5.	Use templates for curated analysis from a public gallery
6.	Leverage ARM programmability to create and manage their workbook assets.

These capabilities of workbooks can be used to create curated and customized reports, analytic narratives, dashboards, etc. 

Use these links to learn more about workbooks:

* [Data Sources](Documentation/DataSources/DataSources.md)
* [Visualizations](Documentation/Visualizations/Visualizations.md)
* [Parameters](Documentation/Parameters/Parameters.md)
* [Groups](Documentation/Groups/Groups.md)
* [Interactivity](Documentation/Interactivity.md)
* [Manage programmatically](Documentation/Programmatically.md)
* [Sample Gallery](Documentation/Samples/Samples.md)
* [Contributing](Documentation/Contributing.md) 
* [Testing](Documentation/Contributing.md#how-to-test-your-changes)
* [Previews](Documentation/Contributing.md#testing-preview-workbook-templates)
* [Auto-Refresh](Documentation/AutoRefresh.md)
* [New Gallery](Documentation/Gallery.md)

### Sample Workbook 
![Image of a sample workbook](Documentation/Images/WorkbookExample.png)

## How to contribute?

For more details about getting started, see the [Contributing guidelines](CONTRIBUTING.md).

Note that templates in this repo will show up for all users of Azure who open the specified gallery. For this reason, the templates gallery is curated by Microsoft. If the submitted template is useful to the community and it does not place undue stress on the underlying infrastructure, it will be accepted to be part of the gallery.

## Status
This repo is [supported by Microsoft](https://docs.microsoft.com/en-us/azure/azure-monitor).
* [File an issue](new-issue) or submit a pull request on GitHub
* Request or upvote features on [UserVoice](https://feedback.azure.com/forums/913690-azure-monitor)
* File a [support](https://docs.microsoft.com/en-us/azure/azure-supportability/how-to-create-azure-support-request) ticket with Azure

We are constantly working to improve, and we value your feedback.
"
119,microsoft/PowerPlatformConnectors,Python,"# Microsoft Power Platform Connectors

Welcome to the Microsoft Power Platform Connectors open source repository. This repository contains custom connectors, certified connectors, and related tools to facilitate connector development for Azure Logic Apps, Microsoft Power Apps, and Microsoft Power Automate.

## Custom Connectors

The ```custom-connectors``` folder contains fully functional connector samples which can be deployed to the Power Platform for extension and use. These samples may not be certified connectors, but should be maintained by the open source community to offer useful scenarios or examples of connector concepts.

## Certified Connectors

The ```certified-connectors``` folder contains certified connectors which are already deployed and available out-of-box within the Power Platform for use. 
A requirement of our [connector certification program](https://docs.microsoft.com/connectors/custom-connectors/submit-certification) is that new certified connectors be open sourced for community contributions. 
The ```certified-connectors``` folder is managed by the Microsoft Connector Certification Team to ensure that within the ```master``` branch, the connector version is identical to that deployed in the Power Platform. 
The ```dev``` branch is maintained by the connector owner and the Microsoft Connector Certification Team to allow community development of the connector prior to certification and deployment of a version. 

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

### Creating a Fork

To contibute to this open source repository, start by creating a fork on this repository. To do so, select the ""fork"" button on the upper right corner, and create your own copy of the repository. Next, sync your fork with the remote repository and clone your forked repository to your local machine.

```git clone https://github.com/YOUR-USERNAME/PowerPlatformConnectors.git```

Check your remote URL.

```git remote -v```

```
> origin  https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (fetch)
> origin  https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (push)
```

Add an upstream repository for your clone.

```git remote add upstream https://github.com/microsoft/PowerPlatformConnectors.git```

Verify the upstream links.

```git remote -v```

```
> origin    https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (fetch) 
> origin    https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (push)
> upstream  https://github.com/microsoft/PowerPlatformConnectors.git (fetch)
> upstream  https://github.com/microsoft/PowerPlatformConnectors.git (push)
```

To keep your fork up to date with this repository's updates, run these commands:

```git fetch upstream```

```git checkout master```

```git merge upstream/master```

You are now ready to develop your connector in your own branch.

### Submitting to the Open Source Repository

Contributions to the open source repository are made through pull requests. 
Prior to submitting a pull request, ensure that your pull request does not contain any sensitive or specific information, for example Client IDs or Client Secrets. 
Any sensitive values can be replaced with fake or dummy values for the purposes of submission as long as it is clearly indicated. 
Also, ensure that the readme.md of the connector is updated with the latest information, or created for new connector submissions. 
An example of a clear, structured, readme.md can be found in the [Azure Key Vault](https://github.com/microsoft/PowerPlatformConnectors/tree/master/custom-connectors/AzureKeyVault) connector repository. 
Include this completed `readme.md` in same connector directory which contains the artifacts. 

#### Custom Connectors

Updates to an existing custom connector can be made through a simple pull request to update the custom connector files.

For new custom connectors, create a directory under the ```custom-connectors``` directory and place the connector files in the sub-folder. Ensure that a clear, structured, readme.md is included. 

#### Certified Connectors

Updates to certified connectors must first be made through a pull request to the ```dev``` branch for review by the connector owner. 
Once a pull request has been merged to the ```dev``` branch, the connector owner can submit the connector for certification through the Connector certification tab in [ISV Studio](https://isvstudio.powerapps.com). Once certified, the Microsoft Certification team will handle merging the updates from ```dev``` to ```master```. 

Updates to an existing custom connector can be made through a simple pull request to the ```dev``` branch to update the custom connector files.

For new connectors which will be submitted for certification, create a directory under the ```certified-connectors``` directory, place the connector files in the sub-folder, and submit a pull request to the ```dev``` branch. Ensure that a clear, structured, readme.md is included. 

### Tooling and Validation

#### CLA

When a pull request is submitted, a CLA-bot will automatically determine whether you need to provide
a CLA and annotate the PR appropriately. Simply follow the instructions
provided by the bot to ensure your pull request can be properly reviewed.
You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

#### Swagger Validation

A submitted pull request will also be validated against our Swagger Validator tool, which checks the connector files to ensure it is a proper Swagger file and adheres to our connector requirements and guidelines. Any errors or warnings will be added to the PR for both the submitter and the reviewer to understand. We do not accept pull requests with outstanding unresolved Swagger Validator issues. 

#### Breaking Change Detector

Another validation which runs on a submitted pull request is the breaking changes validator. This is to catch any inadvertent, non-backwards-compatible (i.e. breaking) changes which may break a current user experience, for example, deleting a published operation. The Breaking Change Detector compares the previous version of the Swagger with the new submission and raises awareness of any breaking change. The submitter and reviewer must both acknowledge any breaking changes submitted and ensure that no end users are inadvertently negatively affected. 

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/en-us/

Microsoft and any contributors reserve all others rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
120,microsoft/Application-Insights-Workbooks,JSON,"# Azure Monitor Workbook Templates [![Build Status](https://github.com/microsoft/Application-Insights-Workbooks/workflows/Template%20Validation/badge.svg)](https://travis-ci.org/microsoft/Application-Insights-Workbooks)



## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a 
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us 
the rights to use your contribution. For details, visit https://cla.microsoft.com.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments. 

## What is this repository for?
This repository contains the templates shown in the workbook galleries of [Azure Monitor Workbooks](https://docs.microsoft.com/azure/application-insights/app-insights-usage-workbooks). Templates added to this repository will show up in the various Workbook Galleries for users of Azure Monitor Workbooks. By contributing templates, you can help others solve interesting problems using the workbooks you've found helpful on your own team.

## Azure Monitor Workbooks

Workbooks allow Azure Monitor users to create customizable interactive reports and analytic narratives by providing a flexible canvas that allows them to:

1.	Create rich visual reports and analytics experiences within the Azure portal.
2.	Use metric, log and Azure Resource Graph data
3.	Build interactive experiences based on user input
4.	Customize their analysis and persist for later use.
5.	Use templates for curated analysis from a public gallery
6.	Leverage ARM programmability to create and manage their workbook assets.

These capabilities of workbooks can be used to create curated and customized reports, analytic narratives, dashboards, etc. 

Use these links to learn more about workbooks:

* [Data Sources](Documentation/DataSources/DataSources.md)
* [Visualizations](Documentation/Visualizations/Visualizations.md)
* [Parameters](Documentation/Parameters/Parameters.md)
* [Groups](Documentation/Groups/Groups.md)
* [Interactivity](Documentation/Interactivity.md)
* [Manage programmatically](Documentation/Programmatically.md)
* [Sample Gallery](Documentation/Samples/Samples.md)
* [Contributing](Documentation/Contributing.md) 
* [Testing](Documentation/Contributing.md#how-to-test-your-changes)
* [Previews](Documentation/Contributing.md#testing-preview-workbook-templates)
* [Auto-Refresh](Documentation/AutoRefresh.md)
* [New Gallery](Documentation/Gallery.md)

### Sample Workbook 
![Image of a sample workbook](Documentation/Images/WorkbookExample.png)

## How to contribute?

For more details about getting started, see the [Contributing guidelines](CONTRIBUTING.md).

Note that templates in this repo will show up for all users of Azure who open the specified gallery. For this reason, the templates gallery is curated by Microsoft. If the submitted template is useful to the community and it does not place undue stress on the underlying infrastructure, it will be accepted to be part of the gallery.

## Status
This repo is [supported by Microsoft](https://docs.microsoft.com/en-us/azure/azure-monitor).
* [File an issue](new-issue) or submit a pull request on GitHub
* Request or upvote features on [UserVoice](https://feedback.azure.com/forums/913690-azure-monitor)
* File a [support](https://docs.microsoft.com/en-us/azure/azure-supportability/how-to-create-azure-support-request) ticket with Azure

We are constantly working to improve, and we value your feedback.
"
121,microsoft/PowerPlatformConnectors,Python,"# Microsoft Power Platform Connectors

Welcome to the Microsoft Power Platform Connectors open source repository. This repository contains custom connectors, certified connectors, and related tools to facilitate connector development for Azure Logic Apps, Microsoft Power Apps, and Microsoft Power Automate.

## Custom Connectors

The ```custom-connectors``` folder contains fully functional connector samples which can be deployed to the Power Platform for extension and use. These samples may not be certified connectors, but should be maintained by the open source community to offer useful scenarios or examples of connector concepts.

## Certified Connectors

The ```certified-connectors``` folder contains certified connectors which are already deployed and available out-of-box within the Power Platform for use. 
A requirement of our [connector certification program](https://docs.microsoft.com/connectors/custom-connectors/submit-certification) is that new certified connectors be open sourced for community contributions. 
The ```certified-connectors``` folder is managed by the Microsoft Connector Certification Team to ensure that within the ```master``` branch, the connector version is identical to that deployed in the Power Platform. 
The ```dev``` branch is maintained by the connector owner and the Microsoft Connector Certification Team to allow community development of the connector prior to certification and deployment of a version. 

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

### Creating a Fork

To contibute to this open source repository, start by creating a fork on this repository. To do so, select the ""fork"" button on the upper right corner, and create your own copy of the repository. Next, sync your fork with the remote repository and clone your forked repository to your local machine.

```git clone https://github.com/YOUR-USERNAME/PowerPlatformConnectors.git```

Check your remote URL.

```git remote -v```

```
> origin  https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (fetch)
> origin  https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (push)
```

Add an upstream repository for your clone.

```git remote add upstream https://github.com/microsoft/PowerPlatformConnectors.git```

Verify the upstream links.

```git remote -v```

```
> origin    https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (fetch) 
> origin    https://github.com/YOUR_USERNAME/PowerPlatformConnectors.git (push)
> upstream  https://github.com/microsoft/PowerPlatformConnectors.git (fetch)
> upstream  https://github.com/microsoft/PowerPlatformConnectors.git (push)
```

To keep your fork up to date with this repository's updates, run these commands:

```git fetch upstream```

```git checkout master```

```git merge upstream/master```

You are now ready to develop your connector in your own branch.

### Submitting to the Open Source Repository

Contributions to the open source repository are made through pull requests. 
Prior to submitting a pull request, ensure that your pull request does not contain any sensitive or specific information, for example Client IDs or Client Secrets. 
Any sensitive values can be replaced with fake or dummy values for the purposes of submission as long as it is clearly indicated. 
Also, ensure that the readme.md of the connector is updated with the latest information, or created for new connector submissions. 
An example of a clear, structured, readme.md can be found in the [Azure Key Vault](https://github.com/microsoft/PowerPlatformConnectors/tree/master/custom-connectors/AzureKeyVault) connector repository. 
Include this completed `readme.md` in same connector directory which contains the artifacts. 

#### Custom Connectors

Updates to an existing custom connector can be made through a simple pull request to update the custom connector files.

For new custom connectors, create a directory under the ```custom-connectors``` directory and place the connector files in the sub-folder. Ensure that a clear, structured, readme.md is included. 

#### Certified Connectors

Updates to certified connectors must first be made through a pull request to the ```dev``` branch for review by the connector owner. 
Once a pull request has been merged to the ```dev``` branch, the connector owner can submit the connector for certification through the Connector certification tab in [ISV Studio](https://isvstudio.powerapps.com). Once certified, the Microsoft Certification team will handle merging the updates from ```dev``` to ```master```. 

Updates to an existing custom connector can be made through a simple pull request to the ```dev``` branch to update the custom connector files.

For new connectors which will be submitted for certification, create a directory under the ```certified-connectors``` directory, place the connector files in the sub-folder, and submit a pull request to the ```dev``` branch. Ensure that a clear, structured, readme.md is included. 

### Tooling and Validation

#### CLA

When a pull request is submitted, a CLA-bot will automatically determine whether you need to provide
a CLA and annotate the PR appropriately. Simply follow the instructions
provided by the bot to ensure your pull request can be properly reviewed.
You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

#### Swagger Validation

A submitted pull request will also be validated against our Swagger Validator tool, which checks the connector files to ensure it is a proper Swagger file and adheres to our connector requirements and guidelines. Any errors or warnings will be added to the PR for both the submitter and the reviewer to understand. We do not accept pull requests with outstanding unresolved Swagger Validator issues. 

#### Breaking Change Detector

Another validation which runs on a submitted pull request is the breaking changes validator. This is to catch any inadvertent, non-backwards-compatible (i.e. breaking) changes which may break a current user experience, for example, deleting a published operation. The Breaking Change Detector compares the previous version of the Swagger with the new submission and raises awareness of any breaking change. The submitter and reviewer must both acknowledge any breaking changes submitted and ensure that no end users are inadvertently negatively affected. 

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/en-us/

Microsoft and any contributors reserve all others rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
122,microsoft/qlib,Python,"[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


<p align=""center"">
  <img src=""http://fintech.msra.cn/images_v060/logo/1.png"" />
</p>


Qlib is an AI-oriented quantitative investment platform, which aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 

With Qlib, users can easily try ideas to create better Quant investment strategies.

For more details, please refer to our paper [""Qlib: An AI-oriented Quantitative Investment Platform""](https://arxiv.org/abs/2009.11189).

- [**News and Plans**](#news-and-plans)
- [Framework of Qlib](#framework-of-qlib)
- [Quick Start](#quick-start)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Auto Quant Research Workflow](#auto-quant-research-workflow)
  - [Building Customized Quant Research Workflow by Code](#building-customized-quant-research-workflow-by-code)
- [**Quant Model Zoo**](#quant-model-zoo)
  - [Run a single model](#run-a-single-model)
  - [Run multiple models](#run-multiple-models)
- [**Quant Dataset Zoo**](#quant-dataset-zoo)
- [More About Qlib](#more-about-qlib)
- [Offline Mode and Online Mode](#offline-mode-and-online-mode)
  - [Performance of Qlib Data Server](#performance-of-qlib-data-server)
- [Related Reports](#related-reports)
- [Contact Us](#contact-us)
- [Contributing](#contributing)


# News And Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
| Feature                        | Status      |
| --                      | ------    |
| Online serving and automatic model rolling | Under review: https://github.com/microsoft/qlib/pull/290 | 
| Planning-based portfolio optimization | Under review:  https://github.com/microsoft/qlib/pull/280 | 
| Fund data supporting and analysis  |  Under review: https://github.com/microsoft/qlib/pull/292 |
| Point-in-Time database | Under review: https://github.com/microsoft/qlib/pull/343 |
| High-frequency trading | Under review: https://github.com/microsoft/qlib/pull/408 | 
| Meta-Learning-based data selection | Initial opensource version under development |

Recent released features
| Feature | Status |
| --                      | ------    |
| DoubleEnsemble Model | Released https://github.com/microsoft/qlib/pull/286 | 
| High-frequency data processing example | Released https://github.com/microsoft/qlib/pull/257 |
| High-frequency trading example | Part of code released https://github.com/microsoft/qlib/pull/227 | 
| High-frequency data(1min) | Released https://github.com/microsoft/qlib/pull/221 |
| Tabnet Model | Released https://github.com/microsoft/qlib/pull/205 | 

Features released before 2021 are not listed here.

# Framework of Qlib

<div style=""align: center"">
<img src=""http://fintech.msra.cn/images_v060/framework.png?v=0.1"" />
</div>


At the module level, Qlib is a platform that consists of the above components. The components are designed as loose-coupled modules, and each component could be used stand-alone.

| Name                   | Description                                                                                                                                                                                                                                                                                                                                                             |
| ------                 | -----                                                                                                                                                                                                                                                                                                                                                                   |
| `Infrastructure` layer | `Infrastructure` layer provides underlying support for Quant research. `DataServer` provides a high-performance infrastructure for users to manage and retrieve raw data. `Trainer` provides a flexible interface to control the training process of models, which enable algorithms to control the training process.                                                       |
| `Workflow` layer       | `Workflow` layer covers the whole workflow of quantitative investment. `Information Extractor` extracts data for models. `Forecast Model` focuses on producing all kinds of forecast signals (e.g. _alpha_, risk) for other modules. With these signals `Portfolio Generator` will generate the target portfolio and produce orders to be executed by `Order Executor`. |
| `Interface` layer      | `Interface` layer tries to present a user-friendly interface for the underlying system. `Analyser` module will provide users detailed analysis reports of forecasting signals, portfolios and execution results                                                                                                                                                                 |

* The modules with hand-drawn style are under development and will be released in the future.
* The modules with dashed borders are highly user-customizable and extendible.


# Quick Start

This quick start guide tries to demonstrate
1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip           | install from source  | plot |
| ------------- |:---------------------:|:--------------------:|:----:|
| Python 3.6    | :heavy_check_mark:    | :heavy_check_mark: (only with `Anaconda`)                  | :heavy_check_mark: |
| Python 3.7    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :x:                   | :heavy_check_mark:   | :x: |

**Note**: 
1. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.7 or use `conda`'s Python to install ``Qlib`` from source.
2. For Python 3.9, `Qlib` supports running workflows such as training models, doing backtest and plot most of the related figures (those included in [notebook](examples/workflow_by_code.ipynb)). However, plotting for the *model performance* is not supported for now and we will fix this when the dependent packages are upgraded in the future.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade  cython
  ```

* Clone the repository and install ``Qlib`` as follows.
  * If you haven't installed qlib by the command ``pip install pyqlib`` before:
    ```bash
    git clone https://github.com/microsoft/qlib.git && cd qlib
    python setup.py install
    ```
  * If you have already installed the stable version by the command ``pip install pyqlib``:
    ```bash
    git clone https://github.com/microsoft/qlib.git && cd qlib
    pip install .
    ```
  **Note**: **Only** the command ``pip install .`` **can** overwrite the stable version installed by ``pip install pyqlib``, while the command ``python setup.py install`` **can't**.

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test.yml) may help you find the problem.

## Data Preparation
Load and prepare data by running the following code:
  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. 

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

<!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.config import REG_CN

  # Initialization
  mount_path = ""~/.qlib/qlib_data/cn_data""  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments('csi500')
  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = ['SH600000']
  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
  ```
 -->

## Auto Quant Research Workflow
Qlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: 

1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.
    ```bash
      cd examples  # Avoid running program under the directory contains `qlib`
      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
    ```
    If users want to use `qrun` under debug mode, please use the following command:
    ```bash
    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
    ```
    The result of `qrun` is as follows, please refer to [Intraday Trading](https://qlib.readthedocs.io/en/latest/component/backtest.html) for more details about the result. 

    ```bash

    'The following are analysis results of the excess return without cost.'
                           risk
    mean               0.000708
    std                0.005626
    annualized_return  0.178316
    information_ratio  1.996555
    max_drawdown      -0.081806
    'The following are analysis results of the excess return with cost.'
                           risk
    mean               0.000512
    std                0.005626
    annualized_return  0.128982
    information_ratio  1.444287
    max_drawdown      -0.091078
    ```
    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).

2. Graphical Reports Analysis: Run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports
    - Forecasting signal (model prediction) analysis
      - Cumulative Return of groups
      ![Cumulative Return](http://fintech.msra.cn/images_v060/analysis/analysis_model_cumulative_return.png?v=0.1)
      - Return distribution
      ![long_short](http://fintech.msra.cn/images_v060/analysis/analysis_model_long_short.png?v=0.1)
      - Information Coefficient (IC)
      ![Information Coefficient](http://fintech.msra.cn/images_v060/analysis/analysis_model_IC.png?v=0.1)        
      ![Monthly IC](http://fintech.msra.cn/images_v060/analysis/analysis_model_monthly_IC.png?v=0.1)
      ![IC](http://fintech.msra.cn/images_v060/analysis/analysis_model_NDQ.png?v=0.1)
      - Auto Correlation of forecasting signal (model prediction)
      ![Auto Correlation](http://fintech.msra.cn/images_v060/analysis/analysis_model_auto_correlation.png?v=0.1)

    - Portfolio analysis
      - Backtest return
      ![Report](http://fintech.msra.cn/images_v060/analysis/report.png?v=0.1)
      <!-- 
      - Score IC
      ![Score IC](docs/_static/img/score_ic.png)
      - Cumulative Return
      ![Cumulative Return](docs/_static/img/cumulative_return.png)
      - Risk Analysis
      ![Risk Analysis](docs/_static/img/risk_analysis.png)
      - Rank Label
      ![Rank Label](docs/_static/img/rank_label.png)
      -->
   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results

## Building Customized Quant Research Workflow by Code
The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.


# [Quant Model Zoo](examples/benchmarks)

Here is a list of models built on `Qlib`.
- [GBDT based on XGBoost (Tianqi Chen, et al. 2016)](qlib/contrib/model/xgboost.py)
- [GBDT based on LightGBM (Guolin Ke, et al. 2017)](qlib/contrib/model/gbdt.py)
- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. 2017)](qlib/contrib/model/catboost_model.py)
- [MLP based on pytorch](qlib/contrib/model/pytorch_nn.py)
- [LSTM based on pytorch (Sepp Hochreiter, et al. 1997)](qlib/contrib/model/pytorch_lstm.py)
- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](qlib/contrib/model/pytorch_gru.py)
- [ALSTM based on pytorch (Yao Qin, et al. 2017)](qlib/contrib/model/pytorch_alstm.py)
- [GATs based on pytorch (Petar Velickovic, et al. 2017)](qlib/contrib/model/pytorch_gats.py)
- [SFM based on pytorch (Liheng Zhang, et al. 2017)](qlib/contrib/model/pytorch_sfm.py)
- [TFT based on tensorflow (Bryan Lim, et al. 2019)](examples/benchmarks/TFT/tft.py)
- [TabNet based on pytorch (Sercan O. Arik, et al. 2019)](qlib/contrib/model/pytorch_tabnet.py)
- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. 2020)](qlib/contrib/model/double_ensemble.py)

Your PR of new Quant models is highly welcomed.

The performance of each model on the `Alpha158` and `Alpha360` dataset can be found [here](examples/benchmarks/README.md).

## Run a single model
All the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.

`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:
- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.
- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.

- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).

## Run multiple models
`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parrallel running the same model for multiple times as well, and this will be fixed in the future development too.)

The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.

Here is an example of running all the models for 10 iterations:
```python
python run_all_model.py 10
```

It also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). 


# Quant Dataset Zoo
Dataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:

| Dataset                                    | US Market | China Market |
| --                                         | --        | --           |
| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |
| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |

[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.
Your PR to build new Quant dataset is highly welcomed.

# More About Qlib
The detailed documents are organized in [docs](docs/).
[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. 
```bash
cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
```
You can also view the [latest document](http://qlib.readthedocs.io/) online directly.

Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).



# Offline Mode and Online Mode
The data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.

Under `Offline` mode, the data will be deployed locally. 

Under `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).

## Performance of Qlib Data Server
The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we
compare it with several other data storage solutions. 

We evaluate the performance of several storage solutions by finishing the same task,
which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.

|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |
| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |
| Total (1CPU) (seconds)  | 184.4±3.7 | 365.3±7.5 | 253.6±6.7 | 368.2±3.6 | 147.0±8.8   | 47.6±1.0     | **7.4±0.3** |
| Total (64CPU) (seconds) |           |           |           |           | 8.8±0.6     | **4.2±0.2**  |             |
* `+(-)E` indicates with (out) `ExpressionCache`
* `+(-)D` indicates with (out) `DatasetCache`

Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.
Such overheads greatly slow down the data loading process.
Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.

# Related Reports
- [【华泰金工林晓明团队】图神经网络选股与Qlib实践——华泰人工智能系列之四十二](https://mp.weixin.qq.com/s/w5fDB6oAv9dO6vlhf1kmhA)
- [Guide To Qlib: Microsoft’s AI Investment Platform](https://analyticsindiamag.com/qlib/)
- [【华泰金工林晓明团队】微软AI量化投资平台Qlib体验——华泰人工智能系列之四十](https://mp.weixin.qq.com/s/Brcd7im4NibJOJzZfMn6tQ)
- [微软也搞AI量化平台？还是开源的！](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)
- [微矿Qlib：业内首个AI量化投资开源平台](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)

# Contact Us
- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).
- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). 
- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).
  - We are recruiting new members(both FTEs and interns), your resumes are welcome!

Join IM discussion groups:
|[Gitter](https://gitter.im/Microsoft/qlib)|
|----|
|![image](http://fintech.msra.cn/images_v060/qrcode/gitter_qr.png)|

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the right to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
123,microsoft/vscode-powerquery,TypeScript,"# Power Query language service for VS Code

Provides a language service for the [Power Query / M formula language](https://powerquery.microsoft.com/) with the following capabilities:

-   Suggestions / Auto complete (Based on M standard library functions, and keywords)
-   Parameter hints
-   Hover
-   Code formatting
-   Syntax validation

Now available in the [Visual Studio Code Marketplace](https://marketplace.visualstudio.com/items?itemName=PowerQuery.vscode-powerquery).

## How to build

1. install dependencies:

```cmd
npm install
```

2. build all packages:

```cmd
npm run build
```

## How to run command line tests

```cmd
cd server
npm run test
```

There is also a UI test suite that can be run from VS Code.

## Generate vscode extension

Install the [vsce](https://code.visualstudio.com/api/working-with-extensions/publishing-extension) CLI utility.

```cmd
npm install --global vsce
```

Generate vsix package:

```cmd
vsce package
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
124,microsoft/PowerToys,C#,"# Microsoft PowerToys

<img src=""./doc/images/overview/PT%20hero%20image.png""/>

[Downloads & Release notes][github-release-link] | [Contributing to PowerToys](#contributing) | [What's Happening](#whats-happening) | [Roadmap](#powertoys-roadmap)

## Build status

| Architecture | Master | Stable | Installer |
|--------------|--------|--------|-----------|
| x64 | [![Build Status for Master](https://dev.azure.com/ms/PowerToys/_apis/build/status/microsoft.PowerToys?branchName=master)](https://dev.azure.com/ms/PowerToys/_build/latest?definitionId=219&branchName=master) | [![Build Status for Stable](https://dev.azure.com/ms/PowerToys/_apis/build/status/microsoft.PowerToys?branchName=stable)](https://dev.azure.com/ms/PowerToys/_build/latest?definitionId=219&branchName=stable) | [![Build Status for Installer](https://github-private.visualstudio.com/microsoft/_apis/build/status/CDPX/powertoys/powertoys-Windows-Official-master-Test?branchName=master)](https://github-private.visualstudio.com/microsoft/_build/latest?definitionId=61&branchName=master) |

## About

Microsoft PowerToys is a set of utilities for power users to tune and streamline their Windows 10 experience for greater productivity. For more info on [PowerToys overviews and guides][usingPowerToys-docs-link], or any other tools and resources for [Windows development environments](https://docs.microsoft.com/windows/dev-environment/overview), head over to [docs.microsoft.com][usingPowerToys-docs-link]! 

|              | Current utilities: |              |
|--------------|--------------------|--------------|
| [Color Picker](https://aka.ms/PowerToysOverview_ColorPicker) |  [FancyZones](https://aka.ms/PowerToysOverview_FancyZones) | [File Explorer Add-ons](https://aka.ms/PowerToysOverview_FileExplorerAddOns) |
| [Image Resizer](https://aka.ms/PowerToysOverview_ImageResizer) | [Keyboard Manager](https://aka.ms/PowerToysOverview_KeyboardManager) | [PowerRename](https://aka.ms/PowerToysOverview_PowerRename) |
| [PowerToys Run](https://aka.ms/PowerToysOverview_PowerToysRun) | [Shortcut Guide](https://aka.ms/PowerToysOverview_ShortcutGuide) | [Video Conference Mute (Experimental)](https://aka.ms/PowerToysOverview_VideoConference) |

## Installing and running Microsoft PowerToys

### Requirements

- Windows 10 v1903 (build 18362) or newer.
   - ⚠️ PowerToys minimum version of Windows 10 is v1903 starting with the 0.37 release
- Have [.NET Core 3.1.14 Desktop Runtime](https://dotnet.microsoft.com/download/dotnet/thank-you/runtime-desktop-3.1.14-windows-x64-installer). The installer should handle this but we want to directly make people aware.

### Via GitHub with EXE [Recommended]

#### Stable version

Install from the [Microsoft PowerToys GitHub releases page][github-release-link]. Click on `Assets` to show the files available in the release and then click on `PowerToysSetup-0.37.0-x64.exe` to download the PowerToys installer.

This is our preferred method.

#### Experimental version
To install the Video Conference mute, please use the [v0.36 experimental version of PowerToys][github-prerelease-link] to try out this version. It includes all improvements from v0.35 in addition to the Video conference utility. Click on `Assets` to show the files available in the release and then download the .exe installer.

### Via WinGet (Preview)
Download PowerToys from [WinGet](https://github.com/microsoft/winget-cli#installing-the-client). To install PowerToys, run the following command from the command line / PowerShell:

```powershell
WinGet install powertoys
```

### Other install methods

There are [community driven install methods](./doc/unofficialInstallMethods.md) such as Chocolatey and Scoop.  If these are your preferred install solutions, this will have the install instructions.

### Processor support

We currently support the matrix below.

| x64 | x86 | ARM64 |
|:---:|:---:|:---:|
| [Supported][github-release-link] | [Issue #602](https://github.com/microsoft/PowerToys/issues/602) | [Issue #490](https://github.com/microsoft/PowerToys/issues/490) |

## Contributing

This project welcomes contributions of all types. Help spec'ing, design, documentation, finding bugs are ways everyone can help on top of coding features / bug fixes. We are excited to work with the power user community to build a set of tools for helping you get the most out of Windows.

We ask that **before you start work on a feature that you would like to contribute**, please read our [Contributor's Guide](CONTRIBUTING.md). We will be happy to work with you to figure out the best approach, provide guidance and mentorship throughout feature development, and help avoid any wasted or duplicate effort.

Most contributions require you to agree to a [Contributor License Agreement (CLA)][oss-CLA] declaring that you have the right to, and actually do, grant us the rights to use your contribution.

For guidance on developing for PowerToys, please read the [developer docs](/doc/devdocs) for a detailed breakdown. This includes how to setup your computer to compile.

## What's Happening

### PowerToys Roadmap

Our [prioritized roadmap][roadmap] of features and utilities that the core team is focusing on.

### 0.37 - April 2021 Update

Our goals for [v0.37 release cycle](https://github.com/microsoft/PowerToys/issues?q=is%3Aopen+is%3Aissue+project%3Amicrosoft%2FPowerToys%2F19) Video Conference Mute work so we can bring it into the stable branch, general bug fixes, moving Keyboard manager out, and removing the legacy settings app.

The 0.36 experimental release was released this month as well which includes Video Conference Mute which is based off the 0.35 code base.

Our [prioritized roadmap][roadmap] of features and utilities will dictate what the core team is focusing on for the near future. 

#### Highlights from v0.37

**General**

- PowerToys now requires Windows 10, version 1903 or higher
- FancyZones editor default launching key is <kbd>Win</kbd>+<kbd>Shift</kbd>+<kbd>`</kbd>
   - Windows Terminal's new Quake mode will use <kbd>Win</kbd>+<kbd>`</kbd>.  We feel this is a far better use of the keystroke. 
   - Current PowerToys users can update this in our settings in the FancyZone section.
- Removed our v1 HTML based settings system

## New Spec - Feedback please!

- What is new in PowerToys (SCOOBE) - [Pull Request](https://github.com/microsoft/PowerToys/pull/10978)

### FancyZones
- Editor UX bug fixes. Thanks [@niels9001](https://github.com/niels9001)
- Monitor resolution is added to the top to directly infer the boxes on top are your monitors
- Fix for editor crash when editing a custom layout

### PowerRename
- Option added for capitalization.  
- Improved loading responsiveness with large sums of files.

### PowerToys Run
- Changed XAML to improve rendering. Thanks [@niels9001](https://github.com/niels9001)
- Disabled plugins are no longer loaded
- VS Code plugin workspaces showing up now.  Thanks [@ricardosantos9521](https://github.com/ricardosantos9521)

### Keyboard manager 
- Now an independent exe.  This now runs high priority in its own process.  When your CPU is under load, this should allow the process to continue to be prioritized

### Color Picker 
- uses a centralized keyhook.  This should improve activation
- Esc for closing will no longer bubble through.  Thanks [@DoctorNefario](https://github.com/DoctorNefario)

### Settings / Welcome to PowerToys
- Shortcuts will stand out more
- Few accessability bugs fixed. Thanks [@niels9001](https://github.com/niels9001)

### Shortcut Guide
- Excluded apps for Shortcut Guide.  Thanks [@davidegiacometti ](https://github.com/davidegiacometti)

### Installer
- new arg for starting PT after silent install

### Developer quality of life
- Ability to directly debug against Settings

## Community contributions

We'd like to directly mention (in alphabetical order) for their continued community support this month and helping directly make PowerToys a better piece of software.  

[@Aaron-Junker](https://github.com/Aaron-Junker), [@addrum](https://github.com/addrum), [@davidegiacometti ](https://github.com/davidegiacometti), [@DoctorNefario](https://github.com/DoctorNefario), [@htcfreek](https://github.com/htcfreek), [@Jay-o-Way](https://github.com/Jay-o-Way), [@niels9001](https://github.com/niels9001), and [@ricardosantos9521](https://github.com/ricardosantos9521)

#### What is being planned for v0.39 - May 2021

For [v0.39][github-next-release-work], we are planning to work on:

- Stability and bug fixes
- Improving VCM
- Moving Shortcutguide out of the main exe

## PowerToys Community

The PowerToys team is extremely grateful to have the [support of an amazing active community][community-link]. The work you do is incredibly important. PowerToys wouldn’t be nearly what it is today without your help filing bugs, updating documentation, guiding the design, or writing features. We want to say thank you and take time to recognize your work.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct][oss-conduct-code].

## Privacy Statement

The application logs basic telemetry. Our Telemetry Data page (Coming Soon) has the trends from the telemetry. Please read the [Microsoft privacy statement][privacy-link] for more information.

[oss-CLA]: https://cla.opensource.microsoft.com
[oss-conduct-code]: CODE_OF_CONDUCT.md
[community-link]: COMMUNITY.md
[github-release-link]: https://github.com/microsoft/PowerToys/releases/
[roadmap]: https://github.com/microsoft/PowerToys/wiki/Roadmap
[privacy-link]: http://go.microsoft.com/fwlink/?LinkId=521839
[vidConfOverview]: https://aka.ms/PowerToysOverview_VideoConference
[loc-bug]: https://github.com/microsoft/PowerToys/issues/new?assignees=&labels=&template=translation_issue.md&title=
[usingPowerToys-docs-link]: https://docs.microsoft.com/windows/powertoys/

<!-- items that need to be updated release to release -->
[github-next-release-work]: https://github.com/microsoft/PowerToys/issues?q=is%3Aopen+is%3Aissue+project%3Amicrosoft%2FPowerToys%2F20
[github-prerelease-link]: https://github.com/microsoft/PowerToys/releases/tag/v0.36.0
"
125,microsoft/fuse-webui,TypeScript,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
126,microsoft/powerbi-visuals-sampleslicer,TypeScript,"# PowerBI Slicer custom visual sample
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-sampleslicer.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-sampleslicer)

Demonstrates the use of the Advanced Filtering API introduced in the version 1.7 of [PowerBI Visuals Tools](https://github.com/Microsoft/PowerBI-visuals-tools). 

This PowerBI Custom Visual relies on the Advanced Filter API for bulk data-point selection and [PowerBI Visuals Interactivity Utils](https://github.com/Microsoft/powerbi-visuals-utils-interactivityutils) for discrete data-point selection.

### Understanding the visual
The visual lets the user select numeric data inputs to be displayed in all other visuals in the same report sheet. The user can either select discrete values or a range by adjusting the sliders. 

See a [demo PowerBI report](doc/SampleSlicer.pbix) to get an idea about the visual's functionality.

![](doc/images/SampleSlicer.PNG)

### Setting up the environment

You will first need to set up your environment as detailed [here](https://github.com/Microsoft/PowerBI-visuals/blob/master/Readme.md#setting-up-environment).

### Installing dev dependencies

Once you have cloned this example, run these commands to install dependencies and to connect the visual into powerbi.

```
npm install # This command will install all necessary modules
```

### Starting the dev app
```
pbiviz start
```

### Understanding the code
1. [Code structure](doc/CodeStructure.md)
2. Discrete selection with the PowerBI Visuals Interactivity Utils
  - [Adding the Interactivity Utils to the project](doc/AddingInteractivityUtils.md)
  - [Using the Interactivity Utils](doc/UsingInteractivityUtils.md)
3. Advanced selection with the Advanced Filter API
  - [Adding the Advanced Filter API to the project](doc/AddingAdvancedFilterAPI.md)
  - [Using the Advanced Filter API](doc/UsingAdvancedFilterAPI.md)
4. Bookmarks support
  - [Adding bookmarks support to the project](doc/AddingBookmarksSuppoprt.md)
5. Slicer synchronization support
  - [Enable Sync Slicers](doc/SlicerSynchronizationSupport.md)"
127,microsoft/electionguard-admin-device,TypeScript,"![Microsoft Defending Democracy Program: ElectionGuard](images/electionguard-banner.svg)

# ElectionGuard Admin Device

![package](https://github.com/microsoft/electionguard-admin-device/workflows/Package/badge.svg)
[![license](https://img.shields.io/github/license/microsoft/electionguard-admin-device)](License)

The ElectionGuard Admin Device is a fully functional
implementation built in ReactJS. It connects to an ElectionGuard SDK API to
complete two main tasks.

1. Start an Election
2. Tally Voting Results

This reference implementation is meant to demonstrate a possible use case of the
ElectionGuard SDK.

## Running the sample

This project was bootstrapped with
[Create React App](https://github.com/facebook/create-react-app).

This project can be run in two configurations.

1. Full Hardware Demo
   - **OS:** Linux (Unbuntu)
   - **Hardware:** Card reader, smart card, and usb stick
   - **Reference Implementation APIs:**
     - [Usb stick Api](https://github.com/InfernoRed/module-usbstick)
     - [Smart Card Api](https://github.com/InfernoRed/module-smartcards)
     - [ElectionGuard Api](https://github.com/microsoft/ElectionGuard-SDK-DotNetCore-Reference-Web-API)
2. Mock Demo
   - To demo, create a `.env.local` and enable the mocks shown in the `.env`
     file.

`yarn start`

Runs the app in the development mode.<br /> Open
[http://localhost:3000](http://localhost:3000) to view it in the browser.

The page will reload if you make edits.<br /> You will also see any lint errors
in the console.

`yarn test`

Launches the test runner in the interactive watch mode.<br /> See the section
about
[running tests](https://facebook.github.io/create-react-app/docs/running-tests)
for more information.

`yarn build`

Builds the app for production to the `build` folder.<br /> It correctly bundles
React in production mode and optimizes the build for the best performance.

The build is minified and the filenames include the hashes.<br /> Your app is
ready to be deployed!

See the section about
[deployment](https://facebook.github.io/create-react-app/docs/deployment) for
more information.

## Contributing

Help defend democracy and [contribute to the project](CONTRIBUTING).

<!--
Guidelines on README format: https://review.docs.microsoft.com/help/onboard/admin/samples/concepts/readme-template?branch=master

Guidance on onboarding samples to docs.microsoft.com/samples: https://review.docs.microsoft.com/help/onboard/admin/samples/process/onboarding?branch=master

Taxonomies for products and languages: https://review.docs.microsoft.com/new-hope/information-architecture/metadata/taxonomies?branch=master
-->
"
128,microsoft/code-with-engineering-playbook,Ruby,"# CSE Code-With Customer/Partner Engineering Playbook

An engineer working for a [CSE](docs/CSE.md) project...

* Has responsibilities to their team – mentor, coach, and lead.
* Knows their **playbook**. Follows their playbook. Fixes their playbook if it is broken. If they find a better playbook, they copy it. If somebody could use your playbook, give them yours.
* Leads by example. Models the behaviors we desire both interpersonally and technically.
* Strives to understand how their work fits into a broader context and ensures the outcome.

This is our playbook. All contributions welcome! Please feel free to submit a [pull request](https://github.com/microsoft/code-with-engineering-playbook/pulls) to get involved.

> **Note:** If you are reading this on github - head over to [https://microsoft.github.io/code-with-engineering-playbook/](https://microsoft.github.io/code-with-engineering-playbook/) for a better reading experience

## Why Have A Playbook

* To increase overall efficiency for team members and the whole team in general.
* Reduce the number of mistakes and avoid common pitfalls.
* Strive to be a better engineer and learn from other people's shared experience.

## ""The"" Checklist

If you do nothing else follow the [Engineering Fundamentals Checklist](docs/ENG-FUNDAMENTALS-CHECKLIST.md)! It's here to help follow the Engineering Fundamentals.

## Structure of a Sprint

A [breakdown of sections](docs/SPRINT-STRUCTURE.md) according to the structure of an Agile sprint.

## General Guidance

* Keep the code quality bar high.
* Value quality and precision over ‘getting things done’.
* Work diligently on the one important thing.
* As a distributed team take time to share context via wiki, teams and backlog items.
* Make the simple thing work now. Build fewer features today, but ensure they work amazingly. Then add more features tomorrow.
* Avoid adding scope to a backlog item, instead add a new backlog item.
* Our goal is to ship incremental customer value.
* Keep backlog item details up to date to communicate the state of things with the rest of your team.
* Report product issues found and provide clear and repeatable engineering feedback!
* We all own our code and each one of us has an obligation to make all parts of the solution great.

## QuickLinks

* [Engineering Fundamentals Checklist](docs/ENG-FUNDAMENTALS-CHECKLIST.md)
* [Structure of a Sprint](docs/SPRINT-STRUCTURE.md)

## Engineering Fundamentals

* [Agile Development](docs/agile-development/README.md)
* [Automated Testing](docs/automated-testing/README.md)
* [Code Reviews](docs/code-reviews/README.md)
* [Continuous Delivery (CD)](docs/continuous-delivery/README.md)
* [Continuous Integration (CI)](docs/continuous-integration/README.md)
* [Design Decision Logs](docs/design-reviews/decision-log/README.md)
* [Design Reviews](docs/design-reviews/README.md)
* [Developer Experience](docs/developer-experience/README.md)
* [Engineering Feedback](docs/engineering-feedback/README.md)
* [Observability](docs/observability/README.md)
* [Security](docs/security/README.md)
* [Source Control](docs/source-control/README.md)
* [Reliability](docs/reliability/README.md)

## Fundamentals for Specific Technology Areas

* [Data and DataOps Fundamentals](docs/data-fundamentals/README.md)
* [Machine Learning Fundamentals](docs/ml-fundamentals/README.md)

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.
"
129,microsoft/vsts-extension-multivalue-control,TypeScript,"> Currently only available on TFS 2017 or later and Visual Studio Team Services. 

![Work Item Form](img/form.png)

# Select multiple values for your fields
![Control](img/operatingSystem.png)

# Expand the control only when needed
![Control Collapsed](img/operatingSystemCollapsed.png)

![Control Expanded](img/operatingSystemExpanded.png)

# How to get started
## Azure Devops Services

Navigate to your work item form customization page and add a multivalue control.

![Layout Customization](img/layoutCustomization.png)

Edit the control so it can use the right field to store your selection and the right set of values to be displayed.

![Options](img/options.png)

Be sure to allow user inputed values if a picklisk (string) field is used to back the extension.

![check the box to allow users to enter their own values](img/allowedValues.png)

## Azure Devops Server
We recommend TFS 2017 RC2 or later when running this extension.

[Learn more](https://github.com/Microsoft/vsts-extension-multivalue-control/blob/master/xmldetails.md) about how to customize the multivalue control directly on XML.

# How to query

The selected values are stored in a semicolon separated format.  To search for items that have a specific value use the ""Contains Words"" operator.  If searching for multiple values, use multipe ""Contains Words"" clauses for that field.

# Source code 

The [source](https://github.com/Microsoft/vsts-extension-multivalue-control) for this extension can be found on Github - feel free to take, fork and extend. 

You can also learn how to build your own custom control extension for the work item form [here](https://www.visualstudio.com/en-us/docs/integrate/extensions/develop/custom-control). 

# Feedback 

We appreciate your feedback! Here are some ways to connect with us:

* Add a review.
* Report issues in [GitHub](https://github.com/Microsoft/vsts-extension-multivalue-control/issues).

> Microsoft DevLabs is an outlet for experiments from Microsoft, experiments that represent some of the latest ideas around developer tools. Solutions in this category are designed for broad usage, and you are encouraged to use and provide feedback on them; however, these extensions are not supported nor are any commitments made as to their longevity.
"
130,microsoft/openpaimarketplace,JavaScript,"<p align=""center"">
  <img src=""./docs/images/marketplace.svg"" width=""160"" alt=""Marketplace Logo"" /></a>
</p>

<h2 align=""center"">Openpaimarketplace</h2>

<p align=""center"">
  <a href=""https://github.com/microsoft/openpaimarketplace/actions?query=workflow%3A%22Publish+Docker+Image%22""><img src=""https://github.com/microsoft/openpaimarketplace/workflows/Publish%20Docker%20Image/badge.svg"" alt=""Publish Docker Image""></a>
  <a href=""https://github.com/microsoft/openpaimarketplace/actions?query=workflow%3AWebportal""><img src=""https://github.com/microsoft/openpaimarketplace/workflows/Webportal/badge.svg?branch=master"" alt=""Webportal CI""></a>
  <a href=""https://github.com/microsoft/openpaimarketplace/actions?query=workflow%3A%22Rest+Server%22""><img src=""https://github.com/microsoft/openpaimarketplace/workflows/Rest%20Server/badge.svg?branch=master"" alt=""Rest Server CI""></a>
  <a href=""https://openpaimarketplace.readthedocs.io/en/latest/?badge=latest""><img src=""https://readthedocs.org/projects/openpaimarketplace/badge/?version=latest"" alt=""Doc""></a>
  <a href=""https://github.com/microsoft/openpaimarketplace/releases""><img src=""https://img.shields.io/github/v/release/Microsoft/openpaimarketplace"" alt=""Release""></a>
</p>

---

A marketplace which stores data and job templates of openpai. Users could use openpaimarketplace to share their jobs or run-and-learn others' sharing job.

## Components

There are two components of openpaimarketplace, [rest server](https://github.com/microsoft/openpaimarketplace/tree/master/rest_server) and [webportal](https://github.com/microsoft/openpaimarketplace/tree/master/webportal), which are responsible for backend service and frontend UI seperately.

## Getting Started

- For Admin
  
  To the admin user who is responsible for deploying and managing OpenPAI and openpaimarketplace, please check [admin manual](https://openpaimarketplace.readthedocs.io/en/latest/admin/README.html) for help.

- For User

  To the normal user who wants to use marketplace templates in OpenPAI, please check [user manual](https://openpaimarketplace.readthedocs.io/en/latest/user/README.html) for help.

## Reference

- [OpenPAI](https://github.com/microsoft/pai): A complete solution for AI platform. HiveD will be more user-friendly when working in tandem with OpenPAI.

- [OpenPAI Protocol](https://github.com/microsoft/openpai-protocol): The protocol interface between marketplace and OpenPAI platform.

## Developing Guide

This section is a guide for developers who are new to openpaimarketplace. Openpaimarketplace contains 2 sub projects, `rest_server` and `webportal`. For the detailed developing guide, you should refer to the readme doc of each sub project.

### [rest_server](./rest_server)

Rest server uses nodejs as backend service framework. The api follows RESTful API specification.

### [webportal](./webportal)

Currently openpaimarketplace webportal is used as a [pai webportal plugin](https://github.com/microsoft/pai/blob/master/docs/manual/cluster-admin/how-to-customize-cluster-by-plugins.md). It uses react as frontend framework, and builds with webpack bundler.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <https://cla.opensource.microsoft.com>.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
131,microsoft/Bing-COVID-19-Data,,"# Bing COVID-19 data

## FAQ

### What data is available in the Bing COVID-19-Data GitHub repo?
Bing COVID-19 data includes confirmed, fatal, and recovered cases from all regions, updated daily in a .csv file. If there is an update or correction to already-published data, the data file will be updated accordingly. To ensure the stability of the data we share, it will be released with a 24-hour delay.

### What are the sources of the data?
We collect data from multiple trusted, reliable sources, including the [World Health Organization (WHO)](https://www.who.int/emergencies/diseases/novel-coronavirus-2019), [Centers for Disease Control and Prevention (CDC)](https://www.cdc.gov/coronavirus/2019-ncov/index.html), national and state public health departments, [BNO News](https://bnonews.com/index.php/2020/04/the-latest-coronavirus-cases/), [24/7 Wall St.](https://247wallst.com/), and [Wikipedia](https://en.wikipedia.org/wiki/2019%E2%80%9320_coronavirus_pandemic).

### How can I use this data?
This information is available strictly for educational and academic purposes, such as medical research, government agencies, and academic institutions. Data used or cited in publications should include an attribution to 'Bing COVID-19 Tracker' with a link to www.bing.com/covid.

### Why is data missing or inconsistent for some regions
Sources don't always provide counts for all data points, especially recovered case counts. The '-' symbol indicates where data is unavailable. In the dataset published in GitHub, the data field will be blank where data is unavailable.

### Where can I see the latest data?
The most current data available can be found on www.bing.com/covid and in the Bing COVID-19 widget. You can find more information about the Bing COVID-19 widget [here](https://www.bing.com/covid/dev#widget).
 
### Why is COVID data different on every website?
COVID tracking data comes from a wide set of sources that update at different times and may not always align.

### What about the Bing COVID-19 Widget?
Learn more about the Bing COVID-19 widget [here](https://github.com/microsoft/COVID-19-Widget).

#### Please reachout to BingCovid19DataReqs@microsoft.com for any questions or requests regarding the data. 
"
132,microsoft/powerbi-visuals-sunburst,TypeScript,"# powerbi-visuals-sunburst
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-sunburst.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-sunburst) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-sunburst/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-sunburst?branch=master)

Sunburst is a multilevel donut chart, used to visualize hierarchical data, depicted by concentric circles.

![Sunburst chart screenshot](assets/screenshot.png)
# Overview
Sunburst chart is used to visualize hierarchical data, depicted by concentric circles. The circle in the centre represents the root node, with the hierarchy moving outward from the center. A segment of the inner circle bears a hierarchical relationship to those segments of the outer circle which lie within the angular sweep of the parent segment.

See also [Sunburst chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380767&sourcecorrid=dfbfa3b3-75c3-497e-b2b9-ffd93aaca76f&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)


### Sunburst has 2 bucket fields: Category, Values.
1. Category - Field with a list of categories for each circle segment. It can accept many values.
2. Values - Field with values for Category field. It can accept one value. Also this field is used for cross filtering with other visuals.

![Sunburst screenshot 1](assets/Fields.png)

# Selection
You can select any segment of the chart for data filtering. For cleaning of selection you should click outside to ""Сlear"" button on the right top corner of a visual.

# Settings of Sunburst
### Group
- Font size: size of the label in the center of sunburst (see screenshot):
![Sunburst screenshot 2](assets/settings1.png)

- Show category label: show category label in the center of the visual
![Sunburst screenshot 3](assets/settings2.png)
- Show data labels: show text labels inside of arc segments of Sunburst
![Sunburst screenshot 3](assets/settings3.png)
- Category colors: to change colors of each category of the visual. Changing element color will also change its child elements color, but if you changed child element color before then it'll keep unchanged.
![Sunburst screenshot 4](assets/settings4.png)

### Tooltip
- Display Units: tooltip numeric value format. Possible values: Auto, Thousands, Million, Billions, Trillions.
- Decimal places: amount of decimal places to show.
![Sunburst screenshot 5](assets/settings5.png)

### Legend
- Position: Legend location. Possible values: Top, Bottom, Left, Right, Top Center, Bottom Center, Left Center, Right Center. 
- Title: switch on/off the legend Title.
- Legend Name: title caption.
- Color: font color of the legend values.
- Text Size: font size of legend values.
![Sunburst screenshot 6](assets/settings6.png)"
133,microsoft/devops-project-samples,JavaScript,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
134,microsoft/FASTER,C#,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/microsoft/FASTER/master/docs/assets/images/faster-logo.png"" alt=""FASTER logo"" width=""600px"" />
</p>
  
[![NuGet](https://img.shields.io/nuget/v/Microsoft.FASTER.Core.svg)](https://www.nuget.org/packages/Microsoft.FASTER.Core/)
[![Build Status](https://dev.azure.com/ms/FASTER/_apis/build/status/Microsoft.FASTER)](https://dev.azure.com/ms/FASTER/_build/latest?definitionId=8)
[![Gitter](https://badges.gitter.im/Microsoft/FASTER.svg)](https://gitter.im/Microsoft/FASTER?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

# Introduction

Managing large application state easily, resiliently, and with high performance is one of the hardest
problems in the cloud today. The FASTER project offers two artifacts to help tackle this problem.

* **FASTER Log** is a high-performance concurrent persistent recoverable log, iterator, and random 
reader library in C#. It supports very frequent commit operations at low latency, and can quickly saturate 
disk bandwidth. It supports both sync and async interfaces, handles disk errors, and supports checksums.

* **FASTER KV** is a concurrent key-value store + cache (available in C# and C++) that is designed for point 
lookups and heavy updates. FASTER supports data larger than memory, by leveraging fast external 
storage (local or cloud). It also supports consistent recovery using a fast non-blocking checkpointing technique 
that lets applications trade-off performance for commit latency.

Both FASTER KV and FASTER Log offer orders-of-magnitude higher performance than comparable solutions, on standard
workloads. Start learning about FASTER, its unique capabilities, and how to get started at our official website:

[aka.ms/FASTER](https://aka.ms/FASTER)


# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
135,microsoft/antares,C++,"## What is Antares:

**Antares** is an automatic engine to generate multi-platform kernels with optimization for ***DNN developers*** (targeting to backends like CUDA/ROCm/CPU/DirectX12/Graphcore/OneAPI/..). It is also a framework for ***Hardware developers*** to extend new backends/hareware quickly and easily. Antares provides IR that follows ""_One Language Syntax for All Platforms_"", and general-purpose device access APIs that hide the differences of not only DNN description but also device mapping.

1. [Features](#about-antares-features)
    - Backend Extension
    - Effective Auto Tuning
    - Einsum-based Antares IR
    - Framework JIT Extension (Op Maker Plugin for Pytorch/Tensorflow/Tensorflow2)

2. [How to Use Antares](#how-to-use-antares)
    - **Senario-1:** Quick Start for Developers that Use Antares to Tune Operator/Sub-graph in Foreground Terminal
    - **Senario-2:** Quick Start for Developers that Use Antares to Extend Operator/Sub-graph in Pytorch/Tensorflow

3. [Antares Pre-dependencies for Different Backends](#antares-predependencies--for-different-backends)
    - **Linux-based:** cuda, rocm, mcpu, scpu, gc, sycl_intel, sycl_cuda, ocl_amdgpu, ocl_nvidia, ocl_android, ..
    - **Windows-based:** cuda_win64, rocm_win64, hlsl_win64, ..

4. [About Microsft Open Source](#about-microsft-open-source)

## About Antares Features:

#### a. Backend Extension

The current version of Antares supports code generation for the following backends (in orange blocks) and devices (in black blocks):

![](images/antares-backends.svg)

#### b. Effective Auto Tuning

Auto tuning by Antares contributes to not only much less tuning time, but also equivalent or better performance for Intra-op/Inter-op execution (against TVM Ansor).

![](images/tuning-perf.svg)

#### c. Einsum-based Antares IR

- Antares IR is the frontend of both kernel generation and automatic optimization.
- The syntax of Antares IR is slim to describe most MLP/CNN/RNN/LSTM/Transformer based models like MNIST/ResNet/BERT/GPT/..

  **E.g. The following computation logic describes a layer of standard BERT transformer:**

``` sh
  merged_layer_local[R, B, S1, N1, H1] +=! input_tensor[B, S1, N, H] * qkv_weight[R, N, H, N1, H1];
  merged_layer_trans[R, B, N1, S1, H1] = merged_layer_local[R, B, S1, N1, H1] + qkv_bias[R, N1, H1];
  attention_scores[B, N1, S1, S2] +=! merged_layer_trans[0, B, N1, S1, H1] * merged_layer_trans[1, B, N1, S2, H1] / const({H}).cast(`float32`);
    softmax_1_temp0[B, N1] >=! attention_scores[B, N1, S1, S2];
    softmax_1_temp1[B, N1] +=! (attention_scores[B, N1, S1, S2] - softmax_1_temp0[B, N1]).call(`exp`);
  attention_probs[B, N1, S1, S2] = (attention_scores[B, N1, S1, S2] - softmax_1_temp0[B, N1]).call(`exp`) / softmax_1_temp1[B, N1];
  ... ...
  layer_norm_2_src[B, S1, N2, H2] = layer_output[B, S1, N2, H2] + attention_output_norm[B, S1, N2, H2];
    layer_norm_2_temp0[B, S1] += layer_norm_2_src[B, S1, N2, H2];
    layer_norm_2_temp1[B, S1] += layer_norm_2_src[B, S1, N2, H2] * layer_norm_2_src[B, S1, N2, H2];
  layer_output_norm[B, S1, N2, H2] = (layer_norm_2_src[B, S1, N2, H2] * {N * H} - layer_norm_2_temp0[B, S1]) * (layer_norm_2_temp0[B, S1] * {N * H} - layer_norm_2_temp1[B, S1] * layer_norm_2_temp1[B, S1]).call(`max`, [1e-8]).call(`rsqrt`);
```
For more IR usage or examples, please follow documentation here: [Antares IR & Examples](AntaresIR.md)

#### d. Pytorch/Tensorflow/Tensorflow2 Op Maker (JIT Plugin)
  Antares provides JIT plugin for Pytorch/Tensorflow/Tensorflow2 to help frameworks to easily extend new operators, e.g.:

```py
# Tensorflow/Tensorflow2 Example:
op = antares.make_op(ir='dot_0[N, M] +=! data[N, K] * weight[K, M]', feed_dict={'data': x, 'weight': y}).emit()
result_1 = sess.run(op)
print('The custom result_1 is:\n%s' % result_1)
result_2 = sess.run(tf.add(op, op))
print('The custom result_2 is:\n%s' % result_2)  

# Pytorch Example:
custom_op = CustomOp(ir='dot_0[N, M] +=! data[N, K] * weight[K, M]', feed_dict={'data': x, 'weight': y}).to(device, dtype).emit()
result = custom_op()
print('The custom result is:', result)
```
For complete programs, please follow examples here: [Antares Examples for Pytorch](frameworks/pytorch/examples) and [Antares Examples for TF/TF2](frameworks/tensorflow/examples)

## How to Use Antares?

### Senario-1: Quick Start for Developers that Use Antares to Tune Operator/Sub-graph in Foreground Terminal:

- Step-1 (Recommend for User with Root): Prepare Environment for Docker Mode
```sh
# Setup Package dependencies
sudo apt install docker.io

# Get Antares
git clone https://github.com/microsoft/antares --branch v0.2.x
cd antares/

# Set default backend type:
echo 'c-cuda' > backend.default

# Build the environment with sudo (if this step failed, please go to ""Pre-dependencies"" section to check which ""backend-related dependencies"" are missing):
sudo make
```
- Step-1 (Recommend for Non-root Users): Prepare Environment for Host Mode
```sh
# Ensure Package dependencies (Please ensure the following root-required dependencies has been installed.)
sudo apt install git python3-pip g++ make g++-mingw-w64-x86-64

# Get Antares
git clone https://github.com/microsoft/antares --branch v0.2.x
cd antares/

# Set default backend type:
echo 'c-cuda' > backend.default

# Build the environment without sudo (if this step failed, please go to ""Pre-dependencies"" section to check which ""backend-related dependencies"" are missing):
make
```
  All valid backends are listed in directory [antares/backends](backends)

- Step-2: Tune a Specific Workload in Foreground

```sh
# Example-1: Run the following command in bash to tune MatMul (4096, 4096) x (4096, 4096) using 2000 trials:
COMMIT=force STEP=2000 COMPUTE_V1='- S = 4096; einstein_v2(input_dict={""input0"": {""dtype"": ""float32"", ""shape"": [S, S]}, ""input1"": {""dtype"": ""float32"", ""shape"": [S, S]}}, exprss=""output0[N, M] +=! input0[N, K] * input1[K, M]"")' make

# Example-2: Run the following command in bash to tune MNIST-inference using 5000 trials:
COMMIT=force STEP=5000 COMPUTE_V1='- einstein_v2(input_dict={""data"": {""dtype"": ""float32"", ""shape"": [64, 784]}, ""weight_0"": {""dtype"": ""float32"", ""shape"": [784, 512]}, ""weight_1"": {""dtype"": ""float32"", ""shape"": [512, 512]}, ""weight_2"": {""dtype"": ""float32"", ""shape"": [512, 10]}, ""bias_0"": {""dtype"": ""float32"", ""shape"": [512]}, ""bias_1"": {""dtype"": ""float32"", ""shape"": [512]}, ""bias_2"": {""dtype"": ""float32"", ""shape"": [10]}}, extra_outputs=[], exprss=""data_0[N, M] +=!  data[N, K] * weight_0[K, M];   data_1[N, K] =   (data_0[N, K] + bias_0[K]).call(`max`, [0.0]);   data_2[N, M] +=!  data_1[N, K] * weight_1[K, M];   data_3[N, K] =   (data_2[N, K] + bias_1[K]).call(`max`, [0.0]);   data_4[N, M] +=!  data_3[N, K] * weight_2[K, M];   data_5[N, K] =   (data_4[N, K] + bias_2[K]);"")' make

```
  Apart from detailed reporting logs during the tuning procedure, the best kernel record will be saved to directory [antares/codehub](codehub). If you don't want to create/overwrite existing kernel record in codehub, environment variable `COMMIT=force` in the tuning command can be removed.

### Senario-2: Quick Start for Developers that Use Antares to Extend Operator/Sub-graph in Pytorch/Tensorflow (only for CUDA & ROCm backend currently):

- Step-1: Prepare Environment

  You need to follow `Step-1` from Senario-1 to finish environment preparation beforehand. This prevents many environmental issues when walking to the next step.

- Step-2: Set up a corresponding TF/TF2/Pytorch version that matches your CUDA/ROCm driver version. (**If you have installed TF/TF2/Pytorch, please just ignore this step**)

  Here we provide several prebuilt package sources that match different environment requirements:

        For Tensorflow 1.x & 2.x: Recommended Packages (tested in Ubuntu 20.04):
        #   Tensorflow-1 for NVIDIA CUDA 10.0:
        python3 -m pip install --upgrade pip && python3 -m pip install tensorflow-gpu==1.15.4
        #   Tensorflow-1 for NVIDIA CUDA 11.0:
        python3 -m pip install --upgrade pip && python3 -m pip install https://github.com/ghostplant/tensorflow-wheel-collections/releases/download/cuda-11/tensorflow_gpu-1.15.4_cuda11+nv-cp38-cp38-linux_x86_64.whl
        #   Tensorflow-1 for AMD ROCm 4.0:
        python3 -m pip install tensorflow-rocm==1.15.9

        #   Tensorflow-2 for NVIDIA CUDA 11.0:
        python3 -m pip install --upgrade pip && python3 -m pip install tensorflow-gpu==2.4.0
        #   Tensorflow-2 for AMD ROCm 4.0:
        python3 -m pip install tensorflow-rocm==2.4.0

        For Pytorch 1.x: Recommended Packages (tested in Ubuntu 20.04):
        #   Pytorch for NVIDIA CUDA 10.0:
        python3 -m pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html
        #   Pytorch for NVIDIA CUDA 11.0:
        python3 -m pip install torch===1.7.1+cu110 torchvision===0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
        #   Pytorch for AMD ROCm 4.0:
        python3 -m pip install torch torchvision -f https://download.pytorch.org/whl/rocm4.0.1/torch_stable.html

- Step-4: Install JIT Plugin Client and Run Examples

    ```sh
    # If using Pytorch, set up JIT Plugin for Pytorch:
    sudo python3 ./frameworks/pytorch/setup.py

    # If using Tensorflow, set up JIT Plugin for Tensorflow/Tensorflow2:
    sudo python3 ./frameworks/tensorflow/setup.py

    # Set the path of Antares root directory:
    export ANTARES_ROOT=""(..clone path to antares..)/""

    # Test Examples for Pytorch:
    cd ./frameworks/pytorch/examples
    ./1_hello_world.py

    # Test Examples for Tensorflow:
    cd ./frameworks/tensorflow/examples
    ./1_hello_world.py
    ```
  More examples here: [Antares Examples for Pytorch](frameworks/pytorch/examples) and [Antares Examples for TF/TF2](frameworks/tensorflow/examples)

## Antares Predependencies  for Different Backends:

Before running `make` command in antares root directory, you need to ensure the corresponding backend driver is installed correctly.

- Predependencies for backend `c-cuda`, `c-sycl_cuda`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: Install NVIDIA CUDA toolkit (>= 10.0) on Host OS`

    `Requirement: docker`

- Predependencies for backend `c-ocl_nvidia`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: Install NVIDIA CUDA toolkit (>= 10.0) to Host OS`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

- Predependencies for backend `c-ocl_android`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: Install package ""adb"", connect to rooted Android device and ensure command ""adb shell su -c 'ls /sdcard'"" works`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

- Predependencies for backend `c-rocm`, `c-ocl_amdgpu`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: Install AMD ROCm (>= 4.0) package ""rock-dkms"" & ""rock-dkms-firmware"" from repo http://repo.radeon.com/rocm/apt/debian to Host OS`

    `Requirement: docker`

- Predependencies for backend `c-ipu`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: Install Poplar SDK to Host OS, ensure ""popc"" command exists in system PATH`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

- Predependencies for backend `c-scpu`, `c-mcpu`, `c-sycl_intel`:

    `Requirement: Ubuntu >= 18.04`

    `Requirement: docker`

- Predependencies for backend `c-hlsl_win64`, `c-hlsl_xbox`:

    `Requirement: Windows 10 64 bit (>= 2004), run ""dxdiag.exe"" to ensure Direct3D 12.0 Accleration is enabled`

    `Requirement: Windows Subsystem Linux 1.0` [How to Install WSL 1.0](https://docs.microsoft.com/en-us/windows/wsl/install-win10)

    `Requirement: GIT clones antares repo inside WSL environment, and the path of antares directory should be **visible to Windows**, (e.g. ""/../c/Users/me/Desktop/antares"" would be OK, but ""/home/me/antares"" won't).`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

- Predependencies for backend `c-rocm_win64`:

    `Requirement: Windows 10 64 bit (>= 2004)`

    `Requirement: Windows Subsystem Linux 1.0` [How to Install WSL 1.0](https://docs.microsoft.com/en-us/windows/wsl/install-win10)

    `Requirement: Install Official AMD GPU driver (release version >= 2020.11).` Ensure `C:\Windows\System32\amdhip64.dll` exists after installation.

    `Requirement: GIT clones antares repo inside WSL environment, and the path of antares directory should be **visible to Windows**, (e.g. ""/../c/Users/me/Desktop/antares"" would be OK, but ""/home/me/antares"" won't).`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

- Predependencies for backend `c-cuda_win64`:

    `Requirement: Windows 10 64 bit (>= 2004)`

    `Requirement: Windows Subsystem Linux 1.0` [How to Install WSL 1.0](https://docs.microsoft.com/en-us/windows/wsl/install-win10)

    `Requirement: Install Official NVIDIA CUDA driver (>= 10.0).` Ensure `C:\Windows\System32\nvcuda.dll` exists after installation.

    `Requirement: GIT clones antares repo inside WSL environment, and the path of antares directory should be **visible to Windows**, (e.g. ""/../c/Users/me/Desktop/antares"" would be OK, but ""/home/me/antares"" won't).`

    `Requirement: run bash command ""make install_host"" in antares root directory beforehand`

## Current Support Table:

|       | HIP-C(c-rocm/c-rocm_win64) | CUDA(c-cuda/c-cuda_win64) | CPU(c-mcpu/c-scpu) | DirectX12(c-hlsl_win64) | Graphcore(c-ipu) | Intel OneAPI(c-sycl_intel) | Codeplay DPCPP (c-sycl_cuda) |
|---|---|---|---|---|---|---|---|
| Deploy Environment | Linux/WSL1 | Linux | Linux | WSL1 | Linux | Linux |   |
| Target Device | AMDGPU | NVGPU | Generic CPU | Generic Graphic Card | IPU Device | Intel CPU/HD Graphic/FPGA |  NVGPU |
| Global schedules  | Y | Y | Y | Y | Y | Y | Y |
| Local schedules   | Y | Y | Y | Y |   | Y | Y |
| Head fusion       | Y | Y | Y | Y | Y | Y | Y |
| Tail fusion       | Y | Y |   | Y |   |   | Y |
| Evaluator         | Y | Y | Y | Y | Y | Y | Y |
| Tensorflow Plugin | Y | Y | Y (intel-tensorflow) |   |   |   |   |
| Pytorch Plugin    | Y | Y | Y |   |   |   |   |
| Multi Kernel Eval | Y | Y | Y | Y |   | Y | Y |

## About Microsft Open Source
For more information about Microsoft Open Source Policy, please see [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)

"
136,microsoft/powerbi-visuals-mekkochart,TypeScript,"# powerbi-visuals-mekkochart
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-mekkochart.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-mekkochart) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-mekkochart/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-mekkochart?branch=master) 
[![Build Status](https://dev.azure.com/customvisuals/public/_apis/build/status/Microsoft.powerbi-visuals-chord)](https://dev.azure.com/customvisuals/public/_build/latest?definitionId=8)


A mix of 100% stacked column chart and 100% stacked bar chart combined into one view

![Mekko chart screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680589/Asset_694cc686-1c97-4063-b1e1-a67a2f57fab7/MekkoChartscreenshot1.png)

# Overview

Since it captures two dimensions in one chart, you can quickly spot the large segments as well the ones that are underrepresented in one quick glance. You can either use the same measure for the column height and width or use different ones depending on your need.
Similar to a treemap, the dimensional values are represented by the length and width of each rectangle. The width of a column is proportional to the total value of the column.
Segmentation and Pattern analysis are a big part of business analysis and with traditional charts you need to piece multiple individual items together in your mental map to draw conclusions. For dealing with such complex business analysis involving multiple variables/dimensions, the iconic marimekko design is very appealing and the Mekko chart makes it super easy to achieve this in Power BI.
The Mekko chart visual also allows you to control the legends, data colors, and data labels for a truly customized presentation.

See also [Mekko chart at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380785&sourcecorrid=848a7fc8-787d-427c-9364-34c8c9204179&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)"
137,microsoft/data-protection-mapping-project,TypeScript,"Copyright (c) Data Protection Mapping Project. All rights reserved.
Licensed under the MIT License.

## Data Protection/Privacy Mapping Project Mission
The Data Protection/Privacy Mapping Project (the “Project”) facilitates consistent global comprehension and implementation of data protection with an open source mapping between ISO/IEC 27701 and global data protection and/or privacy laws and regulations.
### Principles
* Neutral – The project is not for profit and carries no affiliation or company-driven focus. 
* Open – The data, infrastructure, and processes are available to all
* Consensus – Mapping data can at times be subjective, but consensus can be achieved through engagements with stakeholders
* Transparent – The operations and decision-making process for this project is transparent
### Scope
The Project aims to make the link between ISO/IEC 27001 and data protection regulatory requirements comprehensible to privacy professionals. It seeks to continuously improve and expand the data map between ISO/IEC 27701 and data protection laws and regulations, especially that of regulatory requirements. Initial mapping data is based on existing mapping between ISO/IEC 27701 and GDPR, and additional mappings were prepared by outside counsel for Microsoft between ISO/IEC 27701 and regulations from Australia, California, Canada, Brazil, Hong Kong, Singapore, South Korea, and Turkey.
### What?
Crowdsourcing the mapping between ISO/IEC 27701 controls and various data protection requirements.
### Why?
Data Protection regulatory requirements vary from jurisdiction to jurisdiction. With today’s global economy and shifting technology landscape, it is becoming increasingly difficult for organizations to manage data protection accountability efficiently and effectively. This mapping project leverages the universal data protection controls outlined by ISO/IEC 27701 to help organizations reconcile various laws and regulations with the common controls. Certification bodies, internal auditors and other stakeholders can then review the implementation of these controls to confirm accountability for the mapped regulations. Establishing a common understanding of the relationship between the ISO/IEC 27701 and data protection laws and regulations enables consistent interpretations and shared accountability among organizations globally. This can help strengthen data protection, technological solutions, and commerce.
### Microsoft's Role
Microsoft initiated the Project by donating the initial data visualization code and regulatory mappings with ISO/IEC 27701 except the mapping with GDPR. The intention of publishing the content to open source is to stimulate international cooperation within the global privacy community and to improve the reliability and consistency of privacy practice. Microsoft will initially act as one of the Data Curators and Code Committers while the Project takes shape. Microsoft plans to transition away from the initial role of Data Curator as the Project matures. As part of Microsoft's contribution to the privacy community, the app is freely deployed on Microsoft services.
### How?
The Project encourages mapping contributions from data protection experts. The Data Curators will assess whether to accept, reject, or amend those contributions. Contributions that are accepted (with or without amendments) are then posted publicly with attribution for the public to review and consume. Contributions that are rejected are published separately with explanation from data curators within the site for future reference.
### Future efforts could focus on further topics such as the following examples:
*	Mapping Quality: Improve comprehensiveness, integrity and accuracy of the mapping
*	Mapping Scope: Add new mappings
*	Multilingual Capability: Enable multi-language capability both for mapping and consumption of data
*	Improve User Interface: Include attribution of mapping source and time stamp
*	Additional Data Visualization and Analytics
*	Simplify Data Export
The ordering and completion of this projected work and effort applied will depend entirely on the community and their interests.
## Governance
### Processes
The project undertakes four main operations in support of the stated goals and scope. Those processes are listed here in rough execution order:
#### Consume
The general expectation is that most users for the Project will only consume the data without making contribution. Users may use and download all or part of the mapping data for their own analysis under MIT license. Please note that the mapping data does not expose the full content of ISO/IEC 27701, which is the heart of the Project. Proper consumption of the project content requires acquisition of the standard from ISO, IEC, national standard bodies, such as BSI, ANSI, JISC, or ABNT, or other authorized sellers.
#### Value-added projects
Users are encouraged to create derivative, value-added projects from the mapping data for commercial purposes under the licensing terms of the Project. Users shall cite and acknowledge the Project. As the mapping data will evolve over time, time stamp or continuous data synchronization may be necessary. Commercial use of the mapping content shall respect the copyrights of ISO/IEC.
#### Curate
The curation process is open and transparent. Data Curators work on data contributed by the data protection community to validate presented information. All deliberations, discoveries and discussions are recorded and made available for community inspection.
Initially this workflow will happen in one or more GitHub repositories using standard Pull Request workflows on human-readable and diff-able curation artifacts. The project should develop additional tools to supplement or supplant this flow to better support users who are not technically savvy but will always ensure full transparency.
At least initially, all curated data must be signed off by two or more Data Curators after a due consideration of data quality. This is in the interest of working through thought and mechanical processes and developing a common understanding of the data and determining what is admissible. Disagreement in what is admissible should be resolved through consensus among Data Curators. While the voting process can be used to resolve such dispute, it is intended to be a procedure of last resort.
#### Contribute
Contribution from the data protection community is the most important activity for this project. All contributions are welcomed. Contributions may include correction or improvement of existing mappings, new mappings, and code contribution. Due to the nature and spirit of open source projects, all contributors must be either individually identifiable or representing an organization.
### Roles
#### Data Curator
A Data Curator is akin to a project maintainer or committer in typical open source projects. Data Curators have “write” permissions to the curation repository and are ultimately responsible for admitting data to the data repository. A Data Curator is responsible for data quality control, integrity and accuracy. The role requires reasonable domain context to enable issue identification and resolution. The role also requires technical expertise in running the necessary tools used to manage the project. Each curator must be, and be seen to be, neutral and impartial. 
New Data Curators are nominated and approved by existing curators based on their merits and prior contributions. The role relates to an individual expert, an organization or a position in an organization. Under no circumstances, shall a Data Curator be held responsible for any errors or other flaws in the data merged into the service.
The current Data Curators are: Lanx Goh, Eric Lachaud, and Alex Li on behalf of Microsoft
#### Data Contributor
A Data Contributor for the Project is like a contributor on any other open source project – they identify bugs or improvements, fork the repo and contribute a pull request with their changes. For data contributors, this could be a small change (e.g., spelling correction or URL link submission), a substantive change (e.g., mapping correction), or wholesale data mapping (e.g., providing data mapping for additional regulations). Contributors should, as with any other open source project, expect to substantiate the changes with background information and adequate explanation of correctness. Since most Contributors are unlikely to be knowledgeable users of GitHub, data contribution in the form of spreadsheet sent to the Data Curators will be accepted.
A serial contributor of quality data or code is a candidate to become a curator or code committer.
#### Consumer
A Data Consumer accesses the curated data. They understand that the data is provided “as-is” with no guarantees or warranties as to (a) the correctness of the data or (b) suitability for any particular purpose. All data is fully qualified as to its origin and any clarifications made and it is up to the consumer to decide whether to use the data at their own risk.
#### Code Committer
While the Project is focused on data, the project initially contains a modest amount of code to enable data import and data visualization. Upon the initial release of the project, code committership and data curatorship roles are integrated. But it is recognized that there is a distinct skill set that is required for a Code Committer and a Data Curator. Separation of roles may develop as the project evolve.
The current Code Committer is Alex Li on behalf of Microsoft
#### Removal from role
In the unlikely event that a committer or curator becomes disruptive or falls inactive for an extended period of time, they may be removed from the role through an absolute majority vote of the remaining set of committers and curators. Committers and curators may resign at any time from the Project. Advance notice in case of resignation is encouraged to identify replacement. 
### Voting
Most decisions within the project can be done through informal consensus and recorded in the appropriate public record. When a formal decision is required, for example, when electing committers/curators, a vote is held using the following process:
•	A topic for voting is tabled by a Data Curator or Code Committer by notifying all other Curators and Committers.
•	Once tabled, Data Curators may vote during an open voting period lasting no less than one working week. Voting will occur on an agreed to, mutually convenient, and open medium (e.g., email, GitHub issue, etc.)
•	A minimum of two positive (+1) votes and no negative (-1) votes carries the topic. Note that negative votes must be substantiated.
•	Abstention (0) votes do not affect the outcome.

### Running the app

You can access the app live at the following URL: https://dataprotectionmapping.z21.web.core.windows.net/ or https://aka.ms/dpmap.

### Building the code

#### Prerequisites

Please ensure that you have at least the **minimum** recommended versions

-   Node >= 9.0.0 

#### 1. Clone the repository

-   Clone the repository using the following commands:
    ```bash
    git clone https://github.com/microsoft/data-protection-mapping-project
    ```
-   Select the created directory
    ```bash
    cd data-protection-mapping-project
    ```

#### 2. Install packages

-   Install the Angular CLI and the project dependency packages:

    ```bash
    npm install -g @angular/cli
    npm install
    ```

#### 3. Build and run

-   Run the dev server
    ```bash
    ng serve
    ```

#### 4. Open app in web browser

-   Navigate your browser to: 
	http://localhost:4200


# MIT License

Copyright <2020> Data Protection Mapping Project
  
  MIT License
  
  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
  
  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
  
  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"
138,microsoft/PowerBI-visuals-ChicletSlicer,TypeScript,"# PowerBI-visuals-ChicletSlicer
[![Build Status](https://travis-ci.org/Microsoft/PowerBI-visuals-ChicletSlicer.svg?branch=master)](https://travis-ci.org/Microsoft/PowerBI-visuals-ChicletSlicer) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/PowerBI-visuals-ChicletSlicer/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/PowerBI-visuals-ChicletSlicer?branch=master)

> Use this slicer to display image and/or text buttons that act as an in-canvas filter. Define additional properties for the layout & selection to customize this slicer to meet your specific needs

![ChicletSlicer screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680551/Asset_95650974-8e44-40cf-b041-fe64ca49a6e0/ChicletSlicerscreenshot2.png)

# Overview

The Chiclet Slicer was inspired by the great slicer control found in Excel since 2010, but with much greater customization options.
Chiclet are a slicers made of buttons, that can also be arranged horizontally for a very efficient real estate use, or arranged as a matrix for a super compact form.
Chiclet slicer also supports cross highlighting.
That's not all - they can even contain images!

See also [Chiclet Slicer at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380756&sourcecorrid=1094fb73-f014-4f7a-accf-65142c8316af&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)

ChicletSlicer has 3 bucket fields: Category, Values and Images.
1. Category - Field with a list of categories for each ""chiclet"" item. String values
2. Values - Field with values for Category field. This field use just for cross filtering with other visuals.
3. Image - Field with a list of images for each ""chiclet"" item. Values of this field can be presented in ""base64 image"" format or it can be just external link to image.

# Selection
You can select any chiclet item for data filtering and also you can use multi selection. For cleaning of selection you should click to ""Сlear"" button on the right top corner of a visual.

# Search
You can filter a list of chiclets items by category using string search. For activation of search field you should click to ""search"" in options of the visual. After that you can input search string and data will be filtered immediately.

# Settings of ChicletSlicer
### General options
- Orientation: An order of ""chiclets"" list building. Can be vertical or horizontal
- Columns: Amount of ChicletSlicer columns.
- Rows: Amount of ChicletSlicer rows.
- Show disabled: Setting which regulate where will be displayed disabled chiclets items.
  This setting has 3 selection values:
  - Inplace: Disabled chiclets will be located in a their original positions.
  - Bottom: Disabled chiclets will be located in a bottom of the list.
  - Hide: Disabled chiclets will be hide from list.
- Multiple selection:
  - If this option is turned on you will be able to select chiclets without Ctrl button. (It will be usefull for multiple selection on mobile devices)
  - If this option is turned off you have to use Ctrl button for multiple selection.
- Forced selection: This setting forcibly select first item and doesn't allow to unselect any chiclet item

### Chiclets options
- Text size: Size of text for chiclet item.
- Height: Height of chicklet item.
- Width: Width of chiclet item.
- Background: Background color of all ChicletSlicer.
- Transparency: Transparency of ChicletSlicer background.
- Selected Color: Background color of selected chiclet item.
- Hover color: Text color when you hover to chiclet item.
- Unselected color: Background color of unselected chicklet items.
- Disabled color: Color of disabled chicklet item.
- Outline color: Color for outline of chiclet item.
- Outline weight: Thickness of chiclet item outline.
- Text color: Color of chiclet item text.
- Padding: Indent around chiclet item.
- Outline style: Style of outline.
  This settins has 3 selection values:
  - Rounded: Round outline edges
  - Cut: Semicircular outline edges
  - Square: Square outline edges

### Images options
- Image split: Height of Image for chiclet item
- Round: Shows image as rounded
- Stretch image: Stretch image to full chiclet item width
- Bottom image:
  - If this option is turned on image will be in the bottom of chiclet item, under the text
  - If this option is turned off image will be in the top of chiclet item, above the text"
139,microsoft/vscode-python-devicesimulator,TypeScript,"# Device Simulator Express, a Microsoft Garage project

<a href='https://www.python.org/downloads/'><img src='https://img.shields.io/badge/Python-3.7%2B-blue.svg' alt='Python versions: 3.7+' /></a> <img src='https://img.shields.io/badge/VS%20Code-v1.43+-blue' alt='VS Code version 1.43'> <img src='https://www.repostatus.org/badges/latest/active.svg' alt='Project Status: Active – The project has reached a stable, usable state and is being actively developed.' /> <a href='LICENSE'><img src='https://img.shields.io/badge/license-MIT-blue.svg' alt='License: We are using the MIT License'></a> <a href='CONTRIBUTING.md'><img src='https://img.shields.io/badge/PRs-Welcome-brightgreen.svg' alt='We are welcoming PRS!'></a> <img src='https://img.shields.io/badge/platform-win%20%7C%20osx-lightgrey.svg' alt='Platforms Supported: Windows, MacOSX'/>

<a href='https://microsoftgarage.visualstudio.com/002806e2-ebaa-4672-9d2e-5fe5d29154ef/_boards/board/t/227906bb-31ac-4b07-8626-3d757754a616/Microsoft.RequirementCategory/'><img src='https://microsoftgarage.visualstudio.com/002806e2-ebaa-4672-9d2e-5fe5d29154ef/227906bb-31ac-4b07-8626-3d757754a616/_apis/work/boardbadge/73f82653-3da1-4a6f-bb79-c91c9eecec28' alt='Azure DevOps Board Badge' /></a>

Make without limit! Device Simulator Express, a Microsoft Garage project, allows you to code microcontrollers without the hardware on hand! You can program your Adafruit Circuit Playground Express (CPX), your BBC micro:bit or the Adafruit CLUE! Test and debug your code on the device simulator and see the same
result when you plug in your actual microcontroller. Curious about the output of the device, the serial
monitor allows you to observe the device output.

## Table of Contents
  - [Devices we support](#devices-we-support)
  - [Prerequisites](#prerequisites)
  - [Adafruit Circuit Playground Express (CPX) Simulator](#adafruit-circuit-playground-express-cpx-simulator)
    - [Features](#features)
    - [Useful Links](#useful-links)
    - [Keyboard Shortcuts](#keyboard-shortcuts)
  - [BBC micro:bit Simulator](#bbc-microbit-simulator)
    - [Features](#features-1)
    - [Useful Links](#useful-links-1)
    - [Keyboard Shortcuts](#keyboard-shortcuts-1)
  - [Adafruit CLUE Simulator](#adafruit-clue-simulator)
    - [Features](#features-2)
    - [Useful Links](#useful-links-2)
    - [Keyboard Shortcuts](#keyboard-shortcuts-2)
  - [How to use](#how-to-use)
    - [Commands](#commands)
  - [Contribute](#contribute)
  - [Provide feedback](#provide-feedback)
  - [Privacy and Telemetry Notice](#privacy-and-telemetry-notice)
  - [Third Party Notice](#third-party-notice)
  - [Troubleshooting Tips](#troubleshooting-tips)
  - [License](#license)
  - [Notes](#notes)
  
## Devices we support

-   [**Adafruit Circuit Playground Express (CPX)**](#adafruit-circuit-playground-express-cpx-simulator)

    [<img alt='CircuitPlayground Express' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/cpx/cpx-img.png'>](#adafruit-circuit-playground-express-cpx-simulator)

-   [**BBC micro:bit**](#bbc-microbit-simulator)

    [<img alt='bbc micro:bit' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/microbit/microbit.png'>](#bbc-microbit-simulator)

-   [**Adafruit CLUE**](#adafruit-clue-simulator)

    [<img alt='Adafruit CLUE' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/clue/clue.png'>](#adafruit-clue-simulator)

## Build Status

| Branch  |                                                                                                                    Build Status                                                                                                                    |
| :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| dev     |     [![Build Status](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_apis/build/status/Adafruit/Pacifica-CI?branchName=dev)](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_build/latest?definitionId=304&branchName=dev)     |
| staging | [![Build Status](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_apis/build/status/Adafruit/Pacifica-CI?branchName=staging)](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_build/latest?definitionId=304&branchName=staging) |
| master  |  [![Build Status](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_apis/build/status/Adafruit/Pacifica-CI?branchName=master)](https://microsoftgarage.visualstudio.com/Intern%20GitHub/_build/latest?definitionId=304&branchName=master)  |

## Prerequisites

The following dependencies are required to install before launching Device Simulator Express.  
You will be prompted to install the Python dependencies during the first use.

-   _**[Visual Studio Code](https://code.visualstudio.com/)**_
-   _**[Python 3.7+](https://www.python.org/downloads/)**_: Make sure you've added Python and pip to your PATH in your environment variables. (1)
-   _**[Python VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)**_: This will be installed automatically from the marketplace when you install Device Simulator Express.

## Adafruit Circuit Playground Express (CPX) Simulator

### Features

-   IntelliSense and syntax highlighting for CircuitPython code for the CPX library
-   Template file generation
-   Integrated Python Debugging for the Simulator
-   Serial monitor (available on Windows and Mac only)
-   Output panel for the simulator
-   Deploy CircuitPython code to the physical device.
-   Simulation of the CPX device, including:
    -   Green LED
    -   Red LED
    -   Push Buttons A and B
    -   Slider Switch
    -   Speaker: Play .wav file
    -   10 NeoPixels
    -   Light sensor
    -   Motion sensors
    -   Acceleration detection
    -   Device shake detection
    -   Temperature sensor
    -   7 Capacitive Touch sensors

The simulator supports most of the sensors on CPX except **IR transmitter & Receiver**, **Sound Sensor (microphone)**, **Speaker (Play Tone)** and the **""tap"" on Motion Sensor**.
The code related to these sensors can still run on the actual CPX board and be deployed using Device Simulator Express.  
As we only support CPX library now, other libraries (i.e. simpleio) can’t run on the simulator. But they will work on the actual device!

### Useful Links

-   Tutorials and Example Code for Adafruit CPX:
    -   [Adafruit CPX library tutorial](https://learn.adafruit.com/circuitpython-made-easy-on-circuit-playground-express/circuit-playground-express-library)
    -   [Adafruit CPX Examples on GitHub](https://github.com/adafruit/Adafruit_CircuitPython_CircuitPlayground/tree/master/examples)
    -   [Adafruit CPX Guided Tour (Intro for the Hardware)](https://learn.adafruit.com/adafruit-circuit-playground-express/guided-tour)
-   Format Adafruit CPX device:
    -   [Tutorial for formatting Adafruit CPX for CircuitPython](https://learn.adafruit.com/welcome-to-circuitpython/installing-circuitpython)
    -   [Download Firmware .uf2 file](https://learn.adafruit.com/adafruit-circuit-playground-express/circuitpython-quickstart)
    -   [Download the latest version of the Adafruit CPX library](https://learn.adafruit.com/welcome-to-circuitpython/circuitpython-libraries)

### Keyboard Shortcuts

In Device Simulator Express, you can use keyboard to interact with the device:

-   Push Button: <kbd>A</kbd> for Button A, <kbd>B</kbd> for Button B, <kbd>C</kbd> for Buttons A & B
-   Capacitive Touch Sensor: <kbd>Shift</kbd> + <kbd>1</kbd> ~ <kbd>7</kbd> for GPIO pins A1 - A7
-   Slider Switch: <kbd>Shift</kbd> + <kbd>S</kbd>
-   Refresh the simulator: <kbd>Shift</kbd> + <kbd>R</kbd>
-   Run the simulator: <kbd>Shift</kbd> + <kbd>F</kbd>

## BBC micro:bit Simulator

### Features

-   IntelliSense and syntax highlighting for MicroPython code for the micro:bit library
-   Template file generation
-   Integrated Python Debugging for the Simulator
-   Deploy MicroPython code to the physical device
-   Serial monitor (available on Windows and Mac only)
-   Simulation of the micro:bit device, including:
    -   25 LEDs
    -   Push Buttons A and B
    -   Light sensor
    -   Motion sensors
    -   Acceleration detection including gesture detection
    -   Temperature sensor

### Useful Links

-   Tutorials and Example Code for BBC micro:bit:
    -   [MicroPython documentation](https://microbit-micropython.readthedocs.io/en/latest/)
    -   [BBC micro:bit examples on the official micro:bit website](https://microbit.org/projects/make-it-code-it/?filters=python)

### Keyboard Shortcuts

-   Push Button: <kbd>A</kbd> for Button A, <kbd>B</kbd> for Button B, <kbd>C</kbd> for Buttons A & B
-   Refresh the simulator: <kbd>Shift</kbd> + <kbd>R</kbd>
-   Run the simulator: <kbd>Shift</kbd> + <kbd>F</kbd>

## Adafruit CLUE Simulator

### Features

-   IntelliSense and syntax highlighting for CircuitPython code for the following drivers and libraries:
    -   `adafruit_clue`
    -   `adafruit_slideshow`
    -   `adafruit_display_shapes`
    -   `adafruit_display_text`
    -   `adafruit_bitmap_font`
    -   `adafruit_fancyled`
    -   `neopixel`
    -   `displayio`
-   Template file generation
-   Integrated Python Debugging for the Simulator
-   Deploy CircuitPython code to the physical device
-   Serial monitor (available on Windows and Mac only)
-   Simulation of the CLUE device, including:
    -   240x240 color screen
    -   Push Buttons A and B
    -   Sensors for:
        -   Temperature
        -   Light
        -   Color
        -   Acceleration
        -   Humidity
        -   Pressure
        -   Proximity
        -   Gestures
        -   Gyro
        -   Magnetic Field

### Useful Links

-   Tutorials and Example Code for Adafruit CLUE:
    -   [Adafruit CLUE Overview](https://learn.adafruit.com/adafruit-clue)
    -   [Adafruit CLUE Examples on GitHub](https://github.com/adafruit/Adafruit_CircuitPython_CLUE/tree/master/examples)

### Keyboard Shortcuts

-   Push Button: <kbd>A</kbd> for Button A, <kbd>B</kbd> for Button B, <kbd>C</kbd> for Buttons A & B
-   Refresh the simulator: <kbd>Shift</kbd> + <kbd>R</kbd>
-   Run the simulator: <kbd>Shift</kbd> + <kbd>F</kbd>

## How to use

To use Device Simulator Express, install the extension from the marketplace and reload VS Code.

To access many of the commands, you need to open the command palette. This can be done with <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> for Windows and Linux / <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> for Mac. It can also be accessed from the toolbar by going to `View -> Command Palette`.

### I. Take a look at the ""Device Simulator Express: Getting Started"" Command.

1. Type in `""Device Simulator Express: Getting Started""` in the command palette (<kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> / <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> to open the command palette).
2. Choose the the device you want to play with from the dropdown.
3. Read, copy and learn some of the things you can do with the simulator!

<img alt='Getting Started' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/getting_started.png'>

### II. Start with the ""Device Simulator Express: New File"" Command.

1. Type in `""Device Simulator Express: New File""` in the command palette (<kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> / <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> to open the command palette).
2. Select the device you want to use.
3. Name and save your file somewhere, and we’re good to go!
4. Start with some examples: you can find examples files and tutorials inside the comments at the top of the file.

<img alt='""New File"" animation' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/new_file.gif'>

### III. Start from an existing Python file.

1. Open the folder or your .py file in Visual Studio Code.
2. Run `Device Simulator Express: Open Simulator` from the command palette or icon in the editor toolbar.
3. Select the device you want to use.

### IV. Run your code on the simulator.

1. Run `Run Simulator` from the command palette or use the `Play` button on the simulator webview.

<img alt='How to run the simulator animation' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/run.gif'>

### V. Deploy your code to the physical device

Before deploying the Python code to your CPX device, you need to format your device by following these tutorials:

-   _For the CPX_:

    -   Download the firmware with the .uf2 file (link: https://learn.adafruit.com/adafruit-circuit-playground-express/circuitpython-quickstart).
    -   Download the lastest versions of the cpx libraries (link: https://learn.adafruit.com/welcome-to-circuitpython/circuitpython-libraries).

-   _For the micro:bit_:

    -   Download the firmware with the .hex file (link: https://microbit.org/get-started/user-guide/firmware/).

-   _For the CLUE_:
    -   Download the latest versions of the cpx libraries and follow the instructions here (link:https://learn.adafruit.com/adafruit-clue/circuitpython).

1. Plug in your device (make sure it’s formatted properly already).
2. Run the command `""Device Simulator Express: Deploy to Device""`.

<img alt='Deploy to Device' src='https://raw.githubusercontent.com/microsoft/vscode-python-devicesimulator/dev/assets/readmeFiles/deploy.png'>

### VI. Use the Serial Monitor for your device (available on Windows and Mac only)

1. Plug in your device (make sure it’s formatted properly already).
2. Run the command `""Device Simulator Express: Open Serial Monitor""`.
3. Select your baud rate for the serial port.
4. The `print()` statements in your code will show in the output console.

### VII. Debug your project on the simulator

1. Add breakpoints in your code
2. Press F5 to enter the debugging mode, and you can start debugging line by line!

### Commands

Device Simulator Express provides several commands in the Command Palette (<kbd>F1</kbd> or <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> / <kbd>Cmd</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> for Mac OS) for working with \*.py files:

-   `Device Simulator Express: Getting Started`: Opens a page in VS Code that helps users get started with the extension. Here, users can browse through code that they can use to play with the simulators.
-   `Device Simulator Express: Run Simulator`: Runs Python code on the simulator.
-   `Device Simulator Express: New File`: Opens an unsaved .py file with template code, also opens the simulator for the selected device.
-   `Device Simulator Express: Open Simulator`: Opens the simulator in the simulator window for the selected device
-   `Device Simulator Express: Deploy to Device`: Copies the current file to the selected device.
-   `Device Simulator Express: Open Serial Monitor`: Opens the serial monitor in the integrated output window.
-   `Device Simulator Express: Close Serial Monitor`: Stops the serial monitor and releases the serial port.
-   `Device Simulator Express: Change Baud Rate`: Changes the baud rate of the selected serial port. For Adafruit CPX, the default baud rate is 115200.
-   `Device Simulator Express: Select Serial Port`: Changes the current serial port.

## Contribute

[See here for steps to run the extension locally.](https://github.com/microsoft/vscode-python-devicesimulator/blob/dev/docs/developers-setup.md)

## Provide feedback

To add a review for our extension, please do so on the [Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-python.devicesimulatorexpress)

To report issues, provide feedback or requests, please use this link: [Provide Feedback](https://github.com/microsoft/vscode-python-devicesimulator/issues).  
We would love to hear from you about your experience to keep improving our project.

## Privacy and Telemetry Notice

### Data Collection

The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.

### Disable Telemetry

The Microsoft Device Simulator Express Extension for Visual Studio Code collects usage
data and sends it to Microsoft to help improve our products and
services. Read our
[privacy statement](https://privacy.microsoft.com/privacystatement) to
learn more. This extension respects the `telemetry.enableTelemetry`
setting which you can learn more about at
https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting.

To disable telemetry, follow these steps:

1. Open **File** (Open **Code** on macOS)
2. Select **Preferences**
3. Select **Settings**
4. Search for `telemetry`
5. Uncheck the **Telemetry: Enable Telemetry** setting

## Third Party Notice

A `ThirdPartyNotices.txt` file is provided in the extension's source code listing the appropriate third-party notices.

## Troubleshooting Tips

-   The first time you install the extension, you'll need to execute the `run` command at least once in order to access auto-completion.
-   While running a code file, if you get an error saying it can't find the file, make sure you've clicked on a valid Python code file before running it.
-   To open the output panel again after closing it go to VS Code menu: `View -> Output`.
-   If you try to deploy to the CPX while it's plugged in but you still get an error saying it cannot find the board, make sure your device is formatted correctly and that its name matches `CIRCUITPY`.
-   If you can't get the Simulator communication working while debugging, try to open your `Settings` and check the port used under `""Device Simulator Express: Debugger Server Port""`. You can either change it (usually ports above 5000 should work) or try to free it, then start debugging again.
-   When you are using the serial monitor, if you get some unusual error messages, unplug the device and reload the VS Code windows.
-   If you're using Ubuntu and having some problems with setting up the environment, try reviewing [this article's](https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-ubuntu-16-04) ""Step 1"" section on how to set up Python 3 on Ubuntu 16.04. Then, ensure that you've run `sudo apt-get install -y python3-venv` to allow for virtual environment creation.

## License

    Device Simulator Express, a Microsoft Garage project

    Copyright (c) Microsoft Corporation. All rights reserved.

    MIT License

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the ""Software""), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE.

## Notes

1. Make sure that when you type _python_ in a terminal, the command is recognized and you have the correct version. The easiest way to do it is to select the ""Add to PATH"" option directly when you install Python. Otherwise you can search how to insert it manually.
2. You can choose to see the prompt or not by changing the extension configurations.
"
140,microsoft/vscode-arduino,TypeScript,"# Visual Studio Code extension for Arduino

[![Gitter](https://img.shields.io/badge/chat-on%20gitter-blue.svg)](https://gitter.im/Microsoft/vscode-arduino)
[![Travis CI](https://travis-ci.org/Microsoft/vscode-arduino.svg?branch=master)](https://travis-ci.org/Microsoft/vscode-arduino)

Welcome to the Visual Studio Code extension for **Arduino** <sup>preview</sup> ! The Arduino extension makes it easy to develop, build, deploy and debug your Arduino sketches in Visual Studio Code, with a rich set of functionalities. These include:

* IntelliSense and syntax highlighting for Arduino sketches
* Verify and upload your sketches in Visual Studio Code
* Built-in board and library manager
* Built-in example list
* Built-in serial monitor
* Snippets for sketches
* Automatic Arduino project scaffolding
* Command Palette (<kbd>F1</kbd>) integration of frequently used commands (e.g. Verify, Upload...)
* Integrated Arduino Debugging <sup>New</sup>

## Prerequisites
Either the Arduino IDE or Arduino CLI are required.

### Arduino IDE
The Arduino IDE can be installed the Arduino [download page](https://www.arduino.cc/en/main/software#download).
- The supported Arduino IDE versions are `1.6.x` and later.
- The Windows Store's version of the Arduino IDE is not supported because of the sandbox environment that the application runs in.
- *Note:* Arduino IDE `1.8.7` had some breaking changes, causing board package and library installation failures.  These failures were corrected in `1.8.8` and later.

### Arduino CLI
The Arduino CLI can be downloaded from the repository's [release page](https://github.com/arduino/arduino-cli/releases/tag/0.13.0)
- The extension has only been tested with v0.13.0.
- If you use the CLI you will have to set `arduino.path` since the CLI does not have a default path. 

## Installation
Open VS Code and press <kbd>F1</kbd> or <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd> to open command palette, select **Install Extension** and type `vscode-arduino`.

Or launch VS Code Quick Open (<kbd>Ctrl</kbd> + <kbd>P</kbd>), paste the following command, and press enter.
```bash
ext install vscode-arduino
```

You can also install directly from the Marketplace within Visual Studio Code, searching for `Arduino`.

## Get Started
You can find code samples and tutorials each time that you connect a supported device. Alternatively you can visit our [IoT Developer Blog Space](https://devblogs.microsoft.com/iotdev/) or [Get Started Tutorials](https://docs.microsoft.com/azure/iot-hub/iot-hub-arduino-iot-devkit-az3166-get-started).

## Commands
This extension provides several commands in the Command Palette (<kbd>F1</kbd> or <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>P</kbd>) for working with `*.ino` files:

- **Arduino: Board Manager**: Manage packages for boards. You can add 3rd party Arduino board by configuring `Additional Board Manager URLs` in the board manager.
- **Arduino: Change Baud Rate**: Change the baud rate of the selected serial port.
- **Arduino: Change Board Type**: Change board type or platform.
- **Arduino: Close Serial Monitor**: Stop the serial monitor and release the serial port.
- **Arduino: Examples**: Show list of examples.
- **Arduino: Initialize**: Scaffold a VS Code project with an Arduino sketch.
- **Arduino: Library Manager**: Explore and manage libraries.
- **Arduino: Open Serial Monitor**: Open the serial monitor in the integrated output window.
- **Arduino: Select Serial Port**: Change the current serial port.
- **Arduino: Send Text to Serial Port**: Send a line of text via the current serial port.
- **Arduino: Upload**: Build sketch and upload to Arduino board.
- **Arduino: CLI Upload**: Upload complied code without building sketch (CLI only).
- **Arduino: Upload Using Programmer**: Upload using an external programmer.
- **Arduino: CLI Upload Using Programmer**: Upload using an external programmer without building sketch (CLI only).
- **Arduino: Verify**: Build sketch.
- **Arduino: Rebuild IntelliSense Configuration**: Forced/manual rebuild of the IntelliSense configuration. The extension analyzes Arduino's build output and sets the IntelliSense include paths, defines, compiler arguments accordingly.

## Keybindings
- **Arduino: Upload** <kbd>Alt</kbd> + <kbd>Cmd</kbd> + <kbd>U</kbd> *or* <kbd>Alt</kbd> + <kbd>Ctrl</kbd> + <kbd>U</kbd>
- **Arduino: Verify** <kbd>Alt</kbd> + <kbd>Cmd</kbd> + <kbd>R</kbd> *or* <kbd>Alt</kbd> + <kbd>Ctrl</kbd> + <kbd>R</kbd>
- **Arduino: Rebuild IntelliSense Configuration** <kbd>Alt</kbd> + <kbd>Cmd</kbd> + <kbd>I</kbd> *or* <kbd>Alt</kbd> + <kbd>Ctrl</kbd> + <kbd>I</kbd>

## Options
| Option | Description |
| --- | --- |
| `arduino.path`  | Path to Arduino, you can use a custom version of Arduino by modifying this setting to include the full path. Example: `C:\\Program Files\\Arduino` for Windows, `/Applications` for Mac, `/home/<username>/Downloads/arduino-1.8.1` for Linux. (Requires a restart after change). The default value is automatically detected from your Arduino IDE installation path. |
| `arduino.commandPath` | Path to an executable (or script) relative to `arduino.path`. The default value is `arduino_debug.exe` for Windows, `Contents/MacOS/Arduino` for Mac and `arduino` for Linux, You also can use a custom launch script to run Arduino by modifying this setting. (Requires a restart after change) Example: `run-arduino.bat` for Windows, `Contents/MacOS/run-arduino.sh` for Mac and `bin/run-arduino.sh` for Linux. |
| `arduino.additionalUrls` | Additional Boards Manager URLs for 3rd party packages. You can have multiple URLs in one string with a comma(`,`) as separator, or have a string array. The default value is empty. |
| `arduino.logLevel` | CLI output log level. Could be info or verbose. The default value is `""info""`. |
| `arduino.allowPDEFiletype` | Allow the VSCode Arduino extension to open .pde files from pre-1.0.0 versions of Arduino. Note that this will break Processing code. Default value is `false`. |
| `arduino.enableUSBDetection` | Enable/disable USB detection from the VSCode Arduino extension. The default value is `true`. When your device is plugged in to your computer, it will pop up a message ""`Detected board ****, Would you like to switch to this board type`"". After clicking the `Yes` button, it will automatically detect which serial port (COM) is connected a USB device. If your device does not support this feature, please provide us with the PID/VID of your device; the code format is defined in `misc/usbmapping.json`.To learn more about how to list the vid/pid, use the following tools: https://github.com/EmergingTechnologyAdvisors/node-serialport `npm install -g serialport` `serialport-list -f jsonline`|
| `arduino.disableTestingOpen` | Enable/disable automatic sending of a test message to the serial port for checking the open status. The default value is `false` (a test message will be sent). |
| `arduino.skipHeaderProvider` | Enable/disable the extension providing completion items for headers. This functionality is included in newer versions of the C++ extension. The default value is `false`.|
| `arduino.defaultBaudRate` | Default baud rate for the serial port monitor. The default value is 115200. Supported values are 300, 1200, 2400, 4800, 9600, 19200, 38400, 57600, 74880, 115200, 230400 and 250000 |
| `arduino.disableIntelliSenseAutoGen` | When `true` vscode-arduino will not auto-generate an IntelliSense configuration (i.e. `.vscode/c_cpp_properties.json`) by analyzing Arduino's compiler output. |

The following Visual Studio Code settings are available for the Arduino extension. These can be set in global user preferences <kbd>Ctrl</kbd> + <kbd>,</kbd> or workspace settings (`.vscode/settings.json`). The latter overrides the former.

```json
{
    ""arduino.path"": ""C:/Program Files (x86)/Arduino"",
    ""arduino.commandPath"": ""arduino_debug.exe"",
    ""arduino.logLevel"": ""info"",
    ""arduino.allowPDEFiletype"": false,
    ""arduino.enableUSBDetection"": true,
    ""arduino.disableTestingOpen"": false,
    ""arduino.skipHeaderProvider"": false,
    ""arduino.additionalUrls"": [
        ""https://raw.githubusercontent.com/VSChina/azureiotdevkit_tools/master/package_azureboard_index.json"",
        ""http://arduino.esp8266.com/stable/package_esp8266com_index.json""
    ],
    ""arduino.defaultBaudRate"": 115200
}
```
*Note:* You only need to set `arduino.path` in Visual Studio Code settings, other options are not required.

The following settings are as per sketch settings of the Arduino extension. You can find them in
`.vscode/arduino.json` under the workspace.

```json
{
    ""sketch"": ""example.ino"",
    ""port"": ""COM5"",
    ""board"": ""adafruit:samd:adafruit_feather_m0"",
    ""output"": ""../build"",
    ""debugger"": ""jlink"",
    ""prebuild"": ""./prebuild.sh"",
    ""postbuild"": ""./postbuild.sh"",
    ""intelliSenseGen"": ""global""
}
```
- `sketch` - The main sketch file name of Arduino.
- `port` - Name of the serial port connected to the device. Can be set by the `Arduino: Select Serial Port` command. For Mac users could be ""/dev/cu.wchusbserial1420"".
- `board` - Currently selected Arduino board alias. Can be set by the `Arduino: Change Board Type` command. Also, you can find the board list there.
- `output` - Arduino build output path. If not set, Arduino will create a new temporary output folder each time, which means it cannot reuse the intermediate result of the previous build leading to long verify/upload time, so it is recommended to set the field. Arduino requires that the output path should not be the workspace itself or in a subfolder of the workspace, otherwise, it may not work correctly. By default, this option is not set. It's worth noting that the contents of this file could be deleted during the build process, so pick (or create) a directory that will not store files you want to keep.
- `debugger` - The short name of the debugger that will be used when the board itself does not have a debugger and there is more than one debugger available. You can find the list of debuggers [here](https://github.com/Microsoft/vscode-arduino/blob/master/misc/debuggerUsbMapping.json). By default, this option is not set.
- `prebuild` - External command which will be invoked before any sketch build (verify, upload, ...). For details see the [Pre- and Post-Build Commands](#Pre--and-Post-Build-Commands) section.
- `postbuild` - External command to be run after the sketch has been built successfully. See the afore mentioned section for more details.
- `intelliSenseGen` - Override the global setting for auto-generation of the IntelliSense configuration (i.e. `.vscode/c_cpp_properties.json`). Three options are available:
  - `""global""`: Use the global settings (default)
  - `""disable""`: Disable the auto-generation even if globally enabled
  - `""enable""`: Enable the auto-generation even if globally disabled
- `buildPreferences` - Set Arduino preferences which then are used during any build (verify, upload, ...). This allows for extra defines, compiler options or includes. The preference key-value pairs must be set as follows:
```json
    ""buildPreferences"": [
        [""build.extra_flags"", ""-DMY_DEFINE=666 -DANOTHER_DEFINE=3.14 -Wall""],
        [""compiler.cpp.extra_flags"", ""-DYET_ANOTER=\""hello\""""]
    ]
}
```

## Pre- and Post-Build Commands
On Windows the commands run within a `cmd`-, on Linux and OSX within a `bash`-instance. Therefore your command can be anything what you can run within those shells. Instead of running a command you can invoke a script. This makes writing more complex pre-/post-build mechanisms much easier and opens up the possibility to run python or other scripting languages.
The commands run within the workspace root directory and vscode-arduino sets the following environment variables:  
**`VSCA_BUILD_MODE`** The current build mode, one of `Verifying`, `Uploading`, `Uploading (programmer)` or `Analyzing`. This allows you to run your script on certain build modes only.  
**`VSCA_SKETCH`** The sketch file relative to your workspace root directory.  
**`VSCA_BOARD`** Your board and configuration, e.g. `arduino:avr:nano:cpu=atmega328`.  
**`VSCA_WORKSPACE_DIR`** The absolute path of your workspace root directory.  
**`VSCA_LOG_LEVEL`** The current log level. This allows you to control the verbosity of your scripts.  
**`VSCA_SERIAL`** The serial port used for uploading. Not set if you haven't set one in your `arduino.json`.  
**`VSCA_BUILD_DIR`** The build directory. Not set if you haven't set one in your `arduino.json`.  

For example under Windows the following `arduino.json` setup
```json
{
    ""board"": ""arduino:avr:nano"",
    ""sketch"": ""test.ino"",
    ""configuration"": ""cpu=atmega328"",
    ""prebuild"": ""IF \""%VSCA_BUILD_MODE%\""==\""Verifying\"" (echo VSCA_BUILD_MODE=%VSCA_BUILD_MODE% && echo VSCA_BOARD=%VSCA_BOARD%)""
}
```
will produce
```
[Starting] Verifying sketch 'test.ino'
Running pre-build command: ""IF ""%VSCA_BUILD_MODE%""==""Verifying"" (echo VSCA_BUILD_MODE=%VSCA_BUILD_MODE% && echo VSCA_BOARD=%VSCA_BOARD%)""
VSCA_BUILD_MODE=Verifying 
VSCA_BOARD=arduino:avr:nano:cpu=atmega328
Loading configuration...
<...>
```
when verifying.

## IntelliSense
vscode-arduino auto-configures IntelliSense by default. vscode-arduino analyzes Arduino's compiler output by running a separate build and generates the corresponding configuration file at `.vscode/c_cpp_properties.json`. vscode-arduino tries as hard as possible to keep things up to date, e.g. it runs the analysis when switching the board or the sketch.

It doesn't makes sense though to run the analysis repeatedly. Therefore if the workspace reports problems (""squiggles"") - for instance after adding new includes from a new library - run the analysis manually:

Manual rebuild: **Arduino: Rebuild IntelliSense Configuration**,
Keybindings: <kbd>Alt</kbd> + <kbd>Cmd</kbd> + <kbd>I</kbd> *or* <kbd>Alt</kbd> + <kbd>Ctrl</kbd> + <kbd>I</kbd>

When the analysis is invoked manually it ignores any global and project specific disable.

### IntelliSense Configurations
vscode-arduino's analysis stores the result as a dedicated IntelliSense-configuration named `Arduino`. You have to select it from the far right of the status bar when you're in one of your source files as shown here:

![74001156-cfce8280-496a-11ea-9b9d-7d30c83765c1](https://user-images.githubusercontent.com/21954933/74351237-2696ea80-4db7-11ea-9f7a-1bfc652ad5f5.png)

This system allows you to setup and use own IntelliSense configurations in parallel to the automatically generated configurations provided through vscode-arduino. Just add your configuration to `c_cpp_properties.json` and name it differently from the default configuration (`Arduino`), e.g. `My awesome configuration` and select it from the status bar or via the command palette command **C/C++: Select a Configuration...**

## Debugging Arduino Code <sup>preview</sup>
Before you start to debug your Arduino code, please read [this document](https://code.visualstudio.com/docs/editor/debugging) to learn about the basic mechanisms of debugging in Visual Studio Code. Also see [debugging for C++ in VSCode](https://code.visualstudio.com/docs/languages/cpp#_debugging) for further reference.

Make sure that your Arduino board can work with [STLink](http://www.st.com/en/development-tools/st-link-v2.html), [Jlink](https://www.segger.com/jlink-debug-probes.html) or [EDBG](http://www.atmel.com/webdoc/protocoldocs/ch01s01.html). The debugging support is currently fully tested with the following boards:
- [MXChip IoT Developer Kit - AZ3166](https://microsoft.github.io/azure-iot-developer-kit/)
- [Arduino M0 PRO](https://www.arduino.cc/en/Main/ArduinoBoardM0PRO)
- [Adafruit WICED WiFi Feather](https://www.adafruit.com/product/3056)
- [Adafruit Feather M0](https://www.adafruit.com/product/3010)
- Arduino Zero Pro

Steps to start debugging:
1. Plug in your board to your development machine properly. For those boards that do not have an on-board debugging chip, you need to use a STLink or JLink connector.
2. Go to the **Debug View** (<kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>D</kbd>). and set breakpoints in your source files.
3. Press <kbd>F5</kbd> to select your debugging environment.
4. When your breakpoint is hit, you can see variables and add expression(s) to watch on the Debug Side Bar.

> To learn more about how to debug Arduino code, visit our [team blog](https://blogs.msdn.microsoft.com/iotdev/2017/05/27/debug-your-arduino-code-with-visual-studio-code/).

## Change Log
See the [Change log](https://github.com/Microsoft/vscode-arduino/blob/master/CHANGELOG.md) for details about the changes in each version.

## Supported Operating Systems
Currently this extension supports the following operating systems:

- Windows 7 and later (32-bit and 64-bit)
- macOS 10.10 and later
- Ubuntu 16.04
  - The extension might work on other Linux distributions, as reported by other users, but without guarantee.

## Support
You can find the full list of issues on the [Issue Tracker](https://github.com/Microsoft/vscode-arduino/issues). You can submit a [bug or feature suggestion](https://github.com/Microsoft/vscode-arduino/issues/new), and participate in community driven [discussions](https://gitter.im/Microsoft/vscode-arduino).

## Development

Installation prerequisites:

- [Git](https://git-scm.com/)
- [Node.js](https://nodejs.org/) (>= 6.5.0)
- [Npm](https://www.npmjs.com/) (>= 3.10.3)

To *run and develop*, do the following:
- `git clone https://github.com/microsoft/vscode-arduino`
- `cd vscode-arduino`
- Run `npm i`
- Run `npm i -g gulp`
- Open in Visual Studio Code (`code .`)
- Press <kbd>F5</kbd> to debug.

To *test*, press <kbd>F5</kbd> in VS Code with the ""Launch Tests"" debug configuration.

## Code of Conduct
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct). For more information please see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/#howadopt) or contact opencode@microsoft.com with any additional questions or comments.

## Privacy Statement
The [Microsoft Enterprise and Developer Privacy Statement](https://www.microsoft.com/en-us/privacystatement/EnterpriseDev/default.aspx) describes the privacy statement of this software.

## License
This extension is licensed under the [MIT License](https://github.com/Microsoft/vscode-arduino/blob/master/LICENSE.txt). Please see the [Third Party Notice](https://github.com/Microsoft/vscode-arduino/blob/master/ThirdPartyNotices.txt) file for additional copyright notices and terms.

## Contact Us
If you would like to help build the best Arduino experience with VS Code, you can reach us directly at [gitter chat room](https://gitter.im/Microsoft/vscode-arduino).
"
141,microsoft/FastTrack,PowerShell,"# Microsoft FastTrack Open Source
Welcome to the home for Microsoft FastTrack Open Source Software (FTOSS). Through this initiative we are collecting tools, scripts, and guidance from across the FastTrack program, our partners, and anyone who wants to contribute with the aim to make them easier to find, grow, and improve. Please let us know any questions or feedback you have using the [issues list](https://github.com/Microsoft/FastTrack/issues).

## Scripts

We have a collection of [scripts](scripts) to help with migration related tasks. Each script folder has a readme describing what the script does and how to use it. If you have scripts you would like to add please submit a Pull Request.

## Tools

|Project|Description
|----|--------------------------
|[SMAT Workbook Generator](https://github.com/Microsoft/fasttrack-smat-workbook-generator)|Combines the output of SMAT into a single workbook|
|[IdFix](https://github.com/Microsoft/idfix)|Fix on-premesis AD issues before migration|
|[gscan](https://github.com/microsoft/FastTrack/tree/master/tools/gsuite-scanner)|Provides an inventory of v1 Google Sites|
|[SimpleGraph](https://github.com/microsoft/FastTrack/tree/master/tools/SimpleGraph)|Use PowerShell to make simple requests against Graph API, with full support for complex requests|

## Samples

|Sample|Description
|----|--------------------------
|[teams-spfx-create-team](./samples/teams-spfx-create-team)|Demonstrates how to create a Microsoft Team from a SharePoint Framework Field customizer. Makes use of PnPjs libraries, Office UI Fabric & React Fabric Controls, and React.|
|[Teams upgrade snippets](./samples/teams-upgrade-snippets)|Example PowerShell snippets for common automated tasks when performing an upgrade to Teams Only mode from Skype for Business|

## Ideas Welcome!

If you have ideas for projects that would improve our delivery, experience, or process please [submit an issue](https://github.com/Microsoft/FastTrack/issues) and let us know. We can't promise every idea will be implemented, but we value your feedback. Please be sure to include sufficient information that we can understand your idea and respond.

Do you have a script or tool you use that would be of value to the community? Please let us know so we can discuss potentially adding it to the catalog - or submit a Pull Request to get it added!

## Contributing

If you have a script or sample you would like to contribute to the Microsoft FastTrack repository please review the [contributing](CONTRIBUTING.md) guidance. If you have any questions, please let us know using the issues list. We'd love to discuss how you'd like to contribute!

## Support Statement

The scripts, samples, and tools made available through the FastTrack Open Source initiative are provided as-is. These resources are developed in partnership with the community and do not represent official Microsoft software. As such, support is not available through premier or other Microsoft support channels. If you find an issue or have questions please reach out through the issues list and we'll do our best to assist, however there is no associated SLA.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content in this repository under the [MIT License](https://opensource.org/licenses/MIT), see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the [LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/en-us/

Microsoft and any contributors reserve all others rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
142,microsoft/vscode-apimanagement,TypeScript,"[![Version](https://vsmarketplacebadge.apphb.com/version/ms-azuretools.vscode-apimanagement.svg)](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-apimanagement) [![Installs](https://vsmarketplacebadge.apphb.com/installs-short/ms-azuretools.vscode-apimanagement.svg)](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-apimanagement)
[![Build Status](https://dev.azure.com/ms-azuretools/AzCode/_apis/build/status/vscode-apimanagement?branchName=master)](https://dev.azure.com/ms-azuretools/AzCode/_build/latest?definitionId=20&branchName=master) [![GitHub](https://img.shields.io/github/license/mashape/apistatus.svg)](https://github.com/Microsoft/vscode-apimanagement/blob/master/LICENSE.md)

# Azure API Management Extension for Visual Studio Code

Use the Azure API Management extension to perform common management operations on your Azure API Management service instances without switching away from Visual Studio Code.

[Azure API Management](https://aka.ms/apimrocks) is a fully managed service that helps customers to securely expose their APIs to external and internal consumers. API Management serves as a facade and a front door for the API implementations and enables their frictionless consumption by developers. Visit [this page](https://aka.ms/apimlove) for more information and resources related to Azure API Management.

## Requirements

All you need is an Azure Subscription to get started. If you don't have one, [click here](https://azure.microsoft.com/en-us/free/) for a free subscription with $200 in Azure credits!

## Main Features

-   Create and delete an API Management instance
-   Create an API by importing an OpenAPI specification
-   Edit APIs and operations in Azure Resource Manager or OpenAPI formats
-   Edit policies at any scope
-   Associate an API with a product
-   Create, delete, and edit Named Values
-   Test an API using [REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client)
-   Command Palette support for most features
-   Extract API and service configurations into ARM templates
-   Import Azure Functions - Transform HttpTriggers into APIs
-   Import App Service WebApp
-   Manage Self-Hosted Gateways
-   Policy Debugging
-   Scaffold Azure Functions
-   Show diffs against last saved version
-   Switch / Release API Revisions
-   API Filter
-   Policy Debugging on Self-Hosted Gateways

## Create an API Management instance using defaults

![Create-simple](resources/create-default.gif)

## Create an API Management instance using custom options

![Create-advanced](resources/create-advanced.gif)

## Create an API by importing an OpenAPI specification

Please note: only JSON format is supported currently.
![Import-OAS](resources/import-oas.gif)

## Edit an API in Azure Resource Manager format

![Edit-JSON](resources/edit-json.gif)

## Edit an API in OpenAPI format

![Edit-OAS](resources/edit-oas.gif)

## Edit policies

![Edit-Policy](resources/policy.gif)

## Test an API

![Test-API](resources/test-api.gif)

## Create and edit a Named Value

![Named-Values](resources/namedvalues.gif)

## Extract Service or API

![Extract](resources/extract.gif)

## Import Function App
![importFuncApp](resources/importFunc.gif)

## Import App Service Web App
![importWebApp](resources/importWebapp.gif)

## Deploy Gateway with Docker or Kubernetes
![deployGateway](resources/deployGw.gif)

## Debug Policy
![debugPolicy](resources/debug.gif)

## Scaffold Azure Functions
![generateFunctions](resources/stencil.gif)

## Show diffs against last saved version
![diffing](resources/diffing.gif)

## Switch API Revision
![SwitchRevision](resources/SwitchRevision.gif)

## Release API Revision
![ReleaseRevision](resources/ReleaseRevision.gif)

## API Filter
![apiFilter](resources/apiFilter.gif)

## Self-Hosted Gateway Debugging
![selfHostedDebugging](resources/selfdebug.gif)

## Intellisense for Policy Expressions.

Follow instructions [here](https://github.com/microsoft/vscode-apimanagement/issues/37#issuecomment-516551741).

## Managing Azure Subscriptions

If you are not signed in to Azure, you will see a ""Sign in to Azure..."" link. Alternatively, you can select ""View->Command Palette"" in the VS Code menu, and search for ""Azure: Sign In"".

If you don't have an Azure Account, you can sign up for one today for free and receive $200 in credits by selecting ""Create a Free Azure Account..."" or selecting ""View->Command Palette"" and searching for ""Azure: Create an Account"".

You may sign out of Azure by selecting ""View->Command Palette"" and searching for ""Azure: Sign Out"".

To select which subscriptions show up in the extension's explorer, click on the ""Select Subscriptions..."" button on any subscription node (indicated by a ""filter"" icon when you hover over it), or select ""View->Command Palette"" and search for ""Azure: Select Subscriptions"". Note that this selection affects all VS Code extensions that support the [Azure Account and Sign-In](https://github.com/Microsoft/vscode-azure-account) extension.

## Contributing

There are several ways you can contribute to this repo:

-   **Ideas, feature requests and bugs**: We are open to all ideas and we want to get rid of bugs! Use the [Issues](https://github.com/Microsoft/vscode-apimanagement/issues) section to either report a new issue, share your ideas, or contribute to ongoing discussions.
-   **Documentation**: Found a typo or an awkwardly worded sentence? Submit a PR!
-   **Code**: Contribute bug fixes, features, or design changes:
    -   Clone the repository locally and open in Visual Studio Code.
    -   Install [TSLint for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-vscode.vscode-typescript-tslint-plugin).
    -   Open the terminal (press `` CTRL+` ``) and run `npm install`.
    -   To build, press `F1` and type in `Tasks: Run Build Task`.
    -   Debug: press `F5` to start debugging the extension.

### Legal

Before we can accept your pull request you will need to sign a **Contribution License Agreement**. All you need to do is to submit a pull request, then the PR will get appropriately labelled (e.g. `cla-required`, `cla-norequired`, `cla-signed`, `cla-already-signed`). If you already signed the agreement we will continue with reviewing the PR, otherwise system will tell you how you can sign the CLA. Once you sign the CLA all future PR's will be labeled as `cla-signed`.

### Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Telemetry

VS Code collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://go.microsoft.com/fwlink/?LinkID=528096&clcid=0x409) to learn more. If you don’t wish to send usage data to Microsoft, you can set the `telemetry.enableTelemetry` setting to `false`. Learn more in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).

## License

[MIT](LICENSE.md)
"
143,microsoft/DiskANN,C++,"# DiskANN

The goal of the project is to build scalable, performant and cost-effective approximate nearest neighbor search algorithms.
The initial release has the in-memory version of the [DiskANN paper](https://papers.nips.cc/paper/9527-rand-nsg-fast-accurate-billion-point-nearest-neighbor-search-on-a-single-node.pdf) published in NeurIPS 2019. 
This code reuses and builds upon some of the [code for NSG](https://github.com/ZJULearning/nsg) algoritm.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

See [guidelines](CONTRIBUTING.md) for contributing to this project.



##Linux build:

Install the following packages through apt-get, and Intel MKL either by downloading the installer or using [apt](https://software.intel.com/en-us/articles/installing-intel-free-libs-and-python-apt-repo) (we tested with build 2019.4-070).
```
sudo apt install cmake g++ libaio-dev libgoogle-perftools-dev clang-format-4.0 libboost-dev
```

Build
```
mkdir build && cd build && cmake .. && make -j 
```

##Windows build:

The Windows version has been tested with the Enterprise editions of Visual Studio 2017 and Visual Studio 2019. It should work with the Community and Professional editions as well without any changes. 

**Prerequisites:**

* Install CMAKE (v3.15.2 or later) from https://cmake.org
* Install MKL from https://software.intel.com/en-us/mkl
* Install/download Boost from https://www.boost.org

* Environment variables: 
    * Set a new System environment variable, called INTEL_ROOT to the ""windows"" folder under your MKL installation
	   (For instance, if your install folder is ""C:\Program Files (x86)\IntelSWtools"", set INTEL_ROOT to ""C:\Program Files (x86)\IntelSWtools\compilers_and_libraries\windows"")
    * Set environment variable BOOST_ROOT to your boost folder.

**Build steps:**
-	Open a new command prompt window
-	Create a ""build"" directory under diskann
-	Change to the ""build"" directory and run  
```
<full-path-to-cmake>\cmake -G ""Visual Studio 16 2019"" -B. -A x64 ..
```
OR 
```
<full-path-to-cmake>\cmake -G ""Visual Studio 15 2017"" -B. -A x64 ..
```

**Note: Since VS comes with its own (older) version of cmake, you have to specify the full path to cmake to ensure that the right version is used.**
-	This will create a “diskann” solution file in the ""build"" directory
-	Open the ""diskann"" solution and build the “diskann” project. 
- 	Then build all the other binaries using the ALL_BUILD project that is part of the solution
- 	Generated binaries are stored in the diskann/x64/Debug or diskann/x64/Release directories.

To build from command line, change to the ""build"" directory and use msbuild to first build the ""diskpriority_io"" and ""diskann_dll"" projects. And then build the entire solution, as shown below.
```
msbuild src\dll\diskann.vcxproj
msbuild diskann.sln
```
Check msbuild docs for additional options including choosing between debug and release builds.


##Usage:

We now detail the main binaries using which one can build and search indices which reside in memory as well as SSD-resident indices.

**Usage for SSD-based indices**
===============================

To generate an SSD-friendly index, use the `tests/build_disk_index` program. 
----------------------------------------------------------------------------

```
./tests/build_disk_index  [data_type<float/int8/uint8>]  [data_file.bin]  [index_prefix_path]  [R]  [L]  [B]  [M]  [T]. 
```

The arguments are as follows:

(i) data_type:  The datatype is the type of dataset you wish to build an index. We support byte indices (signed int8 or unsigned uint8) or float indices. 

(ii) data_file: The input data over which to build an index, in .bin format. The first 4 bytes represent number of points as integer. The next 4 bytes represent the dimension of data as integer. The following n*d*sizeof(T) bytes contain the contents of the data one data point in time. sizeof(T) is 1 for byte indices, and 4 for float indices. This will be read by the program as int8_t for signed indices, uint8_t for unsigned indices or float for float indices.

(iii) index_prefix_path: the index will generate a few files, all beginning with the specified prefix path. For example, if you provide ~/index_test as the prefix path, build  generates files such as ~/index_test_pq_pivots.bin, ~/index_test_pq_compressed.bin, ~/index_test_disk.index, etc. There may be between 8 and 10 files generated with this prefix depending on how we construct the index.

(iv) R: the degree of our graph index, typically between 60 and 150. Again, larger values will result in bigger indices (with longer indexing times), but better search quality. Try to ensure that the L value is at least the R value unless you need to build indices really quickly, but can somewhat compromise on quality. 

(v) L: the size of search list we maintain during index building. Typical values are between 75 to 200. Larger values will take more time to build but result in indices that provide higher recall for the same search parameters.

(vi) B: bound on the memory footprint of the index at search time. Once built, the index will use up only the specified RAM limit, the rest will reside on disk. This will dictate how aggressively we compress the data vectors to store in memory. Larger will yield better performance at search time.

(vii) M: Limit on the memory allowed for building the index. If you specify a value less than what is required to build the index in one pass, the index is  built using a divide and conquer approach so that  sub-graphs will fit in the RAM budget. The sub-graphs are  stitched together to build the overall index. This approach can be upto 1.5 times slower than building the index in one shot. Try to allocate as much memory as possible for index build as your RAM allows.

(viii) T: number of threads used by the index build process. Since the code is highly parallel, the  indexing time improves almost linearly with the number of threads (subject to the cores available on the machine).

To search the SSD-index, use the `tests/search_disk_index` program. 
----------------------------------------------------------------------------

```
./tests/search_disk_index  [index_type<float/int8/uint8>]  [index_prefix_path]  [num_nodes_to_cache]  [num_threads]  [beamwidth (use 0 to optimize internally)]  [query_file.bin]  [truthset.bin (use ""null"" for none)]  [K]  [result_output_prefix]  [L1]  [L2] etc.
```

The arguments are as follows:

(i) data type: same as (i) above in building index.

(ii) index_prefix_path: same as (iii) above in building index.

(iii) num_nodes_to_cache: our program stores the entire graph on disk. For faster search performance, we provide the support to cache a few nodes (which are closest to the starting point) in memory. 

(iv) num_threads: search using specified number of threads in parallel, one thread per query. More will result in more IOs, so find the balance depending on the bandwidth of the SSD.

(v) beamwidth: maximum number of IO requests each query will issue per iteration of search code. Larger beamwidth williult in fewer IO round-trips per query, but might result in slightly higher number of IO requests to SSD per query. Specifying 0 will optimize the beamwidth depending on the number of threads performing search.

(vi) query_file.bin: search on these queries, same format as data file (ii) above. The query file must be the same type as specified in (i).

(vii) truthset.bin file. Must be in the following format, or specify ""null"": n, the number of queries (4 bytes) followed by d, the number of ground truth elements per query (4 bytes), followed by n*d entries per query representing the d closest IDs per query in integer format,  followed by n*d entries representing the corresponding distances (float). Total file size is 8 + 4*n*d + 4*n*d. The groundtruth file, if not available, can be calculated using our program, tests/utils/compute_groundtruth. If you just want to measure the latency numbers of search and output the nearest neighbors without calculating recall, enter ""null"".

(viii) K: measure recall@k, meaning the accuracy of retrieving top-k nearest neighbors.

(ix) result output prefix: search results will be stored in files with specified prefix, in bin format.

(x, xi, ...) various search_list sizes to perform search with. Larger will result in slower latencies, but higher accuracies. Must be atleast the recall@ value in (vi).


**Usage for in-memory indices**
================================

To generate index, use the `tests/build_memory_index` program. 
--------------------------------------------------------------

```
./tests/build_memory_index  [data_type<int8/uint8/float>]  [data_file.bin]  [output_index_file]  [R]  [L]  [alpha]  [num_threads_to_use]
```

The arguments are as follows:

(i) data_type: same as (i) above in building disk index.

(ii) data_file: same as (ii) above in building disk index, the input data file in .bin format of type int8/uint8/float.

(iii) output_index_file: memory index will be saved here.

(iv) R: max degree of index: larger is typically better, range (50-150). Preferrably ensure that L is at least R.

(v) L: candidate_list_size for building index, larger is better (typical range: 75 to 200)

(vi) alpha: float value which determines how dense our overall graph will be, and diameter will be log of n base alpha (roughly). Typical values are between 1 to 1.5. 1 will yield sparsest graph, 1.5 will yield denser graphs.

(vii) number of threads to use: indexing uses specified number of threads.


To search the generated index, use the `tests/search_memory_index` program:
---------------------------------------------------------------------------

```
./tests/search_memory_index  [index_type<float/int8/uint8>]  [data_file.bin]  [memory_index_path]  [query_file.bin]  [truthset.bin (use ""null"" for none)] [K]  [result_output_prefix]  [L1]  [L2] etc. 
```

The arguments are as follows:

(i) data type: same as (i) above in building index.

(ii) memory_index_path: enter path of index built (argument (iii) above in building memory index).

(iii) query_bin: search on these queries, same format as data file (ii) above. The query file must be the same type as specified in (i).

(iv) Truthset file. Must be in the following format: n, the number of queries (4 bytes) followed by d, the number of ground truth elements per query (4 bytes), followed by n*d entries per query representing the d closest IDs per query in integer format,  followed by n*d entries representing the corresponding distances (float). Total file size is 8 + 4*n*d + 4*n*d. The groundtruth file, if not available, can be calculated using our program, tests/utils/compute_groundtruth.

(v) K: search for recall@k, meaning accuracy of retrieving top-k nearest neighbors.

(vi) result output prefix: will search and store the computed results in the files with specified prefix in bin format.

(vii, viii, ...) various search_list sizes to perform search with. Larger will result in slower latencies, but higher accuracies. Must be atleast the recall@ value in (vi).
"
144,microsoft/react-native-macos,C++,"<h1 align=""center""> React Native for macOS </h1>

<p align=""center"">
  Build native macOS apps with React.
</p>

<p align=""center"">
  <a href=""https://github.com/microsoft/react-native-macos/blob/master/LICENSE"">
    <img src=""https://img.shields.io/badge/license-MIT-blue.svg"" alt=""React Native for macOS is released under the MIT license."" />
  </a>
  <a href=""https://www.npmjs.org/package/react-native-macos"">
    <img src=""https://img.shields.io/npm/v/react-native-macos?color=e80441&label=react-native-macos"" alt=""Current npm package version."" />
  </a>
  <a href=""https://github.com/microsoft/react-native-macos/blob/master/CONTRIBUTING.md"">
    <img src=""https://img.shields.io/badge/PRs-welcome-brightgreen.svg"" alt=""PRs welcome!"" />
  </a>
</p>

> See the official [React Native website](https://reactnative.dev/) for an introduction to React Native. 

[React Native](https://reactnative.dev) is a framework developed by Facebook that enables you to build world-class application experiences on native platforms using a consistent developer experience based on JavaScript and [React](https://reactjs.org/). The focus of React Native is on developer efficiency across all the platforms you care about - learn once, write anywhere.

This repository is a working fork of **facebook/react-native** that adds support for the official React Native for macOS implementation from Microsoft. 

You can read more about the macOS implementation in our website - [React Native for Windows + macOS](https://microsoft.github.io/react-native-windows/)

## Contents

- [Requirements](#requirements)
- [Getting Started](#getting-started)
- [Contributing](#contributing)
- [Documentation](#documentation)
- [License](#license)
- [Code of Conduct](#code-of-conduct)

## Requirements

You can run React Native for macOS apps on Mac devices with versions Mojave (10.14) or newer.

For a full and detailed list of the system requirements and how to set up your development platform, see our [System Requirements](https://microsoft.github.io/react-native-windows/docs/rnm-dependencies) documentation on our website.

## Getting Started
See the [Getting Started Guide](https://microsoft.github.io/react-native-windows/docs/rnm-getting-started) on our React Native for Windows + macOS website to build your first React Native for macOS app.

### Logging Issues
Search the [existing issues](https://github.com/microsoft/react-native-macos/issues) and try to make sure your problem doesn’t already exist before opening a new issue. If your issue doesn't exist yet, try to make sure you provide as much information as possible to us so we can help you sooner. It’s helpful if you include information like:

- The version of macOS, React Native, React Native macOS extension where you ran into the issue.
- A stack trace and reduced repro case when possible.
- Ensure the [appropriate template](https://github.com/microsoft/react-native-macos/issues/new/choose) is used when filing your issue(s).

##  Contributing
See [Contributing guidelines](https://github.com/microsoft/react-native-macos/blob/master/CONTRIBUTING.md) for how to setup your fork of the repo and start a PR to contribute to React Native for macOS.

[Good First Issue](https://github.com/microsoft/react-native-macos/labels/good%20first%20issue) and [help wanted](https://github.com/microsoft/react-native-macos/labels/help%20wanted) are great starting points for PRs.

## Documentation
[React Native already has great documentation](https://reactnative.dev/docs/getting-started.html) and we're working to ensure the React Native for Windows + macOS are part of that documentation story.

[React Native for Windows + macOS](https://microsoft.github.io/react-native-windows/) has it's own separate documentation site where Windows and macOS
specific information, like API docs and blog updates live. We are bootstrapping documentation for macOS at this time, tune in for updates.

### Examples

- Using the CLI in the [Getting Started](https://microsoft.github.io/react-native-windows/docs/rnm-getting-started) guide will set you up with a sample React Native for macOS app that you can begin editing right away.
- If you're looking for sample code, just browse the [RNTester folder](https://github.com/microsoft/react-native-macos/tree/master/RNTester) for examples

## License

The React Native for macOS extension, including modifications to the original Facebook source code, and all newly contributed code is provided under the [MIT License](LICENSE). Portions of the React Native for macOS extension derived from React Native are copyright Facebook.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

"
145,microsoft/snmalloc,C++,"# snmalloc

snmalloc is a high-performance allocator. 
snmalloc can be used directly in a project as a header-only C++ library, 
it can be `LD_PRELOAD`ed on Elf platforms (e.g. Linux, BSD),
and there is a [crate](https://crates.io/crates/snmalloc-rs) to use it from Rust.

Its key design features are:

* Memory that is freed by the same thread that allocated it does not require any
  synchronising operations.
* Freeing memory in a different thread to initially allocated it, does not take
  any locks and instead uses a novel message passing scheme to return the
  memory to the original allocator, where it is recycled.  This enables 1000s of remote 
  deallocations to be performed with only a single atomic operation enabling great
  scaling with core count. 
* The allocator uses large ranges of pages to reduce the amount of meta-data
  required.
* The fast paths are highly optimised with just two branches on the fast path 
  for malloc (On Linux compiled with Clang).
* The platform dependencies are abstracted away to enable porting to other platforms. 

snmalloc's design is particular well suited to the following two difficult 
scenarios that can be problematic for other allocators:

  * Allocations on one thread are freed by a different thread
  * Deallocations occur in large batches

Both of these can cause massive reductions in performance of other allocators, but 
do not for snmalloc.

Comprehensive details about snmalloc's design can be found in the
[accompanying paper](snmalloc.pdf), and differences between the paper and the
current implementation are [described here](difference.md).
Since writing the paper, the performance of snmalloc has improved considerably.

[![Build Status](https://dev.azure.com/snmalloc/snmalloc/_apis/build/status/Microsoft.snmalloc?branchName=master)](https://dev.azure.com/snmalloc/snmalloc/_build/latest?definitionId=1?branchName=master)

# Further documentation

 - [Instructions for building snmalloc](docs/BUILDING.md)
 - [Instructions for porting snmalloc](docs/PORTING.md)

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
146,microsoft/Docker-Provider,Ruby,"# About

This repository contains source code for Azure Monitor for containers Linux and Windows Agent

# Questions?

Feel free to contact engineering team owners in case you have any questions about this repository or project.

# Prerequisites

## Common
- [Visual Studio Code](https://code.visualstudio.com/) for authoring
- [Go lang](https://golang.org/) for building go code. Go lang version 1.14.1.

> Note: If you are using WSL2, make sure you have cloned the code onto ubuntu not onto windows

## WSL2
- [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install-win10).
- configure [Docker-for-windows-wsl2](https://docs.docker.com/docker-for-windows/wsl/)

## Linux
- Ubuntu 14.04 or higher to build Linux Agent.
- [Docker](https://docs.docker.com/engine/install/ubuntu/) to build the docker image for Linux Agent
> Note: if you are using WSL2, you can ignore Docker since Docker for windows will be used.

## Windows
- Windows 10 Professional machine to build  Windows Agent
- [Docker for Windows](https://docs.docker.com/docker-for-windows/) to build docker image for Windows Agent
- [.NET Core SDK](https://dotnet.microsoft.com/download) to build the Windows Agent code
- [gcc for windows](https://github.com/jmeubank/tdm-gcc/releases/download/v9.2.0-tdm64-1/tdm64-gcc-9.2.0.exe) to build go code


# Repo structure

The general directory structure is:

```
├── .pipelines/                               - files related to azure devops ci and cd pipelines
├── build/                                    - files to related to  compile and build the code
│   ├── version                               - build version used for docker prvider and go shared object(so) files
│   ├── common/                               - common to both windows and linux installers
│   │   ├── installer                         - files related to installer
|   |   |   |── scripts/                      - script files related to configmap parsing
│   ├── linux/                                - Makefile and installer files for the Docker Provider
│   │   ├── Makefile                          - Makefile to build the docker provider
│   │   ├── installer                         - files related to installer
|   |   |   |── bundle/                       - shell scripts to create shell bundle
|   |   |   |── conf/                         - plugin configuration files
|   |   |   |── datafiles/                    - data files for the installer
|   |   |   |── scripts/                      - script files related to livenessproble, tomlparser etc..
|   |   |   |── InstallBuilder/               - python script files for the install builder
│   ├── windows/                              - scripts to build the .net and go code
|   |   |── Makefile.ps1                      - powershell script to build .net and go lang code and copy the files to omsagentwindows directory
│   │   ├── installer                         - files related to installer
|   |   |   |── conf/                         - fluent, fluentbit and out_oms plugin configuration files
|   |   |   |── scripts/                      - script files related to livenessproble, filesystemwatcher, keepCertificateAlive etc..
|   |   |   |── certificategenerator/         - .NET code for the generation self-signed certificate of the windows agent
├── charts/                                   - helm charts
│   ├── azuremonitor-containers/              - azure monitor for containers helm chart used for non-AKS clusters
├── alerts/                                   - alert queries
├── kubernetes/                               - files related to Linux and Windows Agent for Kubernetes
│   ├── linux/                                - scripts to build the Docker image for Linux Agent
│   │   ├── dockerbuild                       - script to build docker provider, docker image and publish docker image
│   │   ├── DockerFile                        - DockerFile for Linux Agent Container Image
│   │   ├── main.sh                           - Linux Agent container entry point
│   │   ├── setup.sh                          - setup file for Linux Agent Container Image
│   │   ├── acrworkflows/                     - acr work flows for the Linux Agent container image
│   │   ├── defaultpromenvvariables           - default environment variables for Prometheus scraping
│   │   ├── defaultpromenvvariables-rs        - cluster level default environment variables for Prometheus scraping
│   │   ├── defaultpromenvvariables-sidecar   - cluster level default environment variables for Prometheus scraping in sidecar
│   ├── windows/                              - scripts to build the Docker image for Windows Agent
│   │   ├── dockerbuild                       - script to build the code and docker imag, and publish docker image
│   │   ├── acrworkflows/                     - acr work flows for the Windows Agent container image
│   │   ├── DockerFile                        - DockerFile for Windows Agent Container Image
│   │   ├── main.ps1                          - Windows Agent container entry point
│   │   ├── setup.ps1                         - setup file for Windows Agent Container Image
│   ├── omsagent.yaml                         - kubernetes yaml for both Linux and Windows Agent
│   ├── container-azm-ms-agentconfig.yaml     - kubernetes yaml for agent configuration
├── scripts/                                  - scripts for onboarding, troubleshooting and preview scripts related to Azure Monitor for containers
│   ├── troubleshoot/                         - scripts for troubleshooting of Azure Monitor for containers onboarding issues
│   ├── onboarding/                           - scripts related to Azure Monitor for containers onboarding.
│   ├── preview/                              - scripts related to preview features ...
│   ├── build/                                - scripts related to build such as installing pre-requisites etc.
│   ├── deployment/                           - scripts related to deployment goes here.
│   ├── release/                              - scripts related to release  goes here.
├── source/                                   - source code
│   ├── plugins/                              - plugins source code
│   │   ├── go/                               - out_oms plugin code in go lang
│   │   ├── ruby/                             - plugins code in ruby
│   │   |   ├── health/                       - code for health feature
│   │   |   ├── lib/                          - lib for app insights ruby and this code of application_insights gem
│   │   |   ...                               - plugins in, out and filters code in ruby
│   ├── toml-parser/                          - code for parsing of toml configuration files
├── test/                                     - source code for tests
│   ├── e2e/                                  - e2e tests to validate agent and e2e workflow(s)
│   ├── unit-tests/                           - unit tests code
│   ├── scenario/                             - scenario tests code
├── !_README.md                               - this file
├── .gitignore                                - git config file with include/exclude file rules
├── LICENSE                                   - License file
├── Rakefile                                  - Rake file to trigger ruby plugin tests
└── ReleaseProcess.md                         - Release process instructions
└── ReleaseNotes.md                           - Release notes for the release of the Azure Monitor for containers agent
```

# Branches

- `ci_prod` branch contains codebase currently in production (or being prepared for release).
- `ci_dev` branch contains version in development.

To contribute: create your private branch off of `ci_dev`, make changes and use pull request to merge back to `ci_dev`.
Pull request must be approved by at least one engineering team members.

# Authoring code

We recommend using [Visual Studio Code](https://code.visualstudio.com/) for authoring. Windows 10 with Ubuntu App can be used for both Windows and Linux  Agent development and recommened to clone the code onto Ubuntu app so that you dont need to worry about line ending issues LF vs CRLF.

# Building code

## Linux Agent

### Install Pre-requisites

1. Install go1.14.1, dotnet, powershell, docker and build dependencies to build go code for both Linux and Windows platforms
```
bash ~/Docker-Provider/scripts/build/linux/install-build-pre-requisites.sh
```
2. Verify python, docker and golang installed properly and also PATH and GOBIN environment variables set with go bin path.
   For some reason go env not set by install-build-pre-requisites.sh script, run the following commands to set them
   ```
   export PATH=$PATH:/usr/local/go/bin
   export GOBIN=/usr/local/go/bin
   ```
3. If you want to use Docker on the WSL2, verify following configuration settings configured on your Ubuntu app
   ```
   echo $DOCKER_HOST
   # if either DOCKER_HOST not set already or doesnt have tcp://localhost:2375 value, set DOCKER_HOST value via this command
   echo ""export DOCKER_HOST=tcp://localhost:2375"" >> ~/.bashrc && source ~/.bashrc
   # on Docker Desktop for Windows make sure docker running linux mode and enabled Expose daemon on tcp://localhost:2375 without TLS
   ```

### Build Docker Provider Shell Bundle and Docker Image and Publish Docker Image

> Note: If you are using WSL2, ensure `Docker for windows` running with Linux containers mode on your windows machine to build Linux agent image successfully

```
cd ~/Docker-Provider/kubernetes/linux/dockerbuild
sudo docker login # if you want to publish the image to acr then login to acr via `docker login <acr-name>`
# build provider, docker image and publish to docker image
bash build-and-publish-docker-image.sh --image <repo>/<imagename>:<imagetag>
```
> Note: format of the imagetag will be `ci<release><MMDDYYYY>`. possible values for release are test, dev, preview, dogfood, prod etc.

If you prefer to build docker provider shell bundle and image separately, then you can follow below instructions

##### Build Docker Provider shell bundle

```
cd ~/Docker-Provider/build/linux
make
```
##### Build and Push Docker Image

```
cd ~/Docker-Provider/kubernetes/linux/
docker build -t <repo>/<imagename>:<imagetag> --build-arg IMAGE_TAG=<imagetag> .
docker push <repo>/<imagename>:<imagetag>
```
## Windows Agent

To build the windows agent, you will have to build .NET and Go code, and docker image for windows agent.
Docker image for windows agent can only build on Windows machine with `Docker for windows` with Windows containers mode but the .NET code and Go code can be built either on Windows or Linux or WSL2.

### Install Pre-requisites

Install pre-requisites based on OS platform you will be using to build the windows agent code

#### Option 1 - Using Windows Machine to Build the Windows agent

```
powershell # launch powershell with elevated admin on your windows machine
Set-ExecutionPolicy -ExecutionPolicy bypass # set the execution policy
cd %userprofile%\Docker-Provider\scripts\build\windows # based on your repo path
.\install-build-pre-requisites.ps1 #
```

#### Option 2 - Using WSL2 to Build the Windows agent

```
powershell # launch powershell with elevated admin on your windows machine
Set-ExecutionPolicy -ExecutionPolicy bypass # set the execution policy
net use z: \\wsl$\Ubuntu-16.04 # map the network drive of the ubuntu app to windows
cd z:\home\sshadmin\Docker-Provider\scripts\build\windows # based on your repo path
.\install-build-pre-requisites.ps1 #
```


### Build Windows Agent code and Docker Image

> Note: format of the windows agent imagetag will be `win-ci<release><MMDDYYYY>`. possible values for release are test, dev, preview, dogfood, prod etc.

#### Option 1 - Using Windows Machine to Build the Windows agent

Execute below instructions on elevated command prompt to build windows agent code and docker image, publishing the image to acr or docker hub

```
cd %userprofile%\Docker-Provider\kubernetes\windows\dockerbuild # based on your repo path
docker login # if you want to publish the image to acr then login to acr via `docker login <acr-name>`
powershell -ExecutionPolicy bypass  # switch to powershell if you are not on powershell already
.\build-and-publish-docker-image.ps1 -image <repo>/<imagename>:<imagetag> # trigger build code and image and publish docker hub or acr
```

#### Option 2 - Using WSL2 to Build the Windows agent

##### On WSL2, Build Certificate Generator Source code and Out OMS Go plugin code

```
cd ~/Docker-Provider/build/windows # based on your repo path on WSL2 Ubuntu app
pwsh #switch to powershell
.\Makefile.ps1 # trigger build and publish of .net and go code
```

####  On Windows machine, build and Push Docker Image

> Note: Docker image for windows container can only built on windows hence you will have to execute below commands on windows via accessing network share or copying published bits omsagentwindows under kubernetes directory on to windows machine

```
net use z: \\wsl$\Ubuntu-16.04 # map the network drive of the ubuntu app to windows
cd z:\home\sshadmin\Docker-Provider\kubernetes\windows # based on your repo path
docker build -t <repo>/<imagename>:<imagetag> --build-arg IMAGE_TAG=<imagetag> .
docker push <repo>/<imagename>:<imagetag>
```

# Azure DevOps Build Pipeline

Navigate to https://github-private.visualstudio.com/microsoft/_build?view=pipelines to see Linux and Windows Agent build pipelines. These pipelines are configured with CI triggers for ci_dev and ci_prod.

Docker Images will be pushed to CDPX ACR repos and these needs to retagged and pushed to corresponding ACR or docker hub. Only onboarded Azure AD AppId has permission to pull the images from CDPx ACRs.

Please reach out the agent engineering team if you need access to it.

## Onboarding feature branch

Here are the instructions to onboard the feature branch to Azure Dev Ops pipeline

 1. Navigate to https://github-private.visualstudio.com/microsoft/_apps/hub/azurecdp.cdpx-onboarding.cdpx-onboarding-tab
 2. Select the repository as ""docker-provider"" from repository drop down
 3. click on validate repository
 4. select the your feature branch from Branch drop down
 5. Select the Operation system as ""Linux"" and Build type as ""buddy""
 6. create build definition
 7. enable continous integration on trigger on the build definition

 This will create build definition for the Linux agent.
 Repeat above steps except that this time select Operation system as ""Windows"" to onboard the pipeline for Windows agent.

# Azure DevOps Release Pipeline

Integrated to Azure DevOps release pipeline for the ci_dev and ci_prod.With this, for every commit to ci_dev branch, latest bits automatically deployded to DEV AKS clusters in Build subscription and similarly for for every commit to ci_prod branch, latest bits automatically deployed to PROD AKS clusters in Build subscription.

For dev, agent image will be in this format mcr.microsoft.com/azuremonitor/containerinsights/cidev:cidev<git-commit-id>.
For prod, agent will be in this format mcr.microsoft.com/azuremonitor/containerinsights/ciprod:ciprod`<MM><DD><YYYY>`.

Navigate to https://github-private.visualstudio.com/microsoft/_release?_a=releases&view=all to see the release pipelines.

# Update Kubernetes yamls

Navigate to Kubernetes directory and update the yamls with latest docker image of Linux and Windows Agent and other relevant updates.

#  Deployment and Validation

For DEV and PROD branches, automatically deployed latest yaml with latest agent image (which automatically built by the azure devops pipeline) onto CIDEV and CIPROD AKS clusters in build subscription.  So, you can use CIDEV and CIPROD AKS cluster to validate E2E. Similarly, you can set up build and release pipelines for your feature branch.

# E2E Tests

## For executing tests

1. Deploy the omsagent.yaml with your agent image. In the yaml, make sure `ISTEST` environment variable set to `true` if its not set already
2. Update the Service Principal CLIENT_ID, CLIENT_SECRET and TENANT_ID placeholder values and apply e2e-tests.yaml to execute the tests 
    > Note: Service Principal requires reader role on log analytics workspace and cluster resource to query LA and metrics
   ```
   cd ~/Docker-Provider/test/e2e # based on your repo path    
   kubectl apply -f e2e-tests.yaml # this will trigger job to run the tests in sonobuoy namespace 
   kubectl get po -n sonobuoy # to check the pods and jobs associated to tests   
   ``` 
3. Download (sonobuoy)[https://github.com/vmware-tanzu/sonobuoy/releases] on your dev box to view the results of the tests
   ```
   results=$(sonobuoy retrieve) # downloads tar file which has logs and test results
   sonobuoy results $results # get the summary of the results
   tar -xzvf <downloaded-tar-file> # extract downloaded tar file and look for pod logs, results and other k8s resources if there are any failures
   ```

## For adding new tests

1. Add the test python file with your test code under `tests` directory
2. Build the docker image, recommended to use ACR & MCR 
  ```
   cd ~/Docker-Provider/test/e2e/src # based on your repo path 
   docker login <acr> -u <user> -p <pwd> # login to acr
   docker build -f ./core/Dockerfile -t <repo>/<imagename>:<imagetag> .
   docker push <repo>/<imagename>:<imagetag>
  ```
3. update existing agentest image tag in e2e-tests.yaml with newly built image tag with MCR repo

# Scenario Tests
Clusters are used in release pipeline already has the yamls under test\scenario deployed. Make sure to validate these scenarios.
If you have new interesting scenarios, please add/update them.

# Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct] (https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ] (https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.

"
147,microsoft/apple-ux-guide,,"# Apple UX Guide

This guide contains best practices for building great experiences and writing sustainable code using Apple's UX Frameworks. Guidance here comes from a combination of our collective practical experiences and documentation from Apple and other 3rd parties. This is meant to be a living document and all contents are perpetually open for debate and improvement.

## Table of Contents

* [Layout](Layout.md)
* [Storyboards and Xibs](StoryboardsAndXibs.md)
* [View Controllers](ViewControllers.md)

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/en-us/

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
148,microsoft/STL,C++,"# Microsoft's C++ Standard Library

This is the official repository for Microsoft's implementation of the C++ Standard Library (also known as the STL),
which ships as part of the MSVC toolset and the Visual Studio IDE.

* Our [Changelog][] tracks which updates to this repository appear in each VS release.
* Our [Status Chart][] displays our overall progress over time.
* Join our [Discord server][].

[![Build Status](https://dev.azure.com/vclibs/STL/_apis/build/status/microsoft.STL?branchName=main)][Pipelines]

# What This Repo Is Useful For

If you're a programmer who just wants to use the STL, you **don't** need this repo. Simply install the Visual Studio IDE
and select the ""Desktop development with C++"" workload.

If you want to participate in the STL's development, welcome! You can report issues, comment on pull requests, and learn
about what we're working on. You can also submit pull requests to fix bugs or add features (see below).

Finally, you can take our code and use it in other apps and libraries (according to the terms of our license, like
everything else).

# GitHub Migration Status

We're in the process of moving all of our work on the STL to GitHub. Current status:

* Code: **Done.** Our source code is available under the Apache License v2.0 with LLVM Exception. (See
[LICENSE.txt][] and [NOTICE.txt][] for more information.)

* Build System: **In progress.** We're working on a CMake build system, which is currently capable of building one
flavor of the STL (native desktop). We need to extend this to build all of the flavors required for the MSVC toolset
(e.g. `/clr`, `/clr:pure`, OneCore, Spectre). Until that's done, we're keeping our legacy build system around in the
`stl/msbuild` subdirectory. (We're keeping those files in this repo, even though they're unusable outside of Microsoft,
because they need to be updated whenever source files are added/renamed/deleted. We'll delete the legacy machinery as
soon as possible.)

* Tests: **In progress.** We rely on three test suites: std, tr1, and [libcxx][]. We've partially ported std and tr1,
and fully ported libcxx to run under [lit][] using the various configurations/compilers we test internally.

* Continuous Integration: **In progress.** We've set up Azure Pipelines to validate changes to the repository.
Currently, it builds the STL (native desktop for x86, x64, ARM, and ARM64). Also, it strictly verifies that all of our
files have been formatted with [clang-format][] and follow our other whitespace conventions.

* Contribution Guidelines: **Coming soon.** Working on the STL's code involves following many rules. We have codebase
conventions, Standard requirements, Microsoft-specific requirements, binary compatibility (ABI) requirements, and more.
We're eager to begin accepting features and fixes from the community, but in addition to setting up a CI system, we need
to write down all of the rules that are currently stored in our brains. (The ABI rules may be useful to other C++
libraries.)

* Issues: **In progress.** We're going to use GitHub issues to track all of the things that we need to work on. This
includes C++20 features, [LWG issues][], conformance bugs, performance improvements, and other todos. There are
approximately 200 active bugs in the STL's Microsoft-internal database; we need to manually replicate all of them to
GitHub issues. Currently, the [cxx20 tag][] and [LWG tag][] are done; every remaining work item is tracked by a GitHub
issue. The [bug tag][] and [enhancement tag][] are being populated.

* Plans: **In progress.** We're writing up our [Roadmap][].

# Goals

We're implementing the latest C++ Working Draft, currently [N4885][], which will eventually become the next C++
International Standard. The terms Working Draft (WD) and Working Paper (WP) are interchangeable; we often
informally refer to these drafts as ""the Standard"" while being aware of the difference. (There are other relevant
Standards; for example, supporting `/std:c++14` and `/std:c++17` involves understanding how the C++14 and C++17
Standards differ from the Working Paper, and we often need to refer to the C Standard Library and ECMAScript regular
expression specifications.) We're currently prioritizing C++20 features before starting any work on C++23.

Our primary goals are conformance, performance, usability, and compatibility.

* Conformance: The Working Paper is a moving target; as features and LWG issue resolutions are added, we need to
implement them. That can involve a lot of work, because the STL is required to behave in very specific ways and to
handle users doing very unusual things.

* Performance: The STL needs to be extremely fast at runtime; speed is one of C++'s core strengths, and most C++
programs use the STL extensively. As a result, we spend more time on optimization than most general-purpose libraries.
(However, we're wary of changes that improve some scenarios at the expense of others, or changes that make code
significantly more complicated and fragile. That is, there's a ""complexity budget"" that must be spent carefully.)

* Usability: This includes parts of the programming experience like compiler throughput, diagnostic messages, and
debugging checks. For example, we've extensively marked the STL with `[[nodiscard]]` attributes because this helps
programmers avoid bugs.

* Compatibility: This includes binary compatibility and source compatibility. We're keeping VS 2019 binary-compatible
with VS 2017 and VS 2015, which restricts what we can change in VS 2019 updates. (We've found that significant changes
are possible even though other changes are impossible, which we'll be documenting in our Contribution Guidelines soon.)
While there are a few exceptions to this rule (e.g. if a feature is added to the Working Paper, we implement it, and
then the feature is significantly changed before the International Standard is finalized, we reserve the right to break
binary compatibility because `/std:c++latest` offers an experimental preview of such features), binary compatibility
generally overrides all other considerations, even conformance. Source compatibility refers to being able to
successfully recompile user code without changes. We consider source compatibility to be important, but not
all-important; breaking source compatibility can be an acceptable cost, if done for the right reasons in the right way
(e.g. in a controlled manner with escape hatches).

# Non-Goals

There are things that we aren't interested in doing with this project, for various reasons (most importantly, we need to
focus development effort on our goals). Some examples:

* Non-goal: Porting to other platforms.

* Non-goal: Adding non-Standard extensions.

* Non-goal: Implementing Technical Specifications. (We're prioritizing features in the Working Paper. Occasionally, we
might implement some or all of a TS, often when we're working on the specification itself.)

If you're proposing a feature to WG21 (the C++ Standardization Committee), you're welcome (and encouraged!) to use our
code as a base for a proof-of-concept implementation. These non-goals simply mean that we're unable to consider pull
requests for a proposed feature until it has been voted into a Working Paper. After that happens, we'll be delighted to
review a production-ready pull request.

# Reporting Issues

You can report STL bugs here, where they'll be directly reviewed by maintainers. You can also report STL bugs through
[Developer Community][], or the VS IDE (Help > Send Feedback > Report a Problem...).

**Please help us** efficiently process bug reports by following these rules:

* Only STL bugs should be reported here. If it's a bug in the compiler, CRT, or IDE, please report it through Developer
Community or Report A Problem. If it's a bug in the Windows SDK, please report it through the [Feedback Hub][hub] app.
If you aren't sure, try to reduce your test case and see if you can eliminate the STL's involvement while still
reproducing the bug.

* You should be reasonably confident that you're looking at an actual implementation bug, instead of undefined behavior
or surprising-yet-Standard behavior. Comparing against other implementations can help (but remember that implementations
can differ while conforming to the Standard); try Godbolt's [Compiler Explorer][] and [Wandbox][]. If you still aren't
sure, ask the nearest C++ expert.

* You should prepare a self-contained command-line test case, ideally as small as possible. We need a source file, a
command line, what happened (e.g. a compiler error, runtime misbehavior), and what you expected to happen. By
""self-contained"", we mean that your source file has to avoid including code that we don't have. Ideally, only CRT and
STL headers should be included. If you have to include other MSVC libraries, or the Windows SDK, to trigger an STL bug,
that's okay. But if you need parts of your own source code to trigger the STL bug, you need to extract that for us. (On
Developer Community, we'll accept zipped IDE projects if you have no other way to reproduce a bug, but this is very
time-consuming for us to reduce.)

* A good title is helpful. We prefer ""`<header_name>`: Short description of your issue"". You don't usually need to
mention `std::` or C++. For example, ""`<type_traits>`: `is_cute` should be true for `enum class FluffyKittens`"".

It's okay if you report an apparent STL bug that turns out to be a compiler bug, or surprising-yet-Standard behavior.
Just try to follow these rules, so we can spend more time fixing bugs and implementing features.

# How To Build With The Visual Studio IDE

The STL uses boost-math headers to provide P0226R1 Mathematical Special Functions. We recommend using [vcpkg][] to
acquire this dependency.

1. Install Visual Studio 2019 16.10 Preview 2 or later.
    * We recommend selecting ""C++ CMake tools for Windows"" in the VS Installer.
    This will ensure that you're using supported versions of CMake and Ninja.
    * Otherwise, install [CMake][] 3.20 or later, and [Ninja][] 1.10.2 or later.
2. Open Visual Studio, and choose the ""Clone or check out code"" option. Enter the URL of this repository,
   `https://github.com/microsoft/STL`.
3. Open a terminal in the IDE with `` Ctrl + ` `` (by default) or press on ""View"" in the top bar, and then ""Terminal"".
4. In the terminal, invoke `git submodule update --init --progress llvm-project vcpkg`
5. In the terminal, invoke `.\vcpkg\bootstrap-vcpkg.bat`
6. In the terminal, invoke `.\vcpkg\vcpkg.exe install boost-math:x86-windows boost-math:x64-windows`
7. Choose the architecture you wish to build in the IDE, and build as you would any other project. All necessary CMake
   settings are set by `CMakeSettings.json`.

# How To Build With A Native Tools Command Prompt

1. Install Visual Studio 2019 16.10 Preview 2 or later.
    * We recommend selecting ""C++ CMake tools for Windows"" in the VS Installer.
    This will ensure that you're using supported versions of CMake and Ninja.
    * Otherwise, install [CMake][] 3.20 or later, and [Ninja][] 1.10.2 or later.
2. Open a command prompt.
3. Change directories to a location where you'd like a clone of this STL repository.
4. `git clone https://github.com/microsoft/STL`
5. `cd STL`
6. `git submodule update --init --progress llvm-project vcpkg`
7. `.\vcpkg\bootstrap-vcpkg.bat`
8. `.\vcpkg\vcpkg.exe install boost-math:x86-windows boost-math:x64-windows`

To build the x86 target:

1. Open an ""x86 Native Tools Command Prompt for VS 2019 Preview"".
2. Change directories to the previously cloned `STL` directory.
3. `cmake -G Ninja -S . -B out\build\x86`
4. `ninja -C out\build\x86`

To build the x64 target:

1. Open an ""x64 Native Tools Command Prompt for VS 2019 Preview"".
2. Change directories to the previously cloned `STL` directory.
3. `cmake -G Ninja -S . -B out\build\x64`
4. `ninja -C out\build\x64`

# How To Consume

Consumption of the built library is largely based on the build system you're using. There are at least 2 directories
you need to hook up. Assuming you built the x64 target with the Visual Studio IDE, with the STL repository cloned to
`C:\Dev\STL`, build outputs will end up at `C:\Dev\STL\out\build\x64\out`. Ensure that the `inc` directory is searched
for headers, and that `lib\{architecture}` is searched for link libraries, before any defaults supplied by MSVC. The
names of the import and static libraries are the same as those that ship with MSVC. As a result, the compiler `/MD`,
`/MDd`, `/MT`, or `/MTd` switches will work without modification of your build scripts or command-line muscle memory.

Should you choose to use the DLL flavors, the DLLs to deploy are built to `bin\{architecture}`. Note that the DLLs
generated by the CMake build system here have a suffix, defaulting to `_oss`, which distinguishes them from the binaries
that ship with MSVC. That avoids any conflict with the DLLs installed by the [redistributables][] into System32, and
ensures that other components wanting to be a ""guest in your process"", like print drivers and shell extensions, see the
export surface of the STL they were built with. Otherwise, the ""`msvcp140.dll`"" you deployed in the same directory as
your .exe would ""win"" over the versions in System32.

## Complete Example Using x64 DLL Flavor

The compiler looks for include directories according to the `INCLUDE` environment variable, and the linker looks for
import library directories according to the `LIB` environment variable, and the Windows loader will (eventually) look
for DLL dependencies according to directories in the `PATH` environment variable. From an
""x64 Native Tools Command Prompt for VS 2019 Preview"":

```
C:\Users\username\Desktop>set INCLUDE=C:\Dev\STL\out\build\x64\out\inc;%INCLUDE%

C:\Users\username\Desktop>set LIB=C:\Dev\STL\out\build\x64\out\lib\amd64;%LIB%

C:\Users\username\Desktop>set PATH=C:\Dev\STL\out\build\x64\out\bin\amd64;%PATH%

C:\Users\username\Desktop>type example.cpp
#include <iostream>

int main() {
    std::cout << ""Hello STL OSS world!\n"";
}

C:\Users\username\Desktop>cl /nologo /EHsc /W4 /WX /MDd /std:c++latest .\example.cpp
example.cpp

C:\Users\username\Desktop>.\example.exe
Hello STL OSS world!

C:\Users\username\Desktop>dumpbin /IMPORTS .\example.exe | findstr msvcp
    msvcp140d_oss.dll
```

# How To Run The Tests With A Native Tools Command Prompt

1. Follow either [How To Build With A Native Tools Command Prompt][] or [How To Build With The Visual Studio IDE][].
2. Acquire [Python][] 3.9.4 or newer and have it on the `PATH` (or run it directly using its absolute or relative path).
3. Have LLVM's `bin` directory on the `PATH` (so `clang-cl.exe` is available).
    * We recommend selecting ""C++ Clang tools for Windows"" in the VS Installer. This will automatically add LLVM to the
    `PATH` of the x86 and x64 Native Tools Command Prompts, and will ensure that you're using a supported version.
    * Otherwise, use [LLVM's installer][] and choose to add LLVM to your `PATH` during installation.
4. Follow the instructions below.

## Running All The Tests

After configuring and building the project, running `ctest` from the build output directory will run all the tests.
CTest will only display the standard error output of tests that failed. In order to get more details from CTest's
`lit` invocations, run the tests with `ctest -V`.

## Running A Subset Of The Tests

`${PROJECT_BINARY_DIR}\tests\utils\stl-lit\stl-lit.py` can be invoked on a subdirectory of a testsuite and will execute
all the tests under that subdirectory. This can mean executing the entirety of a single testsuite, running all tests
under a category in libcxx, or running a single test in `std` and `tr1`.

## Examples

These examples assume that your current directory is `C:\Dev\STL\out\build\x64`.

* This command will run all of the testsuites with verbose output.
  + `ctest -V`
* This command will also run all of the testsuites.
  + `python tests\utils\stl-lit\stl-lit.py ..\..\..\llvm-project\libcxx\test ..\..\..\tests\std ..\..\..\tests\tr1`
* This command will run all of the std testsuite.
  + `python tests\utils\stl-lit\stl-lit.py ..\..\..\tests\std`
* If you want to run a subset of a testsuite, you need to point it to the right place in the sources. The following
will run the single test found under VSO_0000000_any_calling_conventions.
  + `python tests\utils\stl-lit\stl-lit.py ..\..\..\tests\std\tests\VSO_0000000_any_calling_conventions`
* You can invoke `stl-lit` with any arbitrary subdirectory of a testsuite. In libcxx this allows you to have finer
control over what category of tests you would like to run. The following will run all the libcxx map tests.
  + `python tests\utils\stl-lit\stl-lit.py ..\..\..\llvm-project\libcxx\test\std\containers\associative\map`

## Interpreting The Results Of Tests

### CTest

When running the tests via CTest, all of the testsuites are considered to be a single test. If any single test in a
testsuite fails, CTest will simply report that the `stl` test failed.

Example:
```
0% tests passed, 1 tests failed out of 1

Total Test time (real) = 2441.55 sec

The following tests FAILED:
      1 - stl (Failed)
```

The primary utility of CTest in this case is to conveniently invoke `stl-lit.py` with the correct set of arguments.

CTest will output everything that was sent to stderr for each of the failed testsuites, which can be used to identify
which individual test within the testsuite failed. It can sometimes be helpful to run CTest with the `-V` option in
order to see the stdout of the tests.

### stl-lit

When running the tests directly via the generated `stl-lit.py` script the result of each test will be printed. The
format of each result is `{Result Code}: {Testsuite Name} :: {Test Name}:{Configuration Number}`.

Example:
```
-- Testing: 28 tests, 12 workers --
PASS: tr1 :: tests/cwchar1:01 (1 of 28)
PASS: tr1 :: tests/cwchar1:11 (2 of 28)
PASS: tr1 :: tests/cwchar1:02 (3 of 28)
PASS: tr1 :: tests/cwchar1:03 (4 of 28)
PASS: tr1 :: tests/cwchar1:00 (5 of 28)
PASS: tr1 :: tests/cwchar1:04 (6 of 28)
PASS: tr1 :: tests/cwchar1:05 (7 of 28)
PASS: tr1 :: tests/cwchar1:09 (8 of 28)
PASS: tr1 :: tests/cwchar1:06 (9 of 28)
UNSUPPORTED: tr1 :: tests/cwchar1:20 (10 of 28)
UNSUPPORTED: tr1 :: tests/cwchar1:21 (11 of 28)
UNSUPPORTED: tr1 :: tests/cwchar1:22 (12 of 28)
UNSUPPORTED: tr1 :: tests/cwchar1:23 (13 of 28)
UNSUPPORTED: tr1 :: tests/cwchar1:24 (14 of 28)
PASS: tr1 :: tests/cwchar1:07 (15 of 28)
PASS: tr1 :: tests/cwchar1:08 (16 of 28)
PASS: tr1 :: tests/cwchar1:10 (17 of 28)
PASS: tr1 :: tests/cwchar1:16 (18 of 28)
PASS: tr1 :: tests/cwchar1:17 (19 of 28)
PASS: tr1 :: tests/cwchar1:14 (20 of 28)
PASS: tr1 :: tests/cwchar1:12 (21 of 28)
PASS: tr1 :: tests/cwchar1:13 (22 of 28)
PASS: tr1 :: tests/cwchar1:19 (23 of 28)
PASS: tr1 :: tests/cwchar1:18 (24 of 28)
PASS: tr1 :: tests/cwchar1:15 (25 of 28)
PASS: tr1 :: tests/cwchar1:25 (26 of 28)
PASS: tr1 :: tests/cwchar1:26 (27 of 28)
PASS: tr1 :: tests/cwchar1:27 (28 of 28)

Testing Time: 3.96s
  Expected Passes    : 23
  Unsupported Tests  : 5
```

In the above example we see that 23 tests succeeded and 5 were unsupported.

### Result Code Values

Our tests use the standard [lit result codes][], and an undocumented result code: `SKIPPED`. For our tests, only the
`PASS`, `XFAIL`, `XPASS`, `FAIL`, `UNSUPPORTED`, and `SKIPPED` result codes are relevant.

The `PASS` and `FAIL` result codes are self-explanatory. We want our tests to `PASS` and not `FAIL`.

The `XPASS` and `XFAIL` result codes are less obvious. `XPASS` is actually a failure result and indicates that we
expected a test to fail but it passed. `XFAIL` is a successful result and indicates that we expected the test to fail
and it did. Typically an `XPASS` result means that the `expected_results.txt` file for the testsuite needs to be
modified. If the `XPASS` result is a test legitimately passing, the usual course of action would be to remove a `FAIL`
entry from the `expected_results.txt`. However, some tests from `libcxx` mark themselves as `XFAIL` (meaning they
expect to fail) for features they have added tests for but have yet to implement in `libcxx`. If the STL implements
those features first the tests will begin passing unexpectedly for us and return `XPASS` results. In order to resolve
this it is necessary to add a `PASS` entry to the `expected_results.txt` of the testsuite in question.

The `UNSUPPORTED` result code means that the requirements for a test are not met and so it will not be run. Currently
all tests which use the `/clr` or `/clr:pure` options are unsupported. Also, the `/BE` option is unsupported for x64.

The `SKIPPED` result code indicates that a given test was explicitly skipped by adding a `SKIPPED` entry to the
`expected_results.txt`. A test may be skipped for a number of reasons, which include, but are not limited to:
* being an incorrect test
* taking a very long time to run
* failing or passing for the incorrect reason

### Debugging Individual Tests

While `stl-lit` is super awesome in finding out that *something* is wrong or not even compiling, it is not really
helpful in debugging *what* is going wrong. However, debugging individual tests is rather simple given some additional
steps. Let's assume we want to debug a new feature with tests located in `tests\std\tests\GH_XXXX_meow`.

As always, build the STL from your branch and run the tests:
```
C:\STL\out\build\x64> ninja
C:\STL\out\build\x64> python tests\utils\stl-lit\stl-lit.py -v C:\STL\tests\std\tests\GH_XXXX_meow
```

Let's assume one of the tests fails an assert and we want to debug that configuration. `stl-lit` will conveniently print
the build command, which is far too long to provide here in full. The important part is to add the following options to
provide debug symbols: `/Zi /Fdbark.pdb`.

You can replace `bark` with any descriptive name you like. Add these before the `""-link""` option in the command line
and recompile. Example:
```
C:\STL\out\build\x64>cl ""C:\STL\tests\std\tests\GH_XXXX_meow\test.cpp"" [... more arguments ...]
""-FeC:\STL\out\build\x64\tests\std\tests\GH_XXXX_meow\Output\02\GH_XXXX_meow.exe"" /Zi /Fdbark.pdb ""-link""
[... more arguments ...]
```

You can now start debugging the test via:
```
devenv ""C:\STL\out\build\x64\tests\std\tests\GH_XXXX_meow\Output\02\GH_XXXX_meow.exe""
       ""C:\STL\tests\std\tests\GH_XXXX_meow\test.cpp""
```

However, this might not work right away, as Visual Studio may complain about a missing `msvcp140_oss.dll`. The reason
is that the STL builds those and other DLLs itself and we should under no circumstances overwrite the installed ones.
If you are testing one of the configurations with dynamic linkage (`/MD` or `/MDd`) the easiest solution is to add the
build folder to your path:
```
set PATH=C:\STL\out\build\x64\out\bin\amd64;%PATH%
```

# Editing And Testing The Debugger Visualizer

### Modify The Visualizer

To modify how components are visualized in the debugger edit the file `stl\debugger\STL.natvis`. For more information on
how to modify this file check the [natvis documentation][].

### Test Your Changes

You can add the natvis file to any Visual Studio C++ project if you right click your project > Add > Existing Item and
select the STL.natvis file. After doing this you should be able to see your changes in a Visual Studio debugging
session.

# Block Diagram

The STL is built atop other compiler support libraries that ship with Windows and Visual Studio, like the UCRT,
VCRuntime, and VCStartup. The following diagram describes the dependencies between those components and their ship
vehicles.

![MSVC Libraries Block Diagram](docs/msvc_libraries.plantuml.svg)

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

# Code Of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct][]. For more information see the
[Code of Conduct FAQ][] or contact [opencode@microsoft.com][] with any additional questions or comments.

# License

Copyright (c) Microsoft Corporation.

SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

[Changelog]: https://github.com/microsoft/STL/wiki/Changelog
[clang-format]: https://clang.llvm.org/docs/ClangFormat.html
[CMake]: https://cmake.org/download
[Code of Conduct FAQ]: https://opensource.microsoft.com/codeofconduct/faq/
[Compiler Explorer]: https://godbolt.org
[Developer Community]: https://aka.ms/feedback/report?space=62
[Discord server]: https://discord.gg/XWanNww
[How To Build With A Native Tools Command Prompt]: #how-to-build-with-a-native-tools-command-prompt
[How To Build With The Visual Studio IDE]: #how-to-build-with-the-visual-studio-ide
[LICENSE.txt]: LICENSE.txt
[LLVM's installer]: https://releases.llvm.org/download.html
[LWG issues]: https://cplusplus.github.io/LWG/lwg-toc.html
[LWG tag]: https://github.com/microsoft/STL/issues?q=is%3Aopen+is%3Aissue+label%3ALWG
[Microsoft Open Source Code of Conduct]: https://opensource.microsoft.com/codeofconduct/
[N4885]: https://wg21.link/n4885
[NOTICE.txt]: NOTICE.txt
[Ninja]: https://ninja-build.org
[Pipelines]: https://dev.azure.com/vclibs/STL/_build/latest?definitionId=4&branchName=main
[Python]: https://www.python.org/downloads/windows/
[Roadmap]: https://github.com/microsoft/STL/wiki/Roadmap
[Status Chart]: https://microsoft.github.io/STL/
[Wandbox]: https://wandbox.org
[bug tag]: https://github.com/microsoft/STL/issues?q=is%3Aopen+is%3Aissue+label%3Abug
[cxx20 tag]: https://github.com/microsoft/STL/issues?q=is%3Aopen+is%3Aissue+label%3Acxx20
[enhancement tag]: https://github.com/microsoft/STL/issues?q=is%3Aopen+is%3Aissue+label%3Aenhancement
[hub]: https://support.microsoft.com/en-us/help/4021566/windows-10-send-feedback-to-microsoft-with-feedback-hub-app
[libcxx]: https://libcxx.llvm.org
[lit]: https://llvm.org/docs/CommandGuide/lit.html
[lit result codes]: https://llvm.org/docs/CommandGuide/lit.html#test-status-results
[opencode@microsoft.com]: mailto:opencode@microsoft.com
[redistributables]: https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads
[vcpkg]: https://github.com/microsoft/vcpkg
[natvis documentation]: https://docs.microsoft.com/en-us/visualstudio/debugger/create-custom-views-of-native-objects
"
149,microsoft/ai-edu,HTML,"# <font size=5>微软人工智能教育与学习共建社区</font>
本社区是微软亚洲研究院（Microsoft Research Asia，简称MSRA）人工智能教育团队创立的人工智能教育与学习共建社区.

在教育部指导下，依托于新一代人工智能开放科研教育平台，微软亚洲研究院研发团队和学术合作部将为本社区提供全面支持。我们将在此提供人工智能应用开发的真实案例，以及配套的教程、工具等学习资源，人工智能领域的一线教师及学习者也将分享他们的资源与经验。

正如微软的使命“予力全球每一人、每一组织，成就不凡”所指出的，期待借由本社区的建立，能以开源的方式，与广大师生、开发者一起学习、一起贡献，共同丰富、完善本社区，既而为中国人工智能的发展添砖加瓦。


本社区注明版权出处的内容适用于[License](./LICENSE.md)版权许可。

[English Version](./0-EnglishVersion/README.md)

# <font size=5>新闻</font>

**<font size=3>2021-04-20:</font>**  

更新[中文文本蕴含](./B-实践案例/B17-快速构建中文文本蕴含深度学习模型/Readme.md)案例

**<font size=3>2021-03-07:</font>**

A7-强化学习，准备中。

**<font size=3>2021-02-07:</font>**

新书出版！

<img src=""./A-基础教程/A2-神经网络基本原理/Images/cover.png"" width=300/>

目前在各大网店都可以买到。全书400多页，全彩印刷，由高等教育出版社出版，是下面所述的“神经网络基本原理教程”的印刷版。

**<font size=3>2021-01-07:</font>**

A5-现代软件工程，开始编写。


**<font size=3>2020-03-25:</font>**

社区结构更新啦！模块调整并重新命名，结构更清晰！

将[神经网络基本原理教程](https://aka.ms/beginnerAI) 移入 [A-基础教程](https://github.com/microsoft/ai-edu/tree/master/A-基础教程) 模块。该模块下还有 [数学基础](https://github.com/microsoft/ai-edu/tree/master/A-基础教程/A1-PythonBasic/math_intro) 和 [Python语言导论](https://github.com/microsoft/ai-edu/tree/master/A-基础教程/A1-PythonBasic/py_intro)。教程更集中，学习更方便！

实践案例全部汇集在 [B-实践案例](https://github.com/microsoft/ai-edu/tree/master/B-实践案例) 模块，并配上案例概览帮助文档，更有针对性学习案例！

[E-课程集锦](https://github.com/microsoft/ai-edu/tree/master/E-课程集锦/) 模块汇集了微软及多所高校开源人工智能教学大纲及课件。欢迎感兴趣的朋友前往查看！

**<font size=3>2019-11-20:</font>**

首页改版啦！新版本的首页，将社区资源进一步系统化，按认识AI（初级），理解AI（中级）,研究AI（高级）的结构分级编写了学习路径，并给出学习时长参考，先修知识资源参考，循序渐进，旨在帮助广大学习者更最高效地学习AI，赶快学起来吧！

**<font size=3>2019-11-19:</font>**

更新[智能对联](./B-实践案例/B13-AI对联生成案例)案例，案例更加简洁、清晰，方便上手！

**<font size=3>2019-11-15:</font>**

[神经网络基本原理简明教程](https://aka.ms/beginnerAI)暨**9步学习神经网络**全部内容完成！

# <font size=5>学习资源介绍</font>
介绍：

本社区的学习资源优质且免费，绝大部分为原创内容，核心学习资源包括**实战篇**和**理论篇**两大部分，辅以参考学习路径和先修知识参考资源，让广大学习者可以清晰地选择适合自己的学习路径，高效地学习。

**1. 实战篇**

以“做中学“的理念为核心，从人工智能真实的应用场景与案例出发，先讲生动的案例，配合详实的实际操作说明，然后在动手实现场景的基础上，逐步引入人工智能学习中的相关理论知识，以递进学习的新颖方式层层剖析人工智能开发的主流场景，让大家在不需要大量时间学习庞大的理论基础的情况下，也可以真正动手开始进行人工智能应用的开发，提高实际动手的能力.

[点此进入详细内容](https://github.com/microsoft/ai-edu/tree/master/B-实践案例)


**2. 理论篇**

理论篇的内容又称作“[9步学习神经网络](https://aka.ms/beginnerAI )”,为微软亚洲研究院研发团队原创内容，着重讲述偏理论的知识，同样以“做中学”为核心概念，但是独特地以化繁为简，深入浅出为特点，提供通俗易懂的理论讲解，清晰工整的代码，准确无误的内容，完整的作业体系，不但有理论，还有大量实践动手环节，帮助读者不但迅速掌握“深度学习”的基础知识，更好地理解并使用现有框架，而且可以助力读者快速学习最新出现的各种神经网络的扩展或者变型，跟上快速发展的AI浪潮,使学习者从新的角度快速上手神经网络的学习，做到真正的从入门到精通。该部分内容在针对合作伙伴线下的培训中，受到广大学习者的广泛好评。

[点此进入详细内容](https://github.com/microsoft/ai-edu/tree/master/A-基础教程)


# <font size=5>AI 前沿精选</font>
[大规模利用单语数据提升神经机器翻译](https://www.msra.cn/zh-cn/news/features/emnlp-2019-exploiting-monolingual-data-at-scale-for-nmt)

[基于层次化注意力图网络和多视角学习的商品推荐](https://www.msra.cn/zh-cn/news/features/emnlp-2019-rmg)

[AI换脸鉴别率超99.6%，微软用技术应对虚假信息](https://www.msra.cn/zh-cn/news/features/ai-detect-fake-face)

[微软亚洲研究院精选论文解读](https://www.msra.cn/zh-cn/news/features/emnlp-2019)

[查看更多...](https://www.msra.cn/zh-cn/news?wd&content-type=posts)


# <font size=5>等你来战</font>
  - [挑战黄金点](./C-挑战项目/GoldenNumberGame)
  - [北京航空航天大学2019春季](./C-挑战项目/BeihangUniversity2019Spring)
  - [山东大学2019春季](./C-挑战项目/ShandongUniversity2019Spring)
  - [Code Search](./C-挑战项目/CodeSearch)
  - [2019实践空间站](./C-挑战项目/2019studentclub)


# <font size=5>如何贡献</font>
我们非常欢迎您参与到社区共建中来，贡献高质量的内容，丰富社区资源并帮助更多学习者。在您准备贡献之前，请仔细阅读[共建指南](./CONTRIBUTING.md)， 并遵守其中的规范，并确保您所贡献的内容符合我们的[License](./LICENSE.md)。如需了解贡献的步骤和具体技巧，请参阅[如何高效贡献](./contribute_efficiently.md)。

----

<font size=2>[访问旧版主页 (Version 1.0)](./README_1.0.md)</font>

<font size=2>[访问旧版主页 (Version 2.0)](./README_2.0.md)</font>"
150,microsoft/playwright-python,Python,"# 🎭 [Playwright](https://playwright.dev) for Python [![PyPI version](https://badge.fury.io/py/playwright.svg)](https://pypi.python.org/pypi/playwright/) [![Join Slack](https://img.shields.io/badge/join-slack-infomational)](https://aka.ms/playwright-slack)

#### [Docs](https://playwright.dev/python/docs/intro) | [API](https://playwright.dev/python/docs/api/class-playwright)

Playwright is a Python library to automate [Chromium](https://www.chromium.org/Home), [Firefox](https://www.mozilla.org/en-US/firefox/new/) and [WebKit](https://webkit.org/) browsers with a single API. Playwright delivers automation that is **ever-green**, **capable**, **reliable** and **fast**. [See how Playwright is better](https://playwright.dev/python/docs/why-playwright).

|          | Linux | macOS | Windows |
|   :---   | :---: | :---: | :---:   |
| Chromium <!-- GEN:chromium-version -->92.0.4498.0<!-- GEN:stop --> | ✅ | ✅ | ✅ |
| WebKit <!-- GEN:webkit-version -->14.2<!-- GEN:stop --> | ✅ | ✅ | ✅ |
| Firefox <!-- GEN:firefox-version -->89.0b6<!-- GEN:stop --> | ✅ | ✅ | ✅ |

Headless execution is supported for all browsers on all platforms.

- [Usage](#usage)
  - [Record and generate code](#record-and-generate-code)
  - [Sync API](#sync-api)
  - [Async API](#async-api)
  - [With pytest](#with-pytest)
  - [Interactive mode (REPL)](#interactive-mode-repl)
- [Examples](#examples)
  - [Mobile and geolocation](#mobile-and-geolocation)
  - [Evaluate JS in browser](#evaluate-js-in-browser)
  - [Intercept network requests](#intercept-network-requests)
- [Documentation](#documentation)

## Usage

```sh
pip install playwright
playwright install
```

This installs Playwright and browser binaries for Chromium, Firefox and WebKit. Playwright requires Python 3.7+.

#### Record and generate code

Playwright can record user interactions in a browser and generate code. [See demo](https://user-images.githubusercontent.com/284612/95930164-ad52fb80-0d7a-11eb-852d-04bfd19de800.gif).

```sh
# Pass --help to see all options
playwright codegen
```

Playwright offers both sync (blocking) API and async API. They are identical in terms of capabilities and only differ in how one consumes the API.

#### Sync API

This is our default API for short snippets and tests. If you are not using asyncio in your
application, it is the easiest to use Sync API notation.

```py
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    for browser_type in [p.chromium, p.firefox, p.webkit]:
        browser = browser_type.launch()
        page = browser.new_page()
        page.goto('http://whatsmyuseragent.org/')
        page.screenshot(path=f'example-{browser_type.name}.png')
        browser.close()
```

#### Async API

If your app is based on the modern asyncio loop and you are used to async/await constructs,
Playwright exposes Async API for you. You should use this API inside a Python REPL supporting `asyncio` like with `python -m asyncio`

```console
$ python -m asyncio
```

```py
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        for browser_type in [p.chromium, p.firefox, p.webkit]:
            browser = await browser_type.launch()
            page = await browser.new_page()
            await page.goto('http://whatsmyuseragent.org/')
            await page.screenshot(path=f'example-{browser_type.name}.png')
            await browser.close()

asyncio.run(main())
```

#### With pytest

Use our [pytest plugin for Playwright](https://github.com/microsoft/playwright-pytest#readme).

```py
def test_playwright_is_visible_on_google(page):
    page.goto(""https://www.google.com"")
    page.type(""input[name=q]"", ""Playwright GitHub"")
    page.click(""input[type=submit]"")
    page.wait_for_selector(""text=microsoft/Playwright"")
```

#### Interactive mode (REPL)

Blocking REPL, as in CLI:

```py
>>> from playwright.sync_api import sync_playwright
>>> playwright = sync_playwright().start()

# Use playwright.chromium, playwright.firefox or playwright.webkit
# Pass headless=False to see the browser UI
>>> browser = playwright.chromium.launch()
>>> page = browser.new_page()
>>> page.goto(""http://whatsmyuseragent.org/"")
>>> page.screenshot(path=""example.png"")
>>> browser.close()
>>> playwright.stop()
```

Async REPL such as `asyncio` REPL:

```console
$ python -m asyncio
```

```py
>>> from playwright.async_api import async_playwright
>>> playwright = await async_playwright().start()
>>> browser = await playwright.chromium.launch()
>>> page = await browser.new_page()
>>> await page.goto(""http://whatsmyuseragent.org/"")
>>> await page.screenshot(path=""example.png"")
>>> await browser.close()
>>> await playwright.stop()
```

## Examples

#### Mobile and geolocation

This snippet emulates Mobile Safari on a device at a given geolocation, navigates to maps.google.com, performs action and takes a screenshot.

```py
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    iphone_11 = p.devices[""iPhone 11 Pro""]
    browser = p.webkit.launch(headless=False)
    context = browser.new_context(
        **iphone_11,
        locale=""en-US"",
        geolocation={""longitude"": 12.492507, ""latitude"": 41.889938 },
        permissions=[""geolocation""]
    )
    page = context.new_page()
    page.goto(""https://maps.google.com"")
    page.click(""text=Your location"")
    page.screenshot(path=""colosseum-iphone.png"")
    browser.close()
```

<details>
<summary>Async variant</summary>

```py
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        iphone_11 = p.devices[""iPhone 11 Pro""]
        browser = await p.webkit.launch(headless=False)
        context = await browser.new_context(
            **iphone_11,
            locale=""en-US"",
            geolocation={""longitude"": 12.492507, ""latitude"": 41.889938},
            permissions=[""geolocation""]
        )
        page = await context.newPage()
        await page.goto(""https://maps.google.com"")
        await page.click(""text=""Your location"""")
        await page.screenshot(path=""colosseum-iphone.png"")
        await browser.close()

asyncio.run(main())
```

</details>

#### Evaluate JS in browser

This code snippet navigates to example.com in Firefox, and executes a script in the page context.

```py
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.firefox.launch()
    page = browser.new_page()
    page.goto(""https://www.example.com/"")
    dimensions = page.evaluate(""""""() => {
      return {
        width: document.documentElement.clientWidth,
        height: document.documentElement.clientHeight,
        deviceScaleFactor: window.devicePixelRatio
      }
    }"""""")
    print(dimensions)
    browser.close()
```

<details>
<summary>Async variant</summary>

```py
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        browser = await p.firefox.launch()
        page = await browser.new_page()
        await page.goto(""https://www.example.com/"")
        dimensions = await page.evaluate(""""""() => {
          return {
            width: document.documentElement.clientWidth,
            height: document.documentElement.clientHeight,
            deviceScaleFactor: window.devicePixelRatio
          }
        }"""""")
        print(dimensions)
        await browser.close()

asyncio.run(main())
```

</details>

#### Intercept network requests

This code snippet sets up request routing for a Chromium page to log all network requests.

```py
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()

    def log_and_continue_request(route, request):
        print(request.url)
        route.continue_()

    # Log and continue all network requests
    page.route(""**/*"", log_and_continue_request)

    page.goto(""http://todomvc.com"")
    browser.close()
```

<details>
<summary>Async variant</summary>

```py
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()

        async def log_and_continue_request(route, request):
            print(request.url)
            await route.continue_()

        # Log and continue all network requests
        await page.route(""**/*"", log_and_continue_request)
        await page.goto(""http://todomvc.com"")
        await browser.close()

asyncio.run(main())
```

</details>

## Documentation

Check out our [new documentation site](https://playwright.dev/python/docs/intro)!
"
151,microsoft/boll,TypeScript,"# boll

Lint the whole repo.

## Getting started

### Install

To install, add `@boll/cli` as a dev dependency to your package with
your package manager of choice.

```sh
npm install --save-dev @boll/cli
```

### Configure

Next, run the `init` command to generate a configuration file that
will be used when boll runs.

```sh
npx boll init
```

This command will create a configuration file called `.boll.config.js`
in your current directory, implementing the recoommended configuration
by default. It should look like the following.

```js
""use strict"";
module.exports = {
  extends: ""boll:recommended""
};
```

### Run

To run `boll`, simply pass the `run` command.

```sh
npx boll run
```

If everything is configured successfully and your project contains no
boll violations, the command will exit with no output and an exit
status of `0`.

### Next steps

Learn about configuring, tweaking, or adding rules in [the docs](https://microsoft.github.io/boll/).

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
152,microsoft/lisa,Python,"# Linux Integration Services Automation (LISA)

[![CI Workflow](https://github.com/microsoft/lisa/workflows/CI%20Workflow/badge.svg?branch=main)](https://github.com/microsoft/lisa/actions?query=workflow%3A%22CI+Workflow+for+LISAv3%22+event%3Apush+branch%3Amain)
[![GitHub license](https://img.shields.io/github/license/microsoft/lisa)](https://github.com/microsoft/lisa/blob/main/LICENSE)

**Linux Integration Services Automation (LISA)** is designed to be an end-to-end solution for verifying Linux kernels and distributions quality on Microsoft virtualization technologies. It can be used on other quality validation, and virtualization technologies as well.

* **End-to-end**: LISA defines several sets of test suites to validate Linux kernels and distributions in Microsoft Azure, Hyper-V, etc. The test suites can help find integration issues easily.
* **Ease-to-use**: The complexity and diversity of Linux kernels/distributions are wrapped in different components of LISA. When running LISA, it doesn't need to know details. Developers can focus on validation logic, when creating new tests.
* **Extensibility**: LISA is extendable in many components to support various scenarios, including virtualization platforms, commands, Linux distributions, community test suites, etc. LISA supports to validate Microsoft virtualization platforms natively, but also can be extended to other cloud or on-premises platforms.

## Why LISA

There are a lot of classic tools and tests, which focus on the quality of Linux kernels or distributions. They are important to ensure the quality of kernels and distributions. The integration validation on virtualization platforms is a little different with classic Linux testing. It covers diverse types of resources with manageable cost. So that, it needs to plan resources creation and deletion automatically.

LISA focuses on validating the integration of Linux kernels/distributions and virtualization platforms. It needs more interactive with virtualization platforms to run tests for different purposes, like test different capabilities, hardware, and so on.

## Documents

* [Install LISA](docs/install.md)
* [Run tests](docs/run.md)
* [Microsoft tests](docs/microsoft_tests.md)
* [Write test cases in LISA](docs/write_case.md)
* [Command line reference](docs/command_line.md)
* [Runbook reference](docs/runbook.md)
* [Extend and customize LISA](docs/extension.md)
* [Run previous version LISA (aka LISAv2)](docs/run_legacy.md)

## Contribute

You are very welcome to contribute. Please follow [the contribution document](docs/contributing.md) for details.

## History and road map

The previous LISA called LISAv2, which is in [master branch](https://github.com/microsoft/lisa/tree/master). The previous LISA can be used standalone or called from the current LISA. Learn more from [how to run LISAv2 test cases](docs/run_legacy.md).

LISA is in active developing, and a lot of exciting features are implementing. We're listening your [feedback](https://github.com/microsoft/lisa/issues/new).

## License

The entire codebase is under [MIT license](LICENSE).
"
153,microsoft/linkcheckermd,TypeScript,"# LinkCheckMD
Load a Markdown file and get highlights and hovers for links that contain a country code (en-us for example.) 

If you use Alt+L, it will generate a report on the links in the document, including broken links. It attempts to check broken links by trying to resolve HTTP & HTTPS links, and relative links (../folder/file.md) by checking if the file exist on the local file system. The result of these checks are logged in an output window on the right of the editor.

![Animated GIF of URLs being flagged as warnings and Alt+L functionality](./images/working.gif)

Note that checking for broken links is more of an art than a science. Some sites don't actually return 404, but send you to a landing page. For example, Azure.com works this way. You can go to https://Azure.com/foo/bar and it will happily redirect you to https://Azure.com, with no 404 status returned. So take a status of ""OK"" with a grain of salt - you may not be arriving at the page you intend.

## Install

Open Visual Studio Code and press `F1`; a field will appear at the top of the window. Type `ext install linkcheck`, hit enter, and reload the window to enable.

![Animated GIF of installing the extension](./images/install.gif)

## Check for country code

Checking for country codes in links happens as you type, and will underline links with green.

## Check for broken links

To check for broken links, use Alt+L. This will open a new column to the right of the VSCode window and display the status of the links as they are checked.

## Changes

### 0.1.5

- Added country code warnings to the output window for Alt+L checking
- Updated vscode dependency for the latest version

## TODO

* Refactor broken link checking to display the actual URL that you arrived at for ""OK"" results that were redirects to a different URL.

"
154,microsoft/powerquery-parser,TypeScript,"# powerquery-parser

[![Build Status](https://dev.azure.com/ms/powerquery-parser/_apis/build/status/microsoft.powerquery-parser?branchName=master)](https://dev.azure.com/ms/powerquery-parser/_build/latest?definitionId=134&branchName=master)

A parser for the [Power Query/M](https://docs.microsoft.com/en-us/power-query/) language, written in TypeScript. Designed to be consumed by other projects.

## How to use

The most common way to consume the project is to interact with the helper functions found in [src/task.ts](src/task.ts). There are all-in-one functions, such as `tryLexParseInspection`, which does a full pass on a given document. There are also incremental functions, such as `tryLex` and `tryParse`, which perform one step at a time. Minimal code samples can be found in [example.ts](src/example.ts).

## Things to note

### Parser

The parser started off as a naive recursive descent parser with limited backtracking. It mostly followed the [official specification](https://docs.microsoft.com/en-us/powerquery-m/power-query-m-language-specification) released in October 2016. Deviations from the specification should be marked down in [specification.md](specification.md). A combinatorial parser has since been added which uses the naive parser as its base.

### Style

This project uses [prettier](https://github.com/prettier/prettier) as the primary source of style enforcement. Additional style requirements are located in [style.md](style.md).

## How to build

- Install NodeJS
- `npm install`
- `npm run-script build`

## How to run tests

- Install NodeJS
- `npm install`
- `npm test`

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
155,microsoft/BotFramework-Composer,TypeScript,"# ![Microsoft Bot Framework Composer](./docs/Assets/gh-banner.png)

# Microsoft Bot Framework Composer

[![Build Status](https://github.com/microsoft/BotFramework-Composer/workflows/Composer%20CI/badge.svg?branch=main)](https://github.com/microsoft/BotFramework-Composer/actions?query=branch%3Amain)
[![Coverage Status](https://coveralls.io/repos/github/microsoft/BotFramework-Composer/badge.svg?branch=main)](https://coveralls.io/github/microsoft/BotFramework-Composer?branch=main)
[![Total alerts](https://img.shields.io/lgtm/alerts/g/microsoft/BotFramework-Composer.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/microsoft/BotFramework-Composer/alerts/)
[![license](https://img.shields.io/badge/license-MIT%20License-00AAAA.svg)](https://github.com/microsoft/BotFramework-Composer/blob/main/LICENSE.md)

## Overview

Bot Framework Composer is an open-source, visual authoring canvas for developers and multi-disciplinary teams to design and build conversational experiences with Language Understanding and QnA Maker, and a sophisticated composition of bot replies (Language Generation). Within this tool, you'll have everything you need to build a sophisticated conversational experience.

- A visual editing canvas for conversation flows
- In context editing for language understanding (NLU)
- Tools to train, test and manage language understanding (NLU) and QnA components
- Language generation and templating system
- A ready-to-use bot runtime executable

The Bot Framework Composer is an open source tool based on the Bot Framework SDK. It is available as a [desktop application](#get-started) as well as a [web-based component](#build-composer-locally)

<p align=""center"">
    <img alt=""Bot Framework Composer Home Page"" src=""./docs/Assets/Screenshot-Composer-overview.png"" style=""max-width:700px;"" />
</p>

## Get Started

- Download Composer for [Windows][201], [Mac][203] and [Linux][202]. Please see [supported OS versions][205].
- To learn about the Bot Framework Composer, read the [documentation][5].
- To get yourself familiar with the Composer, read [Introduction to Bot Framework Composer][1].
- [Create your first bot][3]!
- To find the most recent release and learn what has changed in Bot Framework Composer, see the [latest release][204].

## Build Composer Locally

To build and run the Composer project locally as a web application, clone the source code from Github and build the application using the instructions below.

Please see [supported NodeJS versions][205] before building.

```
$ git clone https://github.com/microsoft/BotFramework-Composer.git
$ cd BotFramework-Composer
$ cd Composer // switch to Composer folder
$ yarn install // install dependencies
$ yarn build // build extensions and libs
$ yarn startall // start client and server at the same time
```

## Extend Composer with Extensions

Many aspects of Composer's functionality can be customized and extended through extensions. Features such as authentication, storage, publishing and even the samples and templates available on the homescreen can be customized by creating new extensions.

[Read more about building Composer extensions &rarr;](extensions/README.md)

## Support and Feedback

- [Ask a question on Stack Overflow][10]
- [Request a new feature][11]
- [File an issue][12]

## Related project

The Bot Framework Composer is part of the [Bot Framework][20] platform:

- [Bot Framework SDK][21]
- [Bot Framework Emulator][22]

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct][100].
For more information see the [Code of Conduct FAQ][101] or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### Issues and feature requests

Please file issues and feature requests [here](https://github.com/microsoft/BotFramework-Composer/issues/issues).

Also, see current [known issues](https://github.com/microsoft/BotFramework-Composer/labels/known%20issue) for high impact bugs you may experience.

### Submitting pull requests

If you'd like to contribute pull requests to Composer, see the [contributing guide](./CONTRIBUTING.md) for helpful information on our development workflow.

## Reporting security issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP][102] key, can be found in
the [Security TechCenter][103].

[1]: https://aka.ms/bf-composer-docs-introduction
[2]: https://aka.ms/bf-composer-docs-setup-yarn
[3]: https://aka.ms/bf-composer-docs-create-first-bot
[4]: https://aka.ms/BF-Composer-Docs
[5]: https://aka.ms/bf-composer-docs-welcome-page
[10]: https://stackoverflow.com/questions/tagged/botframework?tab=Newest
[11]: https://github.com/microsoft/BotFramework-Composer/issues/new?assignees=&labels=Type%3A+suggestion%2C+Needs-triage&template=bot-framework-composer-feature-request.md&title=
[12]: https://github.com/microsoft/BotFramework-Composer/issues/new?assignees=&labels=Needs-triage%2C+Type%3A+bug&template=bot-framework-composer-bug.md&title=
[20]: https://github.com/microsoft/botframework#microsoft-bot-framework
[21]: https://github.com/microsoft/botframework-sdk#bot-framework-sdk
[22]: https://github.com/Microsoft/BotFramework-Emulator#readme
[100]: https://opensource.microsoft.com/codeofconduct/
[101]: https://opensource.microsoft.com/codeofconduct/faq/
[102]: https://technet.microsoft.com/en-us/security/dn606155
[103]: https://technet.microsoft.com/en-us/security/default
[201]: https://aka.ms/bf-composer-download-win
[202]: https://aka.ms/bf-composer-download-linux
[203]: https://aka.ms/bf-composer-download-mac
[204]: https://github.com/microsoft/BotFramework-Composer/releases/latest
[205]: https://aka.ms/bf-composer-supported-os
"
156,microsoft/hermes-windows,C++,"# Hermes JS Engine for React Native Windows
[![MIT license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/facebook/hermes/blob/master/LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/facebook/hermes/blob/master/CONTRIBUTING.md)
<img src=""./website/static/img/logo.svg"" alt=""Hermes logo - large H with wings"" align=""right"" width=""20%""/>

Hermes is a JavaScript engine optimized for fast start-up of [React Native](https://reactnative.dev/) apps. It features ahead-of-time static optimization and compact bytecode.

If you're only interested in using pre-built Hermes in a new or existing React Native app, you do not need to follow this guide or have direct access to the Hermes source. Instead, just follow [these instructions to enable Hermes](https://reactnative.dev/docs/hermes).

> Noted that each Hermes release is aimed at a specific RN version. The rule of thumb is to always follow [Hermes releases](https://github.com/facebook/hermes/releases) strictly. Version mismatch can result in instant crash of your apps in the worst case scenario.

If you want to know how to build and hack on Hermes directly, and/or integrate Hermes built from source into a React Native app then read on.

The instructions here very briefly cover steps to build the Hermes CLI. They assume you have typical native development tools setup for your OS, and support for cmake and Ninja. For more details of required dependencies, building Hermes with different options, etc. follow these links instead:

* [Building and Running Hermes](doc/BuildingAndRunning.md)
* [Using a custom Hermes build in a React Native app](doc/ReactNativeIntegration.md#using-a-custom-hermes-build-in-a-react-native-app)

To build a local debug version of the Hermes CLI tools the following steps should get you started on macOS/Linux:
The following commands should get you going in a Windows Command Prompt:

```shell
mkdir hermes_workingdir
cd hermes_workingdir
git clone https://github.com/facebook/hermes.git
hermes/utils/build/configure.py
cd build
ninja
```

Or if you're using Windows, the following should get you going in a Git Bash shell:

```shell
mkdir hermes_workingdir
cd hermes_workingdir
git -c core.autocrlf=false clone https://github.com/facebook/hermes.git
hermes/utils/build/configure.py --build-system='Visual Studio 16 2019' --cmake-flags='-A x64' --distribute
cd build
MSBuild.exe ALL_BUILD.vcxproj /p:Configuration=Release
```

You will now be in a directory with the output of building Hermes into CLI tools. From here you can run a piece of JavaScript as follows:

```shell
echo 'use strict'; function hello() { print('Hello World'); } hello(); | .\bin\Release\hermes.exe
```

For more details on Hermes for Android, see [here](https://github.com/facebook/hermes/blob/master/README.md).

## Contributing

The main purpose of this repository is to brings Hermes support to [React Native Windows](https://github.com/microsoft/react-native-windows). We are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can participate.

### Code of Conduct

Both Microsoft and Facebook have adopted [Codes of Conduct](./CODE_OF_CONDUCT.md) that we expect project participants to adhere to. Microsoft's Code of Conduct can be found [here](https://opensource.microsoft.com/codeofconduct)  and Facebook's [here](https://code.fb.com/codeofconduct). Please read through them so that you can understand what actions will and will not be tolerated.

### Contributing Guide

Read our [contributing guide](CONTRIBUTING.md) to learn about our development process as well as how to propose bugfixes and improvements.

### License

Hermes is [MIT licensed](./LICENSE).
"
157,microsoft/PSRule,C#,"# PSRule

A cross-platform module to validate infrastructure as code (IaC) and objects using PowerShell rules.
PSRule works great and integrates with popular continuous integration (CI) systems.

![ci-badge]

Features of PSRule include:

- [Extensible](docs/features.md#extensible) - Use PowerShell, a flexible scripting language.
- [Cross-platform](docs/features.md#cross-platform) - Run on MacOS, Linux, and Windows.
- [Reusable](docs/features.md#reusable) - Share rules across teams or organizations.
- [Recommendations](docs/features.md#recommendations) - Include detailed instructions to remediate issues.

## Project objectives

1. **Extensible**:
   - Provide an execution environment (tools and language) to validate infrastructure code.
   - Handling of common concerns such as input/ output/ reporting should be handled by the engine.
   - Language must be flexible enough to support a wide range of use cases.
2. **DevOps**:
   - Validation should support and enhance DevOps workflows by providing fast feedback in pull requests.
   - Allow quality gates to be implemented between environments such development, test, and production.
3. **Cross-platform**:
   - A wide range of platforms can be used to author and deploy infrastructure code.
PSRule must support rule validation and authoring on Linux, MacOS, and Windows.
   - Runs in a Linux container.
For continuous integration (CI) systems that do not support PowerShell, run in a container.
4. **Reusable**:
   - Validation should plug and play, reusable across teams and organizations.
   - Any reusable validation will have exceptions.
Rules must be able to be disabled where they are not applicable.

Continue reading the [PSRule design specification][spec].

## Support

This project uses GitHub Issues to track bugs and feature requests.
Please search the existing issues before filing new issues to avoid duplicates.

- For new issues, file your bug or feature request as a new [issue].
- For help, discussion, and support questions about using this project, join or start a [discussion].

Support for this project/ product is limited to the resources listed above.

## Getting the module

You can download and install the PSRule module from the PowerShell Gallery.

Module | Description | Downloads / instructions
------ | ----------- | ------------------------
PSRule | Validate infrastructure as code (IaC) and objects using PowerShell rules. | [latest][module-psrule] / [instructions][install]

For rule and integration modules see [related projects](#related-projects).

## Getting extensions

Companion extensions are available for the following platforms.

Platform           | Description | Downloads / instructions
--------           | ----------- | ------------------------
Azure Pipelines    | Validate infrastructure as code (IaC) and DevOps repositories using Azure Pipelines. | [latest][extension-pipelines] / [instructions][install]
GitHub Actions     | Validate infrastructure as code (IaC) and DevOps repositories using GitHub Actions. | [latest][extension-actions] / [instructions][install]
Visual Studio Code | Visual Studio Code extension for PSRule. | [latest][extension-vscode] / [instructions][install]

## Getting started

The following example shows basic PSRule usage for validating PowerShell objects.
For specific use cases see [scenarios](#scenarios).

For frequently asked questions, see the [FAQ](docs/features.md#frequently-asked-questions-faq).

### Define a rule

To define a rule, use a `Rule` block saved to a file with the `.Rule.ps1` extension.

```powershell
Rule 'NameOfRule' {
    # Rule conditions
}
```

Within the body of the rule provide one or more conditions.
A condition is valid PowerShell that results in `$True` or `$False`.

For example:

```powershell
Rule 'isFruit' {
    # Condition to determine if the object is fruit
    $TargetObject.Name -in 'Apple', 'Orange', 'Pear'
}
```

An optional result message can be added to by using the `Recommend` keyword.

```powershell
Rule 'isFruit' {
    # An recommendation to display in output
    Recommend 'Fruit is only Apple, Orange and Pear'

    # Condition to determine if the object is fruit
    $TargetObject.Name -in 'Apple', 'Orange', 'Pear'
}
```

The rule is saved to a file named [`isFruit.Rule.ps1`](docs/scenarios/fruit/isFruit.Rule.ps1) file.
One or more rules can be defined within a single file.

### Execute a rule

To execute the rule use `Invoke-PSRule`.

For example:

```powershell
# Define objects to validate
$items = @();
$items += [PSCustomObject]@{ Name = 'Fridge' };
$items += [PSCustomObject]@{ Name = 'Apple' };

# Validate each item using rules saved in current working path
$items | Invoke-PSRule;
```

The output of this example is:

```text
   TargetName: Fridge

RuleName                            Outcome    Recommendation
--------                            -------    --------------
isFruit                             Fail       Fruit is only Apple, Orange and Pear


   TargetName: Apple

RuleName                            Outcome    Recommendation
--------                            -------    --------------
isFruit                             Pass       Fruit is only Apple, Orange and Pear
```

### Additional options

To filter results to only non-fruit results, use `Invoke-PSRule -Outcome Fail`.
Passed, failed and error results are shown by default.

```powershell
# Only show non-fruit results
$items | Invoke-PSRule -Outcome Fail;
```

For a summary of results for each rule use `Invoke-PSRule -As Summary`.

For example:

```powershell
# Show rule summary
$items | Invoke-PSRule -As Summary;
```

The output of this example is:

```text
RuleName                            Pass  Fail  Outcome
--------                            ----  ----  -------
isFruit                             1     1     Fail
```

An optional failure reason can be added to the rule block by using the `Reason` keyword.

```powershell
Rule 'isFruit' {
    # An recommendation to display in output
    Recommend 'Fruit is only Apple, Orange and Pear'

    # An failure reason to display for non-fruit
    Reason ""$($PSRule.TargetName) is not fruit.""

    # Condition to determine if the object is fruit
    $TargetObject.Name -in 'Apple', 'Orange', 'Pear'
}
```

To include the reason with output use `Invoke-PSRule -OutputFormat Wide`.

For example:

```powershell
# Show failure reason for failing results
$items | Invoke-PSRule -OutputFormat Wide;
```

The output of this example is:

```text

   TargetName: Fridge

RuleName                            Outcome    Reason                              Recommendation
--------                            -------    ------                              --------------
isFruit                             Fail       Fridge is not fruit.                Fruit is only Apple, Orange and Pear


   TargetName: Apple

RuleName                            Outcome    Reason                              Recommendation
--------                            -------    ------                              --------------
isFruit                             Pass                                           Fruit is only Apple, Orange and Pear
```

The final rule is saved to [`isFruit.Rule.ps1`](docs/scenarios/fruit/isFruit.Rule.ps1).

### Scenarios

For walk through examples of PSRule usage see:

- [Validate Azure resource configuration](docs/scenarios/azure-resources/azure-resources.md)
- [Validate Azure resources tags](docs/scenarios/azure-tags/azure-tags.md)
- [Validate Kubernetes resources](docs/scenarios/kubernetes-resources/kubernetes-resources.md)
- [Using within continuous integration](docs/scenarios/validation-pipeline/validation-pipeline.md)
- [Packaging rules in a module](docs/scenarios/rule-module/rule-module.md)
- [Writing rule help](docs/scenarios/rule-docs/rule-docs.md)

## Language reference

PSRule extends PowerShell with domain specific language (DSL) keywords, cmdlets and automatic variables.

### Keywords

The following language keywords are used by the `PSRule` module:

- [Rule](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#rule) - A rule definition.
- [Exists](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#exists) - Assert that a field or property must exist.
- [Match](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#match) - Assert that the field must match any of the regular expressions.
- [AnyOf](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#anyof) - Assert that any of the child expressions must be true.
- [AllOf](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#allof) - Assert that all of the child expressions must be true.
- [Within](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#within) - Assert that the field must match any of the values.
- [TypeOf](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#typeof) - Assert that the object must be of a specific type.
- [Reason](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#reason) - Return a reason for why the rule failed.
- [Recommend](docs/keywords/PSRule/en-US/about_PSRule_Keywords.md#recommend) - Return a recommendation to resolve the issue and pass the rule.

### Commands

The following commands exist in the `PSRule` module:

- [Assert-PSRule](docs/commands/PSRule/en-US/Assert-PSRule.md) - Evaluate objects against matching rules and assert any failures.
- [Get-PSRule](docs/commands/PSRule/en-US/Get-PSRule.md) - Get a list of rule definitions.
- [Get-PSRuleBaseline](docs/commands/PSRule/en-US/Get-PSRuleBaseline.md) - Get a list of baselines.
- [Get-PSRuleHelp](docs/commands/PSRule/en-US/Get-PSRuleHelp.md) - Get documentation for a rule.
- [Get-PSRuleTarget](docs/commands/PSRule/en-US/Get-PSRuleTarget.md) - Get a list of target objects.
- [Invoke-PSRule](docs/commands/PSRule/en-US/Invoke-PSRule.md) - Evaluate objects against matching rules and output the results.
- [New-PSRuleOption](docs/commands/PSRule/en-US/New-PSRuleOption.md) - Create options to configure PSRule execution.
- [Set-PSRuleOption](docs/commands/PSRule/en-US/Set-PSRuleOption.md) - Sets options that configure PSRule execution.
- [Test-PSRuleTarget](docs/commands/PSRule/en-US/Test-PSRuleTarget.md) - Pass or fail objects against matching rules.

### Concepts

The following conceptual topics exist in the `PSRule` module:

- [Assert](docs/concepts/PSRule/en-US/about_PSRule_Assert.md)
  - [Contains](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#contains)
  - [EndsWith](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#endswith)
  - [FileHeader](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#fileheader)
  - [FilePath](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#filepath)
  - [Greater](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#greater)
  - [GreaterOrEqual](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#greaterorequal)
  - [HasDefaultValue](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#hasdefaultvalue)
  - [HasField](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#hasfield)
  - [HasFields](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#hasfields)
  - [HasFieldValue](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#hasfieldvalue)
  - [HasJsonSchema](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#hasjsonschema)
  - [In](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#in)
  - [IsArray](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isarray)
  - [IsBoolean](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isboolean)
  - [IsDateTime](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isdatetime)
  - [IsInteger](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isinteger)
  - [IsLower](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#islower)
  - [IsNumeric](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isnumeric)
  - [IsString](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isstring)
  - [IsUpper](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#isupper)
  - [JsonSchema](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#jsonschema)
  - [Less](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#less)
  - [LessOrEqual](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#lessorequal)
  - [Match](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#match)
  - [NotHasField](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#nothasfield)
  - [NotIn](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#notin)
  - [NotMatch](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#notmatch)
  - [NotNull](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#notnull)
  - [NotWithinPath](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#notwithinpath)
  - [Null](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#null)
  - [NullOrEmpty](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#nullorempty)
  - [TypeOf](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#typeof)
  - [StartsWith](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#startswith)
  - [Version](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#version)
  - [WithinPath](docs/concepts/PSRule/en-US/about_PSRule_Assert.md#withinpath)
- [Baselines](docs/concepts/PSRule/en-US/about_PSRule_Baseline.md)
  - [Baseline specs](docs/concepts/PSRule/en-US/about_PSRule_Baseline.md#baseline-specs)
  - [Baseline scopes](docs/concepts/PSRule/en-US/about_PSRule_Baseline.md#baseline-scopes)
- [Conventions](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md)
  - [Using conventions](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#using-conventions)
  - [Defining conventions](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#defining-conventions)
  - [Begin Process End blocks](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#begin-process-end-blocks)
  - [Including with options](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#including-with-options)
  - [Using within modules](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#using-within-modules)
  - [Execution order](docs/concepts/PSRule/en-US/about_PSRule_Conventions.md#execution-order)
- [Docs](docs/concepts/PSRule/en-US/about_PSRule_Docs.md)
  - [Getting documentation](docs/concepts/PSRule/en-US/about_PSRule_Docs.md#getting-documentation)
  - [Online help](docs/concepts/PSRule/en-US/about_PSRule_Docs.md#online-help)
  - [Creating documentation](docs/concepts/PSRule/en-US/about_PSRule_Docs.md#creating-documentation)
- [Options](docs/concepts/PSRule/en-US/about_PSRule_Options.md)
  - [Binding.Field](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingfield)
  - [Binding.IgnoreCase](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingignorecase)
  - [Binding.NameSeparator](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingnameseparator)
  - [Binding.PreferTargetInfo](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingprefertargetinfo)
  - [Binding.TargetName](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingtargetname)
  - [Binding.TargetType](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingtargettype)
  - [Binding.UseQualifiedName](docs/concepts/PSRule/en-US/about_PSRule_Options.md#bindingusequalifiedname)
  - [Configuration](docs/concepts/PSRule/en-US/about_PSRule_Options.md#configuration)
  - [Convention.Include](docs/concepts/PSRule/en-US/about_PSRule_Options.md#conventioninclude)
  - [Execution.LanguageMode](docs/concepts/PSRule/en-US/about_PSRule_Options.md#executionlanguagemode)
  - [Execution.InconclusiveWarning](docs/concepts/PSRule/en-US/about_PSRule_Options.md#executioninconclusivewarning)
  - [Execution.NotProcessedWarning](docs/concepts/PSRule/en-US/about_PSRule_Options.md#executionnotprocessedwarning)
  - [Input.Format](docs/concepts/PSRule/en-US/about_PSRule_Options.md#inputformat)
  - [Input.IgnoreGitPath](docs/concepts/PSRule/en-US/about_PSRule_Options.md#inputignoregitpath)
  - [Input.ObjectPath](docs/concepts/PSRule/en-US/about_PSRule_Options.md#inputobjectpath)
  - [Input.PathIgnore](docs/concepts/PSRule/en-US/about_PSRule_Options.md#inputpathignore)
  - [Input.TargetType](docs/concepts/PSRule/en-US/about_PSRule_Options.md#inputtargettype)
  - [Logging.LimitDebug](docs/concepts/PSRule/en-US/about_PSRule_Options.md#logginglimitdebug)
  - [Logging.LimitVerbose](docs/concepts/PSRule/en-US/about_PSRule_Options.md#logginglimitverbose)
  - [Logging.RuleFail](docs/concepts/PSRule/en-US/about_PSRule_Options.md#loggingrulefail)
  - [Logging.RulePass](docs/concepts/PSRule/en-US/about_PSRule_Options.md#loggingrulepass)
  - [Output.As](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputas)
  - [Output.Banner](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputbanner)
  - [Output.Culture](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputculture)
  - [Output.Encoding](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputencoding)
  - [Output.Format](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputformat)
  - [Output.Outcome](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputoutcome)
  - [Output.Path](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputpath)
  - [Output.Style](docs/concepts/PSRule/en-US/about_PSRule_Options.md#outputstyle)
  - [Rule.Include](docs/concepts/PSRule/en-US/about_PSRule_Options.md#ruleinclude)
  - [Rule.Exclude](docs/concepts/PSRule/en-US/about_PSRule_Options.md#ruleexclude)
  - [Rule.Tag](docs/concepts/PSRule/en-US/about_PSRule_Options.md#ruletag)
  - [Suppression](docs/concepts/PSRule/en-US/about_PSRule_Options.md#suppression)
- [Selectors](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md)
  - [AllOf](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#allof)
  - [AnyOf](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#anyof)
  - [Exists](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#exists)
  - [Equals](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#equals)
  - [Field](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#field)
  - [Greater](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#greater)
  - [GreaterOrEquals](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#greaterorequals)
  - [HasValue](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#hasvalue)
  - [In](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#in)
  - [Less](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#less)
  - [LessOrEquals](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#lessorequals)
  - [Match](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#match)
  - [Not](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#not)
  - [NotEquals](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#notequals)
  - [NotIn](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#notin)
  - [NotMatch](docs/concepts/PSRule/en-US/about_PSRule_Selectors.md#notmatch)
- [Variables](docs/concepts/PSRule/en-US/about_PSRule_Variables.md)
  - [$Assert](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#assert)
  - [$Configuration](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#configuration)
  - [$LocalizedData](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#localizeddata)
  - [$PSRule](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#psrule)
  - [$Rule](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#rule)
  - [$TargetObject](docs/concepts/PSRule/en-US/about_PSRule_Variables.md#targetobject)

### Schemas

PSRule uses the following schemas:

- [Options](schemas/PSRule-options.schema.json) - Schema for PSRule YAML options file.
- [Resources](schemas/PSRule-language.schema.json) - Schema for PSRule YAML resources such as baselines.

## Related projects

The following projects use or integrate with PSRule.

Name                      | Description
----                      | -----------
[PSRule.Rules.Azure]      | A suite of rules to validate Azure resources and infrastructure as code (IaC) using PSRule.
[PSRule.Rules.Kubernetes] | A suite of rules to validate Kubernetes resources using PSRule.
[PSRule.Rules.CAF]        | A suite of rules to validate Azure resources against the Cloud Adoption Framework (CAF) using PSRule.
[PSRule.Rules.GitHub]     | A suite of rules to validate GitHub repositories using PSRule.
[PSRule.Rules.MSFT.OSS]   | A suite of rules to validate repositories against Microsoft Open Source Software (OSS) requirements.
[PSRule.Monitor]          | Send and query PSRule analysis results in Azure Monitor.
[PSRule-pipelines]        | Validate infrastructure as code (IaC) and DevOps repositories using Azure Pipelines.
[ps-rule]                 | Validate infrastructure as code (IaC) and DevOps repositories using GitHub Actions.
[PSRule-vscode]           | Visual Studio Code extension for PSRule.

## Changes and versioning

Modules in this repository use [semantic versioning](http://semver.org/) to declare breaking changes.
For a list of module changes please see the [change log](CHANGELOG.md).

> Pre-release module versions are created on major commits and can be installed from the PowerShell Gallery.
> Pre-release versions should be considered experimental.
> Modules and change log details for pre-releases will be removed as stable releases are made available.

## Contributing

This project welcomes contributions and suggestions.
If you are ready to contribute, please visit the [contribution guide](CONTRIBUTING.md).

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Maintainers

- [Bernie White](https://github.com/BernieWhite)

## License

This project is [licensed under the MIT License](LICENSE).

[issue]: https://github.com/Microsoft/PSRule/issues
[discussion]: https://github.com/microsoft/PSRule/discussions
[install]: docs/install-instructions.md
[ci-badge]: https://dev.azure.com/bewhite/PSRule/_apis/build/status/PSRule-CI?branchName=main
[module-psrule]: https://www.powershellgallery.com/packages/PSRule
[extension-vscode]: https://marketplace.visualstudio.com/items?itemName=bewhite.psrule-vscode-preview
[extension-pipelines]: https://marketplace.visualstudio.com/items?itemName=bewhite.ps-rule
[extension-actions]: https://github.com/marketplace/actions/psrule
[PSRule.Rules.Azure]: https://github.com/microsoft/PSRule.Rules.Azure
[PSRule.Rules.Kubernetes]: https://github.com/microsoft/PSRule.Rules.Kubernetes
[PSRule.Rules.CAF]: https://github.com/microsoft/PSRule.Rules.CAF
[PSRule.Rules.GitHub]: https://github.com/microsoft/PSRule.Rules.GitHub
[PSRule.Rules.MSFT.OSS]: https://github.com/microsoft/PSRule.Rules.MSFT.OSS
[PSRule.Monitor]: https://github.com/microsoft/PSRule.Monitor
[PSRule-pipelines]: https://github.com/microsoft/PSRule-pipelines
[ps-rule]: https://github.com/microsoft/ps-rule
[PSRule-vscode]: https://github.com/microsoft/PSRule-vscode
[spec]: docs/specs/design-spec.md
"
158,microsoft/checkedc,C,"# Checked C
Checked C adds static and dynamic checking to C to detect or prevent common programming
errors such as buffer overruns and out-of-bounds memory accesses. 
The goal of the project is to improve systems programming by making fundamental improvements to C.
This repo contains
sample code, the [extension specification](https://github.com/Microsoft/checkedc/releases),
and test code.

- For a quick overview of Checked C, more information, and pointers to example code,
  see our [Wiki](https://github.com/Microsoft/checkedc/wiki).
- The PDF of the specification is available [here](https://github.com/Microsoft/checkedc/releases).
- Compilers are available [here](https://github.com/Microsoft/checkedc-clang/releases).
- The Checked C clang repo is
  [here](https://github.com/Microsoft/checkedc-clang).
- The instructions to build and test the Checked C compiler are documented on
  the [Checked C clang wiki](https://github.com/Microsoft/checkedc-clang/wiki).

# Publications and Presentations
- We presented a [research paper](https://www.microsoft.com/en-us/research/publication/checkedc-making-c-safe-by-extension/) on
Checked C at the [IEEE 2018 Cybersecurity Development Conference](https://secdev.ieee.org/2018/home):
""Checked C: Making C Safe by Extension"".   The paper describes the key ideas of Checked C in 8 pages. Note that we have added features to Checked C for improving type safety (and reducing type confusion)
since writing the paper.  The [Wiki](https://github.com/Microsoft/checkedc/wiki) and [specification](https://github.com/Microsoft/checkedc/releases) provide up-to-date descriptions of Checked C.

- We presented another [paper](https://www.microsoft.com/en-us/research/uploads/prod/2019/05/checkedc-post2019.pdf)
on Checked C at the [2019 Principles of Security and Trust Conference](http://www.etaps.org/2019/post): 
""Achieving Safety Incrementally With Checked C"".
This paper describes a tool for converting existing C code to use Ptr types.  It also proves a blame
property about checked regions that shows that checked regions are blameless for any memory corruption.  This proof is formalized for a core subset of the language extension.

- We presented a
[poster](https://github.com/microsoft/checkedc/blob/master/papers/posters/checkedc_for_memory_safety.pdf)
at the [LLVM Dev Meeting
2019](https://llvm.org/devmtg/2019-10/talk-abstracts.html#post6): ""Overflows Be
Gone: Checked C for Memory Safety"". The poster provides an introduction to
Checked C, outlines the compiler implementation and presents an experimental
evaluation of Checked C.

- We presented a
  [talk](https://www.youtube.com/watch?v=AIlBWIiV68U&ab_channel=LLVM) at the
[2020 LLVM Virtual Dev Meeting](https://llvm.org/devmtg/2020-09/program):
""Checked C: Adding memory safety support to LLVM"". The talk describes the
design of bounds annotations for checked pointers and array pointers as well as
the framework for the static checking of the soundness of bounds. We also
briefly describe novel algorithms to automatically widen bounds for
null-terminated arrays and for comparison of expressions for equivalence.

# Build Status

|Configuration|Testing|Status|
|--------|---------------|-------|
|Debug X86 Windows| Checked C and clang regression tests|![Debug X86 Windows status](https://msresearch.visualstudio.com/_apis/public/build/definitions/f6454e27-a46c-49d9-8453-29d89d53d2f9/211/badge)|
|Debug X64 Windows| Checked C and clang regression tests| ![Debug X64 Windows status](https://msresearch.visualstudio.com/_apis/public/build/definitions/f6454e27-a46c-49d9-8453-29d89d53d2f9/205/badge)|
|Debug X64 Linux  | Checked C and clang regression tests| ![Debug X64 Linux status](https://msresearch.visualstudio.com/_apis/public/build/definitions/f6454e27-a46c-49d9-8453-29d89d53d2f9/217/badge)|
|Release X64 Linux| Checked C, clang, and LLVM nightly tests|![Release X64 Linux status](https://msresearch.visualstudio.com/_apis/public/build/definitions/f6454e27-a46c-49d9-8453-29d89d53d2f9/238/badge)|

# Participating
We're happy to have the help! You can contribute by trying out Checked C, 
reporting bugs, and giving us feedback. There are other ways to [contribute](CONTRIBUTING.md) too.
You can join the [mailing lists](https://github.com/Microsoft/CheckedC/blob/master/MAILING-LISTS.md) for
announcements about the project.

# Licensing
The software in this repository is covered by the MIT license.  See the file LICENSE.TXT for the license.   The
Checked C specification is made available by Microsoft under the [OpenWeb Foundation Final
Specification Agreement, version 1.0](http://www.openwebfoundation.org/legal/the-owf-1-0-agreements/owfa-1-0).
Contributions of code to the Checked LLVM/clang repos are
subject to the [CLANG/LLVM licensing terms](https://github.com/Microsoft/checkedc-clang/blob/master/LICENSE.TXT).

# Code of conduct

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.
"
159,microsoft/checkedc-automation,Shell,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
160,microsoft/checkedc-llvm-test-suite,C,"# The Checked C LLVM test-suite repo

This repo contains a version of the LLVM test-suite repo that is being modified
to use Checked C. The modified programs will be used to benchmark the Checked C
version of LLVM/clang.

We have deleted test-only code from the master branch of the repo and left only
benchmarks in the master branch.  That makes the repo easier to work with.  It
decreases disk usage from about 2.3 GBytes to under 500 MBytes when using the
master branch.

Checked C is an extension to C that adds checking to detect or prevent common 
programming  errors such as out-of-bounds memory accesses.  For more information
on Checked C, see the Checked C specification in the
[Checked C repo](https://github.com/Microsoft/checkedc).  The Checked C
version of LLVM/clang lives in two repos: the
[Checked C clang repo](https://github.com/Microsoft/checked-clang)
and the [Checked C LLVM repo](https://github.com/Microsoft/checkedc-llvm).


## Branch organization

There are 3 branches in the repo:
- master: this branch contains benchmarks, some of which may have been modified
to use Checked C.
- baseline: this branch contains benchmarks that have not been modified.
- original: this contains all the tests, including application tests.

This master branch should be used for modifying benchmarks.  This branch can be diffed
against the baseline branch to see the changes in benchmarks.
The original branch can be used to test that
the Checked C implementation has not broken existing tests.

## Running tests

### On Linux
1. Setup LNT
Note: These steps have been adopted from the [LNT Quickstart Guide](http://llvm.org/docs/lnt/quickstart.html).
These instructions are for Ubuntu 20.
```
sudo apt install bison flex tclsh
sudo apt install virtualenv
sudo virtualenv ~/mysandbox
git clone https://github.com/llvm/llvm-lnt.git  ~/lnt
sudo ~/mysandbox/bin/python ~/lnt/setup.py install
```

2. Invoke LNT tests

Prerequisite: Make sure you have checked out and built the Checked C compiler.
```
git clone https://github.com/microsoft/checkedc-automation.git <AUTOMATION_DIR>
export SRC_DIR=</path/to/llvm/src>
export BUILD_DIR=</path/to/llvm/build>
<AUTOMATION_DIR>/UNIX/run-lnt-local.sh
```

Optional flags:
```
TEST_TARGET=""X86_64;ARM""
LNT_BIN=</path/to/lnt> // By default, lnt is picked up from ~/mysandbox/bin/lnt.
```

The test results are generated at:
```
<BUILD_DIR>/LNT-Results-Release-Linux/<TEST_TARGET>/test-<TIME_STAMP>/test.log
```

### On Windows
The LNT tests can also be run on Windows 10 using
the [Windows Subsystem for Linux](https://blogs.msdn.microsoft.com/wsl/2016/04/22/windows-subsystem-for-linux-overview/).
See the directions [here](docs/Benchmarking-on-Windows.md).

## Contributing

We would be happy for people to convert existing benchmarks to use Checked C.
For code contributions, we follow the standard
[Github workflow](https://guides.github.com/introduction/flow/).  See 
[Contributing to Checked C](https://github.com/Microsoft/checkedc/blob/master/CONTRIBUTING.md) for more detail.
You will need to sign a contributor license agreement before contributing a
converted benchmark.

For more information on contributing on the Checked C project, see 
[Contributing to Checked C](https://github.com/Microsoft/checkedc/blob/master/CONTRIBUTING.md).

## Code of conduct

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.
"
161,microsoft/vscode-generator-code,JavaScript,"# Yo Code - Extension and Customization Generator

[![Build Status](https://dev.azure.com/ms/vscode-generator-code/_apis/build/status/Microsoft.vscode-generator-code)](https://dev.azure.com/ms/vscode-generator-code/_build/latest?definitionId=17)

We have written a Yeoman generator to help get you started. We plan to add templates for most extension/customization types into this.

## Install the Generator

Install Yeoman and the VS Code Extension generator:

```bash
npm install -g yo generator-code
```

## Run Yo Code
The Yeoman generator will walk you through the steps required to create your customization or extension prompting for the required information.

To launch the generator simply type:

```bash
yo code
```

![The command generator](yocode.png)

## Generator Output

These templates will
* Create a base folder structure
* Template out a rough `package.json`
* Import any assets required for your extension e.g. tmBundles or the VS Code Library
* For Extensions: Set-up `launch.json` for running your extension and attaching to a process

## Command line

```
Usage:
  yo code [<destination>] [options]

Argument (optional):
  The destination to create the extension in, absolute or relative to the current working
  directory. Use '.' for the current folder.
  If not provided, defaults to a folder in the current working directory with the extension
  display name.

Options:
  -h,   --help                  # Print the generator's options and usage
  -i,   --insiders              # Show the insiders options for the generator
  -q,   --quick                 # Quick mode, skip all optional prompts and use defaults
  -o,   --open                  # Open the generated extension in Visual Studio Code
  -O,   --openInInsiders        # Open the generated extension in Visual Studio Code Insiders
  -t,   --extensionType         # ts, js, colortheme, language, snippets, keymap...
        --extensionId           # Id of the extension
        --extensionDescription  # Description of the extension
        --pkgManager            # 'npm' or 'yarn'
        --webpack               # Bundle the extension with webpack
        --gitInit               # Initialize a git repo

Example usages:
  yo code                       # Create an extension in a folder with the extension's name.
  yo code . -O                  # Create an extension in current folder and open with code-insiders
  yo code Hello -t=ts -q        # Create an TypeScript extension in './Hello', skip prompts, use defaults.
  yo code --insiders            # Show the insiders options for the generator
```

## Run Generator using Docker
If you don't want to install nodejs or any node packages, use this method to containerize the generator. \
\
Go into your project directory
```bash
cd <project directory>
```
Build the docker image from the docker file
```bash
docker build -t vscode-generator-code .
```
Create a docker container with volumes
```bash
docker run -v $(pwd):/usr/src/app vscode-generator-code
```

## History

* 1.0.0: Generates a VS Code extension for TypeScript 2.0.3
* 0.10.x: Generates a VS Code extension for TypeScript 1.8.10

## License

[MIT](LICENSE)
"
162,microsoft/azure-devops-extension-yeoman-generator,TypeScript,"# Azure DevOps extension generator

Generates a basic Azure DevOps extension with support for hot reload and debugging in VS Code. For more information about how hot reload and debugging works with Azure DevOps extensions, please see the [azure-devops-extension-hot-reload-and-debug](https://github.com/microsoft/azure-devops-extension-hot-reload-and-debug) repo, as well as the accompanying [blog post](https://devblogs.microsoft.com/devops/streamlining-azure-devops-extension-development/).

## Installation

First, install [Yeoman](http://yeoman.io) and generator-azure-devops-extension using [npm](https://www.npmjs.com/) (we assume you have pre-installed [Node.js](https://nodejs.org/)).

```shell
npm install -g yo
npm install -g @microsoft/generator-azure-devops-extension
```

Then generate your new project:

```shell
yo @microsoft/azure-devops-extension
```

## Output

Running the generator will result in the following file structure:

```text
.
├── .eslintrc.js
├── .gitignore
├── .vscode
│   └── launch.json
├── README.md
├── configs
│   ├── dev.json
│   └── release.json
├── img
│   └── world.png
├── package.json
├── src
│   └── hub
│       ├── hub.html
│       ├── hub.scss
│       └── hub.tsx
├── tsconfig.json
├── vss-extension.json
└── webpack.config.js
```

## What's next?

Now that you have generated a new project, you are ready to start debugging. Refer to the generated readme in your new project for instructions on how to get started. You should also check out our [azure-devops-extension-hot-reload-and-debug](https://github.com/microsoft/azure-devops-extension-hot-reload-and-debug#deploy-your-dev-extension-to-azure-devops) repo for an in-depth look at how these features work.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
163,microsoft/timelinestoryteller,JavaScript,"# Timeline Storyteller

![""The Daily Routines of Famous Creative People"": A Story made with Timeline Storyteller](https://github.com/Microsoft/timelinestoryteller/blob/master/public/img/dailyroutines.gif ""'The Daily Routines of Famous Creative People': A Story made with Timeline Storyteller"")

[Timeline Storyteller](https://timelinestoryteller.com/) is an expressive visual storytelling environment for presenting timelines in the browser or in [Microsoft Power BI](https://powerbi.microsoft.com/en-us/).

Use it to present different aspects of timeline data using a palette of timeline representations, scales, and layouts, as well as controls for filtering, highlighting, and annotation.

![Timeline Design dimensions](https://github.com/Microsoft/timelinestoryteller/blob/master/public/img/dims.png ""Timeline Design dimensions"")

To learn more about the research that informed this project, see [timelinesrevisited.github.io](https://timelinesrevisited.github.io/), which includes a survey of timeline tools and more than 200 bespoke timelines.

See [these examples](https://timelinestoryteller.com/#examples) of timelines and timeline stories made with Timeline Storyteller.

## Project Team

- [Matthew Brehmer](http://mattbrehmer.github.io/)
- [Bonghsin Lee](http://research.microsoft.com/en-us/um/people/bongshin/)
- [Nathalie Henry Riche](http://research.microsoft.com/en-us/um/people/nath/)
- [Darren Edge](https://www.microsoft.com/en-us/research/people/daedge/)
- [Christopher White](https://www.microsoft.com/en-us/research/people/chwh/)
- [Kate Lytvynets](mailto:kalytv@microsoft.com)
- [David Tittsworth](mailto:David.Tittsworth@microsoft.com)

## Setup / Testing

1. Clone the main branch of this repository: `git clone https://github.com/Microsoft/timelinestoryteller.git`

2. Ensure that [nodejs](https://nodejs.org/), [npm](https://www.npmjs.com/), and [yarn](https://yarnpkg.com/en/) are installed.

3. Open a terminal at the root of the repository and install node modules: `yarn` OR `npm_install`.

4. Build public/app/timelinestoryteller.js: `npm test`

5. Start the node server: `npm start`

6. Open [localhost:8000](http://localhost:8000/)

The application source code can be found in the [src/](https://github.com/Microsoft/timelinestoryteller/tree/master/src) directory.

## The Timeline Storyteller Power BI custom visual

This respository contains the source for Timeline Storyteller as a standalone web application. To generate the Timeline Storyteller custom visual for Power BI, refer to [github.com/Microsoft/PowerBI-visuals-TimelineStoryteller](https://github.com/Microsoft/PowerBI-visuals-TimelineStoryteller). 

## Preparing your data

Timeline Storyteller currently supports datasets of events in CSV, JSON, or Google Spreadsheet format.

Each event is specified by the following attributes:

- __Required__: `start_date`, date: YYYY, YYYY-MM-DD, or YYYY-MM-DD HH:MMZ (ISO 8601) formats are supported (Z necessary for specifying UTC, otherwise HH:MM will be time-zone dependent). BC dates are permitted, e.g., -27, -13800000000
- __Optional__: `end_date`, date: using same format as `start_date`
- __Optional__: `category`, a string corresponding to the category of the event (which Timeline Storyteller encodes as colour)
- __Optional__: `facet`,a string corresponding to another category of the event (which Timeline Storyteller uses to create a faceted timeline layout; `category` and `facet` can be identical if desired)
- __Optional__: `content_text`, a string description of the event (which Timeline Storyteller exposes as event annotations)

### Example event in JSON:

`{
  ""start_date"":""1775"",
  ""end_date"":""1783"",
  ""content_text"":""American Revolutionary War: an armed struggle for secession from the British Empire by the Thirteen Colonies that would subsequently become the United States."",
  ""facet"":""North America"",
  ""category"":""North America""
},`

### Example event in CSV:

header row:

`start_date,end_date,content_text,facet,category`

example event row:

`1775,1783,American Revolutionary War: an armed struggle for secession from the British Empire by the Thirteen Colonies that would subsequently become the United States.,North America,North America`

### Example CSV / Google Spreadsheet

Here is the [The Daily Routines of Famous Creative People](https://podio.com/site/creative-routines) demo dataset used in Timeline Storyteller's demo in a [Google Sheet](https://docs.google.com/spreadsheets/d/1x8N7Z9RUrA9Jmc38Rvw1VkHslp8rgV2Ws3h_5iM-I8M/pubhtml).

- Ensure that the spreadsheet is published (open the Google Spreadsheet 'File' menu, select 'Publish to the Web').
- Ensure that `start_date` and `end_date` columns are formatted as text and not as dates (e.g., `'1926-06-29`).
- __Required__: Spreadsheet URL
- __Optional__: Worksheet title (i.e., tab name) for this dataset: `dailyroutines`
- Enter the spreadsheet URL and worksheet title into Timeline Storyteller's load dialog.

## Usage

Note that more detailed usage instructions are available at [timelinestoryteller.com](https://timelinestoryteller.com/)

1. Load timeline data (demo dataset, JSON, CSV, Google Spreadsheet) or saved timeline story (a JSON Blob with extension .cdc; see step 6)

2. Select a combination of representation, scale, and layout from the menu at the top of the screen; only some combinations are valid; see [our guidance on selecting appropriate combinations for your story](http://timelinesrevisited.github.io/supplemental/gallery/). Mouseover these options to view a tooltip that describes how they might be useful.

3. Edit the canvas

	* Click on events to annotate with their `content_text` label; resize and reposition labels; SHIFT + click to highlight events without showing label.

	* Annotate with captions and images; resize and reposition captions and images.

 	* Filter events by category, facet, or segment. Filter by highlighting emphasizing matching events (de-emphasizing non-matching events).

	* You can also filter by hiding non-matching events.

4. Record current canvas as a scene, which retains labels, captions, and images. Enter playback mode, navigate to previous / next recorded scene.

5. Export current canvas as a PNG, SVG.

6. Export the scenes as an animated GIF or as a JSON Blob (.cdc extension).

## License

Timeline Storyteller

Copyright (c) Microsoft Corporation

All rights reserved.

MIT License

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the Software), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

## Acknowledgements

### Citing us

If you use Timeline Storyteller to make a timeline for a research paper, you can cite us in two ways. You can cite the tool itself:

`@misc{TimelineStoryteller,
author = {Matthew Brehmer and Bongshin Lee and Nathalie Henry Riche and Darren Edge and Christopher White and Kate Lytvynets and David Tittsworth},
title = {Microsoft Timeline Storyteller},
year = {2017},
note = {\url{https://timelinestoryteller.com}}
}`

Or you can cite our recent journal paper about the timeline design space:

`@article{Brehmer2016,
author = {Matthew Brehmer and Bongshin Lee and Benjamin Bach and Nathalie Henry Riche and Tamara Munzner},
title = {Timelines Revisited: A Design Space and Considerations for Expressive Storytelling},
journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
year = {2017},
volume = {23},
issue = {9},
pages = {2151--2164},
doi = {10.1109/TVCG.2016.2614803},
ISSN = {1077-2626}
}`

### Demo dataset provenance

- [Priestley's Chart of Biography](https://upload.wikimedia.org/wikipedia/commons/9/98/PriestleyChart.gif)
- [Great Philosophers since the 8th Century BC](http://bl.ocks.org/rengel-de/5603464)
- [History's Largest Empires](http://nowherenearithaca.github.io/empires/index.html)
- [East Asian Dynasties](http://bl.ocks.org/bunkat/2338034)
- [Epidemics since the 14th Century](https://en.wikipedia.org/wiki/List_of_epidemics)
- [Prime Ministers of Canada](http://www.downloadexcelfiles.com/ca_en/download-excel-file-list-prime-ministers-canada)
- [Presidents of France](http://www.downloadexcelfiles.com/fr_en/download-excel-file-list-presidents-france)
- [Chancellors of Germany](https://en.wikipedia.org/wiki/List_of_Chancellors_of_Germany)
- [Presidents of Italy](http://www.downloadexcelfiles.com/it_en/download-excel-file-list-presidents-italy)
- [Prime Ministers of Japan](http://www.downloadexcelfiles.com/jp_en/download-excel-file-list-prime-ministers-japan)
- [Prime Ministers of the UK](http://www.downloadexcelfiles.com/gb_en/download-excel-file-list-prime-ministers-uk)
- [Presidents of the USA](https://raw.githubusercontent.com/hitch17/sample-data/master/presidents.json)
- [C4-5 Hurricanes: 1960-2010](http://www.aoml.noaa.gov/hrd/hurdat/easyread-2011.html)
- [The Daily Routines of Famous Creative People](https://podio.com/site/creative-routines)
-['Visualizing painters' lives"" by Accurat](http://www.brainpickings.org/2013/06/07/painters-lives-accurat-giorgia-lupi/)
- ['From first published to masterpieces' by Accurat](http://www.brainpickings.org/2013/11/29/accurat-modern-library/)
- [Kurzweil's 'Countdown to Singularity'](http://www.singularity.com/images/charts/CountdowntoSingularityLog.jpg)
- ['A Perspective on Time' by mayra.artes for Wait But Why](http://visual.ly/perspective-time)
- ['Life of a Typical American' by Tim Urban for Wait But Why](http://waitbutwhy.com/2014/05/life-weeks.html)

### Noun Project icons used in the user interface

All Icons [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/us/), by name and author:

- [check-mark](https://thenounproject.com/term/check-mark/608852) (Arthur Shlain)
- [calendar](https://thenounproject.com/term/calendar/38869) (Kiril Tomilov)
- [timeline](https://thenounproject.com/term/timeline/152347) (Alecander Bickov)
- [gif-file](https://thenounproject.com/term/gif-file/446903) (Pranav Grover)
- [png-file](https://thenounproject.com/term/png-file/446907) (Pranav Grover)
- [svg-file](https://thenounproject.com/term/svg-file/446904) (Pranav Grover)
- [json-file](https://thenounproject.com/term/json-file/446959) (Pranav Grover)
- [csv-file](https://thenounproject.com/term/csv-file/446962) (Pranav Grover)
- [drive](https://thenounproject.com/term/drive/128372) (Denis Klyuchnikov)
- [grid](https://thenounproject.com/term/grid/539919) (Doejo)
- [folder](https://thenounproject.com/term/folder/43216) (iconoci)
- [filter](https://thenounproject.com/term/filter/132317) (Creative Shell)
- [image](https://thenounproject.com/term/image/332296) (Creative Shell)
- [quotation-mark](https://thenounproject.com/term/quotation-mark/378366) (Veronika Krpciarova)
- [pin](https://thenounproject.com/term/pin/172903) (Alexandr Cherkinsky)
- [eraser](https://thenounproject.com/term/eraser/3715) (Terrence Kevin Oleary)
- [invisible](https://thenounproject.com/term/invisible/506290) (Kid A)
- [book](https://thenounproject.com/term/book/861149) (Setyo Ari Wibowo)
"
164,microsoft/MCW-Cloud-native-applications,CSS,"### Let us know how we’re doing!  
Please take a moment to fill out the [Microsoft Cloud Workshop Survey](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRyEtIpX7sDdChuWsXhzKJXJUNjFBVkROWDhSSVdYT0dSRkY4UVFCVzZBVy4u) and help us improve our offerings.

# Cloud-native applications

Fabrikam Medical Conferences provides conference web site services, tailored to the medical community. Their business has grown and the management of many instances of the code base and change cycle per tenant has gotten out of control.

The goal of this workshop is to help them build a proof of concept (POC) that will migrate their code to a more manageable process that involves containerization of tenant code, a better DevOps workflow, and a simple lift-and-shift story for their database backend.

November 2020

## Target Audience

- Application developer
- Infrastructure architect

## Abstracts

### Workshop

In this workshop, you will build a proof of concept (POC) that will transform an existing on-premises application to a container-based application. This POC will deliver a multi-tenant web app hosting solution leveraging Azure Kubernetes Service (AKS), Docker containers on Linux nodes, and a migration from MongoDB to CosmosDB.

At the end of this workshop, you will be better able to improve the reliability of and increase the release cadence of your container-based applications through time-tested DevOps practices.

### Whiteboard Design Session

In this whiteboard design session, you will learn about the choices related to building and deploying containerized applications in Azure, critical decisions around this, and other aspects of the solution, including ways to lift-and-shift parts of the application to reduce applications changes.

By the end of this design session, you will be better able to design solutions that target Azure Kubernetes Service (AKS) and define a DevOps workflow for containerized applications.

### Hands-on Lab

This hands-on lab is designed to guide you through the process of building and deploying Docker images to the Kubernetes platform hosted on Azure Kubernetes Services (AKS), in addition to learning how to work with dynamic service discovery, service scale-out, and high-availability.

At the end of this lab, you will be better able to build and deploy containerized applications to Azure Kubernetes Service and perform common DevOps procedures.

## Azure services and related products

- Azure Kubernetes Service (AKS)
- Azure Container Registry
- GitHub
- Docker
- Cosmos DB (including MongoDB API)

## Azure solutions

App Modernization

## Related references

- [MCW](https://github.com/Microsoft/MCW)

## Help & Support

We welcome feedback and comments from Microsoft SMEs & learning partners who deliver MCWs.  

***Having trouble?***

- First, verify you have followed all written lab instructions (including the Before the Hands-on lab document).
- Next, submit an issue with a detailed description of the problem.
- Do not submit pull requests. Our content authors will make all changes and submit pull requests for approval.

If you are planning to present a workshop, *review and test the materials early*! We recommend at least two weeks prior.

### Please allow 5 - 10 business days for review and resolution of issues.
"
165,microsoft/secure-data-sandbox,TypeScript,"# Secure Data Sandbox ![.github/workflows/ci.yml](https://github.com/microsoft/secure-data-sandbox/workflows/.github/workflows/ci.yml/badge.svg)

**`SDS` IS UNDER CONSTRUCTION AND NOT USABLE AT THIS POINT.
THIS PAGE WILL BE UPDATED AS FUNCTIONALITY BECOMES AVAILABLE.**

`SDS` is a secure execution environment for conducting machine learning trials against confidential data.

The goal of `SDS` is to enable collaboration between data scientists and organizations with interesting problems.
The challenge is that interesting problems come with interesting data sets that are almost always proprietary. These data sets are rich with trade secrets and personably identifiable information, and are usually encumbered by contracts, regulated by statute, and subject to corporate data stewardship policies.

In-house data science departments know how to work with this data, but the compliance issues make it is hard for them to collaborate with third parties and experts from industry and academia.

`SDS` aims to solve this problem by creating a sandbox for machine learning experiments inside the environment that hosts sensitive data.
With `SDS`, an organization can host machine learning challenges and invite third parties to submit solutions for evaluation against sensitive data that would otherwise be unavailable.

## Try SDS

### Building SDS
`SDS` is a [Node.js](https://nodejs.org/en/) project,
written in [TypeScript](https://www.typescriptlang.org/).
In order to use `SDS` you must have
[Node](https://nodejs.org/en/download/) installed on your machine.
`SDS` has been tested with Node version [12.16.3](https://nodejs.org/download/release/v12.16.3/).

Here are the steps for cloning and building `SDS`:
~~~
% git clone https://github.com/microsoft/secure-data-sandbox.git
% npm install
% npm run compile
~~~

### Running SDS Locally
Now that we've built `SDS`, let's run a local instance of the Laboratory service.
This local instance does not have a worker pool, so it won't be able to actually run tests, but it allows you to get a feel for the CLI commands. Note that the local instance does not run in a secure environment.

Open two shell windows. In the first window, start the laboratory service:
~~~
% npm run laboratory
~~~

We can run the CLI run the second shell window. Let's start with the `help` command:
~~~
% npm run cli help

Usage: sds [options] [command]

Secure Data Sandbox CLI

Options:
  -h, --help                   display help for command

Commands:
  connect [service]            Connect to a Laboratory [service] or print connection info.
  create <type> <spec>         Create a benchmark, candidate, or suite from a specification where <type> is either ""benchmark"", ""candidate"", or
                               ""suite"".
  demo                         Configures Laboratory service with demo data.
  deploy <server>              NOT YET IMPLEMENTED. Deploy a Laboratory service.
  examples                     Show usage examples.
  list <type>                  Display summary information about benchmarks, candidates, runs, and suites.
  results <benchmark> <suite>  Display the results of all runs against a named benchmark and suite.
  run <candidate> <suite>      Run a named <candidate> against a named <suite>.
  show <type> [name]           Display all benchmarks, candidates, suites, or runs. If optional [name] is specified, only show matching items.
  help [command]               display help for command

For more information and examples, see https://github.com/microsoft/secure-data-sandbox/blob/main/laboratory/README.md
~~~

The first thing we need to do is connect the CLI to the laboratory service that we just started. Currently `packages/laboratory/dist/main.js` listens on port 3000 of localhost.
~~~
% npm run cli connect http://localhost:3000

Connected to http://localhost:3000/.
~~~
This writes the connection information to `~/.sds`, which is consulted every time the CLI is run. If you don't connect to a Laboratory, you will get the following error:
~~~
% npm run cli list benchmark

Error: No laboratory connection. Use the ""connect"" command to specify a laboratory.
~~~

Now that we're connected to a Laboratory service,
we can use the `demo` command to populate the server with sample data, including
* A `benchmark`
* A `candidate`
* A `suite`
* Two `runs` with results.

~~~
% npm run cli demo

=== Sample benchmark ===
name: benchmark1
author: author1
apiVersion: v1alpha1
stages:
  - name: candidate
    kind: candidate
    volumes:
      - name: training
        path: /input
  - name: scoring
    image: benchmark-image
    kind: container
    volumes:
      - name: reference
        path: /reference


=== Sample candidate ===
name: candidate1
author: author1
apiVersion: v1alpha1
benchmark: benchmark1
image: candidate1-image


=== Sample suite ===
name: suite1
author: author1
apiVersion: v1alpha1
benchmark: benchmark1
volumes:
  - name: training
    type: AzureBlob
    target: 'https://sample.blob.core.windows.net/training'
  - name: reference
    type: AzureBlob
    target: 'https://sample.blob.core.windows.net/reference'


Initiated run 0db6c510-d059-11ea-ab64-31e44163fc86
Initiated run 0dba4780-d059-11ea-ab64-31e44163fc86
~~~

If we didn't want to use the built-in `demo` command, we could have created the benchmark, candidate, suite, and runs manually as follows:
~~~
% npm run cli create benchmark sample-data/benchmark1.yaml
benchmark created

% npm run cli create candidate sample-data/candidate1.yaml
candidate created

% npm run cli create suite sample-data/suite1.yaml
suite created

% npm run cli run candidate1 suite1
Scheduling run 1dae9970-d059-11ea-ab64-31e44163fc86

% npm run cli run candidate1 suite1
Scheduling run 1fbe1880-d059-11ea-ab64-31e44163fc86
~~~

The `demo` command does one thing we can't do through the CLI, and that is to pretend to be a worker and report status for the runs.

**List benchmarks, candidates, suites**

~~~
% npm run cli list benchmark
name         submitter   date
benchmark1   author1     2020-07-27 22:32:28 UTC

% npm run cli list candidate
name         submitter   date  
candidate1   author1     2020-07-27 22:32:28 UTC

% npm run cli list suite
name     submitter   date
suite1   author1     2020-07-27 22:32:28 UTC
~~~

**Show benchmarks, candidates, suites**
~~~
% npm run cli show benchmark benchmark1
stages:
  - name: candidate
    kind: candidate
    volumes:
      - name: training
        path: /input
  - name: scoring
    kind: container
    image: benchmark-image
    volumes:
      - name: reference
        path: /reference
name: benchmark1
author: author1
apiVersion: v1alpha1
createdAt: 2020-07-27T22:32:28.865Z
updatedAt: 2020-07-27T22:32:43.284Z


% npm run cli show candidate candidate1
name: candidate1
author: author1
apiVersion: v1alpha1
benchmark: benchmark1
image: candidate1-image
createdAt: 2020-07-27T22:32:28.883Z
updatedAt: 2020-07-27T22:32:47.384Z


% npm run cli show suite suite1
volumes:
  - name: training
    type: AzureBlob
    target: 'https://sample.blob.core.windows.net/training'
  - name: reference
    type: AzureBlob
    target: 'https://sample.blob.core.windows.net/reference'
name: suite1
author: author1
apiVersion: v1alpha1
benchmark: benchmark1
createdAt: 2020-07-27T22:32:28.889Z
updatedAt: 2020-07-27T22:32:50.623Z
~~~

**List runs**
~~~
% npm run cli list run
name                                   submitter   date                      candidate    suite    status   
0db6c510-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:28 UTC   candidate1   suite1   completed
0dba4780-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:28 UTC   candidate1   suite1   completed
1dae9970-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:55 UTC   candidate1   suite1   created  
1fbe1880-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:59 UTC   candidate1   suite1   created  
~~~

**Displaying Run Results**
~~~
% npm run cli results benchmark1 suite1

run                                    submitter   date                      passed   failed   skipped
0db6c510-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:28 UTC        5        6       ---
0dba4780-d059-11ea-ab64-31e44163fc86   unknown     2020-07-27 22:32:28 UTC        3      ---         7
~~~

## Deploying SDS to the cloud

TODO

## [Developer Guide](docs/development.md)

For developers looking to modify SDS itself, please refer to the [developer guide](docs/development.md)

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

"
166,microsoft/vscode-docs,CSS,"<p align=""center"">
  <img alt=""vscode logo"" src=""images/logo-stable.png"" width=""100px"" />
  <h1 align=""center"">Visual Studio Code Documentation</h1>
</p>

You've found the Visual Studio Code documentation GitHub repository, which contains the content for the [Visual Studio Code documentation](https://code.visualstudio.com/docs).

Topics submitted here will be published to the [Visual Studio Code](https://code.visualstudio.com) portal.

If you are looking for the VS Code product GitHub repository, you can find it [here](https://github.com/microsoft/vscode).

## Index

- [Index](#index)
- [Visual Studio Code](#visual-studio-code)
- [Feedback](#feedback)
- [Documentation Issues](#documentation-issues)
- [Contributing](#contributing)
  - [Workflow](#workflow)
  - [Cloning](#cloning)
    - [Cloning without binary files](#cloning-without-binary-files)
- [Publishing](#publishing)

## Visual Studio Code

[VS Code](https://code.visualstudio.com/) is a lightweight source code editor and powerful development environment for building and debugging modern web, mobile and cloud applications. It is free and available on your favorite platform - Linux, macOS, and Windows.

If you landed here looking for other information about VS Code, head over to [our website](https://code.visualstudio.com) for additional information.

## Feedback

If you want to give documentation feedback, please use the feedback control located at the bottom of each documentation page.

## Documentation Issues

To enter documentation bugs, please create a [new GitHub issue](https://github.com/microsoft/vscode-docs/issues). Please check if there is an existing issue first.

If you think the issue is with the VS Code product itself, please enter issues in the VS Code product repo [here](https://github.com/microsoft/vscode/issues).

## Contributing

To contribute new topics/information or make changes to existing documentation, please read the [Contributing Guideline](./CONTRIBUTING.md#contributing).

### Workflow

The two suggested workflows are:

- For small changes, use the ""Edit"" button on each page to edit the Markdown file directly on GitHub.
- If you plan to make significant changes or preview the Markdown files in VS Code, [clone](#cloning) the repo to [edit and preview](https://code.visualstudio.com/docs/languages/markdown) the files directly in VS Code.

![Markdown Preview Button](images/MDPreviewButton.png)

### Cloning

1. Install [Git LFS](https://git-lfs.github.com/).
2. Run `git lfs install` to setup global git hooks. You only need to run this once per machine.
3. `git clone git@github.com:Microsoft/vscode-docs.git`.
4. Now you can `git add` binary files and commit them. They'll be tracked in LFS.

#### Cloning without binary files

You might want to clone the repo without the 1.6GB images. Here are the steps:

1. Install [Git LFS](https://git-lfs.github.com/).
2. Run `git lfs install` to setup global git hooks. You only need to run this once per machine.
3. Clone the repo without binary files.
    - macOS / Linux: `GIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:Microsoft/vscode-docs.git`.
    - Windows: `$env:GIT_LFS_SKIP_SMUDGE=""1""; git clone git@github.com:Microsoft/vscode-docs.git`.
4. Now you can selectively checkout some binary files to work with. For example:
    - `git lfs pull -I ""docs/nodejs""` to only download images in `docs/nodejs`
    - `git lfs pull -I ""release-notes/images/1_4*/*""` to only download images in `release-notes/images/1_4*`
    - `git lfs pull -I ""docs,api""` to download all images in `docs` and in `api`
    - `git lfs pull -I <PATTERN>`, as long as `<PATTERN>` is a valid [Git LFS Include and Exclude pattern](https://github.com/git-lfs/git-lfs/blob/main/docs/man/git-lfs-fetch.1.ronn#include-and-exclude).

The history of this repo before we adopted LFS can be found at [microsoft/vscode-docs-archive](https://github.com/microsoft/vscode-docs-archive).

## Publishing

Steps for how to publish documentation changes can be found [here](https://github.com/microsoft/vscode-website#publishing-a-documentation-change) in the (private) repository of the VS Code website.
"
167,microsoft/msquic,C,"MsQuic
======

MsQuic is a Microsoft implementation of the [IETF QUIC](https://datatracker.ietf.org/wg/quic/about/)
protocol. It is cross platform, written in C and designed to be a general purpose QUIC library.

> **Important** The QUIC protocol is not an official RFC yet. It has been approved by the IESG and now is in the RFC editor queue (final step).

IETF Drafts: [Transport](https://tools.ietf.org/html/draft-ietf-quic-transport), [TLS](https://tools.ietf.org/html/draft-ietf-quic-tls), [Recovery](https://tools.ietf.org/html/draft-ietf-quic-recovery), [Datagram](https://tools.ietf.org/html/draft-ietf-quic-datagram), [Load Balancing](https://tools.ietf.org/html/draft-ietf-quic-load-balancers), [Version Negotiation](https://tools.ietf.org/html/draft-ietf-quic-version-negotiation)

[![Build Status](https://dev.azure.com/ms/msquic/_apis/build/status/CI?branchName=main)](https://dev.azure.com/ms/msquic/_build/latest?definitionId=347&branchName=main) [![Test Status](https://img.shields.io/azure-devops/tests/ms/msquic/347/main)](https://dev.azure.com/ms/msquic/_build/latest?definitionId=347&branchName=main) [![Perf Dashboard](https://img.shields.io/static/v1?label=Performance&message=Dashboard&color=blueviolet)](https://microsoft.github.io/msquic/) [![Code Coverage](https://img.shields.io/azure-devops/coverage/ms/msquic/347/main)](https://dev.azure.com/ms/msquic/_build/latest?definitionId=347&branchName=main) ![CodeQL](https://github.com/microsoft/msquic/workflows/CodeQL/badge.svg?branch=main) [![Language grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/microsoft/msquic.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/microsoft/msquic/context:cpp) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4846/badge)](https://bestpractices.coreinfrastructure.org/projects/4846) [![Discord](https://img.shields.io/discord/827744285595271168?label=Discord&logo=discord&logoColor=white&color=7289DA)](https://discord.gg/YGAtCwTSsc)

## Protocol Features

QUIC has many benefits when compared to existing ""TLS over TCP"" scenarios:

  * All packets are encrypted and handshake is authenticated with TLS 1.3.
  * Parallel streams of (reliable and unreliable) application data.
  * Exchange application data in the first round trip (0-RTT).
  * Improved congestion control and loss recovery.
  * Survives a change in the clients IP address or port.
  * Stateless load balancing.
  * Easily extendable for new features and extensions.

## Library Features

MsQuic has several features that differentiates it from other QUIC implementations:

  * Optimized for client and server.
  * Optimized for maximal throughput and minimal latency.
  * Asynchronous IO.
  * Receive side scaling ([RSS](https://docs.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to-receive-side-scaling)) support.
  * UDP send and receive coalescing support.

# Documentation

  * For frequently asked questions, see the [FAQs](./docs/FAQ.md).
  * For platform support details, see the [Platforms docs](./docs/Platforms.md).
  * For release details, see the [Release docs](./docs/Release.md).
  * For performance data, see the [Performance dashboard](https://aka.ms/msquicperformance).
  * For building the MsQuic library, see the [Build docs](./docs/BUILD.md).
  * For using the MsQuic API, see the [API docs](./docs/API.md) or the [Sample](./src/tools/sample/sample.cpp).
  * For deploying with MsQuic, see the [Deployment docs](./docs/Deployment.md).
  * For diagnosing MsQuic, see the [Diagnostics docs](./docs/Diagnostics.md) and the [Trouble Shooting Guide](./docs/TSG.md).

# Contributing

For information on contributing, please see our [contribution guidlines](./.github/CONTRIBUTING.md).
"
168,microsoft/just,TypeScript,"# Just

[![npm version](https://badge.fury.io/js/just-task.svg)](https://badge.fury.io/js/just-task)
[![NPM Downloads](https://img.shields.io/npm/dm/just-task.svg?style=flat)](https://www.npmjs.com/package/just-task)

`Just` is a library that organizes build tasks for your JS projects. It consists of

- a build task build definition library
- sane preset build flows for node and browser projects featuring TypeScript, Webpack and jest
- project scaffold tool that generates no-ejection needed repos that tracks template changes

# Documentation

All the documentation is online at https://microsoft.github.io/just/

# Building

This README contains only the instructions on how to build and contribute to the project. This is a monorepo that uses the [lerna](https://github.com/lerna/lerna) monorepo management utility. To get started, simply run the following:

`yarn`

and build all the packages this way:

`yarn build`

Development is usually done one package at a time. So go into each package and develop with the innerloop npm script:

```
cd packages/just-task
yarn dev
```

Tests are run with the `test` npm script:

```
cd packages/just-task
yarn test
```

# Packages

| Package            | Description                                                                             |
| ------------------ | --------------------------------------------------------------------------------------- |
| create-just        | Invoked by `npm init just`, a tool that scaffolds project repos                         |
| just-task          | The task definition library that wraps `undertaker` and `yargs` libraries               |
| just-scripts       | A reusable preset of frequently used tasks in node and browser projects                 |
| just-stack-\*      | A set of templates to be used by the scaffold tool `create-just`                        |
| just-scripts-utils | A set of utilities that are shared between `just-scripts` and `create-just`             |
| just-task-logger   | A shared pretty logger used to display timestamps along with a message                  |
| documentation      | The Docusaurus site content and styles which generates the Github page for this library |

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com. Please refer [Contribution guide](https://github.com/microsoft/just/.github/CONTRIBUTING.md) for more details

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
169,microsoft/accessibility-insights-web,TypeScript,"<!--
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
-->

## ![Product Logo](./src/icons/brand/blue/brand-blue-48px.png) Accessibility Insights for Web & Android

[![Build Status](https://dev.azure.com/accessibility-insights/accessibility-insights-web/_apis/build/status/accessibility-insights-web%20CI?branchName=main)](https://dev.azure.com/accessibility-insights/accessibility-insights-web/_build/latest?definitionId=37&branchName=main)
[![codecov](https://codecov.io/gh/microsoft/accessibility-insights-web/branch/main/graph/badge.svg)](https://codecov.io/gh/microsoft/accessibility-insights-web)
[![Chrome Web Store](https://img.shields.io/chrome-web-store/v/pbjjkligggfmakdaogkfomddhfmpjeni.svg?label=Version)](https://chrome.google.com/webstore/detail/accessibility-insights-fo/pbjjkligggfmakdaogkfomddhfmpjeni)
[![Chrome Web Store](https://img.shields.io/chrome-web-store/users/pbjjkligggfmakdaogkfomddhfmpjeni.svg)](https://chrome.google.com/webstore/detail/accessibility-insights-fo/pbjjkligggfmakdaogkfomddhfmpjeni)
[![Chrome Web Store](https://img.shields.io/chrome-web-store/stars/pbjjkligggfmakdaogkfomddhfmpjeni.svg)](https://chrome.google.com/webstore/detail/accessibility-insights-fo/pbjjkligggfmakdaogkfomddhfmpjeni/reviews)
[![Dependabot Status](https://api.dependabot.com/badges/status?host=github&repo=microsoft/accessibility-insights-web)](https://dependabot.com)

Two projects are built from this repository:

-   **Accessibility Insights for Web** is a browser extension for Google Chrome and the new Microsoft Edge, used for assessing the accessibility of web sites and web applications.
-   **Accessibility Insights for Android** is a cross-platform desktop tool used for testing accessibility of Android applications.

### Install Accessibility Insights for Web

-   ![Canary Logo](./src/icons/brand/red/brand-red-16px.png) [Canary](https://chrome.google.com/webstore/detail/hbcplehnakffdldhldncjlnbpfgogbem) (released continuously)
-   ![Insider Logo](./src/icons/brand/violet/brand-violet-16px.png) [Insider](https://chrome.google.com/webstore/detail/nnmjfbmebeckhpejobgjjjnchlljiagp) (on feature completion)
-   ![Production Logo](./src/icons/brand/blue/brand-blue-16px.png) [Production](https://chrome.google.com/webstore/detail/pbjjkligggfmakdaogkfomddhfmpjeni) (after validation in Insider)

### Install Accessibility Insights for Android

-   MacOS ([Canary](https://aka.ms/accessibility-insights-for-android/downloads/CanaryMacOS), [Insider](https://aka.ms/accessibility-insights-for-android/downloads/InsiderMacOS), [Production](https://aka.ms/accessibility-insights-for-android/downloads/MacOS))
-   Windows ([Canary](https://aka.ms/accessibility-insights-for-android/downloads/CanaryWindows), [Insider](https://aka.ms/accessibility-insights-for-android/downloads/InsiderWindows), [Production](https://aka.ms/accessibility-insights-for-android/downloads/Windows))
-   Linux ([Canary](https://aka.ms/accessibility-insights-for-android/downloads/CanaryLinux), [Insider](https://aka.ms/accessibility-insights-for-android/downloads/InsiderLinux), [Production](https://aka.ms/accessibility-insights-for-android/downloads/Linux))

## Data/Telemetry

By opting into telemetry, you [help the community](https://go.microsoft.com/fwlink/?linkid=2077765) develop inclusive software. We collect anonymized data to identify the top accessibility issues found by the users. This will help focus the accessibility tools and standards community to improve guidelines, rules engines, and features.

This project collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://privacy.microsoft.com/en-us/privacystatement) to learn more.

## Reporting security vulnerabilities

If you believe you have found a security vulnerability in this project, please follow [these steps](https://technet.microsoft.com/en-us/security/ff852094.aspx) to report it. For more information on how vulnerabilities are disclosed, see [Coordinated Vulnerability Disclosure](https://technet.microsoft.com/en-us/security/dn467923).

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.

## FAQ

Please visit our [FAQ](https://accessibilityinsights.io/docs/en/web/reference/faq) page.

## Contributing

All contributions are welcome! To get started, please read through our [CONTRIBUTING](./CONTRIBUTING.md) guidelines for this project. After that, see:

-   [Git branch setup](./docs/git-branch-setup.md)
-   [Building Accessibility Insights for Web](./docs/building-web.md)
-   [Building Accessibility Insights for Android (Unified)](./docs/building-unified.md)

## Code of Conduct

Please read through our [Code of Conduct](./CODE_OF_CONDUCT.md) to this project.
"
170,microsoft/flamegrill,HTML,"# flamegrill

flame grill your webpages for easy digestion

## Prerequisites

web page to test

## Usage

```
flamegrill [command] [options]
```

## Commands

### cook (default)

run flamegrill against specified input

## Options

### --name, -n

name for given scenario

### --scenario, -s

URL for scenario under test

### --baseline, -b

optional baseline scenario to compare against

### --temp-dir, -t

location to store intermediate files (default: cwd)

### --out-dir, -o

location to store test results (default: cwd)

### --help, -?, -h

help message

## Examples

The following invocations perform the tests using a scenario that you can find [here](https://github.com/OfficeDev/office-ui-fabric-react/blob/master/apps/perf-test/src/scenarios/SplitButtonNew.tsx).

```
$ flamegrill cook -n SplitButton -s ""http://fabricweb.z5.web.core.windows.net/pr-deploy-site/refs/heads/master/perf-test/index.html?scenario=SplitButtonNew&iterations=5000""

$ flamegrill cook -n SplitButton -s ""http://fabricweb.z5.web.core.windows.net/pr-deploy-site/refs/heads/master/perf-test/index.html?scenario=SplitButtonNew&iterations=5000"" -b ""http://fabricweb.z5.web.core.windows.net/pr-deploy-site/refs/heads/master/perf-test/index.html?scenario=SplitButton&iterations=5000""

$ flamegrill cook -n SplitButtonNew -s ""http://fabricweb.z5.web.core.windows.net/pr-deploy-site/refs/heads/master/perf-test/index.html?scenario=SplitButtonNew&iterations=5000"" -o out -t temp
```

## Open Source Credits

[Flamebearer](https://github.com/mapbox/flamebearer) is an inspiration for this project and is used to generate flamegraphs. Parts of Flamebearer have been modified and expanded upon to add more functionality to the flamegraphs.

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
171,microsoft/azure-iot-developer-kit,,"# Microsoft Azure IoT Developer Kit

Visit the project overview page to get started: [https://microsoft.github.io/azure-iot-developer-kit/](https://microsoft.github.io/azure-iot-developer-kit/)

## Repository Structure

* [/docs](https://github.com/Microsoft/azure-iot-developer-kit/tree/master/docs) - Project home page with all other documents

## Contributing

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
172,microsoft/QuantumLibraries,Q#,"# Microsoft Quantum Development Kit Libraries #

Welcome to the Microsoft Quantum Development Kit!

This repository contains open-source libraries for the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum):

- **[Docs/](./Docs)**: Additional documentation for developing on the libraries. Please see [QDK online documentation](https://docs.microsoft.com/azure/quantum/) for online documentation.
- **[Standard/](./Standard)**: Q# sources used to implement [the Q# standard libraries](https://docs.microsoft.com/azure/quantum/user-guide/libraries/standard).
- **[Chemistry/](./Chemistry)**: Q# and C# sources used to implement a library for [quantum chemistry](https://docs.microsoft.com/azure/quantum/user-guide/libraries/chemistry) and Hamiltonian simulation.
- **[Numerics/](./Numerics)**: Q# sources used to implement the [quantum numerics library](https://docs.microsoft.com/azure/quantum/user-guide/libraries/numerics).
- **[LICENSE](./LICENSE.txt)**: Terms of use and license details for the Quantum Development Kit libraries.

## New to Quantum? ##

See the [introduction to quantum computing](https://docs.microsoft.com/azure/quantum/concepts-overview/) provided with the Quantum Development Kit.

## Getting Started ##

The libraries provided in this repository are built using [.NET Core](https://docs.microsoft.com/en-us/dotnet/core/) and the
[Quantum Development Kit](https://docs.microsoft.com/azure/quantum).
Please see the [installation guide](https://docs.microsoft.com/azure/quantum/install-overview-qdk) for how to get up and running.

You may also visit our [Quantum](https://github.com/Microsoft/Quantum) repository, which offers a wide variety
of samples on how to use these libraries to write quantum based programs.

## Build Status ##

| branch | status    |
|--------|-----------|
| main | [![Build Status](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_apis/build/status/Microsoft.QuantumLibraries?branchName=main)](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_build/latest?definitionId=1&branchName=main) |

## Feedback ##

If you have feedback about the content in this repository, please let us know by filing a [new issue](https://github.com/microsoft/quantumlibraries/issues/new/choose)!
If you have feedback about some other part of the Microsoft Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview/) for more information.

## Contributing ##

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## [Optional] Using Prerelease Versions ##

If you're interested in helping test the Quantum Development Kit libraries, or if you want to try out new features before they are released, you can add the [Quantum Development Kit prerelease feed](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_packaging?_a=feed&feed=alpha) to your .NET Core SDK configuration.
Packages on the prerelease feed are marked with `-alpha` in their version number, so that projects built using released versions of Quantum Development Kit libraries will not be affected.
Note that the prerelease feed is used automatically when building libraries in this repository.

To use the prerelease feed, edit your `NuGet.Config` file to include the prerelease feed URL (`https://pkgs.dev.azure.com/ms-quantum-public/Microsoft Quantum (public)/_packaging/alpha/nuget/v3/index.json`) as a package source.
The location of this file varies depending on your operating system:

| OS | NuGet config file location |
|----|----------------------------|
| Windows | `$Env:APPDATA/Roaming/NuGet/NuGet.Config` |
| macOS / Linux | `~/.config/NuGet/NuGet.Config` or `~/.nuget/NuGet/NuGet.Config` |

Note that this file may not already exist, depending on your configuration.

For example, the following `NuGet.Config` file includes both the main NuGet package feed, and the Quantum Development Kit prerelease feed:

```xml
<?xml version=""1.0"" encoding=""utf-8""?>
<configuration>
  <packageSources>
    <add key=""nuget.org"" value=""https://api.nuget.org/v3/index.json"" protocolVersion=""3"" />
    <add key=""qdk-alpha"" value=""https://pkgs.dev.azure.com/ms-quantum-public/Microsoft Quantum (public)/_packaging/alpha/nuget/v3/index.json"" protocolVersion=""3"" />
  </packageSources>
</configuration>
```
"
173,microsoft/QuantumKatas,Jupyter Notebook,"# Introduction

The Quantum Katas are a collection of self-paced tutorials and programming exercises to help you learn quantum computing and Q# programming.

Each kata is a separate set of exercises that includes:

* A sequence of tasks progressing from easy to hard.
  Each task requires you to fill in some code. The first task might require just one line, and the last one might require rather complicated code.
* A testing framework that sets up, runs, and validates your solutions.
  Each task is covered by a [unit test](https://docs.microsoft.com/visualstudio/test/getting-started-with-unit-testing) which initially fails. Once you write the code to make the test pass, you can move on to the next task.
* Links to quantum computing and Q# reference material you might need to solve the tasks.
* Hints, reference solutions and detailed explanations to help you if you're stuck.

The Quantum Katas also include *tutorials* that introduce the learner to the basic concepts and algorithms used in quantum computing, starting with the necessary math (complex numbers and linear algebra). They follow the same pattern of supplementing the theory with Q# demos and hands-on programming exercises. 

## Table of contents ##

* [Learning path](#learning-path)
* [Run the katas and tutorials online](#run-online)
* [Run the katas locally](#kata-locally)
  * [Quantum Development Kit installation](#install)
  * [Download the Quantum Katas](#download)
  * [Run a kata as a Jupyter Notebook](#kata-as-notebook)
  * [Run a kata as a Q# project](#kata-as-project)
  * [Run kata tests](#tests)
  * [Run katas locally with Docker](#docker)
* [Contributing](#contributing)
* [Code of Conduct](#code-of-conduct)

## Learning path <a name=""learning-path"" /> ##

Here is the learning path we suggest you to follow if you are starting to learn quantum computing and quantum programming. Once you're comfortable with the basics, you're welcome to jump ahead to the topics that pique your interest!

#### Quantum Computing Concepts: Qubits and Gates

* **[Complex arithmetic (tutorial)](./tutorials/ComplexArithmetic/)**.
  Learn about complex numbers and the mathematics required to work with quantum computing.
* **[Linear algebra (tutorial)](./tutorials/LinearAlgebra/)**.
  Learn about vectors and matrices used to represent quantum states and quantum operations.
* **[The qubit (tutorial)](./tutorials/Qubit/)**.
  Learn what a qubit is.
* **[Single-qubit gates (tutorial)](./tutorials/SingleQubitGates/)**.
  Learn what a quantum gate is and about the most common single-qubit gates.
* **[Basic quantum computing gates](./BasicGates/)**.
  Learn to apply the most common gates used in quantum computing.
* **[Multi-qubit systems (tutorial)](./tutorials/MultiQubitSystems/)**.
  Learn to represent multi-qubit systems.
* **[Multi-qubit gates (tutorial)](./tutorials/MultiQubitGates/)**.
  Learn about the most common multi-qubit gates.
* **[Superposition](./Superposition/)**.
  Learn to prepare superposition states.

#### Quantum Computing Concepts: Measurements

* **[Single-qubit measurements (tutorial)](./tutorials/SingleQubitSystemMeasurements/)**.
  Learn what quantum measurement is and how to use it for single-qubit systems.
* **[Measurements](./Measurements/)**.
  Learn to distinguish quantum states using measurements.
* **[Distinguish unitaries](./DistinguishUnitaries/)**.
  Learn to distinguish unitaries by designing and performing experiments with them.
* **[Joint measurements](./JointMeasurements/)**.
  Learn about using joint (parity) measurements to distinguish quantum states and to perform state transformations.

#### Simple Algorithms

* **[Random number generation (tutorial)](./tutorials/RandomNumberGeneration/)**.
  Learn to generate random numbers using the principles of quantum computing.
* **[Teleportation](./Teleportation/)**.
  Implement standard teleportation protocol and its variations.
* **[Superdense coding](./SuperdenseCoding/)**.
  Implement the superdense coding protocol.

#### Quantum Oracles and Simple Oracle Algorithms

* **[Quantum oracles (tutorial)](./tutorials/Oracles/)**.
  Learn to implement classical functions as equivalent quantum oracles. 
* **[Exploring Deutsch–Jozsa algorithm (tutorial)](./tutorials/ExploringDeutschJozsaAlgorithm/)**.
  Learn to implement classical functions and equivalent quantum oracles, and compare the quantum
  solution to the Deutsch–Jozsa problem to a classical one.
* **[Deutsch–Jozsa algorithm](./DeutschJozsaAlgorithm/)**.
  Learn about quantum oracles which implement classical functions, and implement Bernstein–Vazirani and Deutsch–Jozsa algorithms.
* **[Simon's algorithm](./SimonsAlgorithm/)**.
  Learn about Simon's algorithm.

#### Grover's search algorithm

* **[Implementing Grover's algorithm](./GroversAlgorithm/)**.
  Learn about Grover's search algorithm and how to write quantum oracles to use with it.
* **[Exploring Grover's search algorithm (tutorial)](./tutorials/ExploringGroversAlgorithm/)**.
  Learn more about Grover's search algorithm, picking up where the [Grover's algorithm kata](./GroversAlgorithm/) left off.
* **[Solving SAT problems using Grover's algorithm](./SolveSATWithGrover/)**.
  Explore Grover's search algorithm, using SAT problems as an example. Learn to implement quantum oracles based on the problem description instead of a hard-coded answer. Use Grover's algorithm to solve problems with an unknown number of solutions.
* **[Solving graph coloring problems using Grover's algorithm](./GraphColoring/)**.
  Continue the exploration of Grover's search algorithm, using graph coloring problems as an example.

#### Tools and libraries/Building up to Shor's algorithm

* **[Quantum Fourier transform](./QFT/)**.
  Learn to implement quantum Fourier transform and to use it to perform simple state transformations.
* **[Phase estimation](./PhaseEstimation/)**.
  Learn about phase estimation algorithms.

#### Entanglement games

* **[CHSH game](./CHSHGame/)**.
* **[GHZ game](./GHZGame/)**.
* **[Mermin-Peres magic square game](./MagicSquareGame)**.

#### Reversible computing

* **[Truth tables](./TruthTables/)**.
  Learn to represent and manipulate Boolean functions as truth tables and to implement them as quantum operations.
* **[Ripple-carry adder](./RippleCarryAdder/)**.
  Build a ripple-carry adder on a quantum computer.

#### Miscellaneous

* **[BB84 protocol](./KeyDistribution_BB84/)**.
  Implement the BB84 key distribution algorithm.
* **[Bit-flip error correcting code](./QEC_BitFlipCode/)**.
  Learn about a 3-qubit error correcting code for protecting against bit-flip errors.
* **[Unitary patterns](./UnitaryPatterns/)**.
  Learn to implement unitaries with matrices that follow certain patterns of zero and non-zero elements.
* **[Quantum classification (tutorial)](./tutorials/QuantumClassification/)**.
  Learn about circuit-centric classifiers and the quantum machine learning library included in the QDK.

> For a Q# programming language quick reference sheet, see [Q# Language Quick Reference](./quickref/qsharp-quick-reference.pdf).

## Run the katas and tutorials online <a name=""run-online"" /> ##

The Quantum Katas are now available as Jupyter Notebooks online! See [index.ipynb](https://mybinder.org/v2/gh/Microsoft/QuantumKatas/main?filepath=index.ipynb) for the list of all katas and tutorials, and instructions for running them online.

> While running the Katas online is the easiest option to get started, if you want to save your progress and enjoy better performance, we recommend you to choose the local option. 

## Run the katas locally <a name=""kata-locally"" /> ##

### Quantum Development Kit Installation <a name=""install"" /> ###

To use the Quantum Katas locally, you'll need the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum), available for Windows 10, macOS, and Linux.
If you don't already have the Quantum Development Kit installed, see the [install guide for the Quantum Development Kit](https://docs.microsoft.com/azure/quantum/install-overview-qdk).

**If you want to run the katas and tutorials locally as Jupyter Notebooks**:
1. Follow the steps in the [QDK install guide for Python](https://docs.microsoft.com/azure/quantum/install-python-qdk) 
  and the [QDK install guide for Jupyter Notebooks](https://docs.microsoft.com/azure/quantum/install-jupyter-qkd).
2. Several tutorials require installing additional Python packages:
   * ""Complex arithmetic"" and ""Linear algebra"" require the [`pytest` package](https://docs.pytest.org/en/latest/getting-started.html).
   * ""Exploring Grover's search algorithm"" requires the [`matplotlib` package](https://matplotlib.org/3.1.1/users/installing.html).
   * ""Quantum classification"" requires [`matplotlib`](https://matplotlib.org/3.1.1/users/installing.html) and [`numpy`](https://numpy.org/install/) packages.

**If you want to run the katas and tutorials locally as Q# projects**:

Follow the steps in the [QDK install guide](https://docs.microsoft.com/azure/quantum/install-command-line-qdk) for Visual Studio, 
 Visual Studio Code or other editors.


### Download the Quantum Katas <a name=""download"" /> ###

If you have Git installed, clone the Microsoft/QuantumKatas repository:

```bash
$ git clone https://github.com/Microsoft/QuantumKatas.git
```

> [!TIP]
> Both Visual Studio 2019 and Visual Studio Code make it easy to clone repositories from within your development environment.
> For details, see the [Visual Studio 2019](https://docs.microsoft.com/azure/devops/repos/git/clone?view=azure-devops&tabs=visual-studio#clone-from-another-git-provider) and [Visual Studio Code](https://code.visualstudio.com/docs/editor/versioncontrol#_cloning-a-repository) documentation.

If you don't have Git installed, download the katas from https://github.com/Microsoft/QuantumKatas/archive/main.zip.


### Run a kata as a Jupyter Notebook <a name=""kata-as-notebook"" /> ###

The best way to run the katas as Jupyter Notebooks is to navigate to the root folder of the repository and to open `index.ipynb` using Jupyter:

```bash
$ cd QuantumKatas/
$ jupyter notebook index.ipynb
```

This will open the notebook that contains a list of all katas and tutorials, and you will be able to navigate to the one you want using links.

> Note that this will start Jupyter Notebooks server in the same command line window you used to run the command. If you want to keep using that window for navigation, you can launch Jupyter Notebooks server in a new window using the following commands (on Windows):
> ```bash
> $ cd QuantumKatas/
> $ start jupyter notebook index.ipynb
> ```

You can also open an individual notebook directly, but this might render internal links invalid:

```bash
$ cd QuantumKatas/tutorials/ComplexArithmetic
$ jupyter notebook ComplexArithmetic.ipynb
```


### Run a kata as a Q# project <a name=""kata-as-project"" /> ###

Each kata is in its own directory as a self-contained Q# project, solution and Jupyter Notebook triplet.
For instance, the BasicGates directory structure is:

```bash
QuantumKatas/
  BasicGates/
    README.md                  # Instructions specific to this kata.
    .vscode/                   # Metadata used by Visual Studio Code.
    BasicGates.sln             # Visual Studio 2019 solution file.
    BasicGates.csproj          # Project file used to build both classical and quantum code.
    BasicGates.ipynb           # Jupyter Notebook front-end for this kata.

    Tasks.qs                   # Q# source code that you will fill as you solve each task.
    Tests.qs                   # Q# tests that verify your solutions.
    TestSuiteRunner.cs         # C# source code used to run the Q# tests.
    ReferenceImplementation.qs # Q# source code containing solutions to the tasks.
```

To open the **BasicGates** kata in Visual Studio 2019, open the **QuantumKatas/BasicGates/BasicGates.sln** solution file.

To open the **BasicGates** kata in Visual Studio Code, open the **QuantumKatas/BasicGates/** folder.
Press **Ctrl + Shift + P** (or **⌘ + Shift + P** on macOS) to open the **Command Palette**. Type **Open Folder** on Windows 10 or Linux or **Open** on macOS.

> [!TIP]
> Almost all commands available in Visual Studio Code are in the Command Palette.
> If you get stuck, press **Ctrl + Shift + P** (or **⌘ + Shift + P** on macOS) and start typing to search through all available commands.
>
> You can also launch Visual Studio Code from the command line:
> ```bash
> $ code QuantumKatas/BasicGates/
> ```

### Run kata tests <a name=""tests"" /> ###

Once you have a kata open, it's time to run the tests using the following instructions.
Initially all tests will fail. Don't panic!
Open **Tasks.qs** and start filling in the code to complete the tasks. Each task is covered by a unit test. Once you fill in the correct code for a task, rebuild the project and re-run the tests, and the corresponding unit test will pass.

#### Visual Studio 2019

1. Build the solution.
2. From the main menu, open **Test Explorer** (**Test** > **Windows**) and select **Run All** to run all unit tests at once.
3. Work on the tasks in the **Tasks.qs** file.
4. To test your code changes for a task, rebuild the solution and re-run all unit tests using **Run All**, or run just the test for that task by right-clicking the test and selecting **Run Selected Tests**.

#### Visual Studio Code

1. Press **Ctrl + \`** (or **⌘ + \`** on macOS) to open the integrated terminal.
   The terminal should open to the kata directory. If it doesn't, navigate to the folder containing the *.csproj file for the kata using `cd` command.
2. Run `dotnet test` in the integrated terminal.
   This should build the kata project and run all of the unit tests. All of the unit tests should fail.
3. Work on the tasks in the **Tasks.qs** file.
4. To test your code changes for a task, from the integrated terminal run `dotnet test` again.

For convenience, a tasks.json configuration file exists for each kata. It allows Visual Studio Code to run the build and test steps from the Command Palette.
Press **Ctrl + Shift + P** (or **⌘ + Shift + P** on macOS) to open the Palette and type **Run Build Task** or **Run Test Task** and press **Enter**.

## Run katas locally with Docker <a name=""docker"" /> ##

You can use the included [Dockerfile](./Dockerfile) to create a docker image with all the necessary tools to run the katas from the command line or Jupyter.

1. Install [Docker](https://docs.docker.com/install/).
2. Build the docker image and tag it `katas`:

```bash
docker build -t katas .
```

3. Run the image in the container named `katas-container` with interactive command-line and redirect container port `8888` to local port `8888` (needed to run Jupyter):

```bash
docker run -it --name katas-container -p 8888:8888 katas /bin/bash
```

4. From the same command line that you used to run the container, run the C# version of the **BasicGates** kata:

```bash
cd ~/BasicGates/
dotnet test
```

5. Start a Jupyter Notebook within the image for the **BasicGates** kata:

```bash
cd ~/BasicGates/ && jupyter notebook --ip=0.0.0.0 --no-browser
```

6. Once Jupyter has started, use your browser to open the kata in notebook format. You
will need a token generated by Jupyter when it started on the previous step:

```
http://localhost:8888/notebooks/BasicGates.ipynb
```

To exit a docker container without killing it (daemon mode), press **Ctrl+P, Ctrl+Q**

To re-enter the existing `katas-container` (in daemon mode):

```bash
docker attach katas-container
```

Once you're done, remove the `katas-container`:

```bash
docker rm --force katas-container
```

# Contributing <a name=""contributing"" /> #

This project welcomes contributions and suggestions.  See [How Can I Contribute?](.github/CONTRIBUTING.md) for details.

# Code of Conduct <a name=""code-of-conduct"" /> #

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
174,microsoft/Quantum-NC,F#,"# Microsoft Quantum Development Kit: Non-Commercial Libraries

Welcome to the Microsoft Quantum Development Kit!

This repository contains shared-source libraries that can be used for research and academics, but that cannot be used for commercial purposes.
Please note that these libraries are not intended for production use, and may be modified as research proceeds.
For more information please refer to the [LICENSE](LICENSE).

## Using the non-commercial research libraries

The non-commercial libraries in this repository can be used via NuGet packages beginning with the prefix [""Microsoft.Quantum.Research.""](https://www.nuget.org/packages?q=owner:QuantumEngineering%20id:research)
For more details, please see:
- [Research packages](https://github.com/microsoft/Quantum-NC/wiki/Research-packages) on the [Quantum-NC wiki](https://github.com/microsoft/Quantum-NC/wiki/)

## Feedback

If you have feedback about the libraries in this repository, please let us know by filing a [new issue](https://github.com/microsoft/Quantum-NC/issues/new)!
If you have feedback about some other part of the Microsoft Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview) for more information on the best places to file it.

## Contributing

Please note: **this project does not accept external contributions**.

If you'd like to contribute to the rest of the Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview).
"
175,microsoft/Quantum,Jupyter Notebook,"﻿# Microsoft Quantum Development Kit Samples

 [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Microsoft/Quantum/⭐binder)

These samples demonstrate the use of the Quantum Development Kit for a variety of different quantum computing tasks.

Each sample is self-contained in a folder, and demonstrates how to use Q# to develop quantum applications.

A small number of the samples have additional installation requirements beyond those for the rest of the Quantum Development Kit.
These are noted in the README.md files for each sample, along with complete installation instructions.

## Getting started

You can find instructions on how to install the Quantum Development Kit in [our online documentation](https://docs.microsoft.com/azure/quantum/install-overview-qdk/), which also includes
an introduction to [quantum programming concepts](https://docs.microsoft.com/azure/quantum/concepts-overview/).

For a quick guide on how to set up a development environment from scratch using [Visual Studio Code](https://code.visualstudio.com) or [Visual Studio Codespaces](https://online.visualstudio.com/login), see [here](#setting-up-your-development-environment).

A [Docker](https://docs.docker.com/install/) image definition is also provided for your convenience, see [here](#running-a-jupyter-notebook-with-docker) for instructions on how to build and use it.

### First samples

If you're new to quantum or to the Quantum Development Kit, we recommend starting with the [Getting Started samples](./samples/getting-started/).

After setting up your development environment using one of the options above, try to browse to `samples/getting-started/teleportation` via the terminal and run `dotnet run`. You should see something like the following:
```
Round 1: Sent False, got False.
Teleportation successful!
Round 2: Sent True, got True.
Teleportation successful!
Round 3: Sent False, got False.
Teleportation successful!
Round 4: Sent False, got False.
Teleportation successful!
Round 5: Sent False, got False.
Teleportation successful!
Round 6: Sent False, got False.
Teleportation successful!
Round 7: Sent True, got True.
Teleportation successful!
Round 8: Sent False, got False.
Teleportation successful!
```

Congratulations, you can now start quantum programming!

## Going further

As you go further with quantum development, we provide several different categories of samples for you to explore:

- **[Algorithms](./samples/algorithms)**:
  These samples demonstrate various quantum algorithms, such as database search and integer factorization.
- **[Arithmetic](./samples/arithmetic)**:
  These samples show how to coherently transform arithmetic data.
- **[Characterization](./samples/characterization)**:
  These samples demonstrate how to learn properties of quantum systems from classical data.
- **[Chemistry](./samples/chemistry)**:
- **[Diagnostics](./samples/diagnostics)**:
  These samples show how to diagnose and test Q# applications.
- **[Error Correction](./samples/error-correction)**:
  These samples show how to work with quantum error correcting codes in Q# programs.
- **[Interoperability](./samples/interoperability)**:
  These samples show how to use Q# with different host languages.
- **[Numerics](./samples/numerics)**:
  The samples in this folder show how to use the numerics library.
- **[Runtime](./samples/runtime)**:
  These samples show how to work with the Q# simulation runtime.
- **[Simulation](./samples/simulation)**:
  These samples show how to simulate evolution under different Hamiltonians.

We also encourage taking a look at the [unit tests](./samples/tests) used to check the correctness of the Quantum Development Kit samples.

## Setting up your development environment

This repo contains several configuration files that will make it easy to get started with coding. Below we lay out some instructions for getting started with [VSCode](#visual-studio-code) or with [Jupyter notebooks](#running-a-jupyter-notebook-with-docker).

### Visual Studio Code

If you prefer to develop code locally, we recommend to install an editor such as [Visual Studio Code](https://code.visualstudio.com/download). Make sure to install the [.NET Core SDK 3.1 or later](https://www.microsoft.com/net/download) on your local machine. For more detailed instructions on how to set up VS Code for development with the QDK, go to our docs [here](https://docs.microsoft.com/azure/quantum/install-command-line-qdk).

Once you have installed VS Code and the .NET Core SDK, download this repository to your computer and open the folder in VS Code. The editor will automatically recognize the files in the `.vscode` folder and request you to install the recommended extension. This includes the [Microsoft Quantum Development Kit for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=quantum.quantum-devkit-vscode) extension, which is the fastest way to get started with the QDK.

Open a terminal to start running your first samples (see [here](#first-samples)).

### Running a Jupyter Notebook with Docker

Another way to quickly start developing in Q# is to use Docker and launch a Jupyter notebook on your local machine. You can use the included [Dockerfile](./Dockerfile) to create a docker image with all the necessary libraries to use the Quantum Development Kit to build quantum applications in C#, Python or Jupyter.

Once you have installed [Docker](https://docs.docker.com/install/), you can
use the following commands to get you started:

To build the docker image and tag it `iqsharp`:
```sh
docker build -t iqsharp .
```

To run the image in the container named `iqsharp-container` with interactive command-line and 
redirect container port 8888 to local port 8888 (needed to run jupyter):
```sh
docker run -it --name iqsharp-container -p 8888:8888 iqsharp /bin/bash
```

From the corresponding container command line, you can run the C# version of the Teleportation sample using: 
```sh
cd ~/samples/getting-started/teleportation && dotnet run
```

Similarly, you can run the Python version of the Teleportation sample using: 
```sh
cd ~/samples/getting-started/teleportation && python host.py
```

Finally, to start jupyter notebook within the image for the Teleportation sample, use:
```sh
cd ~/samples/getting-started/teleportation && jupyter notebook --ip=0.0.0.0 --no-browser 
```

Once Jupyter has started, you can open in your browser the Teleportation notebook (you
will need a token generated by jupyter when it started on the previous step):

> http://localhost:8888/notebooks/Notebook.ipynb

Once you're done, to remove container named `iqsharp-container`:
```sh
docker rm --force iqsharp-container
```
"
176,microsoft/qsharp-runtime,C#,"# Microsoft Quantum Development Kit: Q# runtime #

Welcome to the Microsoft Quantum Development Kit!

This repository contains the runtime components for the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum/).
It consists of the libraries and packages needed to create and simulate quantum applications using Q#.

- **[Azure/](./src/Azure/)**: Source for client package to create and manage jobs in Azure Quantum.
- **[Simulation/](./src/Simulation/)**: Source for Q# simulation. Includes code generation, full-state and other simulators.
- **[xUnit/](./src/Xunit/)**: Source for the xUnit's Q# test-case discoverer.

## New to Quantum? ##

See the [introduction to quantum computing](https://docs.microsoft.com/azure/quantum/concepts-overview) provided with the Quantum Development Kit.


## Installing the Quantum Development Kit

**If you're looking to use Q# to write quantum applications, please see the instructions on how to get started with using the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum/install-overview-qdk) including the Q# compiler, language server, and development environment extensions.**

Please see the [installation guide](https://docs.microsoft.com/azure/quantum/install-overview-qdk) for further information on how to get started using the Quantum Development Kit to develop quantum applications.
You may also visit our [Quantum](https://github.com/microsoft/quantum) repository, which offers a wide variety of samples on how to write quantum based programs.


## Building from Source ##

[![Build Status](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_apis/build/status/microsoft.qsharp-runtime?branchName=main)](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_build/latest?definitionId=15&branchName=main)

Note that when building from source, this repository is configured so that .NET Core will automatically look at the [Quantum Development Kit prerelease feed](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_packaging?_a=feed&feed=alpha) in addition to any other feeds you may have configured.

Building **QIR Runtime** isn't enabled by default yet. Please see [its readme](./src/Qir/Runtime/README.md) for details.

### Windows ###

To build on Windows:

1. Install the pre-reqs:
    * Install [CMake](https://cmake.org/install/)
    * Install [Visual Studio 2019 (version 16.3 or later)](https://visualstudio.microsoft.com/downloads/). Make sure you install the following workloads:
        * **Desktop development with C++**
        * **From the Individual Components tab in VS Installer add Spectre-mitigated libs that match your C++ build tools version**
        * **.NET Core 3 cross-platform development**
2. Run [bootstrap.ps1](bootstrap.ps1) from PowerShell
    * pre-req (in PowerShell): `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`
    * The script might install additional tools (a specific compiler, build tools, etc)
    * Then it builds release flavor of the native (C++) full-state simulator and debug flavor of the Simulation solution.
    * You only need to run it once.
3. Open and build the [`Simulation.sln`](./Simulation.sln) solution in Visual Studio.

The `Simulation.sln` solution does not include the full-state quantum simulator. To change it, you can open the `quantum-simulator.sln` solution created during bootstrap in the `src\Simulation\Native\build`. To integrate your changes with the rest of the simulation components, you must manually build it.


### macOS/Linux ###

To build on other platforms:

1. Install the pre-reqs:
    * Install [CMake](https://cmake.org/install/)
    * Install [.NET Core 3 SDK](https://dotnet.microsoft.com/download)
    * On [WSL](https://docs.microsoft.com/en-us/windows/wsl/)/Linux:
      * Install `g++` (e.g. in Ubuntu 20.04 `sudo apt-get install g++`).
      * The build does not accept `dotnet-*-5.0` packages, install `dotnet-*-3.1`
        (`sudo apt-get install dotnet-sdk-3.1`). The possible result can be:

```sh
qsharp-runtime$ dpkg -l *dotnet*
Desired=Unknown/Install/Remove/Purge/Hold
| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend
|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)
||/ Name                      Version      Architecture Description
+++-=========================-============-============-=================================================================
un  dotnet                    <none>       <none>       (no description available)
ii  dotnet-apphost-pack-3.1   3.1.13-1     amd64        Microsoft.NETCore.App.Host 3.1.13
ii  dotnet-host               5.0.4-1      amd64        Microsoft .NET Host - 5.0.4
ii  dotnet-hostfxr-3.1        3.1.13-1     amd64        Microsoft .NET Core Host FX Resolver - 3.1.13 3.1.13
un  dotnet-nightly            <none>       <none>       (no description available)
ii  dotnet-runtime-3.1        3.1.13-1     amd64        Microsoft .NET Core Runtime - 3.1.13 Microsoft.NETCore.App 3.1.13
ii  dotnet-runtime-deps-3.1   3.1.13-1     amd64        dotnet-runtime-deps-3.1 3.1.13
ii  dotnet-sdk-3.1            3.1.407-1    amd64        Microsoft .NET Core SDK 3.1.407
ii  dotnet-targeting-pack-3.1 3.1.0-1      amd64        Microsoft.NETCore.App.Ref 3.1.0
```
2. Run [bootstrap.ps1](./bootstrap.ps1)
    * The script might install additional tools (a specific compiler, build tools, etc)
    * Then it builds release flavor of the native (C++) full-state simulator and debug flavor of the Simulation solution.
    * You only need to run it once.
3. From the command line, run:
    * `dotnet build Simulation.sln`

The `Simulation.sln` solution does not include the full-state simulator. To integrate any changes with the rest of the simulation components, you need to manually build it using `make` in the `src\Simulation\Native\build` folder.


## Testing ##

All unit tests are part of the `Simulation.sln` solution. To run the tests:

* From [Visual Studio](https://docs.microsoft.com/en-us/visualstudio/test/getting-started-with-unit-testing?view=vs-2019#run-unit-tests):
    * Open Test Explorer by choosing Test > Windows > Test Explorer from the top menu bar.
    * Run your unit tests by clicking Run All.
* From the command line, run:
    * `dotnet test Simulation.sln`


## Feedback ##

If you have feedback about the Q# simulators or any other runtime component, please let us know by filing a [new issue](https://github.com/microsoft/qsharp-runtime/issues/new)!
If you have feedback about some other part of the Microsoft Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview) for more information.


## Reporting Security Issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in
the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).


## Legal and Licensing ##


## Contributing ##

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For more details, please see [CONTRIBUTING.md](./CONTRIBUTING.md), or the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview).
"
177,microsoft/qsharp-compiler,C#,"# Microsoft Quantum Development Kit: <br>Q# Compiler and Language Server #

Welcome to the Microsoft Quantum Development Kit!

This repository contains the Q# compiler included in the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum/),
as well as the Q# language server included in our [Visual Studio extension](https://marketplace.visualstudio.com/items?itemName=quantum.DevKit) and our [Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=quantum.quantum-devkit-vscode).
For more information related to the language server protocol take a look at [this repository](https://github.com/Microsoft/language-server-protocol).
These extensions provide the IDE integration for Q#, and can be found on this repository as well.

The Q# [compiler](./src/QsCompiler/Compiler) is distributed as a [NuGet package](https://www.nuget.org/packages/Microsoft.Quantum.Compiler), and the [CompilationLoader class](https://github.com/microsoft/qsharp-compiler/blob/main/src/QsCompiler/Compiler/CompilationLoader.cs) exposes the different configuration options for building a compilation.
The Q# [command line compiler](./src/QsCompiler/CommandLineTool) is included as a tool in the [Microsoft.Quantum.Sdk](./src/QuantumSdk) and provides an [extensibility mechanism](https://devblogs.microsoft.com/qsharp/extending-the-q-compiler/) for compilation steps. See the list of [project properties](./src/QuantumSdk#defined-project-properties) for more information about possible configuration options for Q# projects.

- **[QsCompiler](./src/QsCompiler/)**: Q# compiler including the command line tool
- **[QsCompiler/LanguageServer](./src/QsCompiler/LanguageServer/)**: Q# language server
- **[Microsoft.Quantum.Sdk](./src/QuantumSdk)**: Sdk for building Q# projects and support for [compiler extensions](https://github.com/microsoft/qsharp-compiler/tree/main/examples/CompilerExtensions)
- **[VSCodeExtension](./src/VSCodeExtension/)**: Visual Studio Code extension
- **[VisualStudioExtension](./src/VisualStudioExtension/)**: Visual Studio extension

Q# executables can be compiled into an LLVM-based [Quantum Intermediate Representation (QIR)](https://github.com/microsoft/qsharp-language/tree/main/Specifications/QIR). More details on that capability and how to use it can be found in this [README](https://github.com/microsoft/qsharp-compiler/tree/main/src/QsCompiler/QirGeneration).

## New to Quantum? ##

See the [introduction to quantum computing](https://docs.microsoft.com/azure/quantum/concepts-overview/) provided with the Quantum Development Kit.

## Installing the Quantum Development Kit

**If you're looking to use Q# to write quantum applications, please see the instructions on how to get started with using the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum/install-overview-qdk/) including the Q# compiler, language server, and development environment extensions.**

Please see the [installation guide](https://docs.microsoft.com/azure/quantum/install-overview-qdk) for further information on how to get started using the Quantum Development Kit to develop quantum applications.
You may also visit our [Quantum](https://github.com/microsoft/quantum) repository, which offers a wide variety of samples on how to write quantum based programs.

## Building from Source ##

Before you can build the source code on this repository and start contributing to the Q# compiler and extensions you need to run the PowerShell script [bootstrap.ps1](./bootstrap.ps1) to set up your environment.
We refer to the [PowerShell GitHub repository](https://github.com/powershell/powershell) for instructions on how to install PowerShell.
The script in particular generates the files that are needed for building based on the templates in this repository.

The Q# compiler and language server in this repository are built using [.NET Core](https://docs.microsoft.com/dotnet/core/). Building the [QsCompiler.sln](./QsCompiler.sln) builds the Q# compiler and language server. To test your changes to the compiler, open the project file of a Q# project that uses the latest version of the [Microsoft.Quantum.Sdk](https://www.nuget.org/packages/Microsoft.Quantum.Sdk/) in a text editor. You can confirm the Sdk version that the project is using by looking at the first line in the project file. You may need to edit that line to update to the latest version, and adjust your project as needed. Confirm that the project is building correctly using that version by executing the command
```
dotnet build MyProject.csproj
```
If your project builds successfully, edit the project file in the text editor to add the following project property, adjusting the path as needed:
```
  <PropertyGroup>
    <QscExe>dotnet $(MSBuildThisFileDirectory)src/QsCompiler/CommandLineTool/bin/$(Configuration)/netcoreapp3.1/qsc.dll</QscExe>
  </PropertyGroup>
```
To confirm that indeed the locally built compiler version is used, you can edit `Run<T>` in your local [Project.cs](./src/QsCompiler/CommandLineTool/Program.cs) file to include the following line:
```csharp
private static int Run<T>(Func<T, ConsoleLogger, int> compile, T options)
where T : Options
{
    Console.WriteLine(""Hi from your locally built compiler!"");
    ...
```
From the root of this repository, build the compiler by executing the two commands
```
dotnet clean QsCompiler.sln
dotnet build QsCompiler.sln -c Debug
```
Build the Q# project as usual by invoking the following two commands:
```
dotnet clean MyProject.csproj
dotnet build MyProject.csproj -c Debug
```
In the build output you should now see the print statement inserted above.
You can also execute the project that has now been built using your local source code version of the compiler by executing the command
```
dotnet run --project MyProject.csproj -c Debug
```

If you edit the [Microsoft.Quantum.Sdk](./src/QuantumSdk) as part of your changes, you will need to pack it using [NuGet 5.8.1](https://docs.microsoft.com/en-us/nuget/release-notes/nuget-5.8). Download it and use it to pack the Sdk by executing the following commands from the root of this repository:
```
dotnet publish src/QuantumSdk/Tools/Tools.sln -c Debug
dotnet publish src/QsCompiler/CommandLineTool/CommandLineTool.csproj -c Debug
nuget.exe pack src/QuantumSdk/QuantumSdk.nuspec -Version 1.0.0 -Properties Configuration=Debug
```
Move the created .nupkg file into your [local NuGet folder](https://docs.microsoft.com/en-us/nuget/hosting-packages/local-feeds). You can now use the package to build any Q# project by opening the project file in a text editor, and editing the Sdk version number in the first line to be
```
<Project Sdk=""Microsoft.Quantum.Sdk/1.0.0"">
```
If you are working in Visual Studio, you may need to unload and then reload the project. When you build the project it will now use your locally built version of the Microsoft.Quantum.Sdk.

For instructions on how to build and debug the Visual Studio Code extension take a look at [this file](./src/VSCodeExtension/BUILDING.md).
Building and debugging the Visual Studio extension requires Visual Studio 2019. Open [the corresponding solution](./VisualStudioExtension.sln) and set the [QSharpVsix project](./src/VisualStudioExtension/QSharpVsix/) as startup project, then launch and debug the extension as usual.
The Visual Studio extension is built on the [.NET Framework 4.7.2](https://dotnet.microsoft.com/download/dotnet-framework/net472) that comes with Visual Studio 2019. Alternatively you can easily obtain it via the Visual Studio Installer.

We recommend uninstalling any other Q# extensions when working on the extensions in this repository.

### Tips for using VSCode ###
This repository includes both C# and F# code, as well as .csproj and .fsproj projects organizing that code. The recommended extensions for interacting with these language types are the [Microsoft C# extension powered by OmniSharp](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csharp) and the [Ionide FSharp extension](https://marketplace.visualstudio.com/items?itemName=Ionide.Ionide-fsharp). Several of the projects in each language express dependencies on the other language, which can cause errors resolving namespaces even when the builds succeed without errors. To resolve these errors in C# projects that depend on F# resources, ensure the the MSBuild utilized by Omnisharp comes from an install of Visual Studio or Visual Studio Community edition with support for F# installed. To resolve errors loading .csproj files in the Ionide extension, use the ""Change Workspace or Solution"" option in the F#: Solution Explorer to select the top level ""qsharp-compiler"" folder. This will allow Ionide to find only the .fsproj projects instead of trying to load both .csproj and .fsproj listed in the solution files.

## Build Status ##

| branch | status    |
|--------|-----------|
| main | [![Build Status](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_apis/build/status/microsoft.qsharp-compiler?branchName=main)](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_build/latest?definitionId=14&branchName=main) |

## Feedback ##

If you have feedback about the content in this repository, please let us know by filing a [new issue](https://github.com/microsoft/qsharp-compiler/issues/new/choose)!
If you have feedback about some other part of the Microsoft Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/azure/quantum/contributing-overview/) for more information.

## Reporting Security Issues ##

Security issues and bugs should be reported privately, via email, to the Microsoft Security
Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should
receive a response within 24 hours. If for some reason you do not, please follow up via
email to ensure we received your original message. Further information, including the
[MSRC PGP](https://technet.microsoft.com/en-us/security/dn606155) key, can be found in
the [Security TechCenter](https://technet.microsoft.com/en-us/security/default).

## Legal and Licensing ##

### Telemetry ###

By default, sending out telemetry is disabled for all code in this repository, but it can be enabled via compilation flag.
Our shipped extensions that are built based on the code in this repository support collecting telemetry.
In that case, opt-in or opt-out works via the corresponding setting in Visual Studio and Visual Studio Code,
and the telemetry we collect falls under the [Microsoft Privacy Statement](https://privacy.microsoft.com/privacystatement).

### Data Collection ###

The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.

## Contributing ##

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For more details, please see [CONTRIBUTING.md](./CONTRIBUTING.md).

"
178,microsoft/qdk-python,Python,"[![Build Status](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_apis/build/status/microsoft.qdk-python?branchName=main)](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_build/latest?definitionId=32&branchName=main)

# QDK-Python

## Introduction

QDK-Python is the repository for Python packages of the Quantum Development Kit (QDK). Currently, this consists of the following packages:

- `qdk` [![PyPI version](https://badge.fury.io/py/qdk.svg)](https://badge.fury.io/py/qdk)
- `azure-quantum` [![PyPI version](https://badge.fury.io/py/azure-quantum.svg)](https://badge.fury.io/py/azure-quantum)

Coming soon:

- qsharp

## Installation and getting started

To install the packages, we recommend installing the Anaconda Python distribution. For instructions on installing Conda on your system, please follow the [Conda user guide](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

To install the QDK package, run

```bash
pip install qdk
```

To install the Azure Quantum package, run

```bash
pip install azure-quantum
```

To get started running examples, start a Jupyter notebook:

```bash
cd examples
jupyter notebook
```

## Development

Install pre-reqs:

```bash
pip install azure_devtools pytest pytest-azurepipelines pytest-cov
```

To create a new Conda environment, run:

```bash
conda env create -f environment.yml
```

in the root directory of the given package (`qdk` or `azure-quantum`).

Then to activate the environment:

```bash
conda activate <env name>
```

where `<env name>` is the environment name (`qdk` or `azurequantum`).

To install the package in development mode, run:

```bash
pip install -e .
```

### Integration tests

For instructions on how to run integration tests for the Azure Quantum package, please refer to the [README](azure-quantum/tests/integration/README.md) file.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
179,microsoft/iqsharp,C#,"# Microsoft Quantum Development Kit: IQ# Kernel #

Welcome to the Microsoft Quantum Development Kit!

This repository contains the IQ# kernel for the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum/).
This kernel provides Q# support for the Jupyter platform, as well as the backend used by the Python client for Q#.

- **[src/Core/](./src/Core/)**: Core of the IQ# kernel.
- **[src/Kernel/](./src/Kernel/)**: Assembly used to interoperate between Jupyter and the rest of the IQ# kernel.
- **[src/Python/](./src/Python)**: Python package for accessing IQ#.
- **[src/Tests/](./src/Tests/)**: Unit tests for IQ#.
- **[src/Tool/](./src/Tool/)**: .NET Core Global Tool used to install and launch IQ#.
- **[src/Web/](./src/Web/)**: Provides a RESTful API into IQ#.

## New to Quantum? ##

See the [introduction to quantum computing](https://docs.microsoft.com/azure/quantum/concepts-overview) provided with the Quantum Development Kit.

## Getting Started ##

The Jupyter kernel provided in this repository is built using [.NET Core](https://docs.microsoft.com/dotnet/core/) (2.2 or later) and the compiler infrastructure provided with the [Quantum Development Kit](https://docs.microsoft.com/azure/quantum).
Please see the [getting started guide](https://docs.microsoft.com/azure/quantum/install-overview-qdk) for how to get up and running.

You may also visit the [**microsoft/quantum**](https://github.com/microsoft/quantum) repository, which offers a wide variety
of samples on how to use this kernel to run Q# in Jupyter Notebooks, or from Python.

### Building IQ# from Source ###

To obtain prerequisites, ensure that [Node.js](https://nodejs.org/) is installed, and then run `npm install` from the [src/Kernel/](./src/Kernel/) folder:

```
cd src/Kernel/
npm install
```

To build IQ# from Visual Studio 2017 or later, please use the [`iqsharp.sln`](./iqsharp.sln) solution file.
To build using the .NET Core SDK, please run `dotnet build iqsharp.sln`.

In either case, the IQ# kernel can be installed by using `dotnet run`:

```
cd src/Tool/
dotnet run -- install
```

Optionally, you can install IQ# in _development mode_, which instructs the Jupyter platform to rebuild IQ# whenever a new kernel is started:

```
cd src/Tool/
dotnet run -- install --develop
```

This can cause some issues, especially when running multiple instances of IQ#, such that we recommend against using development mode in general usage.

Note that when building IQ# from source, this repository is configured so that .NET Core will automatically look at the [Quantum Development Kit prerelease feed](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_packaging?_a=feed&feed=alpha) in addition to any other feeds you may have configured.

### Using IQ# as a Container ###

This repository provides a [Dockerfile](./images/iqsharp-base/Dockerfile) that includes the .NET Core SDK, Python, Jupyter Notebook, and the IQ# kernel.

The image built from this Dockerfile is hosted on the [Microsoft Container Registry](https://github.com/microsoft/ContainerRegistry) as the `quantum/iqsharp-base` repository.
The `iqsharp-base` image can be used, for instance, to quickly enable using [Binder](https://gke.mybinder.org/) with Q#-language repositories, or as a base image for [Visual Studio Code Development Containers](https://code.visualstudio.com/docs/remote/containers).

To use the `iqsharp-base` image in your own Dockerfile, make sure to begin your Dockerfile with a `FROM` line that points to the Microsoft Container Registry:

```Dockerfile
FROM mcr.microsoft.com/quantum/iqsharp-base:latest
```

To use the `iqsharp-base` image as a development container for Visual Studio Code, add a [`.devcontainer` folder](https://code.visualstudio.com/docs/remote/containers#_using-an-image-or-dockerfile) that points to the Microsoft Container Registry:

```json
{
    ""image"": ""mcr.microsoft.com/quantum/iqsharp-base:latest"",
    ""extensions"": [
        ""quantum.quantum-devkit-vscode"",
        ""ms-vscode.csharp""
    ]
}
```

In either case, you can also use a Quantum Development Kit version number (0.8 or later) in place of `latest` to point to a specific version.

## Build Status ##

| branch | status    |
|--------|-----------|
| main | [![Build Status](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_apis/build/status/microsoft.iqsharp?branchName=main)](https://dev.azure.com/ms-quantum-public/Microsoft%20Quantum%20(public)/_build/latest?definitionId=14&branchName=main) |

## Feedback ##

If you have feedback about IQ#, please let us know by filing a [new issue](https://github.com/microsoft/iqsharp/issues/new)!
If you have feedback about some other part of the Microsoft Quantum Development Kit, please see the [contribution guide](https://docs.microsoft.com/en-us/azure/quantum/contributing-overview) for more information.

## Legal and Licensing ##

### Telemetry ###

By default, IQ# collects information about the runtime performance of IQ#.
To opt-out of sending telemetry, create an environment variable called IQSHARP_TELEMETRY_OPT_OUT set to a value of 1 before starting IQ#.
The telemetry we collect falls under the [Microsoft Privacy Statement](https://privacy.microsoft.com/privacystatement).

### Data Collection ###

The software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft's privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.

## Contributing ##

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For more details, please see [CONTRIBUTING.md](./CONTRIBUTING.md), or the [contribution guide](https://docs.microsoft.com/en-us/azure/quantum/contributing-overview).
"
180,microsoft/terminal,C++,"![terminal-logos](https://user-images.githubusercontent.com/48369326/115790869-4c852b00-a37c-11eb-97f1-f61972c7800c.png)

# Welcome to the Windows Terminal, Console and Command-Line repo

This repository contains the source code for:

* [Windows Terminal](https://aka.ms/terminal)
* [Windows Terminal Preview](https://aka.ms/terminal-preview)
* The Windows console host (`conhost.exe`)
* Components shared between the two projects
* [ColorTool](https://github.com/microsoft/terminal/tree/main/src/tools/ColorTool)
* [Sample projects](https://github.com/microsoft/terminal/tree/main/samples)
  that show how to consume the Windows Console APIs

Related repositories include:

* [Windows Terminal Documentation](https://docs.microsoft.com/windows/terminal)
  ([Repo: Contribute to the docs](https://github.com/MicrosoftDocs/terminal))
* [Console API Documentation](https://github.com/MicrosoftDocs/Console-Docs)
* [Cascadia Code Font](https://github.com/Microsoft/Cascadia-Code)

## Installing and running Windows Terminal

> 🔴 Note: Windows Terminal requires Windows 10 1903 (build 18362) or later

### Microsoft Store [Recommended]

Install the [Windows Terminal from the Microsoft Store][store-install-link].
This allows you to always be on the latest version when we release new builds
with automatic upgrades.

This is our preferred method.

### Other install methods

#### Via GitHub

For users who are unable to install Windows Terminal from the Microsoft Store,
released builds can be manually downloaded from this repository's [Releases
page](https://github.com/microsoft/terminal/releases).

Download the `Microsoft.WindowsTerminal_<versionNumber>.msixbundle` file from
the **Assets** section. To install the app, you can simply double-click on the
`.msixbundle` file, and the app installer should automatically run. If that
fails for any reason, you can try the following command at a PowerShell prompt:

```powershell
# NOTE: If you are using PowerShell 7+, please run
# Import-Module Appx -UseWindowsPowerShell
# before using Add-AppxPackage.

Add-AppxPackage Microsoft.WindowsTerminal_<versionNumber>.msixbundle
```

> 🔴 Note: If you install Terminal manually:
>
> * Terminal will not auto-update when new builds are released so you will need
>   to regularly install the latest Terminal release to receive all the latest
>   fixes and improvements!

#### Via Windows Package Manager CLI (aka winget)

[winget](https://github.com/microsoft/winget-cli) users can download and install
the latest Terminal release by installing the `Microsoft.WindowsTerminal`
package:

```powershell
winget install --id=Microsoft.WindowsTerminal -e
```

#### Via Chocolatey (unofficial)

[Chocolatey](https://chocolatey.org) users can download and install the latest
Terminal release by installing the `microsoft-windows-terminal` package:

```powershell
choco install microsoft-windows-terminal
```

To upgrade Windows Terminal using Chocolatey, run the following:

```powershell
choco upgrade microsoft-windows-terminal
```

If you have any issues when installing/upgrading the package please go to the
[Windows Terminal package
page](https://chocolatey.org/packages/microsoft-windows-terminal) and follow the
[Chocolatey triage process](https://chocolatey.org/docs/package-triage-process)

#### Via Scoop (unofficial)

[Scoop](https://scoop.sh) users can download and install the latest Terminal
release by installing the `windows-terminal` package:

```powershell
scoop bucket add extras
scoop install windows-terminal
```

To update Windows Terminal using Scoop, run the following:

```powershell
scoop update windows-terminal
```

If you have any issues when installing/updating the package, please search for
or report the same on the [issues
page](https://github.com/lukesampson/scoop-extras/issues) of Scoop Extras bucket
repository.

---

## Windows Terminal 2.0 Roadmap

The plan for delivering Windows Terminal 2.0 [is described
here](/doc/terminal-v2-roadmap.md) and will be updated as the project proceeds.

## Project Build Status

Project|Build Status
---|---
Terminal|[![Terminal Build Status](https://dev.azure.com/ms/terminal/_apis/build/status/terminal%20CI?branchName=main)](https://dev.azure.com/ms/terminal/_build?definitionId=136)
ColorTool|![Colortool Build Status](https://microsoft.visualstudio.com/_apis/public/build/definitions/c93e867a-8815-43c1-92c4-e7dd5404f1e1/17023/badge)

---

## Terminal & Console Overview

Please take a few minutes to review the overview below before diving into the
code:

### Windows Terminal

Windows Terminal is a new, modern, feature-rich, productive terminal application
for command-line users. It includes many of the features most frequently
requested by the Windows command-line community including support for tabs, rich
text, globalization, configurability, theming & styling, and more.

The Terminal will also need to meet our goals and measures to ensure it remains
fast and efficient, and doesn't consume vast amounts of memory or power.

### The Windows Console Host

The Windows Console host, `conhost.exe`, is Windows' original command-line user
experience. It also hosts Windows' command-line infrastructure and the Windows
Console API server, input engine, rendering engine, user preferences, etc. The
console host code in this repository is the actual source from which the
`conhost.exe` in Windows itself is built.

Since taking ownership of the Windows command-line in 2014, the team added
several new features to the Console, including background transparency,
line-based selection, support for [ANSI / Virtual Terminal
sequences](https://en.wikipedia.org/wiki/ANSI_escape_code), [24-bit
color](https://devblogs.microsoft.com/commandline/24-bit-color-in-the-windows-console/),
a [Pseudoconsole
(""ConPTY"")](https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/),
and more.

However, because Windows Console's primary goal is to maintain backward
compatibility, we have been unable to add many of the features the community
(and the team) have been wanting for the last several years including tabs,
unicode text, and emoji.

These limitations led us to create the new Windows Terminal.

> You can read more about the evolution of the command-line in general, and the
> Windows command-line specifically in [this accompanying series of blog
> posts](https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/)
> on the Command-Line team's blog.

### Shared Components

While overhauling Windows Console, we modernized its codebase considerably,
cleanly separating logical entities into modules and classes, introduced some
key extensibility points, replaced several old, home-grown collections and
containers with safer, more efficient [STL
containers](https://docs.microsoft.com/en-us/cpp/standard-library/stl-containers?view=vs-2019),
and made the code simpler and safer by using Microsoft's [Windows Implementation
Libraries - WIL](https://github.com/Microsoft/wil).

This overhaul resulted in several of Console's key components being available
for re-use in any terminal implementation on Windows. These components include a
new DirectWrite-based text layout and rendering engine, a text buffer capable of
storing both UTF-16 and UTF-8, a VT parser/emitter, and more.

### Creating the new Windows Terminal

When we started planning the new Windows Terminal application, we explored and
evaluated several approaches and technology stacks. We ultimately decided that
our goals would be best met by continuing our investment in our C++ codebase,
which would allow us to reuse several of the aforementioned modernized
components in both the existing Console and the new Terminal. Further, we
realized that this would allow us to build much of the Terminal's core itself as
a reusable UI control that others can incorporate into their own applications.

The result of this work is contained within this repo and delivered as the
Windows Terminal application you can download from the Microsoft Store, or
[directly from this repo's
releases](https://github.com/microsoft/terminal/releases).

---

## Resources

For more information about Windows Terminal, you may find some of these
resources useful and interesting:

* [Command-Line Blog](https://devblogs.microsoft.com/commandline)
* [Command-Line Backgrounder Blog
  Series](https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/)
* Windows Terminal Launch: [Terminal ""Sizzle
  Video""](https://www.youtube.com/watch?v=8gw0rXPMMPE&list=PLEHMQNlPj-Jzh9DkNpqipDGCZZuOwrQwR&index=2&t=0s)
* Windows Terminal Launch: [Build 2019
  Session](https://www.youtube.com/watch?v=KMudkRcwjCw)
* Run As Radio: [Show 645 - Windows Terminal with Richard
  Turner](http://www.runasradio.com/Shows/Show/645)
* Azure Devops Podcast: [Episode 54 - Kayla Cinnamon and Rich Turner on DevOps
  on the Windows
  Terminal](http://azuredevopspodcast.clear-measure.com/kayla-cinnamon-and-rich-turner-on-devops-on-the-windows-terminal-team-episode-54)
* Microsoft Ignite 2019 Session: [The Modern Windows Command Line: Windows
  Terminal -
  BRK3321](https://myignite.techcommunity.microsoft.com/sessions/81329?source=sessions)

---

## FAQ

### I built and ran the new Terminal, but it looks just like the old console

Cause: You're launching the incorrect solution in Visual Studio.

Solution: Make sure you're building & deploying the `CascadiaPackage` project in
Visual Studio.

> ⚠ Note: `OpenConsole.exe` is just a locally-built `conhost.exe`, the classic
> Windows Console that hosts Windows' command-line infrastructure. OpenConsole
> is used by Windows Terminal to connect to and communicate with command-line
> applications (via
> [ConPty](https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/)).

---

## Documentation

All project documentation is located at [aka.ms/terminal-docs](https://aka.ms/terminal-docs). If you would like
to contribute to the documentation, please submit a pull request on the [Windows
Terminal Documentation repo](https://github.com/MicrosoftDocs/terminal).

---

## Contributing

We are excited to work alongside you, our amazing community, to build and
enhance Windows Terminal\!

***BEFORE you start work on a feature/fix***, please read & follow our
[Contributor's
Guide](https://github.com/microsoft/terminal/blob/main/CONTRIBUTING.md) to
help avoid any wasted or duplicate effort.

## Communicating with the Team

The easiest way to communicate with the team is via GitHub issues.

Please file new issues, feature requests and suggestions, but **DO search for
similar open/closed pre-existing issues before creating a new issue.**

If you would like to ask a question that you feel doesn't warrant an issue
(yet), please reach out to us via Twitter:

* Kayla Cinnamon, Program Manager:
  [@cinnamon\_msft](https://twitter.com/cinnamon_msft)
* Dustin Howett, Engineering Lead: [@dhowett](https://twitter.com/DHowett)
* Michael Niksa, Senior Developer:
  [@michaelniksa](https://twitter.com/MichaelNiksa)
* Mike Griese, Developer: [@zadjii](https://twitter.com/zadjii)
* Carlos Zamora, Developer: [@cazamor_msft](https://twitter.com/cazamor_msft)
* Leon Liang, Developer: [@leonmsft](https://twitter.com/leonmsft)
* Pankaj Bhojwani, Developer
* Leonard Hecker, Developer: [@LeonardHecker](https://twitter.com/LeonardHecker)

## Developer Guidance

## Prerequisites

* You must be running Windows 1903 (build >= 10.0.18362.0) or later to run
  Windows Terminal
* You must [enable Developer Mode in the Windows Settings
  app](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development)
  to locally install and run Windows Terminal
* You must have the [Windows 10 1903
  SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk)
  installed
* You must have at least [VS
  2019](https://visualstudio.microsoft.com/downloads/) installed
* You must install the following Workloads via the VS Installer. Note: Opening
  the solution in VS 2019 will [prompt you to install missing components
  automatically](https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/):
  * Desktop Development with C++
  * Universal Windows Platform Development
  * **The following Individual Components**
    * C++ (v142) Universal Windows Platform Tools

## Building the Code

This repository uses [git
submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) for some of its
dependencies. To make sure submodules are restored or updated, be sure to run
the following prior to building:

```shell
git submodule update --init --recursive
```

OpenConsole.sln may be built from within Visual Studio or from the command-line
using a set of convenience scripts & tools in the **/tools** directory:

### Building in PowerShell

```powershell
Import-Module .\tools\OpenConsole.psm1
Set-MsBuildDevEnvironment
Invoke-OpenConsoleBuild
```

### Building in Cmd

```shell
.\tools\razzle.cmd
bcz
```

## Running & Debugging

To debug the Windows Terminal in VS, right click on `CascadiaPackage` (in the
Solution Explorer) and go to properties. In the Debug menu, change ""Application
process"" and ""Background task process"" to ""Native Only"".

You should then be able to build & debug the Terminal project by hitting
<kbd>F5</kbd>.

> 👉 You will _not_ be able to launch the Terminal directly by running the
> WindowsTerminal.exe. For more details on why, see
> [#926](https://github.com/microsoft/terminal/issues/926),
> [#4043](https://github.com/microsoft/terminal/issues/4043)

### Coding Guidance

Please review these brief docs below about our coding practices.

> 👉 If you find something missing from these docs, feel free to contribute to
> any of our documentation files anywhere in the repository (or write some new
> ones!)

This is a work in progress as we learn what we'll need to provide people in
order to be effective contributors to our project.

* [Coding Style](https://github.com/microsoft/terminal/blob/main/doc/STYLE.md)
* [Code Organization](https://github.com/microsoft/terminal/blob/main/doc/ORGANIZATION.md)
* [Exceptions in our legacy codebase](https://github.com/microsoft/terminal/blob/main/doc/EXCEPTIONS.md)
* [Helpful smart pointers and macros for interfacing with Windows in WIL](https://github.com/microsoft/terminal/blob/main/doc/WIL.md)

---

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of
Conduct][conduct-code]. For more information see the [Code of Conduct
FAQ][conduct-FAQ] or contact [opencode@microsoft.com][conduct-email] with any
additional questions or comments.

[conduct-code]: https://opensource.microsoft.com/codeofconduct/
[conduct-FAQ]: https://opensource.microsoft.com/codeofconduct/faq/
[conduct-email]: mailto:opencode@microsoft.com
[store-install-link]: https://aka.ms/terminal
"
181,microsoft/microsoft-ui-xaml,C#,"
# Windows UI Library

[![Follow WinUI on Twitter](https://img.shields.io/twitter/follow/windowsui.svg?label=Follow%20WinUI%20on%20Twitter&style=social)](https://twitter.com/intent/follow?screen_name=windowsui)

WinUI is a user interface layer that contains modern controls and styles for building Windows apps. </span> As the native UI layer in Windows it embodies <a href=""https://www.microsoft.com/design/fluent/#/"">Fluent Design</a>, giving each Windows app the polished feel that customers expect.

WinUI 2 is a library of controls that provides official native Microsoft UI controls and features for Windows [UWP apps](https://docs.microsoft.com/windows/uwp/index). WinUI 2 can be used in any Windows 10 UWP XAML app, or in a Xamarin.Forms app running on Windows 10 using [native view embedding](https://docs.microsoft.com/xamarin/xamarin-forms/platform/native-views).

WinUI 3 is the next version of the WinUI framework, and the first stable, supported version has recently shipped. It dramatically expands WinUI into a full UX framework, making WinUI available for all types of Windows apps – from Win32 to UWP – for use as the UI layer.
 
## WinUI Community Calls

The WinUI community call is your monthly opportunity to learn about native UX development for Windows with WinUI.

In these calls we’ll discuss the WinUI roadmap, our status and your feedback.

You can watch them online here on YouTube at the [Windows Developer channel](https://www.youtube.com/channel/UCzLbHrU7U3cUDNQWWAqjceA).

Add the event to your calendar: [ICS calendar file](communitycalls/WinUICommunityCall.ics)

## WinUI 3 - Project Reunion 0.5 Preview (March 2021)

As outlined in the [roadmap](docs/roadmap.md), we've recently shipped the first stable version of WinUI 3, which will greatly expand the scope of WinUI to include the full native Windows UI platform. We're continuously working on improving WinUI 3 and adding more features.

You can now [download WinUI 3 - Project Reunion 0.5](https://docs.microsoft.com/en-us/windows/apps/winui/winui3/) to try out - we'd love your feedback!

## Using WinUI
You can download and use WinUI packages in your app using the NuGet package manager: see the [Getting Started with the Windows UI Library](https://docs.microsoft.com/uwp/toolkits/winui/getting-started) page for more information.

### Packages

| NuGet Package | Build Status | Latest Versions | Documentation |
| --- | --- | --- | --- |
| [Microsoft.UI.Xaml](https://www.nuget.org/packages/Microsoft.UI.Xaml) <br /> Controls and Fluent Design for UWP apps | [![Build Status](https://dev.azure.com/ms/microsoft-ui-xaml/_apis/build/status/WinUI-Public-MUX-CI?branchName=main)](https://dev.azure.com/ms/microsoft-ui-xaml/_build/latest?definitionId=20?branchName=main) | [![latest stable version](https://img.shields.io/nuget/v/Microsoft.UI.Xaml.svg)](https://www.nuget.org/packages/Microsoft.UI.Xaml) <br /> [![latest prerelease version](https://img.shields.io/nuget/vpre/Microsoft.UI.Xaml.svg)](https://www.nuget.org/packages/Microsoft.UI.Xaml/absoluteLatest) | [2.5 release](https://docs.microsoft.com/windows/apps/winui/winui2/release-notes/winui-2.5) |
| [Microsoft.UI.Xaml.Core.Direct](https://www.nuget.org/packages/Microsoft.UI.Xaml.Core.Direct) <br /> Low-level APIs for middleware components | | [![latest prerelease version](https://img.shields.io/nuget/vpre/Microsoft.UI.Xaml.Core.Direct.svg)](https://www.nuget.org/packages/Microsoft.UI.Xaml.Core.Direct/absoluteLatest) | [2.0 prerelease](https://docs.microsoft.com/uwp/api/microsoft.ui.xaml.core.direct) |

You can also build a WinUI package yourself from source. See [Contributing to the Windows UI Library](CONTRIBUTING.md) for more information on building and contributing to WinUI.

## Documentation

**WinUI usage documentation**:  
https://docs.microsoft.com/windows/apps/winui/

**WinUI 2 Release notes**:  
https://docs.microsoft.com/windows/apps/winui/winui2/release-notes/

**WinUI 3 Release notes**:
https://docs.microsoft.com/windows/apps/winui/winui3/release-notes/

**Sample code**:  
To view the WinUI controls in an interactive format, check out the Xaml Controls Gallery:
* Get the XAML Controls Gallery app from the [Microsoft Store](https://www.microsoft.com/store/productId/9MSVH128X2ZT)
* Get the source code on [GitHub](https://github.com/Microsoft/Xaml-Controls-Gallery)

[WinUI](https://microsoft.github.io/microsoft-ui-xaml/) also has its own website where you can learn more about it.

## Contributing to WinUI
The WinUI team welcomes feedback and contributions!

For information on how to contribute please see [Contributing to the Windows UI Library](CONTRIBUTING.md).

For guidelines on making an impact on WinUI through non-code contributions, please see [Contributing ideas, feedback, and requests](CONTRIBUTING_feedback_and_requests.md).

## WinUI features

### Benefits

WinUI 2 provides some useful benefits when building apps for Windows 10:

1. **Helps you stay up to date**  
WinUI helps keep your app up to date with the latest versions of key controls and features of [UWP XAML](https://docs.microsoft.com/windows/uwp/xaml-platform/xaml-overview) and the [Fluent Design System](https://www.microsoft.com/design/fluent)

2. **Provides backward compatibility**  
WinUI is backward-compatible with a wide range of Windows 10 versions: you can start building and shipping apps with new XAML features immediately as soon as they're released, even if your users aren't on the latest version of Windows 10

3. **Makes it simpler to build version adaptive apps**  
You don't need version checks or conditional XAML markup to use WinUI controls or features: WinUI automatically adapts to the user's OS version

### Version support

The Microsoft.UI.Xaml 2.4 NuGet package requires your project to have TargetPlatformVersion &gt;= 10.0.18362.0 and TargetPlatformMinVersion &gt;= 10.0.15063.0 when building. 

Your app's users can be on any of the following supported Windows 10 versions:

* Windows 10 1703 - Build 15063 (Creators Update aka ""Redstone 2"") and newer (including Windows Insider Previews)

Some features may have a reduced or slightly different user experience on older versions.

For WinUI 3, your app's users must be on Windows 10 1809 - Build 17763 or newer (including Windows Insider Previews).

## Roadmap

For info on the WinUI release schedule and high level plans please see the [Windows UI Library Roadmap](docs/roadmap.md).

## WinUI 3 is a part of the Project Reunion family
[Project Reunion](https://github.com/microsoft/ProjectReunion) is a set of libraries, frameworks, components, and tools that you can use in your apps to access powerful Windows platform functionality from all kinds of apps on many versions of Windows. Project Reunion combines the powers of Win32 native applications alongside modern API usage techniques, so your apps light up everywhere your users are. 
 
Other Project Reunion components are: [WebView2](https://docs.microsoft.com/microsoft-edge/webview2/),  [MSIX (MSIX-Core)](https://docs.microsoft.com/windows/msix/overview), [C++/WinRT](https://github.com/microsoft/cppwinrt), [Rust/WinRT](https://github.com/microsoft/winrt-rs), and [C#/WinRT](https://github.com/microsoft/cswinrt). If you'd like to learn more and contribute to Project Reunion, or have **UWP/app model related questions**, visit our [Github repo](https://github.com/microsoft/ProjectReunion). 

## Data/Telemetry

This project collects usage data and sends it to Microsoft to help improve our products and services. See the [privacy statement](privacy.md) for more details.

For more information on telemetry implementation see the [developer guide](docs/developer_guide.md#Telemetry).
"
182,microsoft/PowerApps-Samples,C#,"# Power Apps Samples

Welcome to the samples repo for Power Apps.

For Power Apps developer documentation, see [Power Apps for developers](https://docs.microsoft.com/powerapps/#pivot=home&panel=developer).

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
183,microsoft/powerquery-language-services,TypeScript,"# powerquery-language-services

[![Build Status](https://dev.azure.com/ms/powerquery-language-services/_apis/build/status/Microsoft.powerquery-language-services?branchName=master)](https://dev.azure.com/ms/powerquery-language-services/_build/latest?definitionId=343&branchName=master)

This project contains base functionality for implementing a language service for the Power Query / M language.

## Build and test

Build

```cmd
npm install
npm run build
```

Test

```cmd
npm test
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
184,microsoft/ApplicationInsights-dotnet,C#,"# Application Insights for .NET Apps

This is the .NET SDK for sending data to [Azure Monitor](https://docs.microsoft.com/azure/azure-monitor/overview) & [Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview).

## Getting Started

Please review our How-to guides to review which packages are appropriate for your project:

* [Console App](https://docs.microsoft.com/azure/azure-monitor/app/console)
* [ASP.NET](https://docs.microsoft.com/azure/azure-monitor/app/asp-net)
* [ASP.NET Core](https://docs.microsoft.com/azure/azure-monitor/app/asp-net-core)
* [ILogger](https://docs.microsoft.com/azure/azure-monitor/app/ilogger)
* [WorkerService](https://docs.microsoft.com/azure/azure-monitor/app/worker-service)

### Understanding our SDK

We've gathered a list of concepts, code examples, and links to full guides [here](docs/concepts.md).

## Contributing

We strongly welcome and encourage contributions to this project.
Please review our [Contributing guide](.github/CONTRIBUTING.md).

## Branches

* [master](https://github.com/Microsoft/ApplicationInsights-dotnet/tree/master) contains the *latest* published release located on [NuGet](https://www.nuget.org/packages/Microsoft.ApplicationInsights).
* [develop](https://github.com/Microsoft/ApplicationInsights-dotnet/tree/develop) contains the code for the *next* release.

## NuGet packages

The following packages are published from this repository:

|                                                                                                                                                                | Nightly Build                                                                                                                                                                                                                                                                                         | Latest Official Release                                                                                                                                                                                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------- |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Base SDKs**                                                                                                                                                  |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - [Microsoft.ApplicationInsights](https://www.nuget.org/packages/Microsoft.ApplicationInsights/)                                                               | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights)                                                                       | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights/)                                                                |
| - [Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel](https://www.nuget.org/packages/Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel)  | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel)         | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.WindowsServer.TelemetryChannel/)  |
| **Auto Collectors (Generic)**                                                                                                                                  |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - [Microsoft.ApplicationInsights.DependencyCollector](https://www.nuget.org/packages/Microsoft.ApplicationInsights.DependencyCollector/)                       | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.DependencyCollector?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.DependencyCollector)                               | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.DependencyCollector.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.DependencyCollector)                             |
| - [Microsoft.ApplicationInsights.EventCounterCollector](https://www.nuget.org/packages/Microsoft.ApplicationInsights.EventCounterCollector)                    | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.EventCounterCollector?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.EventCounterCollector)                           | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.EventCounterCollector.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.EventCounterCollector)                         |
| - [Microsoft.ApplicationInsights.PerfCounterCollector](https://www.nuget.org/packages/Microsoft.ApplicationInsights.PerfCounterCollector/)                     | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.PerfCounterCollector?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.PerfCounterCollector)                             | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.PerfCounterCollector.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.PerfCounterCollector)                           |
| - [Microsoft.ApplicationInsights.WindowsServer](https://www.nuget.org/packages/Microsoft.ApplicationInsights.WindowsServer/)                                   | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.WindowsServer?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.WindowsServer)                                           | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.WindowsServer.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.WindowsServer)                                         |
| **Auto Collectors (ASP.NET)**                                                                                                                                  |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - [Microsoft.ApplicationInsights.Web](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Web/)                                                       | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.Web?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.Web)                                                               | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.Web.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.Web)                                                             |
| **Auto Collectors (ASP.NET Core)**                                                                                                                             |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - [Microsoft.ApplicationInsights.AspNetCore](https://www.nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore/)                                         | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.AspNetCore?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.AspNetCore)                                                 | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.AspNetCore.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.AspNetCore)                                               |
| **Auto Collectors (WorkerService, Console Application, etc.)**                                                                                                 |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - [Microsoft.ApplicationInsights.WorkerService](https://www.nuget.org/packages/Microsoft.ApplicationInsights.WorkerService/)                                   | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.WorkerService?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.WorkerService)                                           | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.WorkerService.svg)](https://nuget.org/packages/Microsoft.ApplicationInsights.WorkerService)                                         |
| **Logging Adapters**                                                                                                                                           |                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                               |
| - For `ILogger`: [Microsoft.Extensions.Logging.ApplicationInsights](https://www.nuget.org/packages/Microsoft.Extensions.Logging.ApplicationInsights/)          | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.Extensions.Logging.ApplicationInsights?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.Extensions.Logging.ApplicationInsights)                                 | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.Extensions.Logging.ApplicationInsights.svg)](https://www.nuget.org/packages/Microsoft.Extensions.Logging.ApplicationInsights/)                          |
| - For `NLog`: [Microsoft.ApplicationInsights.NLogTarget](http://www.nuget.org/packages/Microsoft.ApplicationInsights.NLogTarget/)                              | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.NLogTarget?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.NLogTarget)                                                 | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.NLogTarget.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.NLogTarget/)                                          |
| - For `Log4Net`: [Microsoft.ApplicationInsights.Log4NetAppender](http://www.nuget.org/packages/Microsoft.ApplicationInsights.Log4NetAppender/)                 | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.Log4NetAppender?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.Log4NetAppender)                                       | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.Log4NetAppender.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.Log4NetAppender/)                                |
| - For `System.Diagnostics`: [Microsoft.ApplicationInsights.TraceListener](http://www.nuget.org/packages/Microsoft.ApplicationInsights.TraceListener/)          | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.TraceListener?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.TraceListener)                                           | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.TraceListener.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.TraceListener/)                                    |
| - [Microsoft.ApplicationInsights.DiagnosticSourceListener](http://www.nuget.org/packages/Microsoft.ApplicationInsights.DiagnosticSourceListener/)              | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.DiagnosticSourceListener?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.DiagnosticSourceListener)                     | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.DiagnosticSourceListener.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.DiagnosticSourceListener/)              |
| - [Microsoft.ApplicationInsights.EtwCollector](http://www.nuget.org/packages/Microsoft.ApplicationInsights.EtwCollector/)                                      | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.EtwCollector?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.EtwCollector)                                             | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.EtwCollector.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.EtwCollector/)                                      |
| - [Microsoft.ApplicationInsights.EventSourceListener](http://www.nuget.org/packages/Microsoft.ApplicationInsights.EventSourceListener/)                        | [![Nightly](https://img.shields.io/myget/applicationinsights-dotnet-nightly/v/Microsoft.ApplicationInsights.EventSourceListener?label=)](https://www.myget.org/feed/applicationinsights-dotnet-nightly/package/nuget/Microsoft.ApplicationInsights.EventSourceListener)                               | [![Nuget](https://img.shields.io/nuget/vpre/Microsoft.ApplicationInsights.EventSourceListener.svg)](https://www.nuget.org/packages/Microsoft.ApplicationInsights.EventSourceListener/)                        |

Nightly Builds are available on our MyGet feed:
`https://www.myget.org/F/applicationinsights-dotnet-nightly/api/v3/index.json`
These builds come from the develop branch. These are not signed and are not intended for production workloads.

## Releases 
Refer to our [Milestones](https://github.com/microsoft/ApplicationInsights-dotnet/milestones) for progress on our next releases.

## Support

For immediate support relating to the Application Insights .NET SDK we encourage you to file an [Azure Support Request](https://docs.microsoft.com/azure/azure-portal/supportability/how-to-create-azure-support-request) with Microsoft Azure instead of filing a GitHub Issue in this repository. 
You can do so by going online to the [Azure portal](https://portal.azure.com/) and submitting a support request. Access to subscription management and billing support is included with your Microsoft Azure subscription, and technical support is provided through one of the [Azure Support Plans](https://azure.microsoft.com/support/plans/). For step-by-step guidance for the Azure portal, see [How to create an Azure support request](https://docs.microsoft.com/azure/azure-portal/supportability/how-to-create-azure-support-request). Alternatively, you can create and manage your support tickets programmatically using the [Azure Support ticket REST API](https://docs.microsoft.com/rest/api/support/).
"
185,microsoft/powerbi-visuals-utils-formattingutils,TypeScript,"# Microsoft Power BI visuals FormattingUtils
![Build](https://github.com/microsoft/powerbi-visuals-utils-formattingutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-formattingutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-formattingutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-formattingutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-formattingutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-formattingutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-formattingutils)

> FormattingUtils is a set of functions and classes in order to format values for Power BI custom visuals

## Usage
Learn how to install and use the FormattingUtils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-formatting)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-formattingutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
186,microsoft/ms-tpm-20-ref,C,"# Official TPM 2.0 Reference Implementation (by Microsoft) #

[![Build Status](https://travis-ci.org/Microsoft/ms-tpm-20-ref.svg?branch=master)](https://travis-ci.org/Microsoft/ms-tpm-20-ref)

This is the official TCG reference implementation of the [TPM 2.0 Specification](https://trustedcomputinggroup.org/tpm-library-specification). The project contains complete source code of the reference implementation with a Microsoft Visual Studio solution and Linux autotools build scripts.

See the definition of the `SPEC_VERSION`, `SPEC_YEAR` and `SPEC_DAY_OF_YEAR` values in the [TpmTypes.h](TPMCmd/tpm/include/TpmTypes.h) header for the exact revision/date of the TPM 2.0 specification, which the given source tree snapshot corresponds to.

The reference implementation can be directly used via the [TPM 2.0 simulator](TPMCmd/Simulator) that emulates a TPM 2.0 device and can be accessed via a custom TCP based protocol. The simplest way to work with the simulator is to use a [TSS library](https://github.com/Microsoft/TSS.MSR) for the programming language of your choice - C#/.Net, C++, Java, Python, JavaScript/Node.js are currently supported. The C language TSS implementing the TCG's TSS API specifiaction is available [here](https://github.com/tpm2-software/tpm2-tss).

## Windows build ##

Windows build is implemented as a Visual Studio 2017 solution. Before building it:

* Setup one or both of the following underlying cryptographic libraries:

   ### OpenSSL library ###

   1. Create `TPMCmd/lib` folder and place a static OpenSSL library (`libcrypto.lib`) built for the `x86` architecture there. For the `x64` architecture use the `TPMCmd/lib/x64` folder.

        The static libs can be either static libraries proper, or import libraries accompanying the corresponding DLLs. In the latter case you'll need to ensure that ther is a matching copy of the OpenSSL DLL in the standard Windows search path, so that it is available when you run the simulator executable (e.g. copy it into the same folder where `simulator.exe` is located).

        Recommended version of OpenSSL is `1.1.1d` or higher.

   2. Create `TPMCmd/OsslInclude/openssl` folder and copy there the contents of the `openssl/include/openssl` folder in the OpenSSL source tree used to build the OpenSSL library.

      If you enable SM{2,3,4} algorithms in `TpmProfile.h`, the build may fail because of missing `SM{2,3,4}.h` headers. In this case you will need to manually copy them over from OpenSSL’s `include/crypt` folder.

   3. Build the solution with either Debug or Release as the active configuration.

   ### Wolfcrypt library (wolfSSL) ###

   1. WolfSSL is included as a submodule. Initialize and update the submodule to fetch the project and checkout the appropriate commit.

        > git submodule init
        > git submodule update

        The current commit will point the minimum recommended version of wolfSSL. Moving to a more recent tag or commit should also be supported but might not be tested. 

   2. Build the solution with either WolfDebug or WolfRelease as the active configuration, either from inside the Visual Studio or with the following command line:

        > msbuild TPMCmd\simulator.sln /p:Configuration=WolfDebug

* If necessary, update the definitions of the following macros in the [VendorString.h](TPMCmd/tpm/include/VendorString.h) header: `MANUFACTURER`, `VENDOR_STRING_1`, `FIRMWARE_V1 and FIRMWARE_V2`

## Linux build

Follows the common `./bootstrap && ./configure && make` convention.

Note that autotools scripts require the following prerequisite packages: `autoconf-archive`, `pkg-config`, and sometimes `build-essential` and `automake`. Their absence is not automatically detected. The build also needs `gcc` and `libssl-dev` packages.

Similarly to the Windows build, if you enable SM{2,3,4} algorithms in `TpmProfile.h`, the build may fail because of missing `SM{2,3,4}.h` headers. In this case you will need to manually copy them over from OpenSSL’s `include/crypt` folder.

## Mac OS X build

As with the Linux build, use `./bootstrap`, `./configure`, and `make`.
If you used Homebrew to install OpenSSL, you may need to include its path in `PKG_CONFIG_PATH`.
OS X compilers treat uninitialized global variables as
[common symbols](https://developer.apple.com/library/archive/documentation/DeveloperTools/Conceptual/MachOTopics/1-Articles/executing_files.html),
which can be eliminated with the `-fno-common` compiler option.
Future updates to the autotools configurations may automate one or both of these steps.

```
./bootstrap
PKG_CONFIG_PATH=""/usr/local/opt/openssl/lib/pkgconfig"" EXTRA_CFLAGS=-fno-common ./configure
make
```
"
187,microsoft/PSRule-pipelines,PowerShell,"# PSRule extension for Azure Pipelines

An Azure DevOps extension for using PSRule within Azure Pipelines.

![ci-badge] ![extension-version]

## Support

This project uses GitHub Issues to track bugs and feature requests.
Please search the existing issues before filing new issues to avoid duplicates.

- For new issues, file your bug or feature request as a new [issue].
- For help, discussion, and support questions about using this project, join or start a [discussion].

Support for this project/ product is limited to the resources listed above.

## Getting started

The PSRule extension includes the following tasks for Azure Pipelines:

Name                | Friendly name   | Description | Reference
----                | -------------   | ----------- | ---------
`ps-rule-assert`    | PSRule analysis | Run analysis with PSRule. | [reference][ps-rule-assert]
`ps-rule-install`   | Install PSRule module | Install a PowerShell module containing rules. | [reference][ps-rule-install]

To add these tasks, use the name for YAML pipelines or friendly name of classic pipelines.

### Installing PSRule extension

To use PSRule within Azure DevOps Services, install the [extension] from the [Visual Studio Marketplace][extension].
For detailed instructions see [Install extensions][extension-install].

If you don't have permissions to install extensions within your Azure DevOps organization,
you can request it to be installed by an admin instead.

### Using within YAML pipelines

To use these tasks within YAML pipelines:

- Install rule modules with the `ps-rule-install` task (optional).
- Run analysis one or more times with the `ps-rule-assert` task.
- Publish analysis results with the [Publish Test Results](https://docs.microsoft.com/azure/devops/pipelines/tasks/test/publish-test-results?view=azure-devops&tabs=yaml) builtin task.

For example:

```yaml
steps:

# Install PSRule.Rules.Azure from the PowerShell Gallery
- task: ps-rule-install@0
  inputs:
    module: PSRule.Rules.Azure   # Install PSRule.Rules.Azure from the PowerShell Gallery.
    latest: false                # Only install the module if not already installed.
    prerelease: false            # Install stable versions only.

# Run analysis from JSON files using the `PSRule.Rules.Azure` module and custom rules from `.ps-rule/`.
- task: ps-rule-assert@0
  inputs:
    inputType: inputPath
    inputPath: 'out/*.json'                  # Read objects from JSON files in 'out/'.
    modules: 'PSRule.Rules.Azure'            # Analyze objects using the rules within the PSRule.Rules.Azure PowerShell module.
    source: '.ps-rule/'                      # Additionally, analyze object using custom rules from '.ps-rule/'.
    outputFormat: NUnit3                     # Save results to an NUnit report.
    outputPath: reports/ps-rule-results.xml  # Write NUnit report to 'reports/ps-rule-results.xml'.

# Publish NUnit report as test results
- task: PublishTestResults@2
  displayName: 'Publish PSRule results'
  inputs:
    testRunTitle: 'PSRule'                          # The title to use for the test run.
    testRunner: NUnit                               # Import report using the NUnit format.
    testResultsFiles: 'reports/ps-rule-results.xml' # The previously saved NUnit report.
```

## Changes and versioning

Extensions and tasks in this repository will use the [semantic versioning](http://semver.org/) model to declare breaking changes from v1.0.0.
Prior to v1.0.0, breaking changes may be introduced in minor (0.x.0) version increments.
For a list of module changes please see the [change log].

## Contributing

This project welcomes contributions and suggestions.
If you are ready to contribute, please visit the [contribution guide].

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Maintainers

- [Bernie White](https://github.com/BernieWhite)

## License

This project is [licensed under the MIT License][license].

[issue]: https://github.com/microsoft/PSRule-pipelines/issues
[discussion]: https://github.com/microsoft/PSRule-pipelines/discussions
[ci-badge]: https://dev.azure.com/bewhite/PSRule-pipelines/_apis/build/status/PSRule-pipelines-CI?branchName=main
[extension]: https://marketplace.visualstudio.com/items?itemName=bewhite.ps-rule
[extension-install]: https://docs.microsoft.com/en-us/azure/devops/marketplace/install-extension?view=azure-devops&tabs=browser
[extension-version]: https://vsmarketplacebadge.apphb.com/version/bewhite.ps-rule.svg
[ps-rule-assert]: docs/tasks.md#ps-rule-assert
[ps-rule-install]: docs/tasks.md#ps-rule-install
[contribution guide]: https://github.com/Microsoft/PSRule-pipelines/blob/main/CONTRIBUTING.md
[change log]: https://github.com/Microsoft/PSRule-pipelines/blob/main/CHANGELOG.md
[license]: https://github.com/Microsoft/PSRule-pipelines/blob/main/LICENSE
"
188,microsoft/CromwellOnAzure,C#,"# Welcome to Cromwell on Azure
### Latest release
 * [Release 2.3.0](https://github.com/microsoft/CromwellOnAzure/releases/tag/2.3.0)<br/>
 [Release notes for version 2.3.0](docs/release-notes/2.3.0.md)
 
Check the ""Update Instructions"" section in the version 2.3.0 [release notes](docs/release-notes/2.3.0.md/#update-instructions) to learn how to update an existing Cromwell on Azure deployment to version 2.3.0. You can customize some parameters when updating. Please [see these customization instructions](docs/troubleshooting-guide.md/#Customize-your-Cromwell-on-Azure-deployment), specifically the ""Used by update"" and ""Comment"" columns in the table.<br/>

#### Getting started
 * What is [Cromwell on Azure?](#Cromwell-on-Azure) <br/>
 * Deploy Cromwell on Azure now using this [guide](#Deploy-your-instance-of-Cromwell-on-Azure)<br/>
 * A brief [demo video](https://youtu.be/QlRQ63n_mKw) on how to run workflows using Cromwell on Azure<br/>

#### Running workflows
 * Prepare, start or abort your workflow [using this guide](docs/managing-your-workflow.md/#Managing-your-workflow)<br/>
 * Here is an example workflow to [convert FASTQ files to uBAM files](docs/example-fastq-to-ubam.md/#Example-workflow-to-convert-FASTQ-files-to-uBAM-files)<br/>
 * Have an existing WDL file that you want to run on Azure? [Modify your existing WDL with these adaptations for Azure](docs/change-existing-WDL-for-Azure.md/#How-to-modify-an-existing-WDL-file-to-run-on-Cromwell-on-Azure)<br/>
 * Want to run commonly used workflows? [Find links to ready-to-use workflows here](#Run-Common-Workflows)<br/>
 * Want to see some examples of tertiary analysis or other genomics analysis? [Find links to related project here](#Related-Projects)<br/>

#### Questions?
 * See our [Troubleshooting Guide](docs/troubleshooting-guide.md/#FAQs,-advanced-troubleshooting-and-known-issues-for-Cromwell-on-Azure) for more information<br/>
 * Known issues and work-arounds are [documented here](docs/troubleshooting-guide.md/#Known-Issues-And-Mitigation)<br/>

If you are running into an issue and cannot find any information in the troubleshooting guide, please open a GitHub issue!<br/>

![Logo](/docs/screenshots/logo.png)

## Cromwell on Azure 

[Cromwell](https://cromwell.readthedocs.io/en/stable/) is a workflow management system for scientific workflows, orchestrating the computing tasks needed for genomics analysis. Originally developed by the [Broad Institute](https://github.com/broadinstitute/cromwell), Cromwell is also used in the GATK Best Practices genome analysis pipeline. Cromwell supports running scripts at various scales, including your local machine, a local computing cluster, and on the cloud. <br />

Cromwell on Azure configures all Azure resources needed to run workflows through Cromwell on the Azure cloud, and uses the [GA4GH TES](https://cromwell.readthedocs.io/en/develop/backends/TES/) backend for orchestrating the tasks that create a workflow. The installation sets up a VM host to run the Cromwell server and uses Azure Batch to spin up virtual machines that run each task in a workflow. Cromwell workflows can be written using either the [WDL](https://github.com/openwdl/wdl) or the [CWL](https://www.commonwl.org/) scripting languages. To see examples of WDL scripts - see this ['Learn WDL'](https://github.com/openwdl/learn-wdl) repository on GitHub. To see examples of CWL scripts - see this ['CWL search result'](https://dockstore.org/search?descriptorType=CWL&searchMode=files) on Dockstore.<br />

## Deploy your instance of Cromwell on Azure

### Prerequisites

1. You will need an [Azure Subscription](https://portal.azure.com/) to deploy Cromwell on Azure.
2. You must have the proper [Azure role assignments](https://docs.microsoft.com/en-us/azure/role-based-access-control/overview) to deploy Cromwell on Azure.  To check your current role assignments, please follow [these instructions](https://docs.microsoft.com/en-us/azure/role-based-access-control/check-access).  You must have one of the following combinations of [role assignments](https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles):
   1. `Owner` of the subscription<br/>
   2. `Contributor` and `User Access Administrator` of the subscription
   3. `Owner` of the resource group. *Note: this level of access will result in a warning during deployment, and will not use the latest VM pricing data.</i>  [Learn more](/docs/troubleshooting-guide.md/#How-are-Batch-VMs-selected-to-run-tasks-in-a-workflow?).  Also, you must specify the resource group name during deployment with this level of access (see below).*
   4.  Note: if you only have `Service Administrator` as a role assignment, please assign yourself as `Owner` of the subscription.
3. Install the [Azure Command Line Interface (az cli)](https://docs.microsoft.com/en-us/cli/azure/?view=azure-cli-latest), a command line experience for managing Azure resources.
4. Run `az login` to authenticate with Azure.


### Download the deployment executable

Download the required executable from [Releases](https://github.com/microsoft/CromwellOnAzure/releases). Choose the runtime of your choice from `win-x64`, `linux-x64`, `osx-x64`. *On Windows machines, we recommend using the `win-x64` runtime (deployment using the `linux-x64` runtime via the Windows Subsystem for Linux is not supported).*<br/>

### Optional: build the executable yourself
Note: Build instructions only provided for the latest release.

#### Linux
*Preqrequisites*:<br/>
.NET Core 3.1 SDK for [Linux](https://docs.microsoft.com/en-us/dotnet/core/install/linux). Get instructions for your Linux distro and version to install the SDK. 

For example, instructions for *Ubuntu 18.04* are available [here](https://docs.microsoft.com/en-us/dotnet/core/install/linux-ubuntu#1804-) and below for convenience:

```
wget https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
sudo apt-get update && \
sudo apt-get install -y apt-transport-https && \
sudo apt-get update && \
sudo apt-get install -y dotnet-sdk-3.1
```

#### Windows
*Preqrequisites*:<br/>
.NET Core 3.1 SDK for [Windows](https://dotnet.microsoft.com/download). Get the executable and follow the wizard to install the SDK.

*Recommended*:<br/>
VS 2019

#### Build steps
1. Clone the [Cromwell on Azure repository](https://github.com/microsoft/CromwellOnAzure)
2. Build the solution using `dotnet build` on bash or Powershell. For Windows, you can choose to build and test using VS 2019
3. Run tests using `dotnet test` on bash or Powershell
4. [Publish](https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish#synopsis) the `deploy-cromwell-on-azure` project [as a self-contained deployment with your target runtime identifier (RID)](https://docs.microsoft.com/en-us/dotnet/core/deploying/#self-contained-deployments-scd) to produce the executable

Example<br/> 
Linux: `dotnet publish -r linux-x64`<br/>
Windows: `dotnet publish -r win-x64`<br/>

Learn more about `dotnet` commands [here](https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet#dotnet-commands)

### Run the deployment executable

1. **Linux and OS X only**: assign execute permissions to the file by running the following command on the terminal:<br/>
`chmod +x <fileName>`. Replace `<fileName>` with the correct name: `deploy-cromwell-on-azure-linux` or `deploy-cromwell-on-azure-osx.app`
1. You must specify the following parameters:
   1. `SubscriptionId` (**required**)
      1.  This can be obtained by navigating to the [subscriptions blade in the Azure portal](https://portal.azure.com/#blade/Microsoft_Azure_Billing/SubscriptionsBlade)
   1. `RegionName` (**required**)
      1. Specifies the region you would like to use for your Cromwell on Azure instance. To find a list of all available regions, run `az account list-locations` on the command line or in PowerShell and use the desired region's ""name"" property for `RegionName`.
   1. `MainIdentifierPrefix` (*optional*)
      1. This string will be used to prefix the name of your Cromwell on Azure resource group and associated resources. If not specified, the default value of ""coa"" followed by random characters is used as a prefix for the resource group and all Azure resources created for your Cromwell on Azure instance. After installation, you can search for your resources using the `MainIdentifierPrefix` value.<br/>
   1. `ResourceGroupName` (*optional*, **required** when you only have owner-level access of the *resource group*)
      1. Specifies the name of a pre-existing resource group that you wish to deploy into.
      
Run the following at the command line or terminal after navigating to where your executable is saved:
```
.\deploy-cromwell-on-azure.exe --SubscriptionId <Your subscription ID> --RegionName <Your region> --MainIdentifierPrefix <Your string> 
```

**Example:**
```
.\deploy-cromwell-on-azure.exe --SubscriptionId 00000000-0000-0000-0000-000000000000 --RegionName westus2 --MainIdentifierPrefix coa 
```

A [test workflow](#Hello-World-WDL-test) is run to ensure successful deployment. If your [Batch account does not have enough resource quotas](https://docs.microsoft.com/en-us/azure/batch/batch-quota-limit#resource-quotas), you will see the error while deploying. You can request more quotas by following [these instructions](https://docs.microsoft.com/en-us/azure/batch/batch-quota-limit#increase-a-quota).

Deployment, including a small test workflow can take up to 25 minutes to complete. **At installation, a user is created to allow managing the host VM with username ""vmadmin"". The password is randomly generated and shown during installation. You may want to save the username, password and resource group name to allow for advanced debugging later.**

Prepare, start or abort a workflow using instructions [here](docs/managing-your-workflow.md).

### Cromwell on Azure deployed resources

Once deployed, Cromwell on Azure configures the following Azure resources:

* [Host VM](https://azure.microsoft.com/en-us/services/virtual-machines/) - runs [Ubuntu 18.04 LTS](https://github.com/microsoft/CromwellOnAzure/blob/421ccd163bfd53807413ed696c0dab31fb2478aa/src/deploy-cromwell-on-azure/Configuration.cs#L16) and [Docker Compose with four containers](https://github.com/microsoft/CromwellOnAzure/blob/master/src/deploy-cromwell-on-azure/scripts/docker-compose.yml) (Cromwell, MySQL, TES, TriggerService).  [Blobfuse](https://github.com/Azure/azure-storage-fuse) is used to mount the default storage account as a local file system available to the four containers.  Also created are an OS and data disk, network interface, public IP address, virtual network, and network security group. [Learn more](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/)
* [Batch account](https://docs.microsoft.com/en-us/azure/batch/) - The Azure Batch account is used by TES to spin up the virtual machines that run each task in a workflow.  After deployment, create an Azure support request to increase your core quotas if you plan on running large workflows.  [Learn more](https://docs.microsoft.com/en-us/azure/batch/batch-quota-limit#resource-quotas)
* [Storage account](https://docs.microsoft.com/en-us/azure/storage/) - The Azure Storage account is mounted to the host VM using [blobfuse](https://github.com/Azure/azure-storage-fuse), which enables [Azure Block Blobs](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs) to be mounted as a local file system available to the four containers running in Docker. By default, it includes the following Blob containers - `configuration`, `cromwell-executions`, `cromwell-workflow-logs`, `inputs`, `outputs`, and `workflows`.
* [Application Insights](https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) - This contains logs from TES and the Trigger Service to enable debugging.
* [Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction) - This database is used by TES, and includes information and metadata about each TES task that is run as part of a workflow.

![Cromwell-On-Azure](/docs/screenshots/cromwellonazure.png)

All of these resources will be grouped under a single resource group in your account, which you can view on the [Azure Portal](https://portal.azure.com). **Note that your specific resource group name, host VM name and host VM password for username ""vmadmin"" are printed to the screen during deployment. You can store these for your future use, or you can reset the VM's password at a later date via the Azure Portal.**<br/>

You can [follow these steps](/docs/troubleshooting-guide.md/#Use-input-data-files-from-an-existing-Storage-account-that-my-lab-or-team-is-currently-using) if you wish to mount a different Azure Storage account that you manage or own, to your Cromwell on Azure instance.

### Connect to existing Azure resources I own that are not part of the Cromwell on Azure instance by default

Cromwell on Azure uses [managed identities](https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to allow the host VM to connect to Azure resources in a simple and secure manner.  

At the time of installation, a managed identity is created and associated with the host VM. 

**Cromwell on Azure version 2.x**

Since version 2.0, a user managed identity is created with the name `{resource-group-name}-identity` in the deployment resource group.

**Cromwell on Azure version 1.x**

For version 1.x and below, a system managed identity is created. You can find the identity via the Azure Portal by searching for the VM name in Azure Active Directory, under ""All Applications"". Or you may use Azure CLI `show` command as described [here](https://docs.microsoft.com/en-us/cli/azure/vm/identity?view=azure-cli-latest#az-vm-identity-show).

To allow the host VM to connect to **custom** Azure resources like Storage Account, Batch Account etc. you can use the [Azure Portal](https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal) or [Azure CLI](https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-cli) to find the managed identity of the host VM (if using Cromwell on Azure version 1.x) or the user-managed identity (if using Cromwell on Azure version 2.x and above) and add it as a Contributor to the required Azure resource.<br/>

![Add Role](/docs/screenshots/add-role.png)


For convenience, some configuration files are hosted on your Cromwell on Azure Storage account, in the ""configuration"" container - `containers-to-mount`, and `cromwell-application.conf`. You can modify and save these file using Azure Portal UI ""Edit Blob"" option or simply upload a new file to replace the existing one.

![Edit Configuration](/docs/screenshots/edit-config.png)


See [this section in the advanced configuration on details of how to connect a different storage account, batch account, or a private Azure Container Registry](/docs/troubleshooting-guide.md/#Customizing-your-Cromwell-on-Azure-instance).<br/>


For these changes to take effect, be sure to restart your Cromwell on Azure VM through the Azure Portal UI or run `sudo reboot`.

![Restart VM](/docs/screenshots/restartVM.png)


### Hello World WDL test

As part of the Cromwell on Azure deployment, a ""Hello World"" workflow is automatically run as a check. The input files for this workflow are found in the `inputs` container, and the output files can be found in the `cromwell-executions` container of your default storage account. 
Once it runs to completion you can find the trigger JSON file that started the workflow in the `workflows` container in the `succeeded` directory, if it ran successfully.<br/>

Hello World WDL file:
```
task hello {
  String name

  command {
    echo 'Hello ${name}!'
  }
  output {
	File response = stdout()
  }
  runtime {
	docker: 'ubuntu:16.04'
  }
}

workflow test {
  call hello
}
```

Hello World inputs.json file:
```
{
  ""test.hello.name"": ""World""
}
```

Hello World trigger JSON file as seen in your storage account's `workflows` container in the `succeeded` directory:
```
{
  ""WorkflowUrl"": ""/<storageaccountname>/inputs/test/test.wdl"",
  ""WorkflowInputsUrl"": ""/<storageaccountname>/inputs/test/test.json"",
  ""WorkflowOptionsUrl"": null,
  ""WorkflowDependenciesUrl"": null
}
```

If your ""Hello-World"" test workflow or other workflows consistently fail, make sure to [check your Azure Batch account quotas](docs/troubleshooting-guide.md/#Check-Azure-Batch-account-quotas).

## Run Common Workflows

Run Broad Institute of MIT and Harvard's Best Practices Pipelines on Cromwell on Azure:

[Data pre-processing for variant discovery](https://github.com/microsoft/gatk4-data-processing-azure)<br/>

[Germline short variant discovery (SNPs + Indels)](https://github.com/microsoft/gatk4-genome-processing-pipeline-azure)<br/>

[Somatic short variant discovery (SNVs + Indels)](https://github.com/microsoft/gatk4-somatic-snvs-indels-azure)<br/>

[Variant-filtering with Convolutional Neural Networks](https://github.com/microsoft/gatk4-cnn-variant-filter-azure)<br/>

[Sequence data format conversion](https://github.com/microsoft/seq-format-conversion-azure)<br/>

## Related Projects

[Genomics Data Analysis with Jupyter Notebooks on Azure](https://github.com/microsoft/genomicsnotebook)<br/>
"
189,microsoft/EconML,Jupyter Notebook,"[![Build Status](https://dev.azure.com/ms/EconML/_apis/build/status/Microsoft.EconML?branchName=master)](https://dev.azure.com/ms/EconML/_build/latest?definitionId=49&branchName=master)
[![PyPI version](https://img.shields.io/pypi/v/econml.svg)](https://pypi.org/project/econml/)
[![PyPI wheel](https://img.shields.io/pypi/wheel/econml.svg)](https://pypi.org/project/econml/)
[![Supported Python versions](https://img.shields.io/pypi/pyversions/econml.svg)](https://pypi.org/project/econml/)



<h1><img src=""https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/MSR-ALICE-HeaderGraphic-1920x720_1-800x550.jpg"" width=""130px"" align=""left"" style=""margin-right: 10px;""> EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation</h1>

**EconML** is a Python package for estimating heterogeneous treatment effects from observational data via machine learning. This package was designed and built as part of the [ALICE project](https://www.microsoft.com/en-us/research/project/alice/) at Microsoft Research with the goal to combine state-of-the-art machine learning 
techniques with econometrics to bring automation to complex causal inference problems. The promise of EconML:

* Implement recent techniques in the literature at the intersection of econometrics and machine learning
* Maintain flexibility in modeling the effect heterogeneity (via techniques such as random forests, boosting, lasso and neural nets), while preserving the causal interpretation of the learned model and often offering valid confidence intervals
* Use a unified API
* Build on standard Python packages for Machine Learning and Data Analysis

One of the biggest promises of machine learning is to automate decision making in a multitude of domains. At the core of many data-driven personalized decision scenarios is the estimation of heterogeneous treatment effects: what is the causal effect of an intervention on an outcome of interest for a sample with a particular set of features? In a nutshell, this toolkit is designed to measure the causal effect of some treatment variable(s) `T` on an outcome 
variable `Y`, controlling for a set of features `X, W` and how does that effect vary as a function of `X`. The methods implemented are applicable even with observational (non-experimental or historical) datasets. For the estimation results to have a causal interpretation, some methods assume no unobserved confounders (i.e. there is no unobserved variable not included in `X, W` that simultaneously has an effect on both `T` and `Y`), while others assume access to an instrument `Z` (i.e. an observed variable `Z` that has an effect on the treatment `T` but no direct effect on the outcome `Y`). Most methods provide confidence intervals and inference results.

For detailed information about the package, consult the documentation at https://econml.azurewebsites.net/.

For information on use cases and background material on causal inference and heterogeneous treatment effects see our webpage at https://www.microsoft.com/en-us/research/project/econml/

<details>
<summary><strong><em>Table of Contents</em></strong></summary>

- [News](#news)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Usage Examples](#usage-examples)
    - [Estimation Methods](#estimation-methods)
    - [Interpretability](#interpretability)
    - [Causal Model Selection and Cross-Validation](#causal-model-selection-and-cross-validation)
    - [Inference](#inference)
- [For Developers](#for-developers)
  - [Running the tests](#running-the-tests)
  - [Generating the documentation](#generating-the-documentation)
- [Blogs and Publications](#blogs-and-publications)
- [Citation](#citation)
- [Contributing and Feedback](#contributing-and-feedback)
- [References](#references)

</details>

# News

**May 8, 2021:** Release v0.11.0, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.11.0)

<details><summary>Previous releases</summary>

**March 22, 2021:** Release v0.10.0, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.10.0)

**March 11, 2021:** Release v0.9.2, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.9.2)

**March 3, 2021:** Release v0.9.1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.9.1)

**February 20, 2021:** Release v0.9.0, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.9.0)

**January 20, 2021:** Release v0.9.0b1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.9.0b1)

**November 20, 2020:** Release v0.8.1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.8.1)

**November 18, 2020:** Release v0.8.0, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.8.0)

**September 4, 2020:** Release v0.8.0b1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.8.0b1)

**March 6, 2020:** Release v0.7.0, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.7.0)

**February 18, 2020:** Release v0.7.0b1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.7.0b1)

**January 10, 2020:** Release v0.6.1, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.6.1)

**December 6, 2019:** Release v0.6, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.6)

**November 21, 2019:** Release v0.5, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.5). 

**June 3, 2019:** Release v0.4, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.4). 

**May 3, 2019:** Release v0.3, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.3).

**April 10, 2019:** Release v0.2, see release notes [here](https://github.com/Microsoft/EconML/releases/tag/v0.2).

**March 6, 2019:** Release v0.1, welcome to have a try and provide feedback.

</details>

# Getting Started

## Installation

Install the latest release from [PyPI](https://pypi.org/project/econml/):
```
pip install econml
```
To install from source, see [For Developers](#for-developers) section below.

## Usage Examples
### Estimation Methods

<details>
  <summary>Double Machine Learning (aka RLearner) (click to expand)</summary>

  * Linear final stage

  ```Python
  from econml.dml import LinearDML
  from sklearn.linear_model import LassoCV
  from econml.inference import BootstrapInference

  est = LinearDML(model_y=LassoCV(), model_t=LassoCV())
  ### Estimate with OLS confidence intervals
  est.fit(Y, T, X=X, W=W) # W -> high-dimensional confounders, X -> features
  treatment_effects = est.effect(X_test)
  lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals

  ### Estimate with bootstrap confidence intervals
  est.fit(Y, T, X=X, W=W, inference='bootstrap')  # with default bootstrap parameters
  est.fit(Y, T, X=X, W=W, inference=BootstrapInference(n_bootstrap_samples=100))  # or customized
  lb, ub = est.effect_interval(X_test, alpha=0.05) # Bootstrap confidence intervals
  ```

  * Sparse linear final stage

  ```Python
  from econml.dml import SparseLinearDML
  from sklearn.linear_model import LassoCV

  est = SparseLinearDML(model_y=LassoCV(), model_t=LassoCV())
  est.fit(Y, T, X=X, W=W) # X -> high dimensional features
  treatment_effects = est.effect(X_test)
  lb, ub = est.effect_interval(X_test, alpha=0.05) # Confidence intervals via debiased lasso
  ```

  * Generic Machine Learning last stage
  
  ```Python
  from econml.dml import NonParamDML
  from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

  est = NonParamDML(model_y=RandomForestRegressor(),
                    model_t=RandomForestClassifier(),
                    model_final=RandomForestRegressor(),
                    discrete_treatment=True)
  est.fit(Y, T, X=X, W=W) 
  treatment_effects = est.effect(X_test)
  ```

</details>

<details>
  <summary>Causal Forests (click to expand)</summary>

  ```Python
  from econml.dml import CausalForestDML
  from sklearn.linear_model import LassoCV
  # Use defaults
  est = CausalForestDML()
  # Or specify hyperparameters
  est = CausalForestDML(criterion='het', n_estimators=500,       
                        min_samples_leaf=10, 
                        max_depth=10, max_samples=0.5,
                        discrete_treatment=False,
                        model_t=LassoCV(), model_y=LassoCV())
  est.fit(Y, T, X=X, W=W)
  treatment_effects = est.effect(X_test)
  # Confidence intervals via Bootstrap-of-Little-Bags for forests
  lb, ub = est.effect_interval(X_test, alpha=0.05)
  ```
</details>


<details>
  <summary>Orthogonal Random Forests (click to expand)</summary>

  ```Python
  from econml.orf import DMLOrthoForest, DROrthoForest
  from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV
  # Use defaults
  est = DMLOrthoForest()
  est = DROrthoForest()
  # Or specify hyperparameters
  est = DMLOrthoForest(n_trees=500, min_leaf_size=10,
                       max_depth=10, subsample_ratio=0.7,
                       lambda_reg=0.01,
                       discrete_treatment=False,
                       model_T=WeightedLasso(alpha=0.01), model_Y=WeightedLasso(alpha=0.01),
                       model_T_final=WeightedLassoCV(cv=3), model_Y_final=WeightedLassoCV(cv=3))
  est.fit(Y, T, X=X, W=W)
  treatment_effects = est.effect(X_test)
  # Confidence intervals via Bootstrap-of-Little-Bags for forests
  lb, ub = est.effect_interval(X_test, alpha=0.05)
  ```
</details>

<details>

<summary>Meta-Learners (click to expand)</summary>
  
  * XLearner

  ```Python
  from econml.metalearners import XLearner
  from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

  est = XLearner(models=GradientBoostingRegressor(),
                propensity_model=GradientBoostingClassifier(),
                cate_models=GradientBoostingRegressor())
  est.fit(Y, T, X=np.hstack([X, W]))
  treatment_effects = est.effect(np.hstack([X_test, W_test]))

  # Fit with bootstrap confidence interval construction enabled
  est.fit(Y, T, X=np.hstack([X, W]), inference='bootstrap')
  treatment_effects = est.effect(np.hstack([X_test, W_test]))
  lb, ub = est.effect_interval(np.hstack([X_test, W_test]), alpha=0.05) # Bootstrap CIs
  ```
  
  * SLearner

  ```Python
  from econml.metalearners import SLearner
  from sklearn.ensemble import GradientBoostingRegressor

  est = SLearner(overall_model=GradientBoostingRegressor())
  est.fit(Y, T, X=np.hstack([X, W]))
  treatment_effects = est.effect(np.hstack([X_test, W_test]))
  ```

  * TLearner

  ```Python
  from econml.metalearners import TLearner
  from sklearn.ensemble import GradientBoostingRegressor

  est = TLearner(models=GradientBoostingRegressor())
  est.fit(Y, T, X=np.hstack([X, W]))
  treatment_effects = est.effect(np.hstack([X_test, W_test]))
  ```
</details>

<details>
<summary>Doubly Robust Learners (click to expand)
</summary>

* Linear final stage

```Python
from econml.dr import LinearDRLearner
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier

est = LinearDRLearner(model_propensity=GradientBoostingClassifier(),
                      model_regression=GradientBoostingRegressor())
est.fit(Y, T, X=X, W=W)
treatment_effects = est.effect(X_test)
lb, ub = est.effect_interval(X_test, alpha=0.05)
```

* Sparse linear final stage

```Python
from econml.dr import SparseLinearDRLearner
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier

est = SparseLinearDRLearner(model_propensity=GradientBoostingClassifier(),
                            model_regression=GradientBoostingRegressor())
est.fit(Y, T, X=X, W=W)
treatment_effects = est.effect(X_test)
lb, ub = est.effect_interval(X_test, alpha=0.05)
```

* Nonparametric final stage

```Python
from econml.dr import ForestDRLearner
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier

est = ForestDRLearner(model_propensity=GradientBoostingClassifier(),
                      model_regression=GradientBoostingRegressor())
est.fit(Y, T, X=X, W=W) 
treatment_effects = est.effect(X_test)
lb, ub = est.effect_interval(X_test, alpha=0.05)
```
</details>

<details>
<summary>Orthogonal Instrumental Variables (click to expand)</summary>

* Intent to Treat Doubly Robust Learner (discrete instrument, discrete treatment)

```Python
from econml.iv.dr import LinearIntentToTreatDRIV
from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression

est = LinearIntentToTreatDRIV(model_Y_X=GradientBoostingRegressor(),
                              model_T_XZ=GradientBoostingClassifier(),
                              flexible_model_effect=GradientBoostingRegressor())
est.fit(Y, T, Z=Z, X=X) # OLS inference by default
treatment_effects = est.effect(X_test)
lb, ub = est.effect_interval(X_test, alpha=0.05) # OLS confidence intervals
```

</details>

<details>
<summary>Deep Instrumental Variables (click to expand)</summary>

```Python
import keras
from econml.iv.nnet import DeepIV

treatment_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(2,)),
                                    keras.layers.Dropout(0.17),
                                    keras.layers.Dense(64, activation='relu'),
                                    keras.layers.Dropout(0.17),
                                    keras.layers.Dense(32, activation='relu'),
                                    keras.layers.Dropout(0.17)])
response_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(2,)),
                                  keras.layers.Dropout(0.17),
                                  keras.layers.Dense(64, activation='relu'),
                                  keras.layers.Dropout(0.17),
                                  keras.layers.Dense(32, activation='relu'),
                                  keras.layers.Dropout(0.17),
                                  keras.layers.Dense(1)])
est = DeepIV(n_components=10, # Number of gaussians in the mixture density networks)
             m=lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model
             h=lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model
             n_samples=1 # Number of samples used to estimate the response
             )
est.fit(Y, T, X=X, Z=Z) # Z -> instrumental variables
treatment_effects = est.effect(X_test)
```
</details>

See the <a href=""#references"">References</a> section for more details.

### Interpretability
<details>
  <summary>Tree Interpreter of the CATE model (click to expand)</summary>
  
  ```Python
  from econml.cate_interpreter import SingleTreeCateInterpreter
  intrp = SingleTreeCateInterpreter(include_model_uncertainty=True, max_depth=2, min_samples_leaf=10)
  # We interpret the CATE model's behavior based on the features used for heterogeneity
  intrp.interpret(est, X)
  # Plot the tree
  plt.figure(figsize=(25, 5))
  intrp.plot(feature_names=['A', 'B', 'C', 'D'], fontsize=12)
  plt.show()
  ```
  ![image](notebooks/images/dr_cate_tree.png)
  
</details>

<details>
  <summary>Policy Interpreter of the CATE model (click to expand)</summary>

  ```Python
  from econml.cate_interpreter import SingleTreePolicyInterpreter
  # We find a tree-based treatment policy based on the CATE model
  intrp = SingleTreePolicyInterpreter(risk_level=0.05, max_depth=2, min_samples_leaf=1,min_impurity_decrease=.001)
  intrp.interpret(est, X, sample_treatment_costs=0.2)
  # Plot the tree
  plt.figure(figsize=(25, 5))
  intrp.plot(feature_names=['A', 'B', 'C', 'D'], fontsize=12)
  plt.show()
  ```
  ![image](notebooks/images/dr_policy_tree.png)

</details>

<details>
  <summary>SHAP values for the CATE model (click to expand)</summary>

  ```Python
  import shap
  from econml.dml import CausalForestDML
  est = CausalForestDML()
  est.fit(Y, T, X=X, W=W)
  shap_values = est.shap_values(X)
  shap.summary_plot(shap_values['Y0']['T0'])
  ```

</details>


### Causal Model Selection and Cross-Validation


<details>
  <summary>Causal model selection with the `RScorer` (click to expand)</summary>

  ```Python
  from econml.score import Rscorer

  # split data in train-validation
  X_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, y, test_size=.4)

  # define list of CATE estimators to select among
  reg = lambda: RandomForestRegressor(min_samples_leaf=20)
  clf = lambda: RandomForestClassifier(min_samples_leaf=20)
  models = [('ldml', LinearDML(model_y=reg(), model_t=clf(), discrete_treatment=True,
                               linear_first_stages=False, cv=3)),
            ('xlearner', XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),
            ('dalearner', DomainAdaptationLearner(models=reg(), final_models=reg(), propensity_model=clf())),
            ('slearner', SLearner(overall_model=reg())),
            ('drlearner', DRLearner(model_propensity=clf(), model_regression=reg(),
                                    model_final=reg(), cv=3)),
            ('rlearner', NonParamDML(model_y=reg(), model_t=clf(), model_final=reg(),
                                     discrete_treatment=True, cv=3)),
            ('dml3dlasso', DML(model_y=reg(), model_t=clf(),
                               model_final=LassoCV(cv=3, fit_intercept=False),
                               discrete_treatment=True,
                               featurizer=PolynomialFeatures(degree=3),
                               linear_first_stages=False, cv=3))
  ]

  # fit cate models on train data
  models = [(name, mdl.fit(Y_train, T_train, X=X_train)) for name, mdl in models]

  # score cate models on validation data
  scorer = RScorer(model_y=reg(), model_t=clf(),
                   discrete_treatment=True, cv=3, mc_iters=2, mc_agg='median')
  scorer.fit(Y_val, T_val, X=X_val)
  rscore = [scorer.score(mdl) for _, mdl in models]
  # select the best model
  mdl, _ = scorer.best_model([mdl for _, mdl in models])
  # create weighted ensemble model based on score performance
  mdl, _ = scorer.ensemble([mdl for _, mdl in models])
  ```

</details>

<details>
  <summary>First Stage Model Selection (click to expand)</summary>

First stage models can be selected either by passing in cross-validated models (e.g. `sklearn.linear_model.LassoCV`) to EconML's estimators or perform the first stage model selection outside of EconML and pass in the selected model. Unless selecting among a large set of hyperparameters, choosing first stage models externally is the preferred method due to statistical and computational advantages.

```Python
from econml.dml import LinearDML
from sklearn import clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

cv_model = GridSearchCV(
              estimator=RandomForestRegressor(),
              param_grid={
                  ""max_depth"": [3, None],
                  ""n_estimators"": (10, 30, 50, 100, 200),
                  ""max_features"": (2, 4, 6),
              },
              cv=5,
           )
# First stage model selection within EconML
# This is more direct, but computationally and statistically less efficient
est = LinearDML(model_y=cv_model, model_t=cv_model)
# First stage model selection ouside of EconML
# This is the most efficient, but requires boilerplate code
model_t = clone(cv_model).fit(W, T).best_estimator_
model_y = clone(cv_model).fit(W, Y).best_estimator_
est = LinearDML(model_y=model_t, model_t=model_y)
```


</details>

### Inference

Whenever inference is enabled, then one can get a more structure `InferenceResults` object with more elaborate inference information, such
as p-values and z-statistics. When the CATE model is linear and parametric, then a `summary()` method is also enabled. For instance:

  ```Python
  from econml.dml import LinearDML
  # Use defaults
  est = LinearDML()
  est.fit(Y, T, X=X, W=W)
  # Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]
  est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)
  # Get the population summary for the entire sample X
  est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)
  #  Get the parameter inference summary for the final model
  est.summary()
  ```
  
  <details><summary>Example Output (click to expand)</summary>
  
  ```Python
  # Get the effect inference summary, which includes the standard error, z test score, p value, and confidence interval given each sample X[i]
  est.effect_inference(X_test).summary_frame(alpha=0.05, value=0, decimals=3)
  ```
  ![image](notebooks/images/summary_frame.png)
  
  ```Python
  # Get the population summary for the entire sample X
  est.effect_inference(X_test).population_summary(alpha=0.1, value=0, decimals=3, tol=0.001)
  ```
  ![image](notebooks/images/population_summary.png)
  
  ```Python
  #  Get the parameter inference summary for the final model
  est.summary()
  ```
  ![image](notebooks/images/summary.png)
  
  </details>
  

### Policy Learning

You can also perform direct policy learning from observational data, using the doubly robust method for offline
policy learning. These methods directly predict a recommended treatment, without internally fitting an explicit
model of the conditional average treatment effect.

<details>
  <summary>Doubly Robust Policy Learning (click to expand)</summary>

```Python
from econml.policy import DRPolicyTree, DRPolicyForest
from sklearn.ensemble import RandomForestRegressor

# fit a single binary decision tree policy
policy = DRPolicyTree(max_depth=1, min_impurity_decrease=0.01, honest=True)
policy.fit(y, T, X=X, W=W)
# predict the recommended treatment
recommended_T = policy.predict(X)
# plot the binary decision tree
plt.figure(figsize=(10,5))
policy.plot()
# get feature importances
importances = policy.feature_importances_

# fit a binary decision forest
policy = DRPolicyForest(max_depth=1, min_impurity_decrease=0.01, honest=True)
policy.fit(y, T, X=X, W=W)
# predict the recommended treatment
recommended_T = policy.predict(X)
# plot the first tree in the ensemble
plt.figure(figsize=(10,5))
policy.plot(0)
# get feature importances
importances = policy.feature_importances_
```


  ![image](images/policy_tree.png)
</details>

To see more complex examples, go to the [notebooks](https://github.com/Microsoft/EconML/tree/master/notebooks) section of the repository. For a more detailed description of the treatment effect estimation algorithms, see the EconML [documentation](https://econml.azurewebsites.net/).

# For Developers

You can get started by cloning this repository. We use 
[setuptools](https://setuptools.readthedocs.io/en/latest/index.html) for building and distributing our package.
We rely on some recent features of setuptools, so make sure to upgrade to a recent version with
`pip install setuptools --upgrade`.  Then from your local copy of the repository you can run `python setup.py develop` to get started.

## Running the tests

This project uses [pytest](https://docs.pytest.org/) for testing.  To run tests locally after installing the package, 
you can use `python setup.py pytest`.

## Generating the documentation

This project's documentation is generated via [Sphinx](https://www.sphinx-doc.org/en/master/index.html).  Note that we use [graphviz](https://graphviz.org/)'s 
`dot` application to produce some of the images in our documentation, so you should make sure that `dot` is installed and in your path.

To generate a local copy of the documentation from a clone of this repository, just run `python setup.py build_sphinx -W -E -a`, which will build the documentation and place it under the `build/sphinx/html` path. 

The reStructuredText files that make up the documentation are stored in the [docs directory](https://github.com/Microsoft/EconML/tree/master/doc); module documentation is automatically generated by the Sphinx build process.

# Blogs and Publications

* June 2019: [Treatment Effects with Instruments paper](https://arxiv.org/pdf/1905.10176.pdf)

* May 2019: [Open Data Science Conference Workshop](https://odsc.com/speakers/machine-learning-estimation-of-heterogeneous-treatment-effect-the-microsoft-econml-library/) 

* 2018: [Orthogonal Random Forests paper](http://proceedings.mlr.press/v97/oprescu19a.html)

* 2017: [DeepIV paper](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf)

# Citation

If you use EconML in your research, please cite us as follows:

   Microsoft Research. **EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.** https://github.com/microsoft/EconML, 2019. Version 0.x.

BibTex:

```
@misc{econml,
  author={Microsoft Research},
  title={{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},
  howpublished={https://github.com/microsoft/EconML},
  note={Version 0.x},
  year={2019}
}
```

# Contributing and Feedback

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# References

Athey, Susan, and Stefan Wager.
**Policy learning with observational data.**
Econometrica 89.1 (2021): 133-161.

X Nie, S Wager.
**Quasi-Oracle Estimation of Heterogeneous Treatment Effects.**
[*Biometrika*](https://doi.org/10.1093/biomet/asaa076), 2020

V. Syrgkanis, V. Lei, M. Oprescu, M. Hei, K. Battocchi, G. Lewis.
**Machine Learning Estimation of Heterogeneous Treatment Effects with Instruments.**
[*Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS)*](https://arxiv.org/abs/1905.10176), 2019
**(Spotlight Presentation)**

D. Foster, V. Syrgkanis.
**Orthogonal Statistical Learning.**
[*Proceedings of the 32nd Annual Conference on Learning Theory (COLT)*](https://arxiv.org/pdf/1901.09036.pdf), 2019
**(Best Paper Award)**

M. Oprescu, V. Syrgkanis and Z. S. Wu.
**Orthogonal Random Forest for Causal Inference.**
[*Proceedings of the 36th International Conference on Machine Learning (ICML)*](http://proceedings.mlr.press/v97/oprescu19a.html), 2019.

S. Künzel, J. Sekhon, J. Bickel and B. Yu.
**Metalearners for estimating heterogeneous treatment effects using machine learning.**
[*Proceedings of the national academy of sciences, 116(10), 4156-4165*](https://www.pnas.org/content/116/10/4156), 2019.

S. Athey, J. Tibshirani, S. Wager.
**Generalized random forests.**
[*Annals of Statistics, 47, no. 2, 1148--1178*](https://projecteuclid.org/euclid.aos/1547197251), 2019.

V. Chernozhukov, D. Nekipelov, V. Semenova, V. Syrgkanis.
**Plug-in Regularized Estimation of High-Dimensional Parameters in Nonlinear Semiparametric Models.**
[*Arxiv preprint arxiv:1806.04823*](https://arxiv.org/abs/1806.04823), 2018.

S. Wager, S. Athey.
**Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.**
[*Journal of the American Statistical Association, 113:523, 1228-1242*](https://www.tandfonline.com/doi/citedby/10.1080/01621459.2017.1319839), 2018.

Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. **Deep IV: A flexible approach for counterfactual prediction.** [*Proceedings of the 34th International Conference on Machine Learning, ICML'17*](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf), 2017.

V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, and a. W. Newey. **Double Machine Learning for Treatment and Causal Parameters.** [*ArXiv preprint arXiv:1608.00060*](https://arxiv.org/abs/1608.00060), 2016.

Dudik, M., Erhan, D., Langford, J., & Li, L.
**Doubly robust policy evaluation and optimization.**
Statistical Science, 29(4), 485-511, 2014."
190,microsoft/Workbooks-Localization,,"Localization files for Application-Insights-Workbooks
"
191,microsoft/MLOS,HTML,"[![codecov](https://codecov.io/gh/microsoft/MLOS/branch/main/graph/badge.svg?token=14T6RFL2KR)](https://codecov.io/gh/microsoft/MLOS)

# MLOS: Machine Learning Optimized Systems

## MLOS: An Infrastructure for Automated Software Performance Engineering

> MLOS is an ML-powered infrastructure and methodology to democratize and automate Performance Engineering. MLOS enables continuous, instance-based, robust, and trackable systems optimization.

From the [MLOS paper at DEEM 2020](https://arxiv.org/abs/2006.02155)

## Overview

### Problem

All systems software (e.g. SqlServer, MySQL, LevelDB, OpenSSL, etc.) is full of parameter choices.

Sometimes these are encoded in the software as constants embedded in the code (e.g. choice of abstract data structure implementation, buffer limit size or alignment, etc.).
Other times they may be exposed as configuration parameters either at startup or runtime.

Careful selection of these parameters can yield dramatic performance differences for different _contexts_ of a system (e.g. different workloads, hardware, etc.).
Note that _performance_ can be interpreted in different ways (e.g. reducing average/variability of latency/memory, increasing throughput, decreasing MTTR, etc.)

Generally speaking, this process is referred to as *Software Performance Engineering*, and typically involves a lot of manual effort that is brittle and not well tracked.

### Goals

MLOS is about using machine-learning and data-science to optimize systems for a given context through these tunable choices.

![MLOS data science experience for software performance engineering](./documentation/images/MLOS-Experience.png)

Roughly, this can happen in two modes:

1. Offline (e.g. at development time)

    In this case, developers can use (micro)benchmarks to explore a parameter space for a component either interactively or with a background CI/CD pipeline and then interact with that data through a notebook experience to select the right value to check in to the code, along with the results of the experiments and analysis, all encoded in the notebook.

2. Online (e.g. at runtime)

    In this case a system component provides hooks to adjust its parameters at runtime and exports data about its current state/performance.  These can be combined with additional contextual information from the system to build a model (or simple heuristics) to invoke the hooks to adjust the component to improve performance at runtime.

### Architecture

![MLOS architecture overview](./documentation/images/MLOS-Architecture.png)

To achieve this MLOS provides:

1. *Code Annotations* to help describe additional *settings metadata* for tunables (a.k.a. `Settings`).

    For instance, metadata can include things like constraints on acceptable values a Setting can take on as well as developer intuition to help guide the automated search process.

    Currently these are implemented as C# Attributes to provide reflection and easy cross-platform and cross-compiler support for C++ projects.

2. *Code Generation* tools to use that metadata to expose those settings to different target systems/languages (e.g. Python Notebooks, C++, C#, etc.)

    For instance, we generate efficient messages over shared memory communication channels for

    1. exporting data about the component using that Setting

        For instance, this may include performance statistics, workload traces, etc.

    2. receiving feedback (e.g. to change the Setting's value)

        This may involve a reconfiguration step or simply update a cache for the next instantiation to read.

3. An external agent (`Mlos.Agent.Server`) which can consume the information exported by the target system (e.g. SqlServer, MySQL, LevelDB, etc.) with mimimal impact on the target system.

    The external agent can perform workload summarization, binning, cataloging, model inference, heuristic invocation, etc. based on the events exposed by the target system to then influence it.

    Once hooks are created in the target system, iteration on the external agent can be more rapidly developed and deployed.

## Python Quickstart

The easiest way to get started with MLOS is to just the Python package.
You can find installation instructions in the [Prerequisites: Python Quickstart](./documentation/01-Prerequisites.md#python-quickstart).

## Full Build (C# and C++ components)

MLOS supports Windows and Linux build environments.

For detailed instructions, please refer to:

  1. [Prerequisites](./documentation/01-Prerequisites.md)
  2. [Build](./documentation/02-Build.md)

## Examples

Code and documentation for examples of using MLOS to optimize a system are described in the [Notebooks](https://microsoft.github.io/MLOS/notebooks/) section.
Additional code is in the  [source/Examples](./source/Examples/#mlos-github-tree-view) source directory.
You can find the source of the notebooks [on github as well](./source/Mlos.Notebooks/#mlos-github-tree-view).

> Some of the notebooks have been used as lab assignments for a seminar class run in collaboration between Microsoft and UW-Madison:
> <https://aka.ms/MLOS_Seminar>

## Documentation

- Additional overview documentation is available in the [documentation](./documentation/) tree.

- Individual components may also include more detailed documentation in their respective subdirectories.

## Contributing

We welcome contributions!  Please see [Contributing](./CONTRIBUTING.md) and [Code of Conduct](./CODE_OF_CONDUCT.md) for details.

Also, please see the [Roadmap](#) of planned features.

## Contact

For more formal enquiries, you can [contact us](mailto:mlos-maintainers@service.microsoft.com).

## License

- [MIT License](./LICENSE.txt)
"
192,microsoft/vscode-cmake-tools,TypeScript,"# CMake Tools

[CMake Tools](https://marketplace.visualstudio.com/items?itemName=ms-vscode.cmake-tools) provides the native developer a full-featured, convenient, and powerful workflow for CMake-based projects in Visual Studio Code.

## Important doc links

- [CMake Tools quick start](https://code.visualstudio.com/docs/cpp/CMake-linux)
- [Configure and build a project with CMake Presets](docs/cmake-presets.md)
- [Configure a project with kits and variants](docs/how-to.md#configure-a-project)
- [Build a project with kits and variants](docs/how-to.md#build-a-project)
- [Debug a project](docs/how-to.md#debug-a-project)
- [Configure CMake Tools settings](docs/cmake-settings.md)
- [How to](docs/how-to.md)
- [FAQ](docs/faq.md)
- [Read the online documentation](docs/README.md)
- [Contribute](docs/contribute.md)

## Issues? Questions? Feature requests?

**PLEASE**, if you experience any problems, have any questions, or have an idea
for a new feature, create an issue on [the GitHub page](https://github.com/microsoft/vscode-cmake-tools)!

This extension itself *does not* provide language support for the CMake
scripting language. For that we recommend [this extension](https://marketplace.visualstudio.com/items?itemName=twxs.cmake).

### Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.

### Data/Telemetry

This extension collects usage data and sends it to Microsoft to help improve our products and services. Collection of telemetry is controlled via the same setting provided by Visual Studio Code: `""telemetry.enableTelemetry""`. Read our [privacy statement](https://privacy.microsoft.com/en-us/privacystatement) to learn more.

### Credits

This project was started by [@vector-of-bool](https://github.com/vector-of-bool) and is now currently maintained by Microsoft.
"
193,microsoft/vscode-typescript-next,JavaScript,"# JavaScript and TypeScript Nightly

VS Code extension that enables the nightly build of TypeScript (`typescript@next`) as VS Code's built-in TypeScript version used to power JavaScript and TypeScript IntelliSense.

## Enabling
This extension replaces VS Code's built-in TypeScript version with `typescript@next`. It does not affect workspace versions of TypeScript, or custom user `typescript.tsdk` settings.

To make sure you are using `typescript@next`:

1. Open a JavaScript or TypeScript file in VS Code.
1. In the VS Code command palette, run the `TypeScript: Select TypeScript version` command.
1. Make sure you have `Use VS Code's version selected`

Note that this extension also includes the [latest JavaScript and TypeScript grammar](https://github.com/microsoft/TypeScript-TmLanguage).
"
194,microsoft/Azure-Analytics-and-AI-Engagement,Jupyter Notebook,"# Setting the scene

The intent of Data and AI Engagement Accelerators for PoC is to provide a conceptual starting point for our sellers, using our industry scenario differentiator demos, to gain customer confidence in the feasibility of a Microsoft solution. 

The Data and AI Engagement Accelerators for PoC will leverage our new and existing industry scenario demos to create packaged content (including Azure resource management templates, code, sample data etc.) that sellers can easily deploy and quickly configure on the customer’s Azure subscription and then modify using the customer’s sample data from which the customer can begin to build their own solution.

Industries these days have multiple business, they have multiple factories and multiple subsidiaries, it is important to keep a track of what is happening in each subsidiary through Power BI Analytics. The use of analytics has the potential to unlock how industries understand the story of their factories, ***the people, the machines, and the financials in the past, present and future***. An insight into the above can give us a sneak peek to two different scenarios at the same time.

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
195,microsoft/etcd3,TypeScript,"# etcd3 [![Run Tests](https://github.com/microsoft/etcd3/workflows/Run%20Tests/badge.svg)](https://github.com/microsoft/etcd3/actions?query=workflow%3A%22Run+Tests%22)

etcd3 is a high-quality, production-ready client for the Protocol Buffer-based [etcd](https://etcd.io/) v3 API. It includes:

- [load balancing](https://microsoft.github.io/etcd3/interfaces/ioptions.html)
- [fault handling and reconnections](https://microsoft.github.io/etcd3/interfaces/ioptions.html#faulthandling)
- [transactions](https://microsoft.github.io/etcd3/classes/comparatorbuilder.html)
- [software transactional memory](https://microsoft.github.io/etcd3/classes/softwaretransaction.html)
- [high-level query builders](https://microsoft.github.io/etcd3/classes/etcd3.html)
- [lease management](https://microsoft.github.io/etcd3/classes/lease.html)
- [watchers](https://microsoft.github.io/etcd3/classes/watchbuilder.html)
- [user](https://microsoft.github.io/etcd3/classes/etcd3.html#user) and [role](https://microsoft.github.io/etcd3/classes/etcd3.html#role) [mocking](https://microsoft.github.io/etcd3/classes/etcd3.html#mock) management
- [elections](https://microsoft.github.io/etcd3/classes/election.html)

and is type-safe for TypeScript consumers.

### Quickstart

Install via:

```
npm install --save etcd3
```

Start building!

```js
const { Etcd3 } = require('etcd3');
const client = new Etcd3();

(async () => {
  await client.put('foo').value('bar');

  const fooValue = await client.get('foo').string();
  console.log('foo was:', fooValue);

  const allFValues = await client.getAll().prefix('f').keys();
  console.log('all our keys starting with ""f"":', allFValues);

  await client.delete().all();
})();
```

### API Documentation

Our [TypeDoc docs are available here](https://microsoft.github.io/etcd3/classes/etcd3.html).

Our [test cases](https://github.com/microsoft/etcd3/tree/master/src/test/) are also readable.

### Running tests

```sh
$ npm install
$ cd src/test/containers/3.2 && docker-compose up # in a separate shell
$ npm test
$ docker-compose down
```

### Contributing

Running tests for this module requires running an etcd3 server locally. The tests try to use the default port initially, and you can configure this by setting the `ETCD_ADDR` environment variable, like `export ETCD_ADDR=localhost:12345`.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
196,microsoft/node-pty,TypeScript,"# node-pty

[![Build Status](https://dev.azure.com/vscode/node-pty/_apis/build/status/Microsoft.node-pty)](https://dev.azure.com/vscode/node-pty/_build/latest?definitionId=11)

`forkpty(3)` bindings for node.js. This allows you to fork processes with pseudoterminal file descriptors. It returns a terminal object which allows reads and writes.

This is useful for:

- Writing a terminal emulator (eg. via [xterm.js](https://github.com/sourcelair/xterm.js)).
- Getting certain programs to *think* you're a terminal, such as when you need a program to send you control sequences.

`node-pty` supports Linux, macOS and Windows. Windows support is possible by utilizing the [Windows conpty API](https://blogs.msdn.microsoft.com/commandline/2018/08/02/windows-command-line-introducing-the-windows-pseudo-console-conpty/) on Windows 1809+ and the [winpty](https://github.com/rprichard/winpty) library in older version.

## API

The full API for node-pty is contained within the [TypeScript declaration file](https://github.com/microsoft/node-pty/blob/main/typings/node-pty.d.ts), use the branch/tag picker in GitHub (`w`) to navigate to the correct version of the API.

## Example Usage

```js
var os = require('os');
var pty = require('node-pty');

var shell = os.platform() === 'win32' ? 'powershell.exe' : 'bash';

var ptyProcess = pty.spawn(shell, [], {
  name: 'xterm-color',
  cols: 80,
  rows: 30,
  cwd: process.env.HOME,
  env: process.env
});

ptyProcess.on('data', function(data) {
  process.stdout.write(data);
});

ptyProcess.write('ls\r');
ptyProcess.resize(100, 40);
ptyProcess.write('ls\r');
```

## Real-world Uses

`node-pty` powers many different terminal emulators, including:

- [Microsoft Visual Studio Code](https://code.visualstudio.com)
- [Hyper](https://hyper.is/)
- [Upterm](https://github.com/railsware/upterm)
- [Script Runner](https://github.com/ioquatix/script-runner) for Atom.
- [Theia](https://github.com/theia-ide/theia)
- [FreeMAN](https://github.com/matthew-matvei/freeman) file manager
- [terminus](https://atom.io/packages/terminus) - An Atom plugin for providing terminals inside your Atom workspace.
- [x-terminal](https://atom.io/packages/x-terminal) - Also an Atom plugin that provides terminals inside your Atom workspace.
- [Termination](https://atom.io/packages/termination) - Also an Atom plugin that provides terminals inside your Atom workspace.
- [atom-xterm](https://atom.io/packages/atom-xterm) - Also an Atom plugin that provides terminals inside your Atom workspace.
- [electerm](https://github.com/electerm/electerm) Terminal/SSH/SFTP client(Linux, macOS, Windows).
- [Extraterm](http://extraterm.org/)
- [Wetty](https://github.com/krishnasrinivas/wetty) Browser based Terminal over HTTP and HTTPS
- [nomad](https://github.com/lukebarnard1/nomad-term)
- [DockerStacks](https://github.com/sfx101/docker-stacks) Local LAMP/LEMP stack using Docker
- [TeleType](https://github.com/akshaykmr/TeleType): cli tool that allows you to share your terminal online conveniently. Show off mad cli-fu, help a colleague, teach, or troubleshoot.
- [mesos-term](https://github.com/criteo/mesos-term): A web terminal for Apache Mesos. It allows to execute commands within containers.
- [Commas](https://github.com/CyanSalt/commas): A hackable terminal and command runner.
- [ENiGMA½ BBS Software](https://github.com/NuSkooler/enigma-bbs): A modern BBS software with a nostalgic flair!
- [Tinkerun](https://github.com/tinkerun/tinkerun): A new way of running Tinker.

Do you use node-pty in your application as well? Please open a [Pull Request](https://github.com/Tyriar/node-pty/pulls) to include it here. We would love to have it in our list.

## Building

```bash
# Install dependencies and build C++
npm install
# Compile TypeScript -> JavaScript
npm run build
```

## Dependencies

Node.JS 12+ or Electron 8+ is required to use `node-pty`.

### Linux (apt)

```sh
sudo apt install -y make python build-essential
```

### macOS

Xcode is needed to compile the sources, this can be installed from the App Store.

### Windows

`npm install` requires some tools to be present in the system like Python and C++ compiler. Windows users can easily install them by running the following command in PowerShell as administrator. For more information see https://github.com/felixrieseberg/windows-build-tools:

```sh
npm install --global --production windows-build-tools
```

The following are also needed:

- [Windows SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk) - only the ""Desktop C++ Apps"" components are needed to be installed

## Debugging

[The wiki](https://github.com/Microsoft/node-pty/wiki/Debugging) contains instructions for debugging node-pty.

## Security

All processes launched from node-pty will launch at the same permission level of the parent process. Take care particularly when using node-pty inside a server that's accessible on the internet. We recommend launching the pty inside a container to protect your host machine.

## Thread Safety

Note that node-pty is not thread safe so running it across multiple worker threads in node.js could cause issues.

## Flow Control

Automatic flow control can be enabled by either providing `handleFlowControl = true` in the constructor options or setting it later on:

```js
const PAUSE = '\x13';   // XOFF
const RESUME = '\x11';  // XON

const ptyProcess = pty.spawn(shell, [], {handleFlowControl: true});

// flow control in action
ptyProcess.write(PAUSE);  // pty will block and pause the child program
...
ptyProcess.write(RESUME); // pty will enter flow mode and resume the child program

// temporarily disable/re-enable flow control
ptyProcess.handleFlowControl = false;
...
ptyProcess.handleFlowControl = true;
```

By default `PAUSE` and `RESUME` are XON/XOFF control codes (as shown above). To avoid conflicts in environments that use these control codes for different purposes the messages can be customized as `flowControlPause: string` and `flowControlResume: string` in the constructor options. `PAUSE` and `RESUME` are not passed to the underlying pseudoterminal if flow control is enabled.

## Troubleshooting

### Powershell gives error 8009001d

> Internal Windows PowerShell error.  Loading managed Windows PowerShell failed with error 8009001d.

This happens when PowerShell is launched with no `SystemRoot` environment variable present.

### ConnectNamedPipe failed: Windows error 232

This error can occur due to anti-virus software intercepting winpty from creating a pty. To workaround this you can exclude this file from your anti-virus scanning `node-pty\build\Release\winpty-agent.exe`

## pty.js

This project is forked from [chjj/pty.js](https://github.com/chjj/pty.js) with the primary goals being to provide better support for later Node.JS versions and Windows.

## License

Copyright (c) 2012-2015, Christopher Jeffrey (MIT License).<br>
Copyright (c) 2016, Daniel Imms (MIT License).<br>
Copyright (c) 2018, Microsoft Corporation (MIT License).
"
197,microsoft/moab,SCSS,"# Project Moab microsite

Location for the Project Moab assets, tutorial, documentation and HTML
content.

### Installation
```shell
npm install
```

### Develop

Starts a local server.  This will start a 11ty server and auto reload on changes.

```shell
npm run start
```


### Deploy

Builds and commits to gh-pages branch.

```shell
npm run deploy
```
"
198,microsoft/MentalHealthPlatform,TypeScript,"This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Project Setup

To set up and run the application, please follow the procedures outlined in the following subsections:

## Setting up the local project

1. Clone the repository to your local machine:

   ```
   git clone https://github.com/Microsoft/MentalHealthPlatform.git
   ```

2. Download and install Node.js onto your development machine from
https://nodejs.org/.

3. Check whether the installation of Node was successful by running the following command; a version number should be printed:
   
   ```
   node -v
   ```

4. Download and install Yarn on your development machine from https://yarnpkg.com/lang/en/docs/install/.

5. Similar to that of the installation of Node.js, check whether the installation of Yarn was successful by running the following command; a version number should be printed:
   
   ```
   yarn -v
   ```

## Setting up the database

1.  Download and install MongoDB from the MongoDB website:
  
    https://docs.mongodb.com/manual/installation/

2. If using a Windows machine, one may need to add mongo to the environment variables. The instructions regarding the addition of environment variables can be found in the Microsoft Docs article for installing and configuring MongoDB:

    https://docs.microsoft.com/en-us/azure/virtual-machines/windows/install-mongodb

3. Create a directory to store the data.

    ```
    mkdir <path>
    ```

4. Set the path for storing the data: 

    ```
    mongod --dbpath /data/<path>
    ```
5. Run the service using the following command:

    ```
    brew services start mongodb-community@4.2
    ```

6.  After installing MongoDB, launch the MongoDB terminal by using the following command:
  
    ```
    mongo
    ```

7.  To add sample data, enter the commands listed in the **""docs/database_ commands.txt""** file of this repository

## Setting up the server

1.  In the cloned project, navigate to the **server** directory:

    ```
    cd server
    ```

2.  Install the dependencies:

    ```
    yarn
    ```

3.  Run the server:

    ```
    yarn start
    ```
    
    The console should then print a statement about the server running on a certain port:

    > Server is running on Port 3000...

## Setting up the client

1.  In the cloned project, navigate to the **client** directory:

    ```
    cd client
    ```

2.  Install all dependencies:

    ```
    yarn
    ```

## Running the project

1.  Run the client:

    ```
    yarn start
    ```

    The console should then print a statement with a URL in which the application is running.

    For example:

    > Project is running at http://localhost:8080/

2.  Launch the application in a web browser by navigating to the URL printed by the console

# Pages

## Dashboard (Home) Page

![Topics page](./docs/screenshots/dashboard.png)

## Topics Page

![Topics page](./docs/screenshots/topics.png)

## Forum Page

![Forum page](./docs/screenshots/forum.png)

## Chat Page

![Chat page](./docs/screenshots/chat.png)

## Contacts Page

![Contacts page](./docs/screenshots/contacts.png)

## Events Page

![Events page](./docs/screenshots/events.png)

## News Page

![Events page](./docs/screenshots/news.png)

## Therapists Page

![Events page](./docs/screenshots/therapists.png)

## Crisis Page

![Crisis page](./docs/screenshots/crisis.png)"
199,microsoft/appcenter-cli,JavaScript,"# App Center Command Line Interface (CLI)

Visual Studio App Center command line interface (CLI) is a unified tool for running App Center services from the command line.
Our aim is to offer a concise and powerful tool for our developers to use App Center services and easily script a sequence of
commands that they'd like to execute. You can currently login and view/configure all the apps that you have access to in App Center.

## Prerequisites

The recommended Node.js version is 12 or higher.

## Installation

```
npm install -g appcenter-cli
```

Once installed, use the `appcenter` command. See below for the available commands.

## Getting Help

To get a top level list of the available commands, run `appcenter help`.

To get help on a specific command or category, run `appcenter help command` or pass the `-h` flag to any command or category name.

App Center provides SDK support directly within the App Center portal. Any time you need help, just sign in to [App Center](https://appcenter.ms), then choose **'Contact support'** inside the help menu on the upper right of the App Center portal and our dedicated support team will respond to your questions and feedback. 

## Commands

Below is the list of commands currently supported by Visual Studio App Center CLI:

| Command                               | Description                                                    |
| ------------------------------------- | -------------------------------------------------------------- |
| `appcenter help` | Get help using appcenter commands |
| `appcenter login` | Log in |
| `appcenter logout` | Log out |
| `appcenter setup-autocomplete` | Setup tab completion for your shell |
| | |
| `appcenter analytics app-versions` | Shows versions of the application |
| `appcenter analytics audience` | Show audience statistics |
| `appcenter analytics log-flow` | Command to see the incoming logs in real time |
| `appcenter analytics sessions` | Show statistics for sessions |
| `appcenter analytics events delete` | Delete event |
| `appcenter analytics events show` | Show statistics for events |
| | |
| `appcenter apps create` | Create a new app |
| `appcenter apps delete` | Delete an app |
| `appcenter apps get-current` | Get the application that's set as default for all CLI commands |
| `appcenter apps list` | Get list of configured applications |
| `appcenter apps set-current` | Set default application for all CLI commands. Not compatible when authenticating with '--token' or an environment variable. Use environment variable 'MOBILE_CENTER_CURRENT_APP' to set the default app instead |
| `appcenter apps show` | Get the details of an app |
| `appcenter apps update` | Update an app |
| | |
| `appcenter build download` | Download the binary, logs or symbols for a completed build |
| `appcenter build logs` | Displays log for build |
| `appcenter build queue` | Queue a new build |
| `appcenter build branches list` | Show list of branches |
| `appcenter build branches show` | Show branch build status |
| | |
| `appcenter codepush patch` | Update the metadata for an existing CodePush release |
| `appcenter codepush promote` | Create a new release for the destination deployment, which includes the exact code and metadata from the latest release of the source deployment |
| `appcenter codepush release-cordova` | Release a Cordova update to an app deployment |
| `appcenter codepush release-electron` | Release an Electron update to a deployment |
| `appcenter codepush release-react` | Release a React Native update to an app deployment |
| `appcenter codepush release` | Release an update to an app deployment |
| `appcenter codepush rollback` | Rollback a deployment to a previous release |
| `appcenter codepush deployment add` | Add a new deployment to an app |
| `appcenter codepush deployment clear` | Clear the release history associated with a deployment |
| `appcenter codepush deployment history` | Display the release history for a CodePush deployment |
| `appcenter codepush deployment list` | List the deployments associated with an app |
| `appcenter codepush deployment remove` | Remove CodePush deployment |
| `appcenter codepush deployment rename` | Rename CodePush deployment |
| | |
| `appcenter crashes upload-mappings` | Upload the Android mappings for the application |
| `appcenter crashes upload-missing-symbols` | Upload missing crash symbols for the application (only from macOS) |
| `appcenter crashes upload-symbols` | Upload the crash symbols for the application |
| | |
| `appcenter distribute release` | Upload release binary and trigger distribution, at least one of --store or --group must be specified |
| `appcenter distribute groups create` | Create new distribution group |
| `appcenter distribute groups delete` | Deletes the distribution group |
| `appcenter distribute groups download` | Download release package for the distribution group |
| `appcenter distribute groups list` | Lists all distribution groups of the app |
| `appcenter distribute groups publish` | Publish an app file to a group |
| `appcenter distribute groups show` | Shows information about the distribution group |
| `appcenter distribute groups update` | Update existing distribution group |
| `appcenter distribute releases add-destination` | Distribute an existing release to an additional destination |
| `appcenter distribute releases delete` | Deletes the release |
| `appcenter distribute releases edit-notes` | Update release notes |
| `appcenter distribute releases edit` | Toggles enabling and disabling the specified release |
| `appcenter distribute releases list` | Shows the list of all releases for the application |
| `appcenter distribute releases show` | Shows full details about release |
| `appcenter distribute stores list` | Lists all stores of the app |
| `appcenter distribute stores publish` | Publish an app file to a store |
| | |
| `appcenter orgs create` | Create a new organization |
| `appcenter orgs list` | Lists organizations in which current user is collaborator |
| `appcenter orgs show` | Show information about organization |
| `appcenter orgs update` | Update organization information |
| `appcenter orgs apps create` | Create a new app in an organization |
| `appcenter orgs apps list` | Lists applications of organization |
| `appcenter orgs collaborators list` | Lists collaborators of organization |
| `appcenter orgs collaborators update` | Update list of organization collaborators |
| | |
| `appcenter profile list` | Get information about logged in user |
| `appcenter profile update` | Update user information |
| | |
| `appcenter telemetry off` | Turn off the sending of telemetry |
| `appcenter telemetry on` | Turn on the sending of telemetry |
| | |
| `appcenter test download` | Download the report artifacts, unpack and merge them. This command is only available for UITest and Appium test runs |
| `appcenter test status` | Checks the status of the started test run |
| `appcenter test stop` | Stop the started test run |
| `appcenter test wizard` | Start a test run interactively. All the parameters will be prompted on-the-go |
| `appcenter test generate appium` | Generates an Appium project |
| `appcenter test generate uitest` | Generates a Xamarin.UITest project |
| `appcenter test prepare appium` | Creates an artifacts directory with Appium tests |
| `appcenter test prepare calabash` | Creates an artifacts directory with Calabash tests |
| `appcenter test prepare espresso` | Creates an artifacts directory with Espresso tests |
| `appcenter test prepare uitest` | Creates an artifacts directory with Xamarin UI Tests |
| `appcenter test prepare xcuitest` | Creates an artifacts directory with XCUITest tests |
| `appcenter test run appium` | Starts a test run with Appium tests |
| `appcenter test run calabash` | Starts a test run with Calabash tests |
| `appcenter test run espresso` | Starts a test run with Espresso tests |
| `appcenter test run manifest` | Starts a test run with previously prepared artifacts |
| `appcenter test run uitest` | Starts a test run with Xamarin UI Tests |
| `appcenter test run xcuitest` | Starts a test run with XCUITest tests |
| | |
| `appcenter tokens create` | Create a new API token |
| `appcenter tokens delete` | Delete an API token |
| `appcenter tokens list` | Get a list of API tokens |

Please use the `appcenter help` command to get more information about each one.

## Contributing

Please see the [contributing](./contributing.md) file
for an introduction to the codebase and what the various moving parts are.

## Known issues

Check out [known issues](./KNOWN_ISSUES.md) for a list of known issues, and potential workarounds.

## Security

Check out [SECURITY.md](SECURITY.md) for any security concern with this project.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact opencode@microsoft.com with any additional questions or comments.
"
200,microsoft/pai,JavaScript,"# Open Platform for AI (OpenPAI) ![alt text][logo]

[logo]: ./pailogo.jpg ""OpenPAI""

[![Build Status](https://openpai.visualstudio.com/OpenPAI/_apis/build/status/OpenPAI-nightly-build?branchName=master)](https://openpai.visualstudio.com/OpenPAI/_build/latest?definitionId=25&branchName=master)
[![Join the chat at https://gitter.im/Microsoft/pai](https://badges.gitter.im/Microsoft/pai.svg)](https://gitter.im/Microsoft/pai?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Version](https://img.shields.io/github/release/Microsoft/pai.svg)](https://github.com/Microsoft/pai/releases/latest)

**OpenPAI [v1.7.0](./RELEASE_NOTE.md#April-2021-version-170) has been released!**

With the release of v1.0, OpenPAI is switching to a more robust, more powerful and lightweight architecture. OpenPAI is also becoming more and more modular so that the platform can be easily customized and expanded to suit new needs. OpenPAI also provides many AI user-friendly features, making it easier for end users and administrators to complete daily AI tasks.

 <table>
   <tr>
      <td align=""center"">
        <span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>
        <br/>
        <a href=""https://github.com/microsoft/openpaimarketplace"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture1.svg"" width=""610"" alt=""Marketplace Logo"" />
        </a>
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture2.svg"" width=""200"" alt="" Web Portal"" />
        <a href=""https://github.com/microsoft/openpaisdk"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture3.svg"" width=""200"" alt=""VScode"" />
        </a>
        <a href=""https://github.com/microsoft/openpaivscode"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture4.svg"" width=""200"" alt=""SDK"" />
        </a>
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture5.svg"" width=""610"" alt=""API"" />
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture18.svg"" width=""610"" alt=""Services"" />
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture19.svg"" width=""304"" alt=""User Authentication"" />
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture20.svg"" width=""304"" alt=""User/Group Management"" />
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture21.svg"" width=""304"" alt=""Storage Management"" />
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture22.svg"" width=""304"" alt=""Cluster/Job Monitoring"" />
        <br/>
        <a href=""https://github.com/microsoft/frameworkcontroller"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture23.svg"" width=""304"" alt=""Job Orchestration"" />
        </a>
        <a href=""https://github.com/microsoft/hivedscheduler"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture24.svg"" width=""304"" alt=""Job Scheduling"" />
        </a>
        <br/>
        <a href=""https://github.com/microsoft/openpai-runtime"" target=""_blank"">
          <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture25.svg"" width=""304"" alt=""Job Runtime"" />
        </a>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture26.svg"" width=""304"" alt=""Job Error Analysis"" />
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture15.svg"" width=""610"" alt=""Kubernetes Cluster Management"" />
        <br/>
        <img src=""https://openpai.readthedocs.io/en/latest/images/architecture/Picture16.svg"" width=""610"" alt=""CPU/GPU/FPGA/InfiniBand"" />
     </td>
   </tr>
 </table>

## Table of Contents

  - [When to consider OpenPAI](#when-to-consider-openpai)
  - [Why choose OpenPAI](#why-choose-openpai)
      - [Support on-premises and easy to deploy](#support-on-premises-and-easy-to-deploy)
      - [Support popular AI frameworks and heterogeneous hardware](#support-popular-ai-frameworks-and-heterogeneous-hardware)
      - [Most complete solution and easy to extend](#most-complete-solution-and-easy-to-extend)
  - [Get started](#get-started)
    - [For cluster administrators](#for-cluster-administrators)
    - [For cluster users](#for-cluster-users)
  - [Standalone Components](#standalone-components)
  - [Reference](#reference)
  - [Related Projects](#related-projects)
  - [Get involved](#get-involved)
  - [How to contribute](#how-to-contribute)
    - [Contributor License Agreement](#contributor-license-agreement)
    - [Call for contribution](#call-for-contribution)
    - [Who should consider contributing to OpenPAI](#who-should-consider-contributing-to-openpai)
    - [Contributors](#contributors)

## When to consider OpenPAI

1. When your organization needs to share powerful AI computing resources (GPU/FPGA farm, etc.) among teams.
2. When your organization needs to share and reuse common AI assets like Model, Data, Environment, etc.
3. When your organization needs an easy IT ops platform for AI.
4. When you want to run a complete training pipeline in one place.

## Why choose OpenPAI

The platform incorporates the mature design that has a proven track record in Microsoft's large-scale production environment.

#### Support on-premises and easy to deploy

OpenPAI is a full stack solution. OpenPAI not only supports on-premises, hybrid, or public Cloud deployment but also supports single-box deployment for trial users.

#### Support popular AI frameworks and heterogeneous hardware

Pre-built docker for popular AI frameworks. Easy to include heterogeneous hardware. Support Distributed training, such as distributed TensorFlow.

#### Most complete solution and easy to extend

OpenPAI is a most complete solution for deep learning, support virtual cluster, compatible with Kubernetes eco-system, complete training pipeline at one cluster etc. OpenPAI is architected in a modular way: different module can be plugged in as appropriate. [Here](./docs/system_architecture.md) is the architecture of OpenPAI, highlighting technical innovations of the platform.

## Get started

OpenPAI manages computing resources and is optimized for deep learning. Through docker technology, the computing hardware are decoupled with software, so that it's easy to run distributed jobs, switch with different deep learning frameworks, or run other kinds of jobs on consistent environments.

As OpenPAI is a platform, there are typically two different roles:

- **Cluster users** are the consumers of the cluster's computing resources. According to the deployment scenarios, cluster users could be researchers of Machine Learning and Deep Learning, data scientists, lab teachers, students and so on.
- **Cluster administrators** are the owners and maintainers of computing resources. The administrators are responsible for the deployment and availability of the cluster.

OpenPAI provides end-to-end manuals for both cluster users and administrators.

### For cluster administrators

The [admin manual](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/README.html) is a comprehensive guide for cluster administrators, it covers (but not limited to) the following contents:

- **Installation and upgrade**. The installation is based on Kubespray, and here is the [system requirements](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/installation-guide.html#installation-requirements). OpenPAI provides an [installation guide](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/installation-guide.html) to facilitate the installation.

  If you are considering upgrade from older version to the latest v1.0.0, please refer to the table below for a brief comparison between `v0.14.0` and the `v1.0.0`. More detail about the upgrade considerations can be found [upgrade guide](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/upgrade-guide.html).

  |                   | `v0.14.0`                | `v1.0.0`                |
  | ----------------- | ------------------------ | ----------------------- |
  | Architecture      | Kubernetes + Hadoop YARN | Kubernetes              |
  | Scheduler         | YARN Scheduler           | HiveD / K8S default     |
  | Job Orchestrating | YARN Framework Launcher  | Framework Controller    |
  | RESTful API       | v1 + v2                  | pure v2                 |
  | Storage           | Team-wise storage plugin | PV/PVC storage sharing  |
  | Marketplace       | Marketplace v2           | openpaimarketplace      |
  | SDK               | Python                   | JavaScript / TypeScript |

  _If there is any question during deployment, please check [installation FAQs and troubleshooting](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/installation-faqs-and-troubleshooting.html) first. If it is not covered yet, refer to [here](#get-involved) to ask question or submit an issue._

- **Basic cluster management**. Through the Web-portal and a command-line tool `paictl`, administrators could complete [cluster managements](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/basic-management-operations.html), such as [adding (or removing) nodes](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/how-to-add-and-remove-nodes.html), [monitoring nodes and services](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/basic-management-operations.html#management-on-webportal), and [storages setup and permission control](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/how-to-set-up-storage.html).

- **Users and groups management**. Administrators could manage the [users and groups](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/how-to-manage-users-and-groups.html) easily.

- **Alerts management**. Administrators could [customize alerts rules and actions](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/how-to-customize-alerts.html).

- **Customization**. Administrators could customize the cluster by [plugins](https://openpai.readthedocs.io/en/latest/manual/cluster-admin/how-to-customize-cluster-by-plugins.html). Administrators could also upgrade (or downgrade) a single component (e.g. rest servers) to address customized application demands.

### For cluster users

The [user manual](https://openpai.readthedocs.io/en/latest/manual/cluster-user/README.html) is a guidance for cluster users, who could train and serve deep learning (and other) tasks on OpenPAI.

- **Job submission and monitoring**. The [quick start tutorial](https://openpai.readthedocs.io/en/latest/manual/cluster-user/quick-start.html) is a good start for learning how to train models on OpenPAI. And more examples and supports to multiple mainstream frameworks (out-of-the-box docker images) are in [here](https://openpai.readthedocs.io/en/latest/manual/cluster-user/docker-images-and-job-examples.html). OpenPAI also provides supports for [good debuggability](https://openpai.readthedocs.io/en/latest/manual/cluster-user/how-to-debug-jobs.html) and [advanced job functionalities](https://openpai.readthedocs.io/en/latest/manual/cluster-user/advanced-jobs.html).

- **Data managements**. Users could use cluster provisioned storages and custom storages in their jobs. The cluster provisioned storages are well integrated and easy to configure in a job [(refer to here)](https://openpai.readthedocs.io/en/latest/manual/cluster-user/how-to-manage-data.html).

- **Collaboration and sharing**. OpenPAI provides facilities for collaboration in teams and organizations. The cluster provisioned storages are organized by teams (groups). And users could easily share their works (e.g. jobs) in the [marketplace](https://openpai.readthedocs.io/en/latest/manual/cluster-user/use-marketplace.html), where others could discover and reproduce (clone) by one-click.

Besides the webportal, OpenPAI provides [VS Code extension](https://openpai.readthedocs.io/en/latest/manual/cluster-user/use-vscode-extension.html) and [command line tool (preview)](https://github.com/microsoft/openpaisdk). The VS Code extension is a friendly, GUI based client tool of OpenPAI, and it's highly recommended. It's an extension of Visual Studio Code. It can submit job, simulate jobs locally, manage multiple OpenPAI environments, and so on.

## Standalone Components

With the `v1.0.0` release, OpenPAI starts using a more modularized component design and re-organize the code structure to 1 main repo together with 7 standalone key component repos. [pai](https://github.com/microsoft/pai) is the main repo, and the 7 component repos are:

- [hivedscheduler](https://github.com/microsoft/hivedscheduler) is a Kubernetes Scheduler Extender for Multi-Tenant GPU clusters, which provides various advantages over standard k8s scheduler.
- [frameworkcontroller](https://github.com/microsoft/frameworkcontroller) is built to orchestrate all kinds of applications on Kubernetes by a single controller.
- [openpai-protocol](https://github.com/microsoft/openpai-protocol) is the specification of OpenPAI job protocol.
- [openpai-runtime](https://github.com/microsoft/openpai-runtime) provides runtime support which is necessary for the OpenPAI protocol.
- [openpaisdk](https://github.com/microsoft/openpaisdk) is a JavaScript SDK designed to facilitate the developers of OpenPAI to offer more user-friendly experience.
- [openpaimarketplace](https://github.com/microsoft/openpaimarketplace) is a service which stores examples and job templates. Users can use it from webportal plugin to share their jobs or run-and-learn others' sharing job.
- [openpaivscode](https://github.com/microsoft/openpaivscode) is a VSCode extension, which makes users connect OpenPAI clusters, submit AI jobs, simulate jobs locally and manage files in VSCode easily.

## Reference

- [PyTorch CIFAR-10](https://github.com/microsoft/pai/tree/pai-for-edu/contrib/edu-examples/pytorch_cifar10) and [TensorFlow CIFAR-10](https://github.com/microsoft/pai/tree/pai-for-edu/contrib/edu-examples/tensorflow_cifar10) job examples
- [RESTful API](https://redocly.github.io/redoc/?url=https://raw.githubusercontent.com/microsoft/pai/master/src/rest-server/docs/swagger.yaml)
- Design documents could be found [here](docs) if you are curious.

## Related Projects

Targeting at openness and advancing state-of-art technology, [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-research-group-asia/) and [Microsoft Software Technology Center Asia (STCA)](https://www.microsoft.com/en-us/ard/default.aspx) had also released few other open source projects.

- [NNI](https://github.com/Microsoft/nni) : An open source AutoML toolkit for neural architecture search and hyper-parameter tuning.
  We encourage researchers and students leverage these projects to accelerate the AI development and research.
- [MMdnn](https://github.com/Microsoft/MMdnn) : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The ""MM"" in MMdnn stands for model management and ""dnn"" is an acronym for deep neural network.
- [NeuronBlocks](https://github.com/Microsoft/NeuronBlocks) : An NLP deep learning modeling toolkit that helps engineers to build DNN models like playing Lego. The main goal of this toolkit is to minimize developing cost for NLP deep neural network model building, including both training and inference stages.
- [SPTAG](https://github.com/Microsoft/SPTAG) : Space Partition Tree And Graph (SPTAG) is an open source library for large scale vector approximate nearest neighbor search scenario.

## Get involved

- [Stack Overflow](./docs/stackoverflow.md): If you have questions about OpenPAI, please submit question at Stack Overflow under tag: openpai
- [Gitter chat](https://gitter.im/Microsoft/pai): You can also ask questions in Microsoft/pai conversation.
- [Create an issue or feature request](https://github.com/Microsoft/pai/issues/new/choose): If you have issue/ bug/ new feature, please submit it to GitHub.

## How to contribute

### Contributor License Agreement

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### Call for contribution

We are working on a set of major features improvement and refactor, anyone who is familiar with the features is encouraged to join the design review and discussion in the corresponding issue ticket.

- GPU fairness usage [Issue 4266](https://github.com/Microsoft/pai/issues/4266)

### Who should consider contributing to OpenPAI

- Folks who want to add support for other ML and DL frameworks
- Folks who want to make OpenPAI a richer AI platform (e.g. support for more ML pipelines, hyperparameter tuning)
- Folks who want to write tutorials/blog posts showing how to use OpenPAI to solve AI problems

### Contributors

One key purpose of OpenPAI is to support the highly diversified requirements from academia and industry. OpenPAI is completely open: it is under the MIT license. This makes OpenPAI particularly attractive to evaluate various research ideas, which include but not limited to the [components](./docs/research_education.md).

OpenPAI operates in an open model. It is initially designed and developed by [Microsoft Research (MSR)](https://www.microsoft.com/en-us/research/group/systems-research-group-asia/) and [Microsoft Software Technology Center Asia (STCA)](https://www.microsoft.com/en-us/ard/default.aspx) platform team.
We are glad to have [Peking University](http://eecs.pku.edu.cn/EN/), [Xi'an Jiaotong University](http://www.aiar.xjtu.edu.cn/), [Zhejiang University](http://www.cesc.zju.edu.cn/index_e.htm), [University of Science and Technology of China](http://eeis.ustc.edu.cn/) and [SHANGHAI INESA AI INNOVATION CENTER (SHAIIC)](https://www.shaiic.com/) joined us to develop the platform jointly.
Contributions from academia and industry are all highly welcome.
"
201,microsoft/vsts-team-calendar,TypeScript,"# Team Calendar Extension for Visual Studio Team Services

![buildstatus](https://mseng.visualstudio.com/_apis/public/build/definitions/b924d696-3eae-4116-8443-9a18392d8544/5979/badge)

Team Calendar helps busy teams stay on track and informed about important deadlines, sprint schedules, and upcoming milestones. It is the one place to see and manage the date important to your teams, including sprint schedule, days off (for individuals or the team), and custom events.

Team Calendar installs into either a Visual Studio Team Services account or into Team Foundation Server.

![screenshot](static/v2-images/calendar-screen-shot.png)

See [overview](overview.md) to learn more about the features of the extension.

## About extensions

Extensions enable you to create first-class integration experiences within Visual Studio Team Services, just the way you have always wanted. An extension can be a simple context menu or toolbar action or can be a complex and powerful custom UI experience that light up within the account, collection, or project hubs.

To learn more about Extensions, see the [overview of extensions](https://www.visualstudio.com/docs/integrate/extensions/overview).

## Install

To try out the extension in your VSTS account, visit the [Team Calendar extension](https://marketplace.visualstudio.com/items?itemName=ms-devlabs.team-calendar) page on the Visual Studio Marketplace.

Don't have a [free] VSTS account? [Learn more](https://www.visualstudio.com/team-services/) about getting one.

## Develop

Team Calendar is written in [TypeScript](https://www.typescriptlang.org/). To build and package the extension:

### Get the pre-reqs

1. Get [Node.js](https://nodejs.org/)
2. Install TypeScript: `npm install -g typescript`
3. Install the TFX CLI (needed to package the extension): `npm install -g tfx-cli`
4. Install required modules: `npm install` (from the root of t

### Compile the code

To compile and package the extension run:

```
npm run build
```

This will compile the TypeScript code in the project and create a .vsix file.

### Package the extension

To install your own version of the Team Calendar extension into your VSTS account, you need to create a publisher on the Visual Studio Marketplace. There is no cost for creating or having a publisher. [Learn how to create a publisher](https://www.visualstudio.com/docs/integrate/extensions/publish/overview).

1. Update your version of the extension manifest (`vss-extension.json`) file:
    1. Set the `publisher` property to your Visual Studio Marketplace publisher ID
    2. Set the `public` property to `false`
2. Package the extension (`npm run build`) to produce a .vsix file. Note: you should see your publisher ID in the name of this file.
3. Go to the [manage](https://marketplace.visualstudio.com/manage) page of the Marketplace and click **Upload** to publish your version of the extension (don't worry, only you will be able to see it)
4. After uploading, select the extension, click **Share** ,and enter the name of the VSTS account you want to be able to install the extension into
5. Click the extension's title to open its details page
6. Click the install button, choose your account from the drop-down, and install the extension

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
202,microsoft/powerbi-visuals-pulsechart,TypeScript,"# Pulse Chart
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-pulsechart.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-pulsechart) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-pulsechart/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-pulsechart?branch=master) [![Build Status](https://dev.azure.com/customvisuals/public/_apis/build/status/Microsoft.powerbi-visuals-chord)](https://dev.azure.com/customvisuals/public/_build/latest?definitionId=4)

> Line chart annotated with key events. Perfect for story telling with data.

![stacked area chart screenshot](./assets/screenshot2.png)

# Overview

The Pulse chart shows key events on a timeline, and lets you play back the events to reveal insights.

The Pulse Chart allows you to playback the data to see the trend unfold in front of your eyes. When an event appears, the playback pauses to filter the rest of the report, revealing hidden relationships. You can use this feature to grab your audience’s attention and highlight specific insights. There’s an auto play feature that starts the playback when the report loads. Pulse Charts are ideal for use with publish to web or when sharing reports with your coworkers.

When a data point is selected on the Pulse Chart, you get a customizable popup. You can specify the title and description, and show or hide the timestamp as well. This lets you clearly call attention to what’s important about the data point.

Creating a Pulse Chart is really easy - you just need to provide data that is a time series. You add columns to the time series data that define the events you want to show on the line. For those columns, non-blank values become events and are shown as circles on the Pulse chart.

See also [Pulse Chart at Microsoft AppSource](https://appsource.microsoft.com/en-us/product/power-bi-visuals/WA104381006)
"
203,microsoft/arcade-machine,TypeScript,"# arcade-machine

arcade-machine is an Angular plugin to provide navigation and interactive semantics using the GamePad API. This allows the application to be navigated using a controller connected to a PC, the PC's keyboard, or on Universal Windows Platform (UWP) web applications running on the Xbox.

> See the [reference controller](https://i-msdn.sec.s-msft.com/en-us/windows/uwp/input-and-devices/images/designing-for-tv/hardware-buttons-gamepad-remote.png) for mappings with buttons.

We use [WinJS' navigation algorithm](https://github.com/winjs/winjs/blob/master/src/js/WinJS/XYFocus.ts#L11) to move between focusable components on the page. Most interactive components, including link tags `<a>`, buttons, and form controls are able to receive focus by default, and for those which are not focusable by default you can use the `arc` directive.

> Here we have a basic app which contains three buttons. The user can use their joystick to navigate between each selectable button. Note that the app itself is marked as focusable; this is how bootstrapping is done. More on this in a minute.
>
> ```html
> <my-app arc>
>   <button>Button 1</button>
>   <button>Button 2</button>
>   <button>Button 3</button>
> </my-app>
> ```

For the majority of navigation, we represent controller actions as keyboard events; the left joystick or arrow keys on a keyboard can be used to fire up, down, left, and right events in order to navigate the page. We determine the next element to focus in a direction using WinJS' algorithm based on each focusable element's physical location, but you can also fine tune what happens when via directives. This can help to avoid [inaccessible UI](https://msdn.microsoft.com/windows/uwp/input-and-devices/designing-for-tv#inaccessible-ui) and provide more fined-tuned experiences on each platform.

> By default only elements that explicity have `tabindex >= 0` are considered for focus

## Demo App

You can see a demo Angular 2 app in the `demo` folder and run it locally with `npm start`.

## Usage

### Directives & Attributes

##### arc

You must define `arc` directive on element which you want to be focusable that are not otherwise focusable, or when you want to define custom logic. That is, anything except the following tags:

- `a`
- `button`
- `input`
- `select`
- `textarea`

##### [arc-exclude-this]=""value""

You can pass a value to `arc-exclude-this` which, if not `false`, exclude this element from arcade-machine's focus.

##### [arc-exclude]=""value""

You can pass a value to `arc-exclude-this` which, if not `false`, exclude this element and all its children from arcade-machine's focus.


##### [arc-set-focus]=""Observable\<boolean\>""

You can pass an Observable to `arc-set-focus` which, when fired, will forcefully cause the element to be focused.

##### arc-default-focus

When `arc-focus` is on an element, that element will steal the page focus when it's instantiated. Setting this is a shortcut to passing `Observable.of(undefined)` to `arc-set-focus` to immediately trigger a focus capture.

It can also be used with *ngFor. For instance, following will focus the 3rd element in ngFor
```html
<div
  *ngFor=""let box of boxes; let i = index""
  arc [arc-default-focus]=""i === 2"">
</div>
```

##### arc-focus-inside

If arc-focus-inside is present on a focusable element; on focus, it transfers the focus to its next best child element.
This is particularly useful to make elements focusable that are not directly in the focus direction.

```html
<div tabindex=""0"" arc arc-focus-inside=""true"">
  <div tabindex=""0"">I will be focused instead</div>
</div>
```

##### (arc-capture-outgoing)=""onEvent(IArcEvent)""

`arc-capture-outgoing` can be set to handle, and possibly cancel, events sent while the element or one of its children are focused. See the `IArcEvent` type for more details:

##### (arc-capture-incoming)=""onEvent(IArcEvent)""

`arc-capture-incoming` can be set to handle, and possibly cancel, events sent while the element is the next target of navigation. See the `IArcEvent` type for more details:

```typescript
/**
 * IArcEvents are fired on an element when an input occurs. They include
 * information about the input and provide utilities similar to standard
 * HTML events.
 */
export interface IArcEvent {
  // The 'arc' directive reference, may not be filled for elements which
  // are focusable without the directive, like form controls.
  readonly directive?: IArcHandler;
  // `next` is the element that we'll select next, on directional navigation,
  // unless the element is cancelled. This *is* settable and you can use it
  // to modify the focus target. This will be set to `null` on non-directional
  // navigation or if we can't find a subsequent element to select.
  next?: Element;

  readonly event: Direction;
  readonly target: Element;
  readonly defaultPrevented: boolean;

  stopPropagation(): void;
  preventDefault(): void;
}

/**
 * Direction is an enum of possible gamepad events which can fire.
 */
export enum Direction {
  SUBMIT = 0,
  BACK = 1,
  X = 2,
  Y = 3,
  TABLEFT = 4, // Left Bumper
  TABRIGHT = 5, // Right Bumper
  TABUP = 6, // Left Trigger
  TABDOWN = 7, // Right Trigger
  UP = 12,
  DOWN = 13,
  LEFT = 14,
  RIGHT = 15,
}
```

##### (arc-focus)=""onFocusChange(Element?)""

`arc-focus` is an event that's fired when the element or any of its children gain or lose focus. The newly-selected element will be passed to the function, and `null` will be passed if none of the elements in the node's tree are selected.

##### (arc-submit)=""onSubmit(IArcEvent)""

`arc-submit` is a shortcut to create a handler via `arc-capture-outgoing` that fires when a ""submit"" event is fired.

##### (arc-back)=""onBack(IArcEvent)""

`arc-back` is a shortcut to create a handler via `arc-capture-outgoing` that fires when a ""back"" event is fired.

##### [arc-[left|right|up|down]]=""Element""

Allows you to explicitly tell the directive which element to focus when off the element in the provided direction. Again, this is a shortcut to a `arc-capture-outgoing` handler which sets the `next` element if it matches the target direction.

##### [arc-focus-[left|right|up|down]]=""Element | CSSQueryString""

Allows you to explicitly tell the directive which element to focus when off the element in the provided direction. This will take precedence over all other FindFocus strategies

### Focus Service

#### trapFocus
```typescript
trapFocus(newRootElem: HTMLElement)
```
To trap the focus inside newRootElem.
To release the focus, call releaseFocus

#### releaseFocus

```typescript
releaseFocus(releaseElem?: HTMLElement)
```
To trap the release the previously trapped focus.
Multiple call to this method will precedurally remove focus traps all the way up to body.
Further calls without releaseElem param will throw a warning on console while keeping the focus at body.
If releaseElem is provided, this method will release focus only if the last trapped focus element was releaseElem.

#### releaseFocus
```typescript
clearAllTraps()
```
Useful for resetting all focus traps e.g. on page navigation

### Classes

By default, the `arc--selected-direct` class is added to the selected node.

### Events
#### arcselectingnode

Fired when arcade machine is about to select a node

#### arcfocuschanging

Fire when arcade-machine is about to call native focus method. This event can be canceled for example to smooth-scroll to the element before focusing it in browser.
"
204,microsoft/thematic,TypeScript,"# Thematic

This repository holds all of the packages for creating shareable, perceptually-balanced and tastefully complementary (hopefully!) application and data viz themes. There are adapter libraries and tools to apply themes across several environments, which will be detailed below. We briefly describe the intentions and usage of the library in [a paper on visualizing workgroup collaboration](https://arxiv.org/pdf/2005.00402.pdf).

This repository is structured as a yarn monorepo. It contains separate packages for all of the theme generation, management, and applications. To use thematic in your app, you should only need to install the relevant packages for your use case(s).

The webapp package is a guide for everything Thematic. In particular, it comprises:

- A running application for configuring theme parameters
- An example app using thematic itself, so you can see what controls and charts look like
- A variety of code examples to explore showing how to use thematic in practice

## Organization

Our thematic packages are published under the `@thematic` scope.

These are the core Thematic libraries, see individual README.md files for greater detail:

- [@thematic/color](packages/color/README.md) - this contains color conversion and scale generation logic. If you need functions to convert between color spaces, this is the place. This also has the color scheme compute logic that dictates how all of the color scales are generated from a few selected input parameters.
- [@thematic/core](packages/core/README.md) - this is the main package for working with Themes. You can load them from a Theme JSON specification, then use the attributes directly in your app to apply the generated colors in a consistent way. We've used SVG notation for all of our mark properties (e.g., 'fill').
- [@thematic/d3](packages/d3/README.md) - this package has helpers to apply the themes to viz created with [d3](https://d3js.org/). We provide a variety of SVG mark primitives that operate on a d3 Selection, so you can apply the theme elements using `selection.call(fn, [params])` as needed.
- [@thematic/fluent](packages/fluent/README.md) - helpers for applying Thematic to the [Fluent UI library](https://developer.microsoft.com/en-us/fluentui#/controls/web)
- [@thematic/react](packages/react/README.md) - helpers to bootstrap theming into React-based apps, particularly a provider and context hook (useThematic) for grabbing the theme anywhere it is needed.
- [@thematic/vega](packages/vega/README.md) - this has a helper that applies our theme spec as default configuration for Vega charts so they automatically adopt the theme.
- [@thematic/webapp](packages/webapp/README.md) - this is the theme editor webapp that you can run or access at https://microsoft.github.io/thematic.

## Getting Started

If you want to run locally and work on the app or any theme components, you can run the whole thing from the root folder. The web app will run using `yarn start:webapp` as with our typical development structure, and changes to any of the other packages will be reflected live. See the Available Scripts section below.

## Publishing

Commits to `main` will automatically deploy to the hosted website. Pull requests should use `yarn version check --interactive` to create semantic versioning documents describing the impact of the PR. When a release is ready, run`yarn release_all` to publish packages to npm.

## Available Scripts

In the project directory, you can run:

### `yarn start`

Runs the app in the development mode.
An available port will be selected automatically, such as 8080. Open [http://localhost:8080](http://localhost:8080) to view it in the browser.

The page will reload if you make edits.
You will also see any lint errors in the console.

### `yarn test`

Launches the test runner using Jest.

### `yarn build:all`

Builds packages for production to their respective `dist` and `lib` folders. Note that the webapp uses a `bundle` command more appropriate to creating an optimized web bundle. CI systems will want to invoke both of these to produce complete

### `yarn clean:all`

Cleans out the node_modules and built lib directories for every package.

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
205,microsoft/essex-alpha-build-infra,TypeScript,"![CI](https://github.com/microsoft/essex-alpha-build-infra/workflows/CI/badge.svg?branch=main)

# essex-alpha-build-infra

This project contains build infrastructure for the Project Essex Alpha team.

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Please follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."
206,microsoft/powerbi-visuals-utils-chartutils,TypeScript,"# Microsoft Power BI visuals ChartUtils
![Build status](https://github.com/microsoft/powerbi-visuals-utils-chartutils/workflows/build/badge.svg) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-utils-chartutils/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-utils-chartutils?branch=master) [![npm version](https://img.shields.io/npm/v/powerbi-visuals-utils-chartutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-chartutils) [![npm](https://img.shields.io/npm/dm/powerbi-visuals-utils-chartutils.svg)](https://www.npmjs.com/package/powerbi-visuals-utils-chartutils)

> ChartUtils is a set of interfaces for creating powerbi custom visuals

## Usage
Learn how to install and use the chartutils in your custom visuals:
* [Usage Guide](https://docs.microsoft.com/en-us/power-bi/developer/visuals/utils-chart)

## Contributing
* Read our [contribution guideline](./CONTRIBUTING.md) to find out how to contribute bugs fixes and improvements
* [Issue Tracker](https://github.com/Microsoft/powerbi-visuals-utils-chartutils/issues)
* [Development workflow](./docs/dev/development-workflow.md)
* [How to build](./docs/dev/development-workflow.md#how-to-build)
* [How to run unit tests locally](./docs/dev/development-workflow.md#how-to-run-unit-tests-locally)

## License
See the [LICENSE](./LICENSE) file for license rights and limitations (MIT).
"
207,microsoft/Recognizers-Text,C#,"# Microsoft Recognizers Text Overview

![Build Status](https://msrasia.visualstudio.com/_apis/public/build/definitions/310c848f-b260-4305-9255-b97bfb69974b/116/badge)
![Build Status](https://ci.appveyor.com/api/projects/status/github/Microsoft/Recognizers-Text?branch=master&svg=true&passingText=all%20plats%20-%20OK)

Microsoft.Recognizers.Text provides robust recognition and resolution of entities like numbers, units, and date/time; expressed in multiple languages. Full support for Chinese, English, French, Spanish, Portuguese, German, Italian, Turkish, and Hindi. Partial support for Dutch, Japanese, Korean, and Swedish. More on the way.

# Utilizing the Project

Microsoft.Recognizers.Text powers pre-built entities in both [**LUIS: Language Understanding Intelligent Service**](https://www.luis.ai/home) and [**Microsoft Bot Framework**](https://dev.botframework.com/); base entity types in [**Text Analytics Cognitive Service**](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking); and it is also available as standalone packages (for the base classes and the different entity recognizers).

The Microsoft.Recognizers.Text packages currently target four platforms:
* [C#/.NET](https://github.com/Microsoft/Recognizers-Text/tree/master/.NET) - **NuGet packages** available at: https://www.nuget.org/profiles/Recognizers.Text
* [JavaScript/TypeScript](https://github.com/Microsoft/Recognizers-Text/tree/master/JavaScript/packages/recognizers-text-suite) - **NPM packages** available at: https://www.npmjs.com/~recognizers.text
* [Python](https://github.com/Microsoft/Recognizers-Text/tree/master/Python) - **PyPI packages** available at: https://pypi.org/user/recognizers-text/ (alpha)
* [Java](https://github.com/Microsoft/Recognizers-Text/tree/master/Java) (in progress)

Contributions are greatly welcome! Both for fixes and extensions in the currently supported languages and for expansion to new ones.
Especially for Dutch, Japanese, Korean, Hindi, and others! More info below.

# Help

If you have any questions, please go ahead and [open an issue](https://github.com/Microsoft/Recognizers-Text/issues/new/choose), even if it's not an actual bug. Issues are an acceptable discussion forum as well.

# Contributing

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

Good starting points for contribution are:
* the list of [open issues](https://github.com/Microsoft/Recognizers-Text/issues) (especially those marked as ```help wanted```); 
* the json spec cases temporarily marked as ```NotSupported``` ([Specs](./Specs)); and
* translating json test spec cases that work in English, but don't yet exist in a target language.

The links below describe the project structure and provide both an overview and tips on how to contribute (although some steps may have become a little out-of-date). Thank you!

* [Overview and language resources](https://blog.botframework.com/2018/01/24/contributing-luis-microsoft-recognizers-text-part-1/)
* [Implementing language specific behaviour](https://blog.botframework.com/2018/02/01/contributing-luis-microsoft-recognizers-text-part-2/)
* [Test specs and testing in general](https://blog.botframework.com/2018/02/12/contributing-luis-microsoft-recognizers-text-part-3/)

# Supported Entities across Cultures

The table below summarizes the currently supported entities. Support for English is usually more complete than others. The primary platform is .NET (shown in table) and support should propagate to the others.

| Entity Type       | EN      | ZH-CN   | NL    | FR     | DE    | IT      | JA     | KO     | PT     | ES      |
|:-----------------:|:-------:|:-------:|:-----:|:------:|:-----:|:-------:|:------:|:------:|:------:|:-------:| 
| Number (cardinal)    | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | ✓      | ✓     | ✓       |
| Ordinal              | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | PA/EO  | ✓     | ✓       |
| Percentage           | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | PA/EO  | ✓     | ✓       |
| Number Range         | ✓    | ✓       | ✓    | :x:    | :x:   | ✓      | PA/EO   | PA/EO  | :x:    | ✓      |
| Unit - Age           | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | PA/EO  | ✓     | ✓       |
| Unit - Currency      | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | PA/EO  | ✓     | ✓       |
| Unit - Dimensions    | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | :x:    | PA/EO  | ✓      | ✓      | 
| Unit - Temperature   | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | :x:    | PA/EO  | ✓      | ✓      | 
| Choice - Boolean     | ✓    | ✓       | ✓    | ✓     | ✓     | ✓       | ✓      | **SO** | ✓     | ✓       | 
| Seq. - E-mail        | G    | G*       | G    | G      | G     | G       | G*     | G*     | G      | G       |
| Seq. - GUID          | G    | G        | G    | G      | G     | G       | G      | G      | G      | G       |
| Seq. - Social        | G    | G        | G    | G      | G     | G       | G      | G      | G      | G       |
| Seq. - IP Address    | G    | G        | G    | G      | G     | G       | G      | G      | G      | G       |
| Seq. - Phone Number  | G    | G        | G    | G      | G     | G       | G      | G      | G      | G       |
| Seq. - URL           | G    | G*       | G    | G      | G     | G       | G*     | G*     | G      | G       |
| DateTime (+subtypes) | ✓    | ✓       | **PA** | ✓    | ✓     | ✓      | **SP** | **SP** | ✓     | ✓       | 

| Entity Type       | SV      | BG      | TR    | HI     | AR    |         |        |        |        |         |
|:-----------------:|:-------:|:-------:|:-----:|:------:|:-----:|:-------:|:------:|:------:|:------:|:-------:| 
| Number (cardinal)    | ✓    | :x:     | ✓    | ✓      | PA/EO |         |        |        |        |         |
| Ordinal              | ✓    | :x:     | ✓    | ✓      | PA/EO |         |        |        |        |         |
| Percentage           | ✓    | :x:     | ✓    | ✓      | PA/EO |         |        |        |        |         |
| Number Range         | :x:  | :x:     | ✓     | ✓     | PA/EO |         |        |        |        |         |
| Unit - Age           | ✓    | :x:     | ✓     | ✓     | :x:   |         |        |        |        |         |
| Unit - Currency      | ✓    | :x:     | ✓     | ✓     | :x:   |         |        |        |        |         |
| Unit - Dimensions    | ✓    | :x:     | ✓     | ✓     | :x:   |         |        |        |        |         | 
| Unit - Temperature   | ✓    | :x:     | ✓     | ✓     | :x:   |         |        |        |        |         | 
| Choice - Boolean     | ✓    | ✓      | ✓     | ✓      | ✓    |         |        |        |        |         |
| Seq. - E-mail        | G    | G       | G     | G      | G     |         |        |        |        |         |
| Seq. - GUID          | G    | G       | G     | G      | G     |         |        |        |        |         |
| Seq. - Social        | G    | G       | G     | G      | G     |         |        |        |        |         |
| Seq. - IP Address    | G    | G       | G     | G      | G     |         |        |        |        |         |
| Seq. - Phone Number  | :x:  | :x:     | :x:   | :x:    | :x:   |         |        |        |        |         |
| Seq. - URL           | G    | G       | G     | G*     | G*    |         |        |        |        |         |
| DateTime (+subtypes) | :x:  | :x:     | ✓     | ✓      | :x:   |         |        |        |        |         |

* G: Generic entity, not language-specific (* unicode TLDs not-supported);
* EO: Extraction-only (parsing/resolution/normalization pending);
* PA: Partial support (type not fully supported);
* SO: Specs-only (test specs coverage OK, but support pending);
* SP: Partial specs;
* SI: Very initial specs (typically language support start for a new language).
"
208,microsoft/powerbi-visuals-globemap,TypeScript,"# GlobeMap
[![Build Status](https://travis-ci.org/Microsoft/powerbi-visuals-globemap.svg?branch=master)](https://travis-ci.org/Microsoft/powerbi-visuals-globemap) [![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-globemap/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-globemap?branch=master)

> A 3D visual using WebGL for plotting locations, with category values displayed as bar heights and heat maps. Shift+Click on bar to change center point. Slicing data points will animate to average location. 

![GlobeMap screenshot](https://az158878.vo.msecnd.net/marketing/Partner_21474836617/Product_42949680586/Asset_87f58068-9b83-4a54-8889-66617065ec5a/GlobeMapscreenshot2.png)

# Overview
Globe Map is a 3D Map that makes the map exploration experience more immersive and magical. It provide the sense of connection to the data with the physical world. This, combined with our spatial ability, brings a new perspective to the data when presented as 3D objects.

Use it with any location data. The location could be an address, city, county , state/province or country/region. On this 3D map, you can project a measure as the height of the bar. The 3D bars reduce the clutter of overlapping bubbles and allow you to get instant insight. GlobeMap also allows you to rotate the Globe and see it from different angles.

Globe Map also supports heat map on the spatial map. You can use a second measure for heat intensity and draw immediate attention to the right areas.

See also [Globe Map at Microsoft AppStore](https://appsource.microsoft.com/en-us/product/power-bi-visuals/WA104380799)"
209,microsoft/perfview,C#,"# PerfView Overview
PerfView is a free performance-analysis tool that helps isolate CPU and memory-related performance issues.  It is a Windows tool, but it also has some support for analyzing data collected on Linux machines.  It works for a wide variety of scenarios, but has a number of special features for investigating performance issues in code written for the .NET runtime.  

If you are unfamiliar with PerfView, there are [PerfView video tutorials](http://channel9.msdn.com/Series/PerfView-Tutorial). 
Also, [Vance Morrison's blog](https://docs.microsoft.com/en-us/archive/blogs/vancem/) gives overview and getting 
started information. 

### Getting PerfView 
Please see the [PerfView Download Page](documentation/Downloading.md) for the link and instructions for downloading the 
current version of PerfView.  

### Are you here about the TraceEvent Library?

PerfView is built on a library called Microsoft.Diagnostics.Tracing.TraceEvent, that knows how to both collect and parse Event Tracing for Windows (ETW) data.   Thus if there is any information that PerfView collects and processes that you would like to manipulate yourself programmatically, you would probably be interested in the [TraceEvent Library Documentation](documentation/TraceEvent/TraceEventLibrary.md)

### Learning about PerfView 

The PerfView User's Guide is part of the application itself. In addition, you can click the
[Users Guide link](http://htmlpreview.github.io/?https://github.com/Microsoft/perfview/blob/main/src/PerfView/SupportFiles/UsersGuide.htm) 
to see the [GitHub HTML Source File](src/PerfView/SupportFiles/UsersGuide.htm) rendered in your browser.  You can also simply
download PerfView using the instructions above and select the Help -> User's Guide menu item. 

### Asking Questions / Reporting Bugs 

When you have question about PerfView, your first reaction should be to search the Users Guide (Help -> User's Guide) and 
see if you can find the answer already.   If that does not work you can ask a question by creating a [new PerfView Issue](https://github.com/Microsoft/perfview/issues/new).
State your question succinctly in the title, and if necessary give details in the body of the issue, there is a issue tag
called 'question' that you should use as well that marks your issue as a question rather than some bug report.
If the question is specific to a particular trace (*.ETL.ZIP file) you can drag that file onto the issue and it will be downloaded.
This allows those watching for issues to reproduce your environment and give much more detailed and useful answers.

Note that once you have your question answered, if the issue is likely to be common, you should strongly consider updating the
documentation to include the information.  The documentation is pretty much just
one file https://github.com/Microsoft/perfview/blob/main/src/PerfView/SupportFiles/UsersGuide.htm.
You will need to clone the repository and create a pull request (see [OpenSourceGitWorkflow](https://github.com/Microsoft/perfview/blob/main/documentation/OpenSourceGitWorkflow.md)
for instructions for setting up and creating a pull request.  

Reporting bugs works pretty much the same way as asking a question.  It is very likely that you will want to include the *.ETL.ZIP
file needed to reproduce the problem as well as any steps and the resulting undesirable behavior.

# Building PerfView Yourself

If you just want to do a performance investigation, you don't need to build PerfView yourself.
Just use the one from the [PerfView Download Page](documentation/Downloading.md).
However if you want new features or just want to contribute to PerfView to make it better
(see [issues](https://github.com/Microsoft/perfview/issues) for things people want)
you can do that by following the rest of these instructions.

### Tools Needed to Build PerfView

The only tools you need to build PerfView are Visual Studio 2017 and the .NET Core SDK.   The
[Visual Studio 2017 Community Edition](https://www.visualstudio.com/vs/community/) can be downloaded *for free* and,
along with the .NET Core SDK, has everything you need to fetch PerfView from GitHub, build and test it. We expect you
to download Visual Studio 2017 Community Edition if you don't already have Visual Studio 2017.

PerfView is mostly C# code, however there is a small amount of C++ code to implement some advanced features of PerfView 
(The ETWCLrProfiler dlls that allow PerfView to intercept the .NET Method calls; see .NET Call in the Collect dialog).  
If you downloaded the Visual Studio 2017 Community Edition, it does not install the C++ compilation tools by default and
it also does not include the Windows 8.1 SDK by default (we build PerfView so it can run on Win8 as well as Win10).  Thus
when you install Visual Studio 2017 check the 'Desktop Development with C++' option and then look the right pane to see
the optional sub-components, and make sure the Windws 8.1 SDK is also checked (it typically is not).   If you have
already installed VS 2017, you can add these options by going to Control Panel -> Programs and Features -> Visual Studio 2017, and click 'Modify'.   This will get you to the place where you can selecte the Desktop Development with C++ and the Windows 8.1 SDK. 
If you get any errors compiling the ETWClrProfiler* dlls, it is likely associated with getting this Win 8.1 SDK.  See 
the troubleshooting sections below for more if you need it.  

The .NET Core SDK should be part of the default VS 2017 installation now, but if not it can be installed easily from [here](https://www.microsoft.com/net/download/windows).

### Cloning the PerfView GitHub Repository. 

The first step in getting started with the PerfView source code is to clone the PerfView GitHub repository.
If you are already familiar with how GIT, GitHub, and Visual Studio 2017 GIT support works, then you can skip this section.
However, if not, the [Setting up a Local GitHub repository with Visual Studio 2017](documentation/SettingUpRepoInVS.md) document
will lead you through the basics of doing this. All it assumes is that you have Visual Studio 2017 installed.

### How to Build and Debug PerfView 

PerfView is developed in Visual Studio 2017 using features through C# 6.

  * The solution file is PerfView.sln.  Opening this file in Visual Studio (or double clicking on it in 
  the Windows Explorer) and selecting Build -> Build Solution, will build it. You can also build the 
  non-debug version from the command line using msbuild or the build.cmd file at the base of the repository.
  The build follows standard Visual Studio conventions, and the resulting PerfView.exe file ends up in
  src/PerfView/bin/*BuildType*/PerfView.exe. You need only deploy this one EXE to use it.  

  * The solution consists of 11 projects, representing support DLLs and the main EXE. To run PerfView in the 
  debugger **you need to make sure that the 'Startup Project' is set to the 'PerfView' project** so that it launches 
  the main EXE.   If the PerfView project in the Solution Explorer (on the right) is not bold, right click on the PerfView project 
  and select 'Set as Startup Project'. After doing this 'Start Debugging' (F5) should work.
  (It is annoying that this is not part of the .sln file...).  

### Deploying your new version of Perfview

You will want to deploy the 'Release' rather than the 'Debug' version of PerfView.  Thus, first set your build configuration
to 'Release' (Text window in the top toolbar, or right click on the .SLN file -> Configuration Manager -> Active Solution Configuration).
Next build (Build -> Build Solution (Ctrl-Shift-B)).   The result will be that in the src\perfView\bin\net45\Release directory there will be
among other things a PerfView.exe.   This one file is all you need to deploy.   Simply copy it to where you wish to deploy the app.  

### Information for build troubleshooting.  

  * One of the unusual things about PerfView is that it incorporates its support DLLs into the EXE itself, and these get 
  unpacked on first launch.  This means that there are tricky dependencies in the build that are not typical.    You will 
  see errors that certain DLLs can't be found if there were build problems earlier in the build.   Typically you can fix 
  this simply by doing a normal (non-clean) build, since the missing file will be present from the last compilation.
  If this does not fix things, see if the DLL being looked for actually exists (if it does, then rebuilding should fix it).
  It can make sense to go down the projects one by one and build them individually to see which one fails 'first'.
  
  * Another unusual thing about PerfView is that it includes an extension mechanism complete with samples.
  This extensions mechanism is the 'Global' project (called that because it is the Global Extension whose commands don't have an
  explicit 'scope') and needs to refer to PerfView to resolve some of its references.   Thus you will get many 'not found'
  issues in the 'Global' project.  These can be ignored until you get every other part of the build working.

  * One of the invariants of the repo is that if you are running Visual Studio 2017 and you simply sync and build the
  PerfView.sln file, it is supposed to 'just work'.   If that does not happen, and the advice above does not help, then
  we need to either fix the repo or update the advice above. Thus it is reasonable to open a GitHub issue. If you
  do this, the goal is to fix the problem, which means you have to put enough information into the issue to do that.
  This includes exactly what you tried, and what the error messages were.
  
  * You can also build PerfView from the command line (but you still need VS 2017 installed).   It is a two step process.
  First you must restore all the needed nuget packages, then you do the build itself. To do this:
    1. Open a developer command prompt.  You can do this by hitting the windows key (by the space bar) and type
       'Developer command prompt'.  You should see a entry for this that you can select (if VS 2017 is installed).
    2. Change directory to the base of your PerfView source tree (where PerfView.sln lives). 
    3. Restore the nuget packages by typing the command 'msbuild /t:restore'
    4. Build perfView by typing the command 'msbuild'
  
  * If you get an error ""MSB8036: The Windows SDK version 8.1 was not found"",  Or you get a 'assert.h' not found error, or 
  frankly any error associated with building the ETWClrProfiler dlls, you should make sure that you have the Windows 8.1 
  SDK installed (We like to build PerfView so it works event on Windows 8).    Unfortunately this library tends not to be 
  installed with Visual Studio anymore unless you ask for it explicitly.   To fix it 
     * windows-Key -> type Control panel -> Programs and Features, and right click on your VS2017 and select 'Modify'. Then look under the C++ Desktop Development and check that the Windows SDK 8.1 option is selected.  If not, select it and have the setup install this.  Then try building PerfView again.
  
### Running Tests

PerfView has a number of *.Test projects that have automated tests.  They can be run in Visual Studio by selecting the
Test -> Run -> All Tests menu item.    For the most thorough results (and certainly if you intend to submit changes) you 
need to run these tests with a Debug build of the product (see the text window in the top toolbar, it says 'Debug' or 'Release').
If tests fail you can right click on the failed test and select the 'Debug' context menu item to run the test under 
the debugger to figure out what went wrong.  

### Check in testing and code coverage statistica

This repository uses [AppVeyor](https://www.appveyor.com/) and Azure DevOps to automatically build and test pull requests, which allows
the community to easily view build results. Code coverage is provided by [codecov.io](https://codecov.io). The build and
coverage status reflected here is the AppVeyor and Azure DevOps build status of the **main** branch.

[![Build status](https://dev.azure.com/ms/perfview/_apis/build/status/CI?label=build)](https://dev.azure.com/ms/perfview/_build/latest?definitionId=332)

[![Build status](https://ci.appveyor.com/api/projects/status/fxtu3xa874whk2w0?svg=true)](https://ci.appveyor.com/project/sharwell/perfview)

[![codecov](https://codecov.io/gh/Microsoft/perfview/branch/main/graph/badge.svg)](https://codecov.io/gh/Microsoft/perfview)

> :warning: Builds produced by AppVeyor and Azure DevOps CI are not considered official builds of PerfView, and are not signed or otherwise
> validated for safety or security in any way. This build integration is provided as a convenience for community
> participants, but is not endorsed by Microsoft nor is it considered an official release channel in any way. For
> information about official builds, see the [PerfView Download Page](documentation/Downloading.md) page.\

### Contributing to PerfView 

You can get a lot of value out of the source code base simply by being able to build the code yourself, debug
through it or make a local, specialized feature, but the real power of open source software happens when
you contribute back to the shared code base and thus help the community as a whole.   **While we encourage this it 
requires significantly more effort on your part**.   If you are interested in stepping up, see the 
[PerfView Contribution Guide](CONTRIBUTING.md) and [PerfView Coding Standards](documentation/CodingStandards.md) before you start.

### Code Organization 

The code is broken into several main sections:

  * PerfView - GUI part of the application
    * StackViewer - GUI code for any view with the 'stacks' suffix
    * EventViewer - GUI code for the 'events' view window
    * Dialogs - GUI code for a variety of small dialog boxes (although the CollectingDialog is reasonably complex)
    * Memory - Contains code for memory investigations, in particular it defines 'Graph' and 'MemoryGraph' which are used 
      to display node-arc graphs (e.g. GC heaps)
  * TraceEvent - Library that understands how to decode Event Tracing for Windows (ETW) which is used to actually 
  collect the data for many investigations
  * MainWindow - GUI code for the window that is initially launched (lets you select files or collect new data)
  * ETWClrProfiler* - There are two projects that build the same source either 32 or 64 bit.   This is (the only) native code
  project in PerfView, and implements the CLR Profiler API and emits ETW events. It is used to trace object allocation
  stacks and .NET method calls.  
  * HeapDump* There are 32 and 64 bit versions of this project.  These make standalone executables that can dump the GC
  heap using Microsoft.Diagnostics.Runtime APIs.  This allows getting heap dumps from debugger process dumps.  
  * Global - An example of using PerfView's extensibility mechanism
  * CSVReader - old code that lets PerfView read .ETL.CSV files generated by XPERF (probably will delete)
  * Zip - a clone of System.IO.Compression.dll so that PerfView can run on pre V4.5 runtimes (probably will delete)
  * [PerfViewJS](src/PerfViewJS/README.md) - contains a version of the GUI based on HTML and JavaScript (for Linux support). (experimental)

### Other Documentation

These docs are for specialized scenarios 

  * [Updating SupportFiles](documentation/MakingSupportFilesNugetPackages.md) PerfView uses some binary files that it
does not build itself. We created two nuget packages to hold these.  This document tells you how to update this
nuget package when these files need to be updated. Very few people should care about these instructions.  

  * [Internal Docs](https://devdiv.visualstudio.com/DevDiv/_git/perfview?_a=preview&path=%2Fdocumentation%2Finternal%2FinternalDocs.md&version=GBmain) This is documentation that is only 
  useful for internal Microsoft users. By design the link will not work for most people.
"
210,microsoft/windows-admin-center-sdk,TypeScript,"# Windows Admin Center SDK #

Welcome to the Windows Admin Center SDK!  Windows Admin Center is an evolution of Windows Server in-box management tools; a locally deployed, browser-based management experience that supports scenarios where customers need full control of all aspects of their deployment, including private networks which aren’t Internet-connected.

### Get started with the SDK ###

Getting started with Windows Admin Center development is easy!  Follow along with [step-by-step directions](https://docs.microsoft.com/windows-server/manage/windows-admin-center/extend/prepare-development-environment) to prepare your environment, and learn more about writing and publishing extensions at our [documentation site](https://aka.ms/WACSDKDocs).

Don't have Windows Admin Center installed yet?  [Download](https://aka.ms/WACDownloadPage) Windows Admin Center.

### Sample Code included with the SDK ###

* Sample code can be found for **tool**, **solution**, and **gateway plugin** extension types in our [SDK documentation](https://aka.ms/WACSDKDocs).  There you will leverage the Windows Admin Center CLI to build a new extension project, then follow the individual guides to customize your project to meet your needs.

* [Developer Tools](/windows-admin-center-developer-tools) is a repository of code to give you an example of how to use and include Windows Admin Center controls and styles in your extensions.  Just use the CLI to create your tool or solution, and then reference the code in the repository to see what is available.

To actually see the code in action, Use the [Extension Manager](/https://docs.microsoft.com/en-us/windows-server/manage/windows-admin-center/configure/using-extensions) to find the ""Windows Admin Center Developer Tools (Preview)"" extension and install it.  This will add a new solution to your gateway instance that has tools to help you explore the development environment.

### SDK design toolkit ###

Check out our Windows Admin Center [SDK design toolkit](WindowsAdminCenterDesignToolkit.zip)! This toolkit is designed to help you rapidly mock up extensions in PowerPoint using Windows Admin Center styles, controls, and page templates. See what your extension can look like in Windows Admin Center before you start coding!

### Contributing ###

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
211,microsoft/react-native-dualscreen,TypeScript,"# React Native dual-screen
This repo contains Microsoft's offerings to streamline [dual-screen](https://docs.microsoft.com/en-us/dual-screen/) cross-platform development using React Native. The modules in the repo will work on any platform, but only Android actually has a dual screen device (Duo).

### Repo status
See below.  We currently have three npm packages for dual screen devices.


## Offerings
This repo provides three modules
* **TwoPaneView** layout component
* **DualScreenInfo** lower-level module
* **TwoPane-Navigation** navigation library for dual screen devices

Please find more details about the features as they were proposed and implemented.  Feel free to chime in with comments about [TwoPaneview](https://github.com/react-native-community/discussions-and-proposals/issues/197), [DualScreenInfo](https://github.com/react-native-community/discussions-and-proposals/issues/189) and 
[TwoPane-Navigation](https://github.com/microsoft/react-native-dualscreen/tree/master/twopane-navigation)!

# React Native for Windows
In the meantime, for more information about React Native for Windows, including steps for [getting started](https://microsoft.github.io/react-native-windows/docs/getting-started), please visit our [website](https://microsoft.github.io/react-native-windows/). And keep an eye on our [blog](https://microsoft.github.io/react-native-windows/blog/) or follow [@ReactWindows](https://twitter.com/ReactWindows) on Twitter for new announcements!
"
212,microsoft/sarif-pattern-matcher,C#,"# sarif-pattern-matcher

[![release](https://img.shields.io/github/v/release/microsoft/sarif-pattern-matcher?label=release)](https://github.com/microsoft/sarif-pattern-matcher/releases/latest)
[![releases](https://img.shields.io/github/v/release/microsoft/sarif-pattern-matcher?include_prereleases&label=pre-release)](https://github.com/microsoft/sarif-pattern-matcher/releases)
[![license](https://img.shields.io/github/license/microsoft/sarif-pattern-matcher)](https://github.com/microsoft/sarif-pattern-matcher/blob/master/LICENSE)

Quality domain agnostic regular expression pattern matcher that persists results to SARIF

## NuGet packages

The following packages are published from this repository:

|| Latest Official Release|
|-|-|
| [Sarif.Pattern.Matcher](https://www.nuget.org/packages/Sarif.PatternMatcher/)| [![Nuget](https://img.shields.io/nuget/vpre/Sarif.PatternMatcher)](https://www.nuget.org/packages/Sarif.PatternMatcher/)|
| [Sarif.Pattern.Matcher.Cli](https://www.nuget.org/packages/Sarif.PatternMatcher.Cli/)| [![Nuget](https://img.shields.io/nuget/vpre/Sarif.PatternMatcher.Cli)](https://www.nuget.org/packages/Sarif.PatternMatcher.Cli/)|
| [Sarif.Pattern.Matcher.Sdk](https://www.nuget.org/packages/Sarif.PatternMatcher.Sdk/)| [![Nuget](https://img.shields.io/nuget/vpre/Sarif.PatternMatcher.Sdk)](https://www.nuget.org/packages/Sarif.PatternMatcher.Sdk/)|
| [Sarif.Pattern.Matcher.Security](https://www.nuget.org/packages/Sarif.PatternMatcher.Security/)| [![Nuget](https://img.shields.io/nuget/vpre/Sarif.PatternMatcher.Security)](https://www.nuget.org/packages/Sarif.PatternMatcher.Security/)|
| [RE2.Managed](https://www.nuget.org/packages/RE2.Managed/)| [![Nuget](https://img.shields.io/nuget/vpre/RE2.Managed)](https://www.nuget.org/packages/RE2.Managed/)|
| [Strings.Interop](https://www.nuget.org/packages/Strings.Interop/)| [![Nuget](https://img.shields.io/nuget/vpre/Strings.Interop)](https://www.nuget.org/packages/Strings.Interop/)|

## Getting started
- [Creating plugins](./docs/getting-started/creating-plugins.md)
- [Using the client tool](./docs/getting-started/using-the-client-tool.md)
- [Using the library](./docs/getting-started/using-the-library.md)

## How To Contribute

sarif-pattern-matcher is accepting contributions. If you've submitted a PR for an existing issue, please post a comment in the issue to avoid duplication of effort. See our [CONTRIBUTING](/CONTRIBUTING.md) file for more information - it also contains guidelines for how to submit a PR.

## License

""Sarif-pattern-matcher"" is licensed under `MIT license`. View [license](https://github.com/microsoft/sarif-pattern-matcher/blob/master/LICENSE).
"
213,microsoft/Powerbi-Visuals-SampleMatrix,TypeScript,"# Abstract
This is a reference visual demoing the Subtotals API availiable starting with the API 2.6 and announced/documented here: 
https://powerbi.microsoft.com/da-dk/blog/power-bi-developer-community-april-may-update/


# Note: use pbiviz 2.5 to build the visual

As of Nov 14, 2019 the most recent version of PBIVIZ is producing a malfunctioning visual when packaged with ""pbiviz package"". 

Unless fixed in the future PBIVIZ releases, I would suggest that you use the older PBIVIZ toolset of verions 2.5.0. 

To install it run: 

npm install -g powerbi-visuals-tools@2.5.0

# Known issue: subtotals not working in the debugger visual

As of Nov 17, 2019, subtotals are not working in the debugger visual. It’s a known issue and I am currently investigating the root cause. 

Please note the issue only affects the development process, while the release visuals (i.e., packaged PBIVIZ files) are working correctly in terms of the subtotals. 

Currently, the best (partial) workaround would be to disable the minimization of the visuals in development (--no-minify pbiviz flag) and debugging in F12. 
"
214,microsoft/Ironclad,Dafny,"# About

To learn more about the Ironclad Apps project, please see the related [README](./ironclad-apps/README.md) file.

To learn more about the IronFleet project, please see the related [README](./ironfleet/README.md) file.

"
215,microsoft/setup-msbuild,TypeScript,"# microsoft/setup-msbuild
You know how handy that 'Visual Studio Developer Command Prompt' is on your local machine?  And how it adds several things to `PATH` to allow you to just issue commands like `msbuild` or otherwise?  Use this action to setup similar flexibility in your Windows-based GitHub Actions runners.  This will let you discover where the `MSBuild` tool is and automatically add it to the `PATH` environment variables for you so future steps in your Actions workflow can just initiate `msbuild` commands without knowing the full path.

> Please note this tools is not to replicate the full 'Developer Command Prompt' but only discover and assist with MSBuild and not other tools like cl.exe

## Usage

```yml
- name: Add msbuild to PATH
  uses: microsoft/setup-msbuild@v1.0.2
```

## Specifying specific versions of Visual Studio
You may have a situation where your Actions runner has multiple versions of Visual Studio and you need to find a specific version of the tool.  Simply add the `vs-version` input to specify the range of versions to find.  If looking for a specific version, specify the minimum and maximum versions as shown in the example below, which will look for just 16.4.

```yml
- name: Add msbuild to PATH
  uses: microsoft/setup-msbuild@v1.0.2
  with:
    vs-version: '[16.4,16.5)'
```

The syntax is the same used for Visual Studio extensions, where square brackets like ""["" mean inclusive, and parenthesis like ""("" mean exclusive. A comma is always required, but eliding the minimum version looks for all older versions and eliding the maximum version looks for all newer versions. See the [vswhere wiki](https://github.com/microsoft/vswhere/wiki) for more details.

## How does this work?
This makes use of the vswhere tool which is a tool delivered by Microsoft to help in identifying Visual Studio installs and various components.  This tool is installed on the hosted Windows runners for GitHub Actions.  If you are using a self-hosted runner, you either need to make sure vswhere.exe is in your agent's PATH or specify a full path to the location using:

```yml
- name: Add msbuild to PATH
  uses: microsoft/setup-msbuild@v1.0.2
  with:
    vswhere-path: 'C:\path\to\your\tools\'
```

## Notes on arguments
While the Action enables you to specify a `vswhere` path as well as a `vs-version`, these are more advanced options and when using GitHub-hosted runners you should not need these and is recommended you don't specify them.  Using these require you to fully understand the runner environment, updates to the tools on the runner, and can cause failures if you are out of sync.  For GitHub-hosted runners, omitting these arguments is the preferred usage.

## Building this repo
As with most GitHub Actions, this requires NodeJS development tools.  After installing NodeJS, you can build this by executing:

```bash
npm install
npm run build
npm run pack
```

which will modify/create the /dist folder with the final index.js output

# Credits
Thank you to [Warren Buckley](https://github.com/warrenbuckley) for being a core contributor to this Action for the benefit of all developers!

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
216,microsoft/azure-devops-engineering-extensions,TypeScript,"# Azure DevOps Engineering Marketplace Extensions

AzureDevOps Marketplace Extensions to do various engineering tasks.

## Email Report Task
Generates a Report with the information from the pipeline and sends it as email. Goal is to provide all the relevant info happened in the pipeline, in a concise and neat manner. 

Read more [here](Tasks/emailReportTask/README.md).

## Pull Request Insights Task
PullRequestInsightsTask provides insights into pipelines. This extension has two main functions:

1. Investigating pull request validation failures
2. Alerting pull request owners to the introduction of long running tasks.

Read more [here](Tasks/pullRequestInsightsTask/README.md).

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.
When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

To build:

```
npm install
npm run build // builds all tasks
npm run build:t1 // build specific task - first task
npm run build:t2 // builds second task - Refer package.json
```

To run unit tests (at root): 

```
npm test
```

To run PullRequestInsights task E2E w/o installing it on a pipeline: 

```
npm run e2e:emailreport // or look below
npm run e2e:t1 // proxy for above
npm run e2e:t2 // proxy for prinsights
```
VS Code Debugging option is also available. Use launch.json under .vscode directory.
Note: E2E test(s) will fail by default with 401 errors as the restclients are not authenticated without providing correct credentials. Change E2E test code to supply correct credentials to test the scenario.

Packaging:

Run the pack command for each task. t1/t2/... are shortcuts for tasks in alphabhetical order. Please check package.json to confirm or use the actual names if preferred.
```
npm run pack:t1 // if option not specified, ""dev"" is default. Doesn't create VSIX.
npm run pack:t1 prod // creates VSIX for emailReport
npm run pack:t2  // Doesn't create VSIX
npm run pack:t2 prod // Creates VSIX for pullRequestInsights
```
Note: Option ""dev"" doesn't create a VSIX file. Instead, it prepares the output task folder ready for upload to an AzureDevOps account and test it.

####
"
217,microsoft/azure-repos-pr-multi-cherry-pick,TypeScript,"# Multi Cherry-Pick Tool

[![Build Status](https://dev.azure.com/1es-cat/azure-repos-pr-multi-cherry-pick/_apis/build/status/microsoft.azure-repos-pr-multi-cherry-pick?branchName=master)](https://dev.azure.com/1es-cat/azure-repos-pr-multi-cherry-pick/_build/latest?definitionId=24&branchName=master)
[![Release Status](https://vsrm.dev.azure.com/1es-cat/_apis/public/Release/badge/a185aa03-7d78-4c7d-b5fb-f7d997b096f9/1/1)](https://dev.azure.com/1es-cat/azure-repos-pr-multi-cherry-pick/_release?definitionId=1)

This tool offers an easy way to use the git cherry-pick operation to apply changes to multiple branches.
For each branch selected, a new topic branch will be created with the applied changes.
If the **Pull request** option is selected, a pull request will be opened to the target branch.

<img width=""434"" alt=""Screen Shot 2019-05-13 at 1 00 27 PM"" src=""https://user-images.githubusercontent.com/19557880/57650379-87229e00-757f-11e9-8966-e00bb5416c8f.png""><img width=""436"" alt=""Screen Shot 2019-05-13 at 1 00 46 PM"" src=""https://user-images.githubusercontent.com/19557880/57650380-87229e00-757f-11e9-9143-549002959cea.png"">

## Quick steps to get started using the tool

1. Install the extension from the [marketplace](https://marketplace.visualstudio.com/items?itemName=1ESLighthouseEng.pr-multi-cherry-pick) into your Azure DevOps organization.
2. Navigate to your pull request.
3. Select the context menu (...)
4. Select **Multi-cherry-pick**.

<p style=""padding-left:25px"">
<img width=""409"" alt=""Screen Shot 2019-05-10 at 4 20 10 PM"" src=""https://user-images.githubusercontent.com/19557880/57596172-1a1af400-74fe-11e9-8c0d-18291d20590a.png"">
</p>

5. Add as many cherry-pick targets as you would like.
6. After you click **Complete**, a summary page will appear with links to branches and PRs created from the tool.

## Technologies used to develop the extension

- Code written in Typescript; styling defined using SASS.
- Webpack for watching and building files during development, and for building optimized bundles for production.
- React for rendering a complex UI with user interaction.

## How to build

### **Download the required tools**

You will need:

- [Visual Studio Code](https://code.visualstudio.com/download)
- [Firefox](https://www.mozilla.org/firefox/) (the VS Code Debugger for Chrome extension [doesn’t support iframes](https://github.com/microsoft/vscode-chrome-debug/issues/786) yet)
- The [Debugger for Firefox](https://marketplace.visualstudio.com/items?itemName=hbenl.vscode-firefox-debug) VS Code extension

### **Prereq: Organization permission level**

- To develop and test the extension, you will need an organization in which you have permission to install extensions (e.g. you are the owner).
- If you don't have a personal organization, you can [create an organization for free](https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/create-organization?view=azure-devops).

### **Prereq: Node and NPM**

- **Windows and Mac OSX**: Download and install node from [nodejs.org](http://nodejs.org/).

- **Linux**: Install [using package manager](https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager).

From a terminal ensure at least node 10.15 and npm 6.9. Use the following command to figure out what version you have installed locally:

```bash
node -v && npm -v
```

The following should appear in your terminal:

```bash
v10.15.0
6.9.0
```

To install npm separately and verify that it installed properly:

```
[sudo] npm install npm@6 -g
npm -v
```

**Note:** On Windows, if it's still returning npm 2.x, run `where npm`. Notice hits in program files. Rename those two npm files and the 5.6.0 in AppData will win.

### **Prereq: Create a publisher**

All extensions, including extensions from Microsoft, live under a publisher. Anyone can create a publisher and publish extensions under it. You can also give other people access to your publisher if a team is developing the extension.

You will do one of two things:

- Sign in to the Visual Studio Marketplace management portal
- If you don't already have a publisher, you'll be prompted to create one. Learn how to create one [here](https://docs.microsoft.com/en-us/azure/devops/extend/publish/overview?view=azure-devops)

### **Install dependencies**

Run this command once:

```bash
npm install
```

### **Build the Extension**

This extension uses webpack for bundling and webpack-dev-server for watching files and serving bundles during development.
Two bundles are defined for webpack: one for the main dialog and one for the extension context menu registration.
All actions can be triggered using npm scripts (`npm run <target>`) with no additional task runner required.

### **Deploy the Extension**

You will need to deploy your extension to the marketplace at least once so that you can share it with your organization
and install it. In order to do this, you will need to generate a personal access token (PAT). Learn how to do that [here](https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops). When creating your PAT, under **Organization**, select **All accessible organizations**, and set the **Marketplace** scope to **Publish**.

Then run once, inserting your PAS into [token]:

```bash
npm run publish:dev --  --token [token]
```

You will then need to install and share your extension, learn how to do that [here](https://docs.microsoft.com/en-us/azure/devops/extend/get-started/node?toc=%2Fazure%2Fdevops%2Fextend%2Ftoc.json&bc=%2Fazure%2Fdevops%2Fextend%2Fbreadcrumb%2Ftoc.json&view=azure-devops#install-your-extension).

Once the extension is installed, you will notice that it won’t load correctly. It isn't loading because we configured it to load all its resources (html, images, etc.) from `localhost:3000`, but there is no server running yet.

To start webpack-dev-server run:

```bash
npm run start:dev
```

Now if you go to `localhost:3000` in your browser, you should get an untrusted certificate error page. Select **Advanced** and then trust the certificate. Go back to Azure DevOps and your extension should now load correctly and any changes to the source code will cause webpack to recompile and reload the extension automatically.

Although most code changes will be reflected immediately, you may still need to occasionally update your extension in the marketplace. The dev extension loads all its resources from the webpack-dev-server, but the manifest itself is being loaded from the published code. Therefore, any changes to the manifest file will not be properly reflected in Azure DevOps until the extension has been republished.

### **Configure your VS Code project to debug against Azure DevOps**

In VS Code, press **F5** to start debugging (making sure the webpack-dev-server is still running). The default launch configuration should be set to **Firefox**. 

**Note**: Chrome configurations are included in the sample as well in case the Chrome debugging extension eventually supports iframes. However, debugging iframes is only supported in the Debugger for Firefox extension for now.

Once Firefox starts up, you will have to go through the steps of allowing the `localhost:3000` certificate again and log into your Azure DevOps account. From now on, if you leave this Firefox window open, the debugger will reattach instead of starting a clean Firefox instance each time.

Once you are logged in to Azure DevOps, your extension should be running. Set a breakpoint in a method in VS Code and you should see that breakpoint hit when that method executes.

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
218,microsoft/botframework-components,C#,"# Bot Framework Components

This repository contains *components* published by Microsoft for bots built on the Azure Bot Framework technology stack. They are part of the component model for building bots with re-usable building blocks. The model is built on a configurable [adaptive runtime](#adaptive-runtime), that can be extended by adding your own code, importing [packages](#packages) of functionality or connecting to other bots as [skills](#skills). Getting started [templates](#templates) provide dynamic code scaffolding, helping users get started quickly based on their scenario.

## Using Components

You'll primarily use components through [**Bot Framework Composer**](https://github.com/microsoft/BotFramework-Composer) - our visual bot authoring canvas for developers. From Composer you can add and remove packages from your bot, and the creation process creates bots built from the templates here.

## Creating your own components

You can also create your own packages and templates for use from Composer. We document creating components [here](/docs/overview.md). You can use this repository as examples for building your own components.

## Index of Content

### Templates

Templates are pre-built bot projects designed for specific scenarios. We use [yeoman](https://yeoman.io) generators for scaffolding our templates.

| Name         | npm | Description |
|:------------:|:---:|:------------|
|[Empty Bot](/generators/generator-bot-empty) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-empty.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-empty) | A simple bot with a root dialog and greeting dialog. |
|[Core Bot with Azure Language Understanding](/generators/generator-bot-core-language) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-core-language.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-core-language) | A simple bot with Azure Language Understanding (LUIS) and common trigger phrases used to direct the conversation flow. |
|[Core Assistant Bot](/generators/generator-bot-core-assistant) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-core-assistant.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-core-assistant) | A bot with Azure Language Understanding (LUIS) and common trigger phrases used to direct the conversation flow and help customers accomplish basic tasks. Designed to be extended with skills. |
|[Enterprise Assistant Bot](/generators/generator-bot-enterprise-assistant) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-assistant.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-assistant) | A Core Assistant Bot with Calendar & People as skills. |
|[Enterprise Calendar Bot](/generators/generator-bot-enterprise-calendar) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-calendar.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-calendar) | A bot with the ability to interact with M365 Calendar using Microsoft Graph. |
|[Enterprise People Bot](/generators/generator-bot-enterprise-people) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-people.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-enterprise-people) | A bot with the ability to search for people within Azure Active Directory using Microsoft Graph.|
|[Adaptive Bot Generator](/generators/generator-bot-adaptive) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-adaptive.svg)](https://badge.fury.io/js/%40microsoft%2Fgenerator-bot-adaptive) | Used by other generators to scaffold web app or functions project. |

### Packages

Packages are bits of bots that you can add to your bot project. They can contain coded extensions like custom actions, adapters, or triggers, and declarative assets like dialogs, language generation or language understanding files.

| Name         |Type   | NuGet | npm |Description |
|:------------:|:------|:-----:|:---:|:-----------|
|[Welcome](/packages/Welcome) | Dialogs | [![NuGet Badge](https://buildstats.info/nuget/Microsoft.Bot.Components.Welcome?includePreReleases=true)](https://www.nuget.org/packages/Microsoft.Bot.Components.Welcome/)| [![npm version](https://badge.fury.io/js/%40microsoft%2Fbot-components-welcome.svg)](https://badge.fury.io/js/%40microsoft%2Fbot-components-welcome) | Declarative assets supporting scenarios that welcome new and returning users. |
|[HelpAndCancel](/packages/HelpAndCancel) | Dialogs | [![NuGet Badge](https://buildstats.info/nuget/Microsoft.Bot.Components.HelpAndCancel?includePreReleases=true)](https://www.nuget.org/packages/Microsoft.Bot.Components.HelpAndCancel/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fbot-components-helpandcancel.svg)](https://badge.fury.io/js/%40microsoft%2Fbot-components-helpandcancel) | Declarative assets supporting scenarios for ""help"" and ""cancel"" utterances. |
|[Graph](/packages/Graph) | Custom Actions | [![NuGet Badge](https://buildstats.info/nuget/Microsoft.Bot.Components.Graph?includePreReleases=true)](https://www.nuget.org/packages/Microsoft.Bot.Components.Graph/) | | Custom actions for working with calendars and people through the MS Graph API.|
|[Teams](/packages/Teams) | Triggers Actions | [![NuGet Badge](https://buildstats.info/nuget/Microsoft.Bot.Components.Teams?includePreReleases=true)](https://www.nuget.org/packages/Microsoft.Bot.Components.Teams/) | | Triggers and actions for working with Microsoft Teams.|

### Virtual Assistant skills (Legacy)

Skills built to work with the [Virtual Assistant](https://docs.microsoft.com/azure/bot-service/bot-builder-virtual-assistant-introduction) template. You can find the list of Virtual Assistant skills [here](/skills/csharp/readme.md).

## Need Help?

Please use this GitHub repository issue to raise any [issues](https://github.com/Microsoft/botframework-components/issues/new?assignees=&labels=Type%3A+Bug&template=bug_report.md&title=) you encounter consuming these components, or [feature requests](https://github.com/Microsoft/botframework-components/issues/new?assignees=&labels=Type%3A+Feature&template=feature_request.md&title=) you'd like to see added.

## Contributing

We welcome contributions to this repository! Please see our [wiki](https://github.com/microsoft/botframework-components/wiki) for details on how to contribute.

If you'd like to contribute a completely new package or template, please use our [community repo](https://github.com/BotBuilderCommunity/) and we can help publish them for you, or feel free to blaze your own trail and publish them independently.

## Reporting Security Issues

Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) at [secure@microsoft.com](mailto:secure@microsoft.com). You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the [MSRC PGP](https://technet.microsoft.com/security/dn606155) key, can be found in the [Security TechCenter](https://technet.microsoft.com/security/default).

## License

MIT License

Copyright (c) Microsoft Corporation.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE."
219,microsoft/WinAppDriver,C#,"## Windows Application Driver
Windows Application Driver (WinAppDriver) is a service to support Selenium-like UI Test Automation on Windows Applications. This service supports testing **Universal Windows Platform (UWP)**, **Windows Forms (WinForms)**, **Windows Presentation Foundation (WPF)**, and **Classic Windows (Win32)** apps on **Windows 10 PCs**. 

### Install & Run WinAppDriver
1. Download Windows Application Driver installer from <https://github.com/Microsoft/WinAppDriver/releases>
2. Run the installer on a Windows 10 machine where your application under test is installed and will be tested
3. Enable [Developer Mode](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development) in Windows settings
4. Run `WinAppDriver.exe` from the installation directory (E.g. `C:\Program Files (x86)\Windows Application Driver`)

Windows Application Driver will then be running on the test machine listening to requests on the default IP address and port (`127.0.0.1:4723`). You can then run any of our [Tests](/Tests/) or [Samples](/Samples). `WinAppDriver.exe` can be configured to listen to a different IP address and port as follows:

```
WinAppDriver.exe 4727
WinAppDriver.exe 10.0.0.10 4725
WinAppDriver.exe 10.0.0.10 4723/wd/hub
```

> **Note**: You must run `WinAppDriver.exe` as **administrator** to listen to a different IP address and port.

### Write an Automation Script
Now that you've successfully installed WinAppDriver, you can get started with [authoring your first automation script](./Docs/AuthoringTestScripts.md)! 

### Supported APIs

See [here](./Docs/SupportedAPIs.md) for a list of supported APIs by WinAppDriver. API support may differ from Appium and other counterparts.

## FAQ & Documentation
Additional documentation on WinAppDriver and related topics can be found under [/Docs/](./Docs/), such as the following:
   - [Frequently Asked Questions](./Docs/FAQ.md) 
     - [General Development & Best Practices](./Docs/FAQ.md#general-development--best-practices) 
     - [Using with Appium](./Docs/UsingAppium.md)
   - [Running WinAppDriver in CI (with Azure Pipelines)](./Docs/CI_AzureDevOps.md) 
   - [Using UI Recorder](./Docs/UsingUIRecorder.md)
   - [Authoring Test Scripts](./Docs/AuthoringTestScripts.md)
   - [Using the Selenium Grid](./Docs/SeleniumGrid.md) 
   - [Running On a Remote Machine](./Docs/RunningOnRemoteMachine.md)

## Repository Content
This repository includes the following content:
* [Samples](https://github.com/Microsoft/WinAppDriver/tree/master/Samples) - used to showcase various commands and operations such as opening applications, finding elements, clicking elements, typing keystrokes, reading texts, etc; and can be run against built-in Windows 10 applications such as **Alarms & Clock**, **Calculator**, and **Notepad**. 
* [Tests](https://github.com/Microsoft/WinAppDriver/tree/master/Tests) - used to verify the functionality of **Windows Application Driver** itself. The tests cover each API endpoints extensively and also against all basic UI control scenario, and demonstrate how to invoke certain command in C#. In addition, they show how to interact with some more complex UI elements such as **DatePicker**, **SplitViewPane**, **Slider**, etc.
* [UI Recorder](https://github.com/microsoft/WinAppDriver/tree/master/Tools/UIRecorder) - standalone tool that aims to provide users a simpler way of creating automaton scripts by recording UI events performed by the user and generating XPath queries and C# code on the fly. Read more about it on our [Wiki](https://github.com/Microsoft/WinAppDriver/wiki/WinAppDriver-UI-Recorder). 
* [Docs](./Docs/) - subdirectory hosting WinAppDriver related documentation. 

## Vote on New Features
Add your feature request in [issues](../../issues/) or :+1: (+1) existing issues labeled as **Enhancement**
"
220,microsoft/responsible-ai-widgets,TypeScript,"![Responsible AI Widgets Python Build](https://github.com/microsoft/responsible-ai-widgets/workflows/Responsible%20AI%20Widgets/badge.svg) ![CD](https://github.com/microsoft/responsible-ai-widgets/workflows/CD/badge.svg) ![MIT license](https://img.shields.io/badge/License-MIT-blue.svg) ![PyPI raiwidgets](https://img.shields.io/pypi/v/raiwidgets?color=blue) ![PyPI rai_core_flask](https://img.shields.io/pypi/v/rai_core_flask?color=blue) ![npm fairness](https://img.shields.io/npm/v/@responsible-ai/fairness?label=npm%20%40responsible-ai%2Ffairness) ![npm interpret](https://img.shields.io/npm/v/@responsible-ai/interpret?label=npm%20%40responsible-ai%2Finterpret) ![npm mlchartlib](https://img.shields.io/npm/v/@responsible-ai/mlchartlib?label=npm%20%40responsible-ai%2Fmlchartlib) ![npm core-ui](https://img.shields.io/npm/v/@responsible-ai/core-ui?label=npm%20%40responsible-ai%2Fcore-ui) ![npm dataset-explorer](https://img.shields.io/npm/v/@responsible-ai/dataset-explorer?label=npm%20%40responsible-ai%2Fdataset-explorer) ![npm causality](https://img.shields.io/npm/v/@responsible-ai/causality?label=npm%20%40responsible-ai%2Fcausality) ![npm counterfactuals](https://img.shields.io/npm/v/@responsible-ai/counterfactuals?label=npm%20%40responsible-ai%2Fcounterfactuals)

# Responsible-AI-Widgets


Responsible-AI-Widgets provides a collection of model and data exploration and assessment user interfaces that enable better understanding of AI systems. Together, these interfaces empower developers and stakeholders of AI systems to develop and monitor AI more responsibly. Currently, there are three widgets demonstrating how to interpret models and assess their errors and fairness issues.

This repository contains the Jupyter notebooks with examples to showcase how to use these widgets.

## Contents

- [Overview of Responsible-AI-Widgets](#intro)
- [Interpretability Dashboard](#interpretability-dashboard)
- [Error Analysis Dashboard](#error-analysis-dashboard)
- [Fairness Dashboard](#fairness-dashboard)
- [Supported Models](#supported-models)
- [Getting Started](#getting-started)

<a name=""intro""></a>

## Overview of Responsible-AI-Widgets
Responsible-AI-Widgets extends the [Interpret-Community](https://github.com/interpretml/interpret-community) and [Fairlearn](https://github.com/fairlearn/fairlearn) repositories and provides user interfaces for model interpretability and fairness assessment of machine learning models. It introduces Error Analysis, a toolkit to identify and diagnose errors in machine learning models.  The following table shows a list of the user interfaces available in this repository:

| User Interface | Description | Use Case  (Assessing a loan allocation model to accept or deny home loan applicants.) |
| --- | --- | --- |
| Interpretability Dashboard |  User interface for [Interpret-Community](https://github.com/interpretml/interpret-community) which enables you to 1) evaluate your model by observing its performance metrics, 2) explore your dataset statistics, 3) understand the most important factors impacting your model’s overall (global) and individual (local) predictions, 4) debug models by performing a variety of feature perturbation operations (e.g., what-if analysis and Individual Conditional Expectation Plots), and 5) Understand your model’s explanations on different demographics. | Use the Interpretability dashboard to understand which factors have the most impact on your model's accept/deny decisions. Observe this for the whole population, for a subset of applicants (e.g., females), and individuals (such as why Mary’s loan got rejected). |
| Error Analysis (+ Interpretability) Dashboard |  Use the Error Analysis dashboard to 1) ***Identify*** cohorts with high error rate versus benchmark and visualize how the error rate is distributed. 2) ***Diagnose*** the root causes of the errors by visually diving deeper into the characteristics of data and models (via its embedded interpretability capabilities) | Use Error Analysis to discover that the model has a higher error rate for a specific cohort (e.g., females with income <$50K) vs. the rest of the population. Next, use the embedded interpretability capabilities of this dashboard to understand most impactful factors responsible for this subset’s erroneous predictions. Moreover, use interpretability to inspect some individuals of that cohort receiving erroneous predictions, understand their feature importance values, and perform what-if analysis on them to diagnose the contributing error factors better. |
| Fairness Dashboard |  User interface for [Fairlearn](https://github.com/fairlearn/fairlearn) which enables you to use common fairness metrics to assess which groups of people may be negatively impacted (females vs. males vs. non-binary gender). Also explore Fairlearn's state-of-the-art unfairness mitigation algorithms to mitigate fairness issues in your classification and regression models.  | Use Fairness dashboard to assess harm of allocation (i.e., to understand whether your loan allocation model approves more applications of a specific advantaged group). Use Fairness dashboard to assess harm of quality of service (i.e., Understand how your model performs on applications of your qualified males group vs. qualified females/non-binary gender.) Navigate trade offs between fairness and performance of your loan allocation model. Use [Fairlearn](https://github.com/fairlearn/fairlearn)'s mitigation algorithms to mitigate the observed fairness issues. |

Besides the above functionalities, this repository provides foundational blocks such as 

- A shared Flask service layer which also maintains utilities to determine the environment that it is running in so that it can configure the local flask service accordingly. This layer is published in the ```rai_core_flask``` package on PyPI.

- A base typescript library with common controls used across responsible AI dashboards. For information on how to contribute please refer to our [Contributor Guide](CONTRIBUTING.md).

## Example Notebooks

- [Interpretability for binary classification (employee attrition)](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/interpretability-dashboard-employee-attrition.ipynb)
- [Fairness assessment of a loan allocation model](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/fairness-dashboard-loan-allocation.ipynb)
- [Joint Example: Interpretability and fairness assessment a loan allocation model](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/fairness-interpretability-dashboard-loan-allocation.ipynb)

- [Error analysis and interpretability of a census income prediction model](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/erroranalysis-interpretability-dashboard-census.ipynb)
- [Error analysis and interpretability of a breast cancer prediction model](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/erroranalysis-interpretability-dashboard-breast-cancer.ipynb)


<a name=""interpretability dashboard""></a>

## Interpretability Dashboard

Please refer to [Interpret-Community](https://github.com/interpretml/interpret-community)'s README and [sample notebooks](https://github.com/interpretml/interpret-community/tree/master/notebooks) to learn how you can train and generate model explanations.  Once your model is trained and your explanation object is generated, load the interpretability visualization dashboard in your notebook to understand and interpret your model:

```python
from raiwidgets import ExplanationDashboard

ExplanationDashboard(global_explanation, model, dataset=x_test, true_y=y_test)
```
Once you load the visualization dashboard, you can investigate different aspects of your dataset and trained model via four tab views: 

* Model Performance
* Data Explorer	
* Aggregate Feature Importance
* Individual Feature Importance and what-if	

---
**NOTE**

Click on ""Open in a new tab"" on the top left corner to get a better view of the dashboard in a new tab.

---

You can further create custom cohorts (subgroups of your dataset) to explore the insights across different subgroups (e.g., women vs. men). The created cohorts can contain more than one filter (e.g., age < 30 and sex = female) and will be visible from all of the four tabs. The following sections demonstrate the visualization dashboard capabilities on a [classification model trained on employee attrition dataset](https://github.com/microsoft/responsible-ai-widgets/blob/master/notebooks/interpretability-dashboard-employee-attrition.ipynb). Besides the default cohort (including the whole dataset), there are two additional cohorts created: employees with Age <= 35 and employees with Age > 35.

![Visualization Dashboard Cohorts](./img/Interpretability-Cohorts.png)

### Model performance

This tab enables you to evaluate your model by observing its performance metrics and prediction probabilities/classes/values across different cohorts.

![Visualization Dashboard Cohorts](./img/Interpretability-ModelPerformance.png)

### Dataset explorer
You can explore your dataset statistics by selecting different filters along the X, Y, and color axes of this tab to slice your data into different dimensions.

![Visualization Dashboard Cohorts](./img/Interpretability-DatasetExplorer.png)

The following plots provide a global view of the trained model along with its predictions and explanations.

### Aggregate feature importance (global explanation)

This view consists of two charts:

| Plot | Description |
| --- | --- |
| Feature Importance | Explore the top K important features that impact your overall model predictions (a.k.a. global explanation). Use the slider to show additional less important feature values. Select up to three cohorts to see their feature importance values side by side. |
| Dependence Plot | Click on any of the feature bars in the feature importance graph to see the relationship of the values of the selected feature to its corresponding feature importance values. Overall, this plot show how values of the selected feature impact model prediction. |

![Visualization Dashboard Global](./img/Interpretability-GlobalExplanation.png)

### Individual feature importance (local explanation) and what-if

You can click on any individual data point on the scatter plot to view its local feature importance values (local explanation) and individual conditional expectation (ICE) plot below. These are the capabilities covered in this tab:

| Plot | Description |
| --- | --- |
| Feature Importance Plot | Shows the top K (configurable K) important features for an individual prediction. Helps illustrate the local behavior of the underlying model on a specific data point. |
| Individual Conditional Expectation (ICE) | Allows feature value changes from a minimum value to a maximum value. Helps illustrate how the data point's prediction changes when a feature changes. |
| Perturbation Exploration (what-if analysis) | Allows changes to feature values of the selected data point to observe resulting changes to prediction value. You can then save your hypothetical what-if data point. |

![Visualization Dashboard Global](./img/Interpretability-LocalExplanation.png)

![Visualization Dashboard Global](./img/Interpretability-WhatIf.gif)

<a name=""error analysis dashboard ""></a>

## Error Analysis Dashboard

Introducing the latest addition to the Responsible AI open-source toolkit collection, Error Analysis drives deeper to provide a better understanding of your machine learning model's behaviors. Use Error Analysis to identify cohorts with higher error rates and diagnose the root causes behind these errors. Combined with [Fairlearn](github.com/fairlearn/fairlearn) and [Interpret-Community](https://github.com/interpretml/interpret-community), practitioners can perform a wide variety of assessment operations to build responsible machine learning. Use this dashboard to:

1. Evaluate Cohorts: Learn how errors distribute across different cohorts at different levels of granularity 
2. Explore Predictions: Use built-in interpretability features or combine with InterpretML for boosted debugging capability 
3. Interactive Dashboard View customizable pre-built visuals to quickly identify errors and diagnose root causes

Run the dashboard via:

```python
from raiwidgets import ErrorAnalysisDashboard

ErrorAnalysisDashboard(global_explanation, dashboard_pipeline, dataset=X_test_original,
                       true_y=y_test, categorical_features=categorical_features)
```
Once you load the visualization dashboard, you can investigate different aspects of your dataset and trained model via two stages:

* Identification
* Diagnosis

---
**NOTE**

Click on ""Open in a new tab"" on the top left corner to get a better view of the dashboard in a new tab.

---

### Identification of Errors

Error Analysis identifies cohorts of data with higher error rate than the overall benchmark. These discrepancies might occur when the system or model underperforms for specific demographic groups or infrequently observed input conditions in the training data.

#### Different Methods for Error Identification

1. Decision Tree: Discover cohorts with high error rates across multiple features using the binary tree visualization. Investigate indicators such as error rate, error coverage, and data representation for each discovered cohort. ![Error Analysis tree map](./img/EA-TreeMap.png)

2. Error Heatmap: Once you form hypotheses of the most impactful features for failure, use the Error Heatmap to further investigate how one or two input features impact the error rate across cohorts. ![Error Analysis heat map](./img/EA-Heatmap.png)

### Diagnosis of Errors

After identifying cohorts with higher error rates, Error Analysis enables debugging and exploring these cohorts further. Gain deeper insights about the model or the data through data exploration and model explanation. Different Methods for Error Diagnosis:

1. Data Exploration which explores dataset statistics and feature distributions. Compare cohort data stats with other cohorts or to benchmark data. Investigate whether certain cohorts are underrepresented or if their feature distribution is significantly different from the overall data.

2. Global Explanation which explore the top K important features that impact the overall model global explanation for a selected cohort of data. Understand how values of features impact model prediction. Compare explanations with those from other cohorts or benchmark.

3. Local Explanation which enables observing the raw data in the Instance View. Understand how each data point has correct or incorrect prediction. Visually identify any missing features or label noise that could lead to issues. Explore local feature importance values (local explanation) and individual conditional expectation (ICE) plots.

4. What-if analysis (Perturbation Exploration) which applies changes to feature values of selected data point and observe resulting changes to the prediction.

<a name=""fairness dashboard""></a>

## Fairness Dashboard

Please refer to [Fairlearn](https://github.com/fairlearn/fairlearn)'s README and [user guide](https://fairlearn.github.io/v0.5.0/user_guide/index.html) to learn how you can assess and mitigate model's fairness issues.  Once your model is trained, load the Fairness dashboard in your notebook to understand how your model’s predictions impact different groups (e.g., different ethnicities). Compare multiple models along different fairness and performance metrics.

### Setup and single-model assessment
To assess a single model’s fairness and performance, the dashboard widget can be launched within a Jupyter notebook as follows:

```python
from raiwidgets import FairnessDashboard

# A_test contains your sensitive features (e.g., age, binary gender)
# y_true contains ground truth labels
# y_pred contains prediction labels

FairnessDashboard(sensitive_features=A_test,
                  y_true=Y_test.tolist(),
                  y_pred=[y_pred.tolist()])
```

Once you load the visualization dashboard, the widget walks the user through the assessment setup, where the user is asked to select
![Fairness Dashboard Sensitive Feature](./img/Fairness-Intro.png)

1. The sensitive feature of interest (e.g., ```binary gender``` or ```age```).
![Fairness Dashboard Fairness Metric](./img/Fairness-SensitiveMetric.png)

2. The performance metric (e.g., model precision) along which to evaluate the overall model performance. 
![Fairness Dashboard Fairness Metric](./img/Fairness-PerformanceMetric.png)

3. The fairness metric (e.g., demographic parity ratio) along which to evaluate any disparities across groups. 
![Fairness Dashboard Fairness Metric](./img/Fairness-FairnessMetric.png)

These selections are then used to obtain the visualization of the model’s impact on the subgroups.  (e.g., one is interested to consider non-binary gender for fairness testing and selects ""demographic parity ratio"" as a metric of interest to see how females and males are selected to get a loan).

![Fairness Dashboard Fairness Assessment View 1](./img/Fairness-SelectionRate.png)

![Fairness Dashboard Fairness Assessment View 2](./img/Fairness-DisparityInPerformance.png)

### Comparing multiple models

The dashboard also enables comparison of multiple models, such as the models produced by different learning algorithms and different mitigation approaches, including Fairlearn's [GridSearch](https://fairlearn.github.io/v0.5.0/api_reference/fairlearn.reductions.html#fairlearn.reductions.GridSearch), [ExponentiatedGradient](https://fairlearn.github.io/v0.5.0/api_reference/fairlearn.reductions.html#fairlearn.reductions.ExponentiatedGradient), and [ThresholdOptimizer](https://fairlearn.github.io/v0.5.0/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer).

As before, select the sensitive feature and the performance metric. The model comparison view then depicts the performance and disparity of all the provided models in a scatter plot. This allows the you to examine trade-offs between performance and fairness. Each of the dots can be clicked to open the assessment of the corresponding model. The figure below shows the model comparison view with ```binary gender``` selected as a sensitive feature and accuracy rate selected as the performance metric.

![Fairness Dashboard Model Comparison](./img/Fairness-ModelComparison.png)
 
<a name=""supported models""></a>

## Supported Models

This interpretability and error analysis API supports models that are trained on datasets in Python `numpy.array`, `pandas.DataFrame`, `iml.datatypes.DenseData`, or `scipy.sparse.csr_matrix` format.

The explanation functions of [Interpret-Community](https://github.com/interpretml/interpret-community) accept both models and pipelines as input as long as the model or pipeline implements a `predict` or `predict_proba` function that conforms to the Scikit convention. If not compatible, you can wrap your model's prediction function into a wrapper function that transforms the output into the format that is supported (predict or predict_proba of Scikit), and pass that wrapper function to your selected interpretability techniques.  

If a pipeline script is provided, the explanation function assumes that the running pipeline script returns a prediction. The repository also supports models trained via **PyTorch**, **TensorFlow**, and **Keras** deep learning frameworks.

<a name=""getting started""></a>

## Getting Started

This repository uses Anaconda to simplify package and environment management.

To setup on your local machine:

<details><summary><strong><em>Install Python module, packages and necessary distributions</em></strong></summary>

```
pip install raiwidgets
```

If you intend to run repository tests:

```
pip install -r requirements.txt
```

</details>

<details>
<summary><strong><em>Set up and run Jupyter Notebook server </em></strong></summary>

Install and run Jupyter Notebook

```
if needed:
          pip install jupyter
then:
jupyter notebook
```
</details>

## Maintainers

- [Ke Xu](https://github.com/KeXu444)
- [Roman Lutz](https://github.com/romanlutz)
- [Ilya Matiach](https://github.com/imatiach-msft)
- [Dawei Li](https://github.com/chnldw)
"
221,microsoft/azure-devops-auth-samples,PowerShell,"# Auth samples for Azure DevOps Services

![status](https://dev.azure.com/mseng/_apis/public/build/definitions/b924d696-3eae-4116-8443-9a18392d8544/5326/badge)

Samples that show how to authenticate with Azure DevOps and Azure DevOps Server.

Learn more about [integrating with Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/extend/overview?view=vsts) and [specific authentication guidance](https://docs.microsoft.com/en-us/azure/devops/integrate/get-started/authentication/authentication-guidance?view=vsts)

## Samples

* [Managed client sample (using Azure Active Directory Library)](./ManagedClientConsoleAppSample/README.md)
* [Device profile sample (.NET Core)](./DeviceProfileSample/README.md)
* [ASP.NET Web app OAuth sample](./OAuthWebSample/README.md)
* [Client library sample (using VSSConnection)](./ClientLibraryConsoleAppSample/README.md)
* [Javascript web app sample (using Microsoft Authentication Library for JavaScript)](./JavascriptWebAppSample/README.md)
* [Dual Support (Azure DevOps/TFS) Client Sample (using Azure Active Directory Library and Windows Authentication)](./DualSupportClientSample/README.md)
* [Non-interactive PAT Generation Sample (using Azure Active Directory Library with a Username Password credential)](./NonInteractivePatGenerationSample/README.md)
* [PAT lifecycle management API sample (using Microsoft Authentication Library with authentication code)](./PersonalAccessTokenAPIAppSample/README.md)
"
222,microsoft/angular-react,TypeScript,"# React support for Angular

[![CircleCI](https://circleci.com/gh/microsoft/angular-react.svg?style=svg)](https://circleci.com/gh/microsoft/angular-react)

Industry trends, organizational pressures, and other factors can lead to mandates regarding the use of component libraries or migration from one technology to another. In the case of [Office UI Fabric][fab], where its use is required, the client must be written in React (there is no Angular component library for the latest version). Rewrite from Angular to React may be cost-prohibitive or ill advised for other reasons.

Use of Angular-React allows consuming any React elements, but specifically Office UI Fabric, within an Angular [2+] application. The library of wrappers for Office UI Fabric simplifies the use of these components with Angular. However, any React code can make use of the custom Angular-React renderer.

## Libraries

@angular-react contains two separate libraries:

- [**core**][lib-core]: [![npm version](https://badge.fury.io/js/%40angular-react%2Fcore.svg)](https://www.npmjs.com/package/@angular-react/core) 

    Includes the Renderer and supporting logic to render Angular components with React implementations as React components. 

- [**fabric**][lib-fab]: [![npm version](https://badge.fury.io/js/%40angular-react%2Ffabric.svg)](https://www.npmjs.com/package/@angular-react/fabric)
    
    The light-weight Angular component wrappers that expose the Fabric React component API through common Angular components (including both imperative AND declarative syntax in many cases).


### Quick links

[Documentation, quick start, and guides][ard] |
[Demo][ard-demo] |
[Contributing](https://github.com/microsoft/angular-react/blob/master/CONTRIBUTING.md) |
[StackBlitz Template](https://stackblitz.com/edit/angular-react) |
[Office UI Fabric](https://developer.microsoft.com/en-us/fabric)

### Typical Use Cases

- Use React component libraries with Angular
- Incrementally rewrite an Angular application into React 
(moving from atomic/leaf nodes upward into full features/pages until the entire app is re-written)

## Getting started

See a simple [StackBlitz Template](https://stackblitz.com/edit/angular-react)

# Roadmap & Support

Both the `core` and `fabric` libraries are in production use in consumer-facing applications at Microsoft. That said, 
we (the team that currently maintains this project) are a product team, and @angular-react is not our primary focus. 
We maintain this because we need it and we share it with the wider community with the hope that it will prove useful to others.
Of course, we attempt to provide help when possible and we love getting pull requests for 
improvements/enhancement/fixes from community members. But we don't have any specific plans for the future of this project.

Please take this in to consideration when evaluating this project's suitability for your own needs. 

## Contributing

If you'd like to contribute, you must follow our [contributing guidelines](https://github.com/microsoft/angular-react/blob/master/CONTRIBUTING.md).
You can look through the issues (which should be up-to-date on who is working on which features and which pieces are blocked) and make a comment.

[ard]: https://microsoft.github.io/angular-react
[ard-demo]: https://microsoft.github.io/angular-react/demo
[getting-started]: https://microsoft.github.io/angular-react/docs/getting-started
[fab]: https://developer.microsoft.com/en-us/fabric
[fab-c]: https://developer.microsoft.com/en-us/fabric#/components
[lib-core]: ./libs/core/README.md
[lib-fab]: ./libs/fabric/README.md
"
223,microsoft/health-cards-tests,TypeScript,"# SMART Health Card Tests

This fork provides additional tests to the [base project](https://github.com/smart-on-fhir/health-cards-tests). See the original [README.md](https://github.com/smart-on-fhir/health-cards-tests/blob/master/README.md).

The `demo` folder constains a project illustrating the issuance and validation of SMART Health Cards; see its [README.md](demo/README.md) for details.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
224,microsoft/gather,Jupyter Notebook,"# nbgather: 🧽✨ Spit shine for Jupyter notebooks 

Tools for cleaning code, recovering lost code, and comparing
versions of code in Jupyter Lab.

Download the alpha extension with the following command:

```bash
jupyter labextension install nbgather
```

Then you can clean and compare versions of your code like so:

<img src=docs/demo.gif alt=""Code gathering tools can help you clean your code and review versions of results.""/>

Want to try it out first? [Play around with `nbgather` on an example notebook on BinderHub.](https://gke.mybinder.org/v2/gh/microsoft/gather/master?urlpath=lab/tree/binder%2FTry%20out%20nbgather.ipynb)

**Did the `install` fail?** Make sure Jupyter Lab is
up-to-date, and that you are running Jupyter Lab from Python 3.

**This project is in alpha**: The code this collects will
sometimes be more than you want. It saves your a history of
all code you've executed and the outputs it produces to the
notebook's metadata. The user interface has a few quirks.

Help us make this a real, practical, and really useful tool.
We welcome any and all feedback and contributions. We are
particularly in need of the opinions and efforts of those
with a penchant for hacking code analysis.

## Usage Tips

**Can it extract more precise slices of code?** Yes. First submit
a pull request telling us the desired extraction behavior, so we
can incorporate this behavior into the tool.

Meanwhile, you can help the backend make more precise slices by
telling the tool which functions don't modify their
arguments. By default, the tool assumes that functions change all
arguments they're called with, and the objects they're called on,
with [exceptions for some common APIs](https://github.com/andrewhead/python-program-analysis/tree/master/src/specs).
To edit the slicing rules, open the *Advanced Settings Editor* in the Jupyter Lab
Settings menu and choose the ""nbgather"" tab. In your
user-defined settings, override `moduleMap`, following
[this format](https://github.com/andrewhead/python-program-analysis#api-specs)
to specify which functions don't modify their arguments.

**How do I clear the notebook's history?** Open up your `.ipynb`
file in a text editor, find the `history` key in the
top-level `metadata` object, and set `history` to `[]`.

## Contributing

To run the development version of nbgather, run:

```bash
git clone <this-repository-url>  # clone the repository
npm install                      # download dependencies
jupyter labextension link .      # install this package in Jupyter Lab
npm run watch                    # automatically recompile source code
jupyter lab --watch              # launch Jupyter Lab, automatically re-load extension
```

This requires npm version 4 or later, and was tested most
recently with Node v9.5.0.

Submit all change as a pull request. Feel free to author the
the lead contributor (Andrew Head, <andrewhead@berkeley.edu>) if
you have any questions about getting started with the code or
about features or updates you'd like to contribute.

Also, make sure to format the code and test it before submitting
a pull request, as described below:

### Formatting the code

Before submitting a pull request with changed code, format the code
files by running `npm run format:all`.

### Testing the code

To run the tests from the command line, call:

```bash
npm run test
```

The first time you run tests, they will take about a minute
to finish. The second time, and all subsequent times, the
tests will take only a few seconds. The first test run takes
longer because the Jest test runner transpiles dependencies
like the '@jupyterlab' libraries into a dialect of
JavaScript it expects before running the tests.

### Troubleshooting

Here are some tips for dealing with build errors we've encountered
while developing code gathering tools:

* **Errors about missing semicolons in React types files**: upgrade the `typescript` and `ts-node` packages
* **Conflicting dependencies**: upgrade either the Python Jupyter Lab (may require Python upgrade to Python 3 to get the most recent version of Jupyter Lab) or the Jupyter Lab npm pacakges
* **Other build issues**: we've found some issues can be solved by just deleting your `node_modules/` directory and reinstalling it.
"
225,microsoft/GSL,C++,"# GSL: Guidelines Support Library
[![Build Status](https://dev.azure.com/cppstat/GSL/_apis/build/status/microsoft.GSL?branchName=main)](https://dev.azure.com/cppstat/GSL/_build/latest?definitionId=1&branchName=main)

The Guidelines Support Library (GSL) contains functions and types that are suggested for use by the
[C++ Core Guidelines](https://github.com/isocpp/CppCoreGuidelines) maintained by the [Standard C++ Foundation](https://isocpp.org).
This repo contains Microsoft's implementation of GSL.

The entire implementation is provided inline in the headers under the [gsl](./include/gsl) directory. The implementation generally assumes a platform that implements C++14 support.

While some types have been broken out into their own headers (e.g. [gsl/span](./include/gsl/span)),
it is simplest to just include [gsl/gsl](./include/gsl/gsl) and gain access to the entire library.

> NOTE: We encourage contributions that improve or refine any of the types in this library as well as ports to
other platforms. Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for more information about contributing.

# Project Code of Conduct
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Usage of Third Party Libraries
This project makes use of the [Google Test](https://github.com/google/googletest) testing library. Please see the [ThirdPartyNotices.txt](./ThirdPartyNotices.txt) file for details regarding the licensing of Google Test.

# Supported features
## Microsoft GSL implements the following from the C++ Core Guidelines:

Feature                            | Supported? | Description
-----------------------------------|:----------:|-------------
[**1. Views**][cg-views]           |            |
owner                              | &#x2611;   | an alias for a raw pointer
not_null                           | &#x2611;   | restricts a pointer / smart pointer to hold non-null values
span                               | &#x2611;   | a view over a contiguous sequence of memory. Based on the standardized verison of `std::span`, however `gsl::span` enforces bounds checking. See the [wiki](https://github.com/microsoft/GSL/wiki/gsl::span-and-std::span) for additional information.
span_p                             | &#x2610;   | spans a range starting from a pointer to the first place for which the predicate is true
basic_zstring                      | &#x2611;   | A pointer to a C-string (zero-terminated array) with a templated char type
zstring                            | &#x2611;   | An alias to `basic_zstring` with a char type of char
czstring                           | &#x2611;   | An alias to `basic_zstring` with a char type of const char
wzstring                           | &#x2611;   | An alias to `basic_zstring` with a char type of wchar_t
cwzstring                          | &#x2611;   | An alias to `basic_zstring` with a char type of const wchar_t
u16zstring                         | &#x2611;   | An alias to `basic_zstring` with a char type of char16_t
cu16zstring                        | &#x2611;   | An alias to `basic_zstring` with a char type of const char16_t
u32zstring                         | &#x2611;   | An alias to `basic_zstring` with a char type of char32_t
cu32zstring                        | &#x2611;   | An alias to `basic_zstring` with a char type of const char32_t
[**2. Owners**][cg-owners]         |            |
unique_ptr                         | &#x2611;   | an alias to `std::unique_ptr`
shared_ptr                         | &#x2611;   | an alias to `std::shared_ptr`
stack_array                        | &#x2610;   | a stack-allocated array
dyn_array                          | &#x2610;   | a heap-allocated array
[**3. Assertions**][cg-assertions] |            |
Expects                            | &#x2611;   | a precondition assertion; on failure it terminates
Ensures                            | &#x2611;   | a postcondition assertion; on failure it terminates
[**4. Utilities**][cg-utilities]   |            |
move_owner                         | &#x2610;   | a helper function that moves one `owner` to the other
byte                               | &#x2611;   | either an alias to std::byte or a byte type
final_action                       | &#x2611;   | a RAII style class that invokes a functor on its destruction
finally                            | &#x2611;   | a helper function instantiating `final_action`
GSL_SUPPRESS                       | &#x2611;   | a macro that takes an argument and turns it into `[[gsl::suppress(x)]]` or `[[gsl::suppress(""x"")]]`
[[implicit]]                       | &#x2610;   | a ""marker"" to put on single-argument constructors to explicitly make them non-explicit
index                              | &#x2611;   | a type to use for all container and array indexing (currently an alias for std::ptrdiff_t)
joining_thread                     | &#x2610;   | a RAII style version of `std::thread` that joins
narrow                             | &#x2611;   | a checked version of narrow_cast; it can throw `narrowing_error`
narrow_cast                        | &#x2611;   | a narrowing cast for values and a synonym for static_cast
narrowing_error                    | &#x2611;   | a custom exception type thrown by `narrow()`
[**5. Concepts**][cg-concepts]     | &#x2610;   |

## The following features do not exist in or have been removed from the C++ Core Guidelines:
Feature                            | Supported? | Description
-----------------------------------|:----------:|-------------
strict_not_null                    | &#x2611;   | A stricter version of `not_null` with explicit constructors
multi_span                         | &#x2610;   | Deprecated. Multi-dimensional span.
strided_span                       | &#x2610;   | Deprecated. Support for this type has been discontinued.
basic_string_span                  | &#x2610;   | Deprecated. Like `span` but for strings with a templated char type
string_span                        | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of char
cstring_span                       | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of const char
wstring_span                       | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of wchar_t
cwstring_span                      | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of const wchar_t
u16string_span                     | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of char16_t
cu16string_span                    | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of const char16_t
u32string_span                     | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of char32_t
cu32string_span                    | &#x2610;   | Deprecated. An alias to `basic_string_span` with a char type of const char32_t

This is based on [CppCoreGuidelines semi-specification](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gsl-guidelines-support-library).

[cg-views]: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gslview-views
[cg-owners]: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gslowner-ownership-pointers
[cg-assertions]: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gslassert-assertions
[cg-utilities]: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gslutil-utilities
[cg-concepts]: https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#gslconcept-concepts

# Quick Start
## Supported Compilers
The GSL officially supports the current and previous major release of MSVC, GCC, Clang, and XCode's Apple-Clang.
See our latest test results for the most up-to-date list of supported configurations.

Compiler |Toolset Versions Currently Tested
:------- |--:
 XCode |11.4 & 10.3
 GCC |9 & 8
 Clang |11 &  10
 Visual Studio with MSVC | VS2017 (15.9) & VS2019 (16.4) 
 Visual Studio with LLVM | VS2017 (Clang 9) & VS2019 (Clang 10)

---
If you successfully port GSL to another platform, we would love to hear from you!
- Submit an issue specifying the platform and target.
- Consider contributing your changes by filing a pull request with any necessary changes.
- If at all possible, add a CI/CD step and add the button to the table below!

Target | CI/CD Status
:------- | -----------:
iOS | ![CI_iOS](https://github.com/microsoft/GSL/workflows/CI_iOS/badge.svg)
Android | ![CI_Android](https://github.com/microsoft/GSL/workflows/CI_Android/badge.svg)

Note: These CI/CD steps are run with each pull request, however failures in them are non-blocking.

## Building the tests
To build the tests, you will require the following:

* [CMake](http://cmake.org), version 3.1.3 (3.2.3 for AppleClang) or later to be installed and in your PATH.

These steps assume the source code of this repository has been cloned into a directory named `c:\GSL`.

1. Create a directory to contain the build outputs for a particular architecture (we name it c:\GSL\build-x86 in this example).

        cd GSL
        md build-x86
        cd build-x86

2. Configure CMake to use the compiler of your choice (you can see a list by running `cmake --help`).

        cmake -G ""Visual Studio 15 2017"" c:\GSL

3. Build the test suite (in this case, in the Debug configuration, Release is another good choice).

        cmake --build . --config Debug

4. Run the test suite.

        ctest -C Debug

All tests should pass - indicating your platform is fully supported and you are ready to use the GSL types!

## Building GSL - Using vcpkg

You can download and install GSL using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:

    git clone https://github.com/Microsoft/vcpkg.git
    cd vcpkg
    ./bootstrap-vcpkg.sh
    ./vcpkg integrate install
    vcpkg install ms-gsl

The GSL port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

## Using the libraries
As the types are entirely implemented inline in headers, there are no linking requirements.

You can copy the [gsl](./include/gsl) directory into your source tree so it is available
to your compiler, then include the appropriate headers in your program.

Alternatively set your compiler's *include path* flag to point to the GSL development folder (`c:\GSL\include` in the example above) or installation folder (after running the install). Eg.

MSVC++

    /I c:\GSL\include

GCC/clang

    -I$HOME/dev/GSL/include

Include the library using:

    #include <gsl/gsl>

## Usage in CMake

The library provides a Config file for CMake, once installed it can be found via

    find_package(Microsoft.GSL CONFIG)

Which, when successful, will add library target called `Microsoft.GSL::GSL` which you can use via the usual
`target_link_libraries` mechanism.

### FetchContent

If you are using cmake version 3.11+ you can use the offical FetchContent module.
This allows you to easily incorporate GSL into your project.

```cmake
# NOTE: This example uses cmake version 3.14 (FetchContent_MakeAvailable).
# Since it streamlines the FetchContent process
cmake_minimum_required(VERSION 3.14)

include(FetchContent)

# In this example we are picking a specific tag.
# You can also pick a specific commit, if you need to.
FetchContent_Declare(GSL
    GIT_REPOSITORY ""https://github.com/microsoft/GSL""
    GIT_TAG ""v3.1.0""
)

FetchContent_MakeAvailable(GSL)

# Now you can link against the GSL interface library
add_executable(foobar)

# Link against the interface library (IE header only library)
target_link_libraries(foobar PRIVATE GSL)
```

## Debugging visualization support
For Visual Studio users, the file [GSL.natvis](./GSL.natvis) in the root directory of the repository can be added to your project if you would like more helpful visualization of GSL types in the Visual Studio debugger than would be offered by default.

If you are using cmake this will be done automatically for you.
See 'GSL_VS_ADD_NATIVE_VISUALIZERS'
"
226,microsoft/GLUECoS,Python,"# GLUECoS: An Evaluation Benchmark for Code-Switched NLP
**NEW (Mar - 2021): We have added a new Code-Mixed Machine Translation Dataset to GLUECoS. Please check [this](#code-mixed-machine-translation-task) section**

**NEW (Oct - 2020): Please check our updated policy about making submissions for evaluation [here](#submission-policy)**

**NEW (Sep - 2020): NLI dataset preprocess script updated to fix repetitions in data. If you have downloaded the datasets before, please check [this](#nli-preprocess-script-update) section**

**NEW (Aug - 2020): Evaluation is now automated and results are presented instantly. Please check [this](#submitting-predictions-for-evaluation) section**

This is the repo for the ACL 2020 paper [GLUECoS: An Evaluation Benchmark for Code-Switched NLP](https://www.aclweb.org/anthology/2020.acl-main.329/)

GLUECoS is a benchmark comprising of multiple code-mixed tasks across 2 language pairs (En-Es and En-Hi)

Recording of talk given at ACL: [Link](https://slideslive.com/38928983)

Below are instructions for obtaining the datasets that comprise the benchmark and training transformer based models on this data. Both steps can be run on separate systems and the instructions are structured in such a way. All the user has to do is to copy over the `Data/Processed_Data` folder over to perform training

## Obtaining Datasets
Follow the following instructions to download and process the datasets. All the steps should work in a brand new conda environment with `python==3.6.10` or a docker container with the `python:3.6` image. Please note that the splits for some of the datasets are different from their original releases.
1. Install the requirements for the preprocessing scripts
    ```
    pip install -r requirements.txt
    ```
2. Create a twitter developer account and fill in the 4 keys, one per line,  in `twitter_authentication.txt`. The file should look like this
    ```
    consumer_key
    secret_key
    access_token
    access_secret_token
    ```
    
3. Obtain a key for Microsoft Translator. This is needed as the preprocessing steps involve conversion of Romanized datasets into Devanagari. Instructions for obtaining this key can be found [here](https://docs.microsoft.com/en-us/azure/cognitive-services/translator/translator-how-to-signup). While creating the translator instance, please set the region to global. The number of queries made fall within the free tier. This key will be referred to as SUBSCRIPTION_KEY in the next step
4. To download the data, run the command below. This will download the original datasets, perform all the preprocessing needed and bring them into a format that the training scripts can use
    ```
    ./download_data.sh SUBSCRIPTION_KEY
    ```
    The dowloaded and processed data is stored in `Data/Processed_Data`. 
    
    Some of the datasets did not have predefined splits, so the splits used for those can be found in `Data/Original_Data`.

    Please note that the labels for the test sets are not the gold labels. They have been assigned a separate token to maintain fairness in the benchmarking.

    This will not download/preprocess the QA dataset. For that, please check the next step

5. The original QA dataset (Chandu et. al, 2018) contains contexts only for some examples. To obtain contexts for the rest, [DrQA](https://github.com/facebookresearch/DrQA) is used to obtain contexts from a Wikipedia dump. To run this, you will need atleast 20GB of disk storage (to store the wikidump) and 16GB+ of RAM (to run DrQA). DrQA uses PyTorch, so having a GPU will help speed it up (although it isn't necessary).

    First, install a suitable version of PyTorch for your system. In most cases, a `pip install torch` should do

    To download and process the QA dataset, run the following command
    ```
    bash Data/Preprocess_Scripts/preprocess_qa.sh
    ```
### NLI Preprocess Script Update
The data downloading and preprocessing scripts were updated in Sep - 2020 to fix an issue with the creation of the NLI train and test sets. Running the scripts as is will download all the datasets, so you do not have to make any changes if you're doing it for the first time. If you downloaded the datasets before this fix was added, you can follow these steps to get the updated NLI data alone.  
1. Make sure you have the latest version of the repo
2. Comment out lines 390-397 and 399-401 of `download_data.sh`
3. Run the updated `download_data.sh` to create the new NLI dataset alone

## Training models on the data
The code contains 4 different evaluation scripts
1. One script for token level tasks:
    - LID (en_es/en_hi)
    - NER (en_es/en_hi),
    - POS (en_es/en_hi_fg/en_hi_ud)
2. One script for the sentence level tasks:
    - Sentiment (en_es/en_hi)
3. One script for the QA task 
    - QA (en_hi)
4. One script for the NLI task
    - NLI (en_hi)

You can train the models on your system or via Azure Machine Learning. To know more about the latter, please refer to [this README](azure_ml/README.md).

### Install the training requirements  
Note: The requirements for dataset preprocessing and training have been separately mentioned, as you may run them on different systems
1. Install a suitable version of pytorch for your system, `pip install torch` should work in most cases
2. The requirements from the file in `Code/requirements.txt`
    ```
    pip install -r Code/requirements.txt
    ```
### Training
Run the below command to fine-tune your model on any of the task. The training scripts uses the Huggingface library and support any models based on BERT, XLM, XLM-Roberta and similar models.

```
bash train.sh MODEL MODEL_TYPE TASK 
```
Example Usage :
```    
bash train.sh bert-base-multilingual-cased bert POS_EN_HI_FG
```
You can also run fine-tuning for all tasks with the following command :
```
bash train.sh bert-base-multilingual-cased bert ALL
```

## Submitting Predictions for Evaluation
Submission is done by uploading the results to a fork of this repo and making a pull request to the main repo. The evaluation is done automatically by a set of actions that run for the PR.

The training scripts supplied write predictions for the test set into the `Results` folder.
1. Zip this folder into results.zip with `zip results.zip -r Results`.
2. Create a fork of `microsoft/GLUECoS` on Github.
3. Add this `results.zip` file to the root directory of your fork and make a pull request to the main repo.

A set of actions will run for your pull request. Clicking on ""Show all checks"" will reveal that one of these is named ""Eval script"". Clicking on ""Details"" will take you to the sequence of steps run for the action. Expanding the ""Run Eval"" stage will show you the results of the eval script.
<p float=""left"">
  <img src=""docs/github_pr.png"" width=""500"" />
  <img src=""docs/eval_script.png"" width=""300"" /> 
</p>

If you would like to make another submission, you can update the same PR with the new `results.zip` file and the action will run again. You DO NOT need to open a new PR each time. Please wait till the current action finishes running before updating the PR with the new submission.

Please ensure that this is the exact structure of the zip file. The eval script will fail if there are any differences in the names or the structure
```
results.zip
    └── Results
        ├── NLI_EN_HI
        │   └── test_predictions.txt
        ├── QA_EN_HI
        │   └── predictions.json
        .
        .
        .
        └── Sentiment_EN_HI
            └── test_predictions.txt

```
<p id=""submission-policy"">
You can make as many submissions as you want. Beyond the 5th submission, your best score will be added to the leaderboard. We will use your Github username for the leaderboard. Instead, if you would like your group's name/affilication to appear on the leaderboard, please mention this along with details about the model in the pull request.
</p>

## Code-Mixed Machine Translation Task
We have added a code-mixed machine translation dataset to GLUECoS. The dataset and task are for translation from English to Hindi-English. The dataset has been provided by Prof. Alan Black's group from CMU. Since BERT like models aren't suitable for this task, we offer this as a separate part of the benchmark with a separate leaderboard. The baseline method for this task is mBART finetuned on this dataset.

### Obtaining the dataset
The `download_data.sh` script does the downloading and preprocessing of this dataset, but the MT dataset part is disabled by deafult. To enable this, you'll have to comment out lines 409-420 and uncomment line 421, and then run the script just like before.

### Train Scripts
We have supplied training scripts that can be used to finetune mBART or related models on the dataset. The train script is at `Code/run_seq2seq.py`. You can run that script directly, or alternatively start training this way
```
bash train.sh facebook/mbart-large-cc25 mbart MT_EN_HI
```
You can have a look at the training arguments in `Code/train_mt.sh` and modify them as per your needs. You might need to adjust batch size depending on the GPU that you are using.

### Evaluation
The train scripts write the predictions on the test set to the `Results` directory and you can submit these for evaluation similar to how you do for the other tasks. The predictions are written to `Results/MT_EN_HI/translations.txt`. The zip file that gets uploaded is expected to follow this structure
```
results.zip
    └── Results
        └── MT_EN_HI
            └── translations.txt
```
The MT task will have a leaderboard that is separate from the other tasks. The evaluation for the MT task (via pull requests) will be enabled in April.

## Citation
Please use the following citation if you use this benchmark:
```
@inproceedings{khanuja-etal-2020-gluecos,
    title = ""{GLUEC}o{S}: An Evaluation Benchmark for Code-Switched {NLP}"",
    author = ""Khanuja, Simran  and
      Dandapat, Sandipan  and
      Srinivasan, Anirudh  and
      Sitaram, Sunayana  and
      Choudhury, Monojit"",
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/2020.acl-main.329"",
    pages = ""3575--3585""
}
```

# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
227,microsoft/service-fabric-services-and-actors-dotnet,C#,"# Azure/service-fabric-services-and-actors-dotnet

Reliable Services and Reliable Actors are Service Fabric application frameworks for building highly-scalable distributed cloud applications.

Reliable Services is a light-weight framework for writing services that integrate with the Service Fabric platform and benefit from the full set of platform features. Built on top of Reliable Services, the Reliable Actor framework is an application framework that implements the Virtual Actor model, based on the actor design pattern. More information on Service Fabric programming models can be found in the [Service Fabric documentation](https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-choose-framework).

This repo builds the following packages:
 - Microsoft.ServiceFabric.Services
 - Microsoft.ServiceFabric.Services.Remoting
 - Microsoft.ServiceFabric.Services.Wcf
 - Microsoft.ServiceFabric.Actors
 - Microsoft.ServiceFabric.Actors.Wcf

For more Service Fabric open source projects, visit the Service Fabric [home repo](https://github.com/microsoft/service-fabric).

## Getting Started

### Prerequesites
Each project is a normal C# Visual Studio 2019 project. At minimum, you need [MSBuild 16](https://docs.microsoft.com/en-us/visualstudio/msbuild/whats-new-msbuild-16-0), [PowerShell](https://msdn.microsoft.com/powershell/mt173057.aspx), [.NET Core SDK](https://www.microsoft.com/net/download/windows) and [.NET Framework 4.6](https://www.microsoft.com/en-US/download/details.aspx?id=48130) to build and generate NuGet packages.

We recommend installing [Visual Studio 2019](https://www.visualstudio.com/vs/) which will set you up with all the .NET build tools and allow you to open the solution files. Community Edition is free and can be used to build everything here.

### Build
To build everything and generate NuGet packages, run the **build.ps1** script. NuGet packages will be dropped in a *drop* directory at the repo root.

Each project can also be built individually directly through Visual Studio or by running the solution file through MSBuild.

Binaries in the build are delay signed, these are fully signed in the official builds released by Microsoft. To use the binaries or to run unit tests from the build of this repository, strong name validation needs to be skipped for these assemblies. This can be done by running **SkipStrongName.ps1** script available in the root of the repository.

For branches, please see [Branching Information](CONTRIBUTING.md#BranchingInformation)

## Releases and Support
Official releases from Microsoft of the NuGet packages in this repo are released directly to NuGet and Web Platform Installer. Get the latest official release [here](http://www.microsoft.com/web/handlers/webpi.ashx?command=getinstallerredirect&appid=MicrosoftAzure-ServiceFabric-VS2015).

**Only officially released NuGet packages from Microsoft are supported for use in production.** If you have a feature or bug fix that you would like to use in your application, please issue a pull request so we can get it into an official release.

## Reporting issues and feedback
Please refer to [Contributing.md](https://github.com/Microsoft/service-fabric/blob/master/CONTRIBUTING.md) at the Service Fabric home repo for details on issue reporting and feedback.

## Contributing code
If you would like to become an active contributor to this project please
follow the instructions provided in [Microsoft Azure Projects Contribution Guidelines](http://azure.github.io/guidelines.html).

For details on contributing to Service Fabric projects, please refer to [Contributing.md](https://github.com/Microsoft/service-fabric/blob/master/CONTRIBUTING.md) at the Service Fabric home repo for details on contributing code.

## How to reflect changes done in Nugets
Nugets from this repo are published via Service Fabric SDK. Once the changes are made in this repo and if there are some changes in nuprojs files, they should reflect in Service Fabric Repo (src\BuildSteps\GenerateNuget\PublicSDK) in respective nuprojs.

## Documentation
Service Fabric has conceptual and reference documentation available at [https://docs.microsoft.com/azure/service-fabric](https://docs.microsoft.com/azure/service-fabric).

These articles will help get you started with Reliable Services and Reliable Actors:

  - [Reliable Services overview](https://docs.microsoft.com/azure/service-fabric/service-fabric-reliable-services-introduction)
  - [Reliable Actors overview](https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-introduction)

## Samples
For Service Fabric sample code, check out the [Azure Code Sample gallery](https://azure.microsoft.com/en-us/resources/samples/?service=service-fabric) or go straight to [Azure-Samples on GitHub](https://github.com/Azure-Samples?q=service-fabric).

## License
[MIT](License.txt)

---
*This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.*
 
"
228,microsoft/AISchoolTutorials,C#,"
# AI School Tutorials

This repository contains a collection of AI tutorials that are featured on [Microsoft AI School](https://aischool.microsoft.com).

## [Sketch2Code](./sketch2code)
See how Azure Custom Vision can create a model that takes a hand drawn wireframe and turns it into valid HTML code.

## [Snip Insights](./snipinsights)
Apply Azure Cognitive Services to gain intelligent insights within a screen capture application.

## Other Labs

You can find other AI Labs in this [Github repo](https://github.com/Microsoft/ailab).

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
229,microsoft/rushstack,TypeScript,"<table><tr><td>
<a href=""https://rushstack.io/""><img src=""https://rushstack.io/images/rushstack.svg"" width=""300px"" /></a>
<p align=""center""><a href=""https://rushstack.io/"">https://rushstack.io/</a></p>
</td></tr></table>

[![Zulip chat room](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://rushstack.zulipchat.com/) &nbsp; [![Build Status](https://dev.azure.com/RushStack/GitHubProjects/_apis/build/status/rushstack/rushstack%20CI%20Build?branchName=master)](https://dev.azure.com/RushStack/GitHubProjects/_build/latest?definitionId=3&branchName=master)

The home for various projects maintained by the Rush Stack community, whose mission is to develop reusable tooling
for large scale TypeScript monorepos.


## Documentation Links

- [What is Rush Stack?](https://rushstack.io/) - learn about the mission behind these projects
- [API reference](https://rushstack.io/pages/api/) - browse API documentation for NPM packages
- [Zulip chat room](https://rushstack.zulipchat.com/) - chat with the Rush Stack developers
- [Rush](https://rushjs.io/) - a build orchestrator for large scale TypeScript monorepos
- [API Extractor](https://api-extractor.com/) - create .d.ts rollups and track your TypeScript API signatures
- [API Documenter](https://api-extractor.com/pages/setup/generating_docs/) - use TSDoc comments to publish an API documentation website

<!-- GENERATED PROJECT SUMMARY START -->

## Published Packages

<!-- the table below was generated using the ./repo-scripts/repo-toolbox script -->

| Folder | Version | Changelog | Package |
| ------ | ------- | --------- | ------- |
| [/apps/api-documenter](./apps/api-documenter/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fapi-documenter.svg)](https://badge.fury.io/js/%40microsoft%2Fapi-documenter) | [changelog](./apps/api-documenter/CHANGELOG.md) | [@microsoft/api-documenter](https://www.npmjs.com/package/@microsoft/api-documenter) |
| [/apps/api-extractor](./apps/api-extractor/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fapi-extractor.svg)](https://badge.fury.io/js/%40microsoft%2Fapi-extractor) | [changelog](./apps/api-extractor/CHANGELOG.md) | [@microsoft/api-extractor](https://www.npmjs.com/package/@microsoft/api-extractor) |
| [/apps/api-extractor-model](./apps/api-extractor-model/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fapi-extractor-model.svg)](https://badge.fury.io/js/%40microsoft%2Fapi-extractor-model) | [changelog](./apps/api-extractor-model/CHANGELOG.md) | [@microsoft/api-extractor-model](https://www.npmjs.com/package/@microsoft/api-extractor-model) |
| [/apps/heft](./apps/heft/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft.svg)](https://badge.fury.io/js/%40rushstack%2Fheft) | [changelog](./apps/heft/CHANGELOG.md) | [@rushstack/heft](https://www.npmjs.com/package/@rushstack/heft) |
| [/apps/rundown](./apps/rundown/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Frundown.svg)](https://badge.fury.io/js/%40rushstack%2Frundown) | [changelog](./apps/rundown/CHANGELOG.md) | [@rushstack/rundown](https://www.npmjs.com/package/@rushstack/rundown) |
| [/apps/rush](./apps/rush/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush.svg)](https://badge.fury.io/js/%40microsoft%2Frush) | [changelog](./apps/rush/CHANGELOG.md) | [@microsoft/rush](https://www.npmjs.com/package/@microsoft/rush) |
| [/apps/rush-lib](./apps/rush-lib/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-lib.svg)](https://badge.fury.io/js/%40microsoft%2Frush-lib) | | [@microsoft/rush-lib](https://www.npmjs.com/package/@microsoft/rush-lib) |
| [/core-build/gulp-core-build](./core-build/gulp-core-build/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build) | [changelog](./core-build/gulp-core-build/CHANGELOG.md) | [@microsoft/gulp-core-build](https://www.npmjs.com/package/@microsoft/gulp-core-build) |
| [/core-build/gulp-core-build-mocha](./core-build/gulp-core-build-mocha/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-mocha.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-mocha) | [changelog](./core-build/gulp-core-build-mocha/CHANGELOG.md) | [@microsoft/gulp-core-build-mocha](https://www.npmjs.com/package/@microsoft/gulp-core-build-mocha) |
| [/core-build/gulp-core-build-sass](./core-build/gulp-core-build-sass/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-sass.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-sass) | [changelog](./core-build/gulp-core-build-sass/CHANGELOG.md) | [@microsoft/gulp-core-build-sass](https://www.npmjs.com/package/@microsoft/gulp-core-build-sass) |
| [/core-build/gulp-core-build-serve](./core-build/gulp-core-build-serve/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-serve.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-serve) | [changelog](./core-build/gulp-core-build-serve/CHANGELOG.md) | [@microsoft/gulp-core-build-serve](https://www.npmjs.com/package/@microsoft/gulp-core-build-serve) |
| [/core-build/gulp-core-build-typescript](./core-build/gulp-core-build-typescript/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-typescript.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-typescript) | [changelog](./core-build/gulp-core-build-typescript/CHANGELOG.md) | [@microsoft/gulp-core-build-typescript](https://www.npmjs.com/package/@microsoft/gulp-core-build-typescript) |
| [/core-build/gulp-core-build-webpack](./core-build/gulp-core-build-webpack/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-webpack.svg)](https://badge.fury.io/js/%40microsoft%2Fgulp-core-build-webpack) | [changelog](./core-build/gulp-core-build-webpack/CHANGELOG.md) | [@microsoft/gulp-core-build-webpack](https://www.npmjs.com/package/@microsoft/gulp-core-build-webpack) |
| [/core-build/node-library-build](./core-build/node-library-build/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fnode-library-build.svg)](https://badge.fury.io/js/%40microsoft%2Fnode-library-build) | [changelog](./core-build/node-library-build/CHANGELOG.md) | [@microsoft/node-library-build](https://www.npmjs.com/package/@microsoft/node-library-build) |
| [/core-build/web-library-build](./core-build/web-library-build/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fweb-library-build.svg)](https://badge.fury.io/js/%40microsoft%2Fweb-library-build) | [changelog](./core-build/web-library-build/CHANGELOG.md) | [@microsoft/web-library-build](https://www.npmjs.com/package/@microsoft/web-library-build) |
| [/heft-plugins/heft-webpack4-plugin](./heft-plugins/heft-webpack4-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft-webpack4-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Fheft-webpack4-plugin) | [changelog](./heft-plugins/heft-webpack4-plugin/CHANGELOG.md) | [@rushstack/heft-webpack4-plugin](https://www.npmjs.com/package/@rushstack/heft-webpack4-plugin) |
| [/heft-plugins/heft-webpack5-plugin](./heft-plugins/heft-webpack5-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft-webpack5-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Fheft-webpack5-plugin) | [changelog](./heft-plugins/heft-webpack5-plugin/CHANGELOG.md) | [@rushstack/heft-webpack5-plugin](https://www.npmjs.com/package/@rushstack/heft-webpack5-plugin) |
| [/libraries/debug-certificate-manager](./libraries/debug-certificate-manager/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fdebug-certificate-manager.svg)](https://badge.fury.io/js/%40rushstack%2Fdebug-certificate-manager) | [changelog](./libraries/debug-certificate-manager/CHANGELOG.md) | [@rushstack/debug-certificate-manager](https://www.npmjs.com/package/@rushstack/debug-certificate-manager) |
| [/libraries/heft-config-file](./libraries/heft-config-file/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft-config-file.svg)](https://badge.fury.io/js/%40rushstack%2Fheft-config-file) | [changelog](./libraries/heft-config-file/CHANGELOG.md) | [@rushstack/heft-config-file](https://www.npmjs.com/package/@rushstack/heft-config-file) |
| [/libraries/load-themed-styles](./libraries/load-themed-styles/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Fload-themed-styles.svg)](https://badge.fury.io/js/%40microsoft%2Fload-themed-styles) | [changelog](./libraries/load-themed-styles/CHANGELOG.md) | [@microsoft/load-themed-styles](https://www.npmjs.com/package/@microsoft/load-themed-styles) |
| [/libraries/node-core-library](./libraries/node-core-library/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fnode-core-library.svg)](https://badge.fury.io/js/%40rushstack%2Fnode-core-library) | [changelog](./libraries/node-core-library/CHANGELOG.md) | [@rushstack/node-core-library](https://www.npmjs.com/package/@rushstack/node-core-library) |
| [/libraries/package-deps-hash](./libraries/package-deps-hash/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fpackage-deps-hash.svg)](https://badge.fury.io/js/%40rushstack%2Fpackage-deps-hash) | [changelog](./libraries/package-deps-hash/CHANGELOG.md) | [@rushstack/package-deps-hash](https://www.npmjs.com/package/@rushstack/package-deps-hash) |
| [/libraries/rig-package](./libraries/rig-package/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Frig-package.svg)](https://badge.fury.io/js/%40rushstack%2Frig-package) | [changelog](./libraries/rig-package/CHANGELOG.md) | [@rushstack/rig-package](https://www.npmjs.com/package/@rushstack/rig-package) |
| [/libraries/stream-collator](./libraries/stream-collator/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fstream-collator.svg)](https://badge.fury.io/js/%40rushstack%2Fstream-collator) | [changelog](./libraries/stream-collator/CHANGELOG.md) | [@rushstack/stream-collator](https://www.npmjs.com/package/@rushstack/stream-collator) |
| [/libraries/terminal](./libraries/terminal/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fterminal.svg)](https://badge.fury.io/js/%40rushstack%2Fterminal) | [changelog](./libraries/terminal/CHANGELOG.md) | [@rushstack/terminal](https://www.npmjs.com/package/@rushstack/terminal) |
| [/libraries/tree-pattern](./libraries/tree-pattern/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Ftree-pattern.svg)](https://badge.fury.io/js/%40rushstack%2Ftree-pattern) | [changelog](./libraries/tree-pattern/CHANGELOG.md) | [@rushstack/tree-pattern](https://www.npmjs.com/package/@rushstack/tree-pattern) |
| [/libraries/ts-command-line](./libraries/ts-command-line/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fts-command-line.svg)](https://badge.fury.io/js/%40rushstack%2Fts-command-line) | [changelog](./libraries/ts-command-line/CHANGELOG.md) | [@rushstack/ts-command-line](https://www.npmjs.com/package/@rushstack/ts-command-line) |
| [/libraries/typings-generator](./libraries/typings-generator/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Ftypings-generator.svg)](https://badge.fury.io/js/%40rushstack%2Ftypings-generator) | [changelog](./libraries/typings-generator/CHANGELOG.md) | [@rushstack/typings-generator](https://www.npmjs.com/package/@rushstack/typings-generator) |
| [/rigs/heft-node-rig](./rigs/heft-node-rig/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft-node-rig.svg)](https://badge.fury.io/js/%40rushstack%2Fheft-node-rig) | [changelog](./rigs/heft-node-rig/CHANGELOG.md) | [@rushstack/heft-node-rig](https://www.npmjs.com/package/@rushstack/heft-node-rig) |
| [/rigs/heft-web-rig](./rigs/heft-web-rig/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fheft-web-rig.svg)](https://badge.fury.io/js/%40rushstack%2Fheft-web-rig) | [changelog](./rigs/heft-web-rig/CHANGELOG.md) | [@rushstack/heft-web-rig](https://www.npmjs.com/package/@rushstack/heft-web-rig) |
| [/stack/eslint-config](./stack/eslint-config/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Feslint-config.svg)](https://badge.fury.io/js/%40rushstack%2Feslint-config) | [changelog](./stack/eslint-config/CHANGELOG.md) | [@rushstack/eslint-config](https://www.npmjs.com/package/@rushstack/eslint-config) |
| [/stack/eslint-patch](./stack/eslint-patch/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Feslint-patch.svg)](https://badge.fury.io/js/%40rushstack%2Feslint-patch) | [changelog](./stack/eslint-patch/CHANGELOG.md) | [@rushstack/eslint-patch](https://www.npmjs.com/package/@rushstack/eslint-patch) |
| [/stack/eslint-plugin](./stack/eslint-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Feslint-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Feslint-plugin) | [changelog](./stack/eslint-plugin/CHANGELOG.md) | [@rushstack/eslint-plugin](https://www.npmjs.com/package/@rushstack/eslint-plugin) |
| [/stack/eslint-plugin-packlets](./stack/eslint-plugin-packlets/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Feslint-plugin-packlets.svg)](https://badge.fury.io/js/%40rushstack%2Feslint-plugin-packlets) | [changelog](./stack/eslint-plugin-packlets/CHANGELOG.md) | [@rushstack/eslint-plugin-packlets](https://www.npmjs.com/package/@rushstack/eslint-plugin-packlets) |
| [/stack/eslint-plugin-security](./stack/eslint-plugin-security/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Feslint-plugin-security.svg)](https://badge.fury.io/js/%40rushstack%2Feslint-plugin-security) | [changelog](./stack/eslint-plugin-security/CHANGELOG.md) | [@rushstack/eslint-plugin-security](https://www.npmjs.com/package/@rushstack/eslint-plugin-security) |
| [/stack/rush-stack-compiler-2.4](./stack/rush-stack-compiler-2.4/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.4.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.4) | [changelog](./stack/rush-stack-compiler-2.4/CHANGELOG.md) | [@microsoft/rush-stack-compiler-2.4](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-2.4) |
| [/stack/rush-stack-compiler-2.7](./stack/rush-stack-compiler-2.7/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.7.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.7) | [changelog](./stack/rush-stack-compiler-2.7/CHANGELOG.md) | [@microsoft/rush-stack-compiler-2.7](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-2.7) |
| [/stack/rush-stack-compiler-2.8](./stack/rush-stack-compiler-2.8/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.8.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.8) | [changelog](./stack/rush-stack-compiler-2.8/CHANGELOG.md) | [@microsoft/rush-stack-compiler-2.8](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-2.8) |
| [/stack/rush-stack-compiler-2.9](./stack/rush-stack-compiler-2.9/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.9.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-2.9) | [changelog](./stack/rush-stack-compiler-2.9/CHANGELOG.md) | [@microsoft/rush-stack-compiler-2.9](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-2.9) |
| [/stack/rush-stack-compiler-3.0](./stack/rush-stack-compiler-3.0/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.0.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.0) | [changelog](./stack/rush-stack-compiler-3.0/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.0](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.0) |
| [/stack/rush-stack-compiler-3.1](./stack/rush-stack-compiler-3.1/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.1.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.1) | [changelog](./stack/rush-stack-compiler-3.1/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.1](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.1) |
| [/stack/rush-stack-compiler-3.2](./stack/rush-stack-compiler-3.2/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.2.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.2) | [changelog](./stack/rush-stack-compiler-3.2/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.2](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.2) |
| [/stack/rush-stack-compiler-3.3](./stack/rush-stack-compiler-3.3/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.3.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.3) | [changelog](./stack/rush-stack-compiler-3.3/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.3](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.3) |
| [/stack/rush-stack-compiler-3.4](./stack/rush-stack-compiler-3.4/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.4.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.4) | [changelog](./stack/rush-stack-compiler-3.4/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.4](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.4) |
| [/stack/rush-stack-compiler-3.5](./stack/rush-stack-compiler-3.5/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.5.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.5) | [changelog](./stack/rush-stack-compiler-3.5/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.5](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.5) |
| [/stack/rush-stack-compiler-3.6](./stack/rush-stack-compiler-3.6/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.6.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.6) | [changelog](./stack/rush-stack-compiler-3.6/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.6](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.6) |
| [/stack/rush-stack-compiler-3.7](./stack/rush-stack-compiler-3.7/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.7.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.7) | [changelog](./stack/rush-stack-compiler-3.7/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.7](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.7) |
| [/stack/rush-stack-compiler-3.8](./stack/rush-stack-compiler-3.8/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.8.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.8) | [changelog](./stack/rush-stack-compiler-3.8/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.8](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.8) |
| [/stack/rush-stack-compiler-3.9](./stack/rush-stack-compiler-3.9/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.9.svg)](https://badge.fury.io/js/%40microsoft%2Frush-stack-compiler-3.9) | [changelog](./stack/rush-stack-compiler-3.9/CHANGELOG.md) | [@microsoft/rush-stack-compiler-3.9](https://www.npmjs.com/package/@microsoft/rush-stack-compiler-3.9) |
| [/webpack/loader-load-themed-styles](./webpack/loader-load-themed-styles/) | [![npm version](https://badge.fury.io/js/%40microsoft%2Floader-load-themed-styles.svg)](https://badge.fury.io/js/%40microsoft%2Floader-load-themed-styles) | [changelog](./webpack/loader-load-themed-styles/CHANGELOG.md) | [@microsoft/loader-load-themed-styles](https://www.npmjs.com/package/@microsoft/loader-load-themed-styles) |
| [/webpack/loader-raw-script](./webpack/loader-raw-script/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Floader-raw-script.svg)](https://badge.fury.io/js/%40rushstack%2Floader-raw-script) | [changelog](./webpack/loader-raw-script/CHANGELOG.md) | [@rushstack/loader-raw-script](https://www.npmjs.com/package/@rushstack/loader-raw-script) |
| [/webpack/localization-plugin](./webpack/localization-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Flocalization-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Flocalization-plugin) | [changelog](./webpack/localization-plugin/CHANGELOG.md) | [@rushstack/localization-plugin](https://www.npmjs.com/package/@rushstack/localization-plugin) |
| [/webpack/module-minifier-plugin](./webpack/module-minifier-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fmodule-minifier-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Fmodule-minifier-plugin) | [changelog](./webpack/module-minifier-plugin/CHANGELOG.md) | [@rushstack/module-minifier-plugin](https://www.npmjs.com/package/@rushstack/module-minifier-plugin) |
| [/webpack/set-webpack-public-path-plugin](./webpack/set-webpack-public-path-plugin/) | [![npm version](https://badge.fury.io/js/%40rushstack%2Fset-webpack-public-path-plugin.svg)](https://badge.fury.io/js/%40rushstack%2Fset-webpack-public-path-plugin) | [changelog](./webpack/set-webpack-public-path-plugin/CHANGELOG.md) | [@rushstack/set-webpack-public-path-plugin](https://www.npmjs.com/package/@rushstack/set-webpack-public-path-plugin) |


## Unpublished Local Projects

<!-- the table below was generated using the ./repo-scripts/repo-toolbox script -->

| Folder | Description |
| ------ | -----------|
| [/build-tests/api-documenter-test](./build-tests/api-documenter-test/) | Building this project is a regression test for api-documenter |
| [/build-tests/api-extractor-lib1-test](./build-tests/api-extractor-lib1-test/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-lib2-test](./build-tests/api-extractor-lib2-test/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-lib3-test](./build-tests/api-extractor-lib3-test/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-scenarios](./build-tests/api-extractor-scenarios/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-test-01](./build-tests/api-extractor-test-01/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-test-02](./build-tests/api-extractor-test-02/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-test-03](./build-tests/api-extractor-test-03/) | Building this project is a regression test for api-extractor |
| [/build-tests/api-extractor-test-04](./build-tests/api-extractor-test-04/) | Building this project is a regression test for api-extractor |
| [/build-tests/heft-action-plugin](./build-tests/heft-action-plugin/) | This project contains a Heft plugin that adds a custom action |
| [/build-tests/heft-action-plugin-test](./build-tests/heft-action-plugin-test/) | This project exercises a custom Heft action |
| [/build-tests/heft-copy-files-test](./build-tests/heft-copy-files-test/) | Building this project tests copying files with Heft |
| [/build-tests/heft-example-plugin-01](./build-tests/heft-example-plugin-01/) | This is an example heft plugin that exposes hooks for other plugins |
| [/build-tests/heft-example-plugin-02](./build-tests/heft-example-plugin-02/) | This is an example heft plugin that taps the hooks exposed from heft-example-plugin-01 |
| [/build-tests/heft-jest-reporters-test](./build-tests/heft-jest-reporters-test/) | This project illustrates configuring Jest reporters in a minimal Heft project |
| [/build-tests/heft-minimal-rig-test](./build-tests/heft-minimal-rig-test/) | This is a minimal rig package that is imported by the 'heft-minimal-rig-usage-test' project |
| [/build-tests/heft-minimal-rig-usage-test](./build-tests/heft-minimal-rig-usage-test/) | A test project for Heft that resolves its compiler from the 'heft-minimal-rig-test' package |
| [/build-tests/heft-node-everything-test](./build-tests/heft-node-everything-test/) | Building this project tests every task and config file for Heft when targeting the Node.js runtime |
| [/build-tests/heft-oldest-compiler-test](./build-tests/heft-oldest-compiler-test/) | Building this project tests Heft with the oldest supported TypeScript compiler version |
| [/build-tests/heft-sass-test](./build-tests/heft-sass-test/) | This project illustrates a minimal tutorial Heft project targeting the web browser runtime |
| [/build-tests/heft-web-rig-library-test](./build-tests/heft-web-rig-library-test/) | A test project for Heft that exercises the '@rushstack/heft-web-rig' package |
| [/build-tests/heft-webpack4-everything-test](./build-tests/heft-webpack4-everything-test/) | Building this project tests every task and config file for Heft when targeting the web browser runtime using Webpack 4 |
| [/build-tests/heft-webpack5-everything-test](./build-tests/heft-webpack5-everything-test/) | Building this project tests every task and config file for Heft when targeting the web browser runtime using Webpack 5 |
| [/build-tests/localization-plugin-test-01](./build-tests/localization-plugin-test-01/) | Building this project exercises @microsoft/localization-plugin. This tests that the plugin works correctly without any localized resources. |
| [/build-tests/localization-plugin-test-02](./build-tests/localization-plugin-test-02/) | Building this project exercises @microsoft/localization-plugin. This tests that the loader works correctly with the exportAsDefault option unset. |
| [/build-tests/localization-plugin-test-03](./build-tests/localization-plugin-test-03/) | Building this project exercises @microsoft/localization-plugin. This tests that the plugin works correctly with the exportAsDefault option set to true. |
| [/build-tests/node-library-build-eslint-test](./build-tests/node-library-build-eslint-test/) |  |
| [/build-tests/node-library-build-tslint-test](./build-tests/node-library-build-tslint-test/) |  |
| [/build-tests/rush-stack-compiler-2.4-library-test](./build-tests/rush-stack-compiler-2.4-library-test/) |  |
| [/build-tests/rush-stack-compiler-2.7-library-test](./build-tests/rush-stack-compiler-2.7-library-test/) |  |
| [/build-tests/rush-stack-compiler-2.8-library-test](./build-tests/rush-stack-compiler-2.8-library-test/) |  |
| [/build-tests/rush-stack-compiler-2.9-library-test](./build-tests/rush-stack-compiler-2.9-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.0-library-test](./build-tests/rush-stack-compiler-3.0-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.1-library-test](./build-tests/rush-stack-compiler-3.1-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.2-library-test](./build-tests/rush-stack-compiler-3.2-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.3-library-test](./build-tests/rush-stack-compiler-3.3-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.4-library-test](./build-tests/rush-stack-compiler-3.4-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.5-library-test](./build-tests/rush-stack-compiler-3.5-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.6-library-test](./build-tests/rush-stack-compiler-3.6-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.7-library-test](./build-tests/rush-stack-compiler-3.7-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.8-library-test](./build-tests/rush-stack-compiler-3.8-library-test/) |  |
| [/build-tests/rush-stack-compiler-3.9-library-test](./build-tests/rush-stack-compiler-3.9-library-test/) |  |
| [/build-tests/ts-command-line-test](./build-tests/ts-command-line-test/) | Building this project is a regression test for ts-command-line |
| [/build-tests/web-library-build-test](./build-tests/web-library-build-test/) |  |
| [/libraries/rushell](./libraries/rushell/) | Execute shell commands using a consistent syntax on every platform |
| [/repo-scripts/doc-plugin-rush-stack](./repo-scripts/doc-plugin-rush-stack/) | API Documenter plugin used with the rushstack.io website |
| [/repo-scripts/generate-api-docs](./repo-scripts/generate-api-docs/) | Used to generate API docs for the rushstack.io website |
| [/repo-scripts/repo-toolbox](./repo-scripts/repo-toolbox/) | Used to execute various operations specific to this repo |
| [/stack/rush-stack-compiler-shared](./stack/rush-stack-compiler-shared/) |  |
| [/tutorials/heft-node-basic-tutorial](./tutorials/heft-node-basic-tutorial/) | This project illustrates a minimal tutorial Heft project targeting the Node.js runtime |
| [/tutorials/heft-node-jest-tutorial](./tutorials/heft-node-jest-tutorial/) | Building this project validates that various Jest features work correctly with Heft |
| [/tutorials/heft-node-rig-tutorial](./tutorials/heft-node-rig-tutorial/) | This project illustrates a minimal tutorial Heft project targeting the Node.js runtime and using a rig package |
| [/tutorials/heft-webpack-basic-tutorial](./tutorials/heft-webpack-basic-tutorial/) | This project illustrates a minimal tutorial Heft project targeting the web browser runtime |
| [/tutorials/packlets-tutorial](./tutorials/packlets-tutorial/) | This project illustrates how to use @rushstack/eslint-plugin-packlets |
<!-- GENERATED PROJECT SUMMARY END -->

## Contributor Notice

This repo welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This repo has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

"
230,microsoft/fluentui-react-native,TypeScript,"# FluentUI React Native

[![npm version](https://badge.fury.io/js/%40fluentui%2Freact-native.svg)](https://badge.fury.io/js/%40fluentui%2Freact-native) [![Build Status](https://dev.azure.com/ms/ui-fabric-react-native/_apis/build/status/PR?branchName=master)](https://dev.azure.com/ms/ui-fabric-react-native/_build/latest?definitionId=226&branchName=master) [![Build Status](https://dev.azure.com/ms/ui-fabric-react-native/_apis/build/status/Publish?branchName=master)](https://dev.azure.com/ms/ui-fabric-react-native/_build/latest?definitionId=229&branchName=master)

FluentUI React Native is a javascript component library that provides developers with controls that are part of the [Fluent Design System](https://www.microsoft.com/design/fluent/). These controls are built on [React Native](https://reactnative.dev/) and fully customizable.

FluentUI React Native is still in the alpha stages of development for both the components and the repo. We encourage anyone who is interested in getting an early glimpse of our plans to download and use our components, but please note that you may hit bumps along the way. Please leave us feedback or file issues if you run into bumps, and we will continue to improve the quality of the repo.

Development status on each platform:
| Windows | macOS | iOS | Android |
|---------------------|---------------------|-------------|-------------|
| Alpha (in progress) | Alpha (in progress) | Alpha (in progress) | Coming Soon |

## Getting Started

If you have an existing React Native project, it's easy to begin using FluentUI React Native. If you need to setup a new React Native project, please see the [React Native Windows Getting Started documentation](https://microsoft.github.io/react-native-windows/docs/getting-started).

### Prerequisites

- [Standard React Native dependencies](https://microsoft.github.io/react-native-windows/docs/rnw-dependencies#manual-setup)
- [Node.js](https://nodejs.org/en/download/)
- [Setting up your React Native Development Environment](https://reactnative.dev/docs/environment-setup)

### Create New React Native project (if needed)

1. Follow the instructions on the [React Native Windows Getting Started documentation](https://microsoft.github.io/react-native-windows/docs/getting-started) to create a React Native project.

2. Navigate to the root folder of your project, and use npm to install the package:

```
 npm i @fluentui/react-native
```

3. After successful installation, you can test the package by importing components at the top of your app's entry file, e.g. `App.js`:

```jsx
import { Checkbox } from '@fluentui/react-native';
```

4. After importing the @fluentui/react-native package, you can use components such as `Text` and `Checkbox` in your JSX.

```jsx
// In App.js in a new project
import React from 'react';
import { View, Text } from 'react-native';
import { Checkbox } from '@fluentui/react-native';
function HelloWorldApp() {
  return (
    <View
      style={{
        flex: 1,
        justifyContent: 'center',
        alignItems: 'center'
      }}
    >
      <Text>Hello, world!</Text>
      <Checkbox label=""Hello World Checkbox"" />
    </View>
  );
}
export default HelloWorldApp;
```

## Documentation

### Components and Controls

Our component documentation is hosted on the [FluentUI documentation](https://developer.microsoft.com/fluentui).

#### Expanding Component documentation

The FluentUI website is built out of the [FluentUI repository](https://github.com/microsoft/fluentui/tree/master/apps/public-docsite). React-Native components and controls are documented in a 'cross' (cross-platform) directory in each component page directory, e.g. [Button 'cross' directory](https://github.com/microsoft/fluentui/tree/master/apps/public-docsite/src/pages/Controls/ButtonPage/docs/cross). The FluentUI website can be run locally to verify changes, and should reflect the current state of controls that have established the _v1_ set of properties on any one platform.

Since the FluentUI React Native controls are cross-platform, but represented by a single page, it's important to distinguish platform differences and limitations. Examples include:

- If the component is not available on all supported platforms.
- If the component has properties not available on all supported platforms.
- If the component has limited support for a given property on any supported platforms.
- If the component has distinguishable behavior on a supported platform that must be minded while used.

### Theming framework

Our FluentUI framework documentation is found in this repository alongside the implementation.

- [Theming Overview](./packages/framework/theming-react-native/README.md)
- [StyleSheets](./packages/framework/themed-stylesheet/README.md)
- [Customizing Theme Settings](./packages/framework/themed-settings/README.md)
- [Theme Registry](./packages/framework/theme-registry/README.md)
- [Tokens](./packages/framework/foundation-tokens/README.md)
- [Settings and Slots](./packages/framework/foundation-settings/README.md)
- [Compose](./packages/framework/foundation-compose/README.md) and [Composable](./packages/framework/foundation-composable/README.md)

## Developing in the repo

### Yarn + Lage

This repo is set up as a monorepo using Yarn workspaces. To install yarn, please follow instructions in the [Yarn documentation](https://classic.yarnpkg.com/en/docs/install/).

For running tasks the repo has switched to using [Lage](https://github.com/microsoft/lage) for task running. The primary tasks that can be executed at the root are:

- `yarn build` - does the typescript build for all packages in the repository
- `yarn test` - will build, lint, and run any applicable tests on all packages in the repo
- `yarn bundle` - will bundle all packages in the repo
- `yarn buildci` - will build, lint, run tests, and bundle everything in the repo

Note that Lage uses caching to avoid redundant steps and has very minimal output. To avoid caching add `--no-cache` as a command line argument. Similarly adding `--verbose` will give more detailed output.

### Setup your development environment

To start developing in the repository you can:

1. `git clone https://github.com/microsoft/fluentui-react-native.git`
1. `cd fluentui-react-native`
1. `yarn`
1. `yarn build`

After a successful yarn build, you can explore FluentUI Tester, our demo application to play with each of the controls. To run FluentUI Tester, please follow instructions in the [FluentUI Tester readme](./apps/fluent-tester/README.md).

### Beachball

This repo manages semantic versioning and publishing using [Beachball](https://github.com/microsoft/beachball). When contributing, make sure to run the following before making a pull request:

1. `yarn change` will take you through a command line wizard to generate change files
2. Make sure to commit and push the newly generated change file

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
231,microsoft/pxt-blockly,JavaScript,"# Blockly (Microsoft MakeCode fork)

This is a fork of [Blockly](https://github.com/google/blockly/), an open source visual programming environment.
The fork is maintained by the Microsoft MakeCode team, and is used to power the blocks environment in [PXT](https://github.com/Microsoft/pxt).


Major additions and changes in this fork:
* [scratch-blocks](https://github.com/llk/scratch-blocks) rendering of the blocks [block_render_svg.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/block_render_svg.js)
* Using insertion markers instead of dragged connections [insertion_marker_manager.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/insertion_marker_manager.js)
* Inverted and coloured toolbox modes [toolbox.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/toolbox.js#L428) 
* Supports disabled categories [toolbox.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/toolbox.js#L360)
* Supports icons in the toolbox
* Adds a number slider field [field_slider.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/field_slider.js)
* Zoom in / out with touch gestures [touch_gesture.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/touch_gesture.js)
* Workspace comments that appear like sticky notes [workspace_comment.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/workspace_comment.js)
* A number of Edge & IE fixes
* Support underlining and icons in flyout labels [flyout_button.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/flyout_button.js#L203)
* Support for multiple flyouts per toolbox for performance reasons [pxt_blockly_functions.js](https://github.com/Microsoft/pxt-blockly/blob/develop/core/pxt_blockly_functions.js#L650)

## Prerequisites

* node, npm
* python

## Development

```
git clone https://github.com/google/closure-library
cd closure-library
git checkout v20180805
cd ../
git clone https://github.com/Microsoft/pxt-blockly
cd pxt-blockly
npm install .
```

## Building

* `npm run build:core --closure-library` to build blockly (install ``gulp`` if needed ``npm install -g gulp``)

## Update Blockly.d.ts

* `gulp typings` to regenerate blockly.d.ts

## Testing local changes in PXT

* `gulp publish --closure-library` from the ``develop`` branch to generate the blockly-compressed, blocks-compressed, and typings files, and copy them to pxt-blockly
* run `gulp` in pxt to rebuild the pxt blockly files
* run `pxt clean && pxt serve` in the **target** directory (eg pxt-arcade, or pxt-minecraft)

This can be combined into one command (starting from the target directory):

```
cd ../pxt && gulp && cd ../pxt-arcade && pxt clean && pxt serve --rebundle
```

**Make sure you've checked out the correct closure-library (see above)**

See [more tips about **pxt+pxt-blockly** testing](https://github.com/Microsoft/pxt/tree/master/scripts).

## Updating pxt-blockly in PXT

* `gulp bump --closure-library` to bump blockly version, commit, and tag.

* After the Travis has deployed the package to npm, update the pxt-blockly version in `package.json` in the pxt repo.

## Playground

There is a playground manual testing page at [tests/playground.html](./tests/playground.html), which requires no build step or server running.

## License

The original Google/Blockly is licensed under Apache License (Version 2.0).

New code is licensed under MIT.
"
232,microsoft/refreshing-config,JavaScript,"![Version](https://img.shields.io/npm/v/refreshing-config.svg)
![License](https://img.shields.io/github/license/Microsoft/refreshing-config.svg)
![Downloads](https://img.shields.io/npm/dt/refreshing-config.svg)

# refreshing-config
Configuration library that can dynamically refresh configuration values.

# Usage
1. Construct your configuration store
2. Instantiate an instance of ```RefreshingConfig``` passing your store to the constructor
3. Optionally, instantiate your refresh policies and/or change notifiers and add them by calling ```withExtension(extension: object)```
4. Call ```get(name: string)``` or ```getAll()``` to retrieve configuration values
5. Call ```set(name: string, value: any)``` or ```delete(name: string)``` to manipulate configuration values

It is important to note that the configuration values are manipulated in place when they are refreshed so if you have an instance of an object returned from ```get``` or ```getAll``` it may be modified
whenever a refresh occurs (this is intentional), if you don't want the values to change you should clone the object and use the clone.

# Stores
refreshing-config requires a store that will store the configuration values. We provide a Redis-backed store in https://npmjs.org/package/refreshing-config-redis but you can implement your own store for
your configuration backend.

### Writing a store
Stores must implement ```getAll(): IPromise<object>``` and can optionally implement ```set(name: string, value: any): IPromise<any>``` and ```delete(name: string): IPromise<void>```. The ```getAll()```
function should return an object whose keys are the names of the configuration values and the value is the configuration value itself. Stores should support the full set of JavaScript data types.

# Events
The following events are emitted from ```RefreshingConfig```:
* ```set(name, value)```: Emitted when a configuration value has been set in the underlying store where ```name```
is the name of the configuration value and ```value``` is the new value.
* ```delete(name)```: Emitted when a configuration value has been deleted where ```name``` is the name
of the configuration value that was deleted.
* ```changed(config, patch)```: Emitted when a change is detected in the configuration values after a refresh where
```config``` is the updated configuration (including unchanged values) and ```patch``` is a JSON patch describing
the changes that were detected.
* ```refresh(config)```: Emitted whenever the configuration is refreshed from the store where ```config```
is the configuration after the refresh.

The object returned from ```getAll()``` also has the ```RefreshingConfig``` instance itself in the ```_config``` property. This is
useful if you want to pass the configuration object around your application and allowing it to subscribe to updates or otherwise
manage the configuration.

# Extensions
You can extend refreshing-config's behavior by attaching extensions using ```withExtension```:

```javascript
const config = new RefreshingConfig.RefreshingConfig(store)
  .withExtension(myExtension1)
  .withExtension(myExtension2);
```

## Refresh policies
Refresh policies define when refreshing-config should go back to the store to get updated configuration values. Refresh policies can either be reactive (refreshing-config asks them if it should go back to the store)
or proactive (they notify refreshing-config that it needs to refresh). If there are multiple refresh policies attached then refreshing-config will go back to the store if **any** of them say a refresh is required.

Refresh policies are bypassed in the following scenarios:

* The read of the first configuration value (to get the initial set of configuration values)
* After a set or delete (because we know the configuration values are stale)

If you do not have a refresh policy in place you can explicitly call ```refresh()``` to force a refresh.

### NeverRefreshPolicy (reactive)
This is the default policy and will only go to the store when the first setting is read or when we know the values have changed (for example, if ```set``` or ```delete``` is called).

```javascript
const config = new RefreshingConfig.RefreshingConfig(store)
  .withExtension(new RefreshingConfig.RefreshPolicy.NeverRefreshPolicy());
```

### AlwaysRefreshPolicy (reactive)
This policy will go back to the store everytime a configuration value is read.

```javascript
const config = new RefreshingConfig.RefreshingConfig(store)
  .withExtension(new RefreshingConfig.RefreshPolicy.AlwaysRefreshPolicy());
```

### StaleRefreshPolicy (reactive)
This policy will go back to the store if it hasn't been back to the store for the specified number of milliseconds. In this example the store will be accessed at most every 30 seconds:

```javascript
const config = new RefreshingConfig.RefreshingConfig(store)
  .withExtension(new RefreshingConfig.RefreshPolicy.StaleRefreshPolicy(30000));
```

### IntervalRefreshPolicy (proactive)
This policy will proactively refresh the configuration values from the store at the defined interval. In this example the configuration values will be refreshed every 30 seconds.

```javascript
const config = new RefreshingConfig.RefreshingConfig(store)
  .withExtension(new RefreshingConfig.RefreshPolicy.IntervalRefreshPolicy(30000));
```

### Writing a refresh policy
A refresh policy must implement either ```shouldRefresh(): boolean``` (for reactive refresh policies) or ```subscribe(subscriber: RefreshingConfig)``` (for proactive refresh policies). Proactive refresh
policies should call ```subscriber.refresh()``` whenever they want the configuration values refreshed from the store.

## Change notifiers
Change notifiers are notified when refreshing-config has modified a configuration value (for example, when ```set``` or ```delete``` is called). This can be used to notify others about the need to refresh config.
Note that these are not called when configuration values are changed externally in the store, if you want to know about those you should subscribe to the ```changed``` event on ```RefreshingConfig```.

Change notifiers are generally paired with a refresh policy, in this pattern the change notifier is told about the change and communicates it to interested consumers, these consumers consume the notification
in their refresh policy which then tells the configuration library to retrieve the new values from the store.

There are no out of the box change notifiers but see https://github.com/Microsoft/refreshing-config-redis to see an example refresh policy/change notifier that use Redis pub/sub to refresh configuration
values automatically when they change.

### Writing a change notifier
A change notifier must implement the ```publish(operation: string, name: string, value: string)``` method which will be called whenever a ```set``` or ```delete``` is performed. The operation will either
be ```set``` or ```delete```, the ```name``` will be the name of the configuration value impacted, and the ```value``` will be the new value (for ```set``` operations).

# Contributing
Pull requests will gladly be considered!

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see
the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com)
with any additional questions or comments."
233,microsoft/Kusto-Query-Language,C#,"# Kusto Query Language

Kusto Query Language is a simple yet powerful language to query structured, semi-structured and unstructured data. It assumes relational data model of tables and columns with a minimal set of data types. The language is very expressive, easy to read and understand the query intent, and optimized for authoring experiences. 

## Content
This repo contains a C# parser and a semantic analyzer as well as a translator project that generates the same libraries in Java Script. See [usage examples](src/Kusto.Language/readme.md)

## API Package
This source code is also available as a [package on nuget.org](https://www.nuget.org/packages/Microsoft.Azure.Kusto.Language/)

## Query Editor
If you need to provide a query authoring experience for the language, consider using the [Kusto language plugin for the Monaco Editor](https://github.com/Azure/monaco-kusto)

## Contribute
  There are many ways to contribute to Kusto Query Language.
* [Submit bugs](https://github.com/microsoft/Kusto-Query-Language/issues) and help us verify fixes as they are checked in.
* Review the [source code changes](https://github.com/microsoft/Kusto-Query-Language/commits/master).
* Engage with other Kusto Query Language users and developers on [StackOverflow](https://stackoverflow.com/questions/tagged/kusto-query-language).

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see
the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com)
with any additional questions or comments.
"
234,microsoft/fast-blazor,C#,"# Microsoft.Fast

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![.NET C#](https://img.shields.io/badge/.NET-C%23-blue)](https://img.shields.io/badge/.NET-C%23-blue)
[![NuGet version](https://badge.fury.io/nu/Microsoft.Fast.Components.FluentUI.svg)](https://badge.fury.io/nu/Microsoft.Fast.Components.FluentUI)

[![Discord](https://img.shields.io/badge/chat%20on-discord-7289da.svg)](https://discord.gg/FcSNfg4)
[![Twitter](https://img.shields.io/twitter/follow/fast_ui.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=fast_ui)

:star: We appreciate your star, it helps!

## Introduction

The `Microsoft.Fast.Components.FluentUI` package provides a lightweight set of wrappers around Microsoft's official FluentUI Web Components. The FluentUI Web Components are built on [FAST](https://www.fast.design/) and work in every major browser. To get up and running with `Microsoft.Fast.Components.FluentUI` see [the Blazor guide](https://www.fast.design/docs/integrations/blazor).

The source for `@fluentui/web-components` is hosted in [the Fluent UI monorepo](https://github.com/microsoft/fluentui/tree/master/packages/web-components).

## Joining the Community

Looking to get answers to questions or engage with us in realtime? Our community is most active [on Discord](https://discord.gg/FcSNfg4). Submit requests and issues on [GitHub](https://github.com/dotnet/blazor-fluentui/issues/new/choose), or join us by contributing on [some good first issues via GitHub](https://github.com/dotnet/blazor-fluentui/labels/community:good-first-issue).

We look forward to building an amazing open source community with you!

## Contact

* Join the community and chat with us in real-time on [Discord](https://discord.gg/FcSNfg4).
* Submit requests and issues on [GitHub](https://github.com/dotnet/blazor-fluentui/issues/new/choose).
* Contribute by helping out on some of our recommended first issues on [GitHub](https://github.com/dotnet/blazor-fluentui/labels/community:good-first-issue).
"
235,microsoft/sonder-ui,HTML,"# Sonder UI
> a collection of tested, accessible components and component pattern documentation.

The purpose of this project is to run usability tests on experimental UI patterns, and showcase the component patterns that have been thoroughly tested for accessibility. Each component's readme will include a description of how it was tested, bugs found, expected functionality, and design considerations for extension or authoring similar patterns.

Each pattern is authored as a web component, and can be dropped directly into a project. However, since this is primarily intended as accessibility documentation + reference implementation, they may not be as fully featured and are not guaranteed to be stable or consistently maintained (translation: don't use this directly in production, but try it out and borrow the patterns you find useful).

Suggestions for additional components to include are very welcome; please file an issue.

## Components
- [Combobox (optionally filterable)](src/components/combobox)
- [Disclosure](src/components/disclosure)
- [Modal](src/components/modal)
- [Multiselect](src/components/multiselect)
- [Select](src/components/select)
- [Tooltip (WIP)](src/components/tooltip)

## Repository Structure

- `src/assets`: Shared assets and sample data.
- `src/components`: Tested, polished components are in here, and each component has its own readme and documentation.
- `src/draft-components`: Experimental patterns live here. There are often multiple variations of the same UI pattern that co-exist for testing.
- `src/shared`: Shared utils used by components in `src/components`.
- `src/studies`: Environments and sample pages used for running usability tests.

## Try it out

To try out the components and usability study environments in this repository, clone it and run the following:

```
npm install
```

then:

```
npm start
```

You should then be able to access the main index at `localhost:3333`, and the usability studies at `localhost:3333/studies`.

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
236,microsoft/banner-settings-ado-extension,TypeScript,"[![Build Status](https://dev.azure.com/ms/banner-settings-ado-extension/_apis/build/status/microsoft.banner-settings-ado-extension?branchName=master)](https://dev.azure.com/ms/banner-settings-ado-extension/_build/latest?definitionId=259&branchName=master)

Banner Settings provides a settings pane under Organization Settings to allow Project Collection Administrators to show sitewide banners. Alert your Azure DevOps users to upcoming changes or events without sending out mass emails. Compatible with Azure DevOps Services and Server.

![](static/screenshot.png)

### Features

- Show banners on any page in Azure DevOps.
- Choose between three types (levels) of messages: Info, Warning, and Error.
- Choose an expiration date for a message.
- Include hyperlinks in your banners using markdown syntax like the banner message below.

```markdown
Windows October Update released! Please visit the [Windows Insider Blog](https://blogs.windows.com/windowsexperience/tag/windows-insider-program/) for more info.
```

### Restrictions

- Only one banner can be shown at a time to keep the interface clean. Banners are prioritized by level. For example, if you have posted a warning message and an info message, the info message will only be shown after a user closes the warning message, or you delete the warning message.
- Banners are restricted to a length of thirty words.

### Building the project

Just run:

    npm run build:dev
    npm run package:dev

This produces a .vsix file which can be uploaded to the [Visual Studio Marketplace](https://marketplace.visualstudio.com/azuredevops)

Publish it to your own publisher by running:

    npm run publish:dev

You can then serve the extension locally and visit your newly published dev environment extension using

    npm run dev

### Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."
237,microsoft/SandDance,TypeScript,"# SandDance

Visually explore, understand, and present your data.

![sanddance-animation](https://user-images.githubusercontent.com/11507384/54236654-52d42800-44d1-11e9-859e-6c5d297a46d2.gif)

By using easy-to-understand views, SandDance helps you find insights about your data, which in turn help you tell stories supported by data, build cases based on evidence, test hypotheses, dig deeper into surface explanations, support decisions for purchases, or relate data into a wider, real world context.

SandDance uses unit visualizations, which apply a one-to-one mapping between rows in your database and marks on the screen.
Smooth animated transitions between views help you to maintain context as you interact with your data.

> This new version of SandDance has been rebuilt from scratch with the goal of being modular, extensible, and embeddable into your custom applications. We are now on GitHub so that we are open and driven by the community through contributions, feature requests, and discussion.

SandDance was created by the [Microsoft Research VIDA Group](https://aka.ms/vida) which explores novel technologies for visualization and immersive data analytics.

## Where can I use SandDance?
* [Try it now on the web](https://microsoft.github.io/SandDance/app/)
* Microsoft apps:
  * [Power BI](https://appsource.microsoft.com/en-us/product/power-bi-visuals/WA200000430) - [*see additional info*](https://github.com/microsoft/SandDance/blob/master/powerbi.md)
  * [Azure Data Studio](https://docs.microsoft.com/en-us/sql/azure-data-studio/sanddance-extension?view=sql-server-2017)
  * [VSCode extension](https://marketplace.visualstudio.com/items?itemName=msrvida.vscode-sanddance)
* 3rd Party apps:
  * [Observable](https://observablehq.com/collection/@danmarshall/sanddance)
  * [Jupyter widget](https://github.com/microsoft/SandDance/tree/master/python/jupyter-widget#sanddance-jupyter-widget)
  * [HASH Core IDE](https://core.hash.ai/) - [*see 'Step Explorer' documentation*](https://docs.hash.ai/core/views#step-explorer)
* In your own JavaScript apps - see below

## Component architecture

SandDance is an offering of several JavaScript components:

* [sanddance](packages/sanddance/README.md) - the core SandDance visualization canvas.
* [sanddance-react](packages/sanddance-react/README.md) - the core SandDance visualization canvas for use in React based applications.
* [sanddance-vue](packages/sanddance-vue/README.md) - the core SandDance visualization canvas for use in Vue based applications.
* [sanddance-explorer](packages/sanddance-explorer/README.md) - the core SandDance visualization canvas with UI to enable data exploration, for use in React based applications.

## Publications

* 2018 - [Atom: A Grammar for Unit Visualizations](https://www.microsoft.com/en-us/research/uploads/prod/2019/01/atom.pdf)
  * Deokgun Park, Steven Drucker, Roland Fernandez, Niklas Elmqvist
  * IEEE Transactions on Visualization and Computer Graphics | December 2018, Vol 24(12): pp. 3032-3043
* 2015 - [A Unifying Framework for Animated and Interactive Unit Visualizations](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/sanddance.pdf)
  * Steven Drucker, Roland Fernandez 
  * MSR-TR-2015-65 | August 2015

## Articles & videos

* [SandDance project @ Microsoft Research](https://www.microsoft.com/en-us/research/project/sanddance/)
* [Microsoft Research webinar / Data Visualization: Bridging the Gap Between Users and Information](https://note.microsoft.com/MSR-Webinar-Data-Visualization-Registration-On-Demand.html).
* [SQL Server Blog / The August release of Azure Data Studio is now available](https://cloudblogs.microsoft.com/sqlserver/2019/08/15/the-august-release-of-azure-data-studio-is-now-available/)
* [Open Source Blog / What’s new in SandDance 3](https://cloudblogs.microsoft.com/opensource/2020/06/23/whats-new-sanddance-3-microsoft-research/)
* [Channel 9 - Data Exposed / Introducing SandDance: Data Visualization in Azure Data Studio](https://channel9.msdn.com/Shows/Data-Exposed/Introducing-SandDance-Data-Visualization-in-Azure-Data-Studio)
* [Channel 9 - Data Exposed / What is SandDance?](https://channel9.msdn.com/Shows/Data-Exposed/What-is-SandDance)
* [Hacker News / Microsoft open sources SandDance, a visual data exploration tool](https://news.ycombinator.com/item?id=21224685)
* [analyticsindiamag.com / Visualizations With SandDance Using Visual Studio Code](https://analyticsindiamag.com/visualizations-with-sanddance-using-visual-studio-code/)
* [codeburst.io / Exploring Titanic Dataset using Microsoft’s Sandance](https://codeburst.io/exploring-titanic-dataset-using-microsofts-sandance-175eb04b3ac2)
* [mathkuro.com / VS Codeのイケメンすぎる分析＆可視化ツールSand Danceの使い方](https://www.mathkuro.com/vs-code/sand-dance/)
* [mathkuro.com / 【SandDanceグラフサンプル】用途に合わせて選択しましょう◎](https://www.mathkuro.com/vs-code/sanddance-charts/)
* [medium.com - @sefaoguzsaglam / how to start data visualizing with Microsoft’s SandDance (for beginners)](https://medium.com/@sefaoguzsaglam/how-to-start-data-visualizing-with-microsofts-sanddance-for-beginners-abe5c0552750)
* [mssqltips.com / SandDance for Azure Data Studio](https://www.mssqltips.com/sqlservertip/6045/sanddance-for-azure-data-studio/)
* [sqlshack.com / Exploring the SandDance Visualizations extension in Azure Data Studio](https://www.sqlshack.com/exploring-the-sanddance-visualizations-extension-in-azure-data-studio/)
* [torbjornzetterlund.com / I got to do some SandDance visualization](https://torbjornzetterlund.com/i-got-to-do-some-sanddance-vizualisation/)
* [YouTube - Anjani Prasad Atluri / SandDance: A tutorial](https://www.youtube.com/watch?v=sI4WIQEz07w)
* [YouTube - BI Tracks / SandDance Visualizations Tutorial - Azure Data Studio](https://www.youtube.com/watch?v=iUhvYMggzAQ)

## Changelog

* June 2020 - Major version bump to v3: Now using Deck.gl@8.
* December 2019 - Major version bump to v2: Now using Vega@5.
* August 2019 - Initial release to AppSource (Power BI marketplace).
* April 2019 - Initial release to GitHub.

## Known issues

* Animations require a WebGL2 enabled browser.

## Roadmap

* ~~PowerBI custom visual based on this new architecture.~~ done!
* ~~Additional views, such as stacks.~~ done!
* Code examples and tutorials.
* ~~Faceting for all chart types.~~ done!
* Better date handling.

## Dependencies

SandDance is created with open source libraries, using [Vega](https://vega.github.io) for chart layout and [Deck.gl](https://deck.gl) for WebGL rendering.

## Development

See [dev.md](dev.md)

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
238,microsoft/fhir-server,C#,"# FHIR Server for Azure

A .NET Core implementation of the FHIR standard.

| CI Build & Deployment | Azure Government Deployment |
|---|---|
| [![Build Status](https://microsofthealthoss.visualstudio.com/FhirServer/_apis/build/status/CI%20Build%20%26%20Deploy?branchName=master)](https://microsofthealthoss.visualstudio.com/FhirServer/_build/latest?definitionId=27&branchName=master) | [![Build Status](https://microsofthealthoss.visualstudio.com/FhirServer/_apis/build/status/CI%20Deployment%20MAG?branchName=master)](https://microsofthealthoss.visualstudio.com/FhirServer/_build/latest?definitionId=28&branchName=master)

FHIR Server for Azure is an open-source implementation of the emerging [HL7 Fast Healthcare Interoperability Resources (FHIR) specification](https://www.hl7.org/fhir/) designed for the Microsoft cloud. The FHIR specification defines how clinical health data can be made interoperable across systems, and the FHIR Server for Azure helps facilitate that interoperability in the cloud. The goal of this Microsoft Healthcare project is to enable developers to rapidly deploy a FHIR service.
 
With data in the FHIR format, the FHIR Server for Azure enables developers to quickly ingest and manage FHIR datasets in the cloud, track and manage data access and normalize data for machine learning workloads. FHIR Server for Azure is optimized for the Azure ecosystem: 
* Scripts and ARM templates are available for immediate provisioning in the Microsoft Cloud 
* Scripts are available to map to Azure AAD and enable role-based access control (RBAC) 

FHIR Server for Azure is built with logical separation, enabling developers with flexibility to modify how it is implemented, and extend its capabilities as needed. The logic layers of the FHIR server are:

* Hosting Layer – Supports hosting in different environments, with custom configuration of Inversion of Control (IoC) containers.
* RESTful API Layer – The implementation of the APIs defined by the HL7 FHIR specification.
* Core Logic Layer – The implementation of the core FHIR logic.
* Persistence Layer – A pluggable persistence provider enabling the FHIR server to connect to virtually any data persistence utility. FHIR Server for Azure includes a ready-to-use data persistence provider for Azure Cosmos DB (a globally replicated database service that offers rich querying over data).

FHIR Server for Azure empowers developers – saving time when they need to quickly integrate a FHIR server into their own applications or providing them with a foundation on which they can customize their own FHIR service. As an open source project, contributions and feedback from the FHIR developer community will continue to improve this project.

Privacy and security are top priorities and the FHIR Server for Azure has been developed in support of requirements for Protected Health Information (PHI). All the Azure services used in FHIR Server for Azure [meet the compliance requirements for Protected Health Information](https://www.microsoft.com/en-us/trustcenter/compliance/complianceofferings).

This open source project is fully backed by the Microsoft Healthcare team, but we know that this project will only get better with your feedback and contributions. We are leading the development of this code base, and test builds and deployments daily.

There is also a managed offering in Azure called the [Azure API for FHIR](https://azure.microsoft.com/services/azure-api-for-fhir/). This Platform as a Service (PaaS) FHIR server is backed by the open source project in this repository and it offers a turn key solution to provisioning a compliant, secure FHIR service.

# Release Notes
To see what is releasing in the FHIR Server, please refer to the [releases](https://github.com/microsoft/fhir-server/releases) section on GitHub. Starting in November 2020, we have tags on the PRs to better describe what is releasing. We have also released documentation on how to test the most recently build [here](docs/Testing-Releases.md). 

# Documentation

## Getting Started
- Quickstart guides to deploy open source using [portal](docs/QuickstartDeployPortal.md), [CLI](docs/QuickstartDeployCLI.md), and [PowerShell](docs/QuickstartDeployPowershell.md).
- [Sql Schema Migration Guide](docs/SchemaMigrationGuide.md): Describes how to upgrade Schema for Sql Server.
- [Register a resource application](docs/Register-Resource-Application.md): Learn how to register a resource application, which is an Azure Active Directory representation of the FHIR server API.
- [Register a client application](docs/Register-Client-Application.md): Learn how to register a client application registration, which is an Azure Active Directory representation of an application that can be used to authenticate on behalf of a user and request access to resource applications.

## Core FHIR Capabilities
- [Azure API for FHIR documentation](https://docs.microsoft.com/azure/healthcare-apis/): Includes all Azure API for FHIR documentation which has many conceptual, how-to guides, and tutorials that can be leveraged in open-source as well.
- [Features](https://docs.microsoft.com/en-us/azure/healthcare-apis/fhir-features-supported): This document lists the main features of the FHIR Server for Azure and Azure API for FHIR.
- [Authentication](docs/Authentication.md): Describes the authentication settings for the FHIR server and how to make use of it in development and test scenarios.
- [Roles](docs/Roles.md): Describes how the FHIR Server for Azure role-based access control (RBAC) system works.
- [Search](docs/SearchArchitecture.md): Describes how search is implemented for the FHIR Server for Azure.

## Additional Capabilities
- [Bulk Export](docs/BulkExport.md): Describes using Bulk Export within the FHIR Server.
- [Convert Data](docs/ConvertDataOperation.md): Describes how to use $convert to convert data into FHIR.
- [FHIR Proxy](https://github.com/microsoft/health-architectures/tree/master/FHIR/FHIRProxy): Secure FHIR Gateway and Proxy to FHIR Servers.

## Tutorials & How-to Guides
- [Health Architectures](https://aka.ms/healtharchitectures): A collection of reference architectures illustrating end-to-end best practices for using the Azure API for FHIR and related technologies.
- [FHIR Server Samples Repo](https://github.com/Microsoft/fhir-server-samples): A demo sandbox using the Azure API for FHIR.
- [SMART on FHIR Proxy tutorial](docs/SMARTonFHIR.md): Describes how to use the proxy to enable SMART on FHIR applications with the FHIR Server.
- [FHIR Postman tutorial](https://docs.microsoft.com/azure/healthcare-apis/access-fhir-postman-tutorial): Describes how to access a FHIR API using Postman.
- [Debugging](docs/HowToDebug.md): Describes how to debug FHIR Server for Azure using Visual Studio.

## Blog Posts
* Blog: [FHIR Server for Azure, an open source project for modern healthcare](https://cloudblogs.microsoft.com/industry-blog/health/2018/11/12/fhir-server-for-azure-an-open-source-project-for-cloud-based-health-solutions/).
* Blog: [Azure API for FHIR moves to general availability](https://azure.microsoft.com/en-us/blog/azure-api-for-fhir-moves-to-general-availability/).
* Twitter: [Health_IT](https://twitter.com/Health_IT)

## Contributing
This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

There are many other ways to contribute to FHIR Server for Azure.
* [Submit bugs](https://github.com/Microsoft/fhir-server/issues) and help us verify fixes as they are checked in.
* Review the [source code changes](https://github.com/Microsoft/fhir-server/pulls).
* Engage with FHIR Server for Azure users and developers on [StackOverflow](https://stackoverflow.com/questions/tagged/fhir-server-for-azure).
* Join the [#fhirforazure](https://twitter.com/hashtag/fhirserverforazure?f=tweets&vertical=default) discussion on Twitter.
* [Contribute bug fixes](CONTRIBUTING.md).

See [Contributing to FHIR Server for Azure](CONTRIBUTING.md) for more information.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

FHIR&reg; is the registered trademark of HL7 and is used with the permission of HL7. 

"
239,microsoft/powerbi-visuals-enhancedscatter,TypeScript,"# EnhancedScatter
![Build](https://github.com/microsoft/powerbi-visuals-utils-testutils/workflows/build/badge.svg)[![Coverage Status](https://coveralls.io/repos/github/Microsoft/powerbi-visuals-enhancedscatter/badge.svg?branch=master)](https://coveralls.io/github/Microsoft/powerbi-visuals-enhancedscatter?branch=master)

> A few more properties were added to the existing scatter chart visual, including shapes as markers, background image support, and developer crosshairs for positioning elements onto an image background.

![Enhancedscatter screenshot](https://raw.githubusercontent.com/microsoft/powerbi-visuals-enhancedscatter/master/assets/screenshot.png)

# Overview
Enhanced Scatter introduces a few more properties that were added on top of the existing scatter chart visual, including shapes as markers, background image support, and developer crosshairs for positioning elements onto an image background.

See also [Enhanced Scatter at Microsoft Office store](https://store.office.com/en-us/app.aspx?assetid=WA104380762&sourcecorrid=dfd34541-621e-4f3b-a6ab-398e528af4ab&searchapppos=0&ui=en-US&rs=en-US&ad=US&appredirect=false)
"
240,microsoft/ts-parsec,TypeScript,"# ts-parsec

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Using ts-parsec with npm

```cmd
npm install -g typescript-parsec
```

## Building this repo

```cmd
yarn
yarn build
yarn test
```

## Packages

- **ts-parsec**: Parser combinator for TypeScript
- **tspc-test**: Unit test project
- **tspc-utilities**: Code generator for developing **ts-parsec**
  - At this moment, running `npm run update` will write overloadings for `alt` and `seq` for you

## Introduction

ts-parsec is a parser combinator library prepared for typescript. By using this library, you are able to create parsers very quickly using just a few lines of code. It provides the following features:

- **Tokenizer based on regular expressions**. This tokenizer is designed for convenience. For some cases its performance may be unsatisfying. In this case, you could write your own tokenizer. It is very easy to plug your tokenizer into ts-parsec.
- **Parser combinators**.
- The ability to support recursive syntax.

You are recommended to learn [EBNF](https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form) before using this library.

Please read [Getting Started](./doc/GettingStarted.md) for ramping up, or our [document page](./doc/README.md) for deeper understanding.

## More Examples

- [A simple calculator](./packages/tspc-test/src/TestRecursiveParser.ts)
- [A minimum Flow parser](https://github.com/microsoft/react-native-tscodegen/blob/master/packages/minimum-flow-parser/src/Parser.ts)

## In the Future

Following combinators will be released soon:

- A context sensitive **apply** combinator.

Context sensitive tokenizer is also comming.
"
241,microsoft/superbenchmark,Python,"# SuperBenchmark

[![Lint](https://github.com/microsoft/superbenchmark/workflows/Lint/badge.svg)](https://github.com/microsoft/superbenchmark/actions?query=workflow%3ALint)
[![Codecov](https://codecov.io/gh/microsoft/superbenchmark/branch/main/graph/badge.svg?token=DDiDLW7pSd)](https://codecov.io/gh/microsoft/superbenchmark)

| Azure Pipelines | Build Status |
| :---: | :---: |
| cpu-unit-test | [![Build Status](https://dev.azure.com/msrasrg/SuperBenchmark/_apis/build/status/microsoft.superbenchmark?branchName=main)](https://dev.azure.com/msrasrg/SuperBenchmark/_build/latest?definitionId=77&branchName=main) |
| gpu-unit-test | [![Build Status](https://dev.azure.com/msrasrg/SuperBenchmark/_apis/build/status/cuda-unit-test?branchName=main)](https://dev.azure.com/msrasrg/SuperBenchmark/_build/latest?definitionId=80&branchName=main) |


SuperBench is a benchmarking and diagnosis tool for AI infrastructure,
which supports:
* Comprehensive AI infrastructure validation
    * Distributed validation tools to validate hundreds or thousands of servers automatically
    * Consider both raw hardware and E2E model performance with ML workload patterns
    * Provide a fast and accurate way to detect and locate hardware problems
    * Performance/Quality Gates for hardware and system release
* Benchmarking with typical AI workload patterns
    * Provide comprehensive performance comparison between different existing hardware
    * Give a better understanding for new DL software & hardware
* Detailed performance analysis and diagnosis
    * Provide detailed performance report and advanced analysis tool   

It includes micro-benchmark for primitive computation and communication benchmarking,
and model-benchmark to measure domain-aware end-to-end deep learning workloads.

> 🔴 __Note__:
SuperBench is in the early pre-alpha stage for open source, and not ready for general public yet.
If you want to jump in early, you can try building latest code yourself.


## Installation

### Using Docker (_Preferred_)

__System Requirements__

* Platform: Ubuntu 18.04 or later (64-bit)
* Docker: Docker CE 19.03 or later

__Install SuperBench__

* Using Pre-Build Images

    ```sh
    docker pull superbench/superbench:dev-cuda11.1.1
    docker run -it --rm \
        --privileged --net=host --ipc=host --gpus=all \
        superbench/superbench:dev-cuda11.1.1 bash
    ```

* Building the Image

    ```sh
    docker build -f dockerfile/cuda11.1.1.dockerfile -t superbench/superbench:dev .
    ```

### Using Python

__System Requirements__

* Platform: Ubuntu 18.04 or later (64-bit); Windows 10 (64-bit) with WSL2
* Python: Python 3.6 or later, pip 18.0 or later

    Check whether Python environment is already configured:
    ```sh
    # check Python version
    python3 --version
    # check pip version
    python3 -m pip --version
    ```
    If not, install the followings:
    * [Python](https://www.python.org/)
    * [pip](https://pip.pypa.io/en/stable/installing/)
    * [venv](https://docs.python.org/3/library/venv.html)

    It's recommended to use a virtual environment (optional):
    ```sh
    # create a new virtual environment
    python3 -m venv --system-site-packages ./venv
    # activate the virtual environment
    source ./venv/bin/activate

    # exit the virtual environment later
    # after you finish running superbench
    deactivate
    ```

__Install SuperBench__

* PyPI Binary

    ```sh
    # not available yet
    ```

* From Source

    ```sh
    # get source code
    git clone https://github.com/microsoft/superbenchmark
    cd superbenchmark

    # install superbench
    python3 -m pip install .
    ```


## Usage

### Run SuperBench

```sh
# run benchmarks in default settings
sb exec

# use a custom config
sb exec --config-file ./superbench/config/default.yaml
```

### Benchmark Gallary

Please find more benchmark examples [here](examples/benchmarks/).


## Developer Guide

Follow [Installation using Python](#using-python).

### Set Up

```sh
# get latest code
git clone https://github.com/microsoft/superbenchmark
cd superbenchmark

# install superbench
python3 -m pip install -e .[dev,test]
```

### Lint and Test

```sh
# format code using yapf
python3 setup.py format

# check code style with mypy and flake8
python3 setup.py lint

# run all unit tests
python3 setup.py test
```

### Submit a Pull Request

Please install `pre-commit` before `git commit` to run all pre-checks.

```sh
pre-commit install
```

Open a pull request to main branch on GitHub.


## Contributing

### Contributor License Agreement

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### Contributing principles

SuperBenchmark is an open-source project. Your participation and contribution are highly appreciated. There are several important things you need know before contributing to this project:

#### What content can be added to SuperBenchmark

1. Bug fixes for existing features.
2. New features for benchmark module (micro-benchmark, model-benchmark, etc.)

   If you would like to contribute a new feature on SuperBenchmark, please submit your proposal first. In [GitHub Issues](https://github.com/microsoft/superbenchmark/issues) module, choose `Enhancement Request` to finish the submission. If the proposal is accepted, you can submit pull requests to origin main branch.

#### Contribution steps

If you would like to contribute to the project, please follow below steps of joint development on GitHub.

1. `Fork` the repo first to your personal GitHub account.
2. Checkout from main branch for feature development.
3. When you finish the feature, please fetch the latest code from origin repo, merge to your branch and resolve conflict.
4. Submit pull requests to origin main branch.
5. Please note that there might be comments or questions from reviewers. It will need your help to update the pull request.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
"
242,microsoft/sarif-sdk,C#,"# sarif-sdk
[![Build Status](https://dev.azure.com/mseng/1ES/_apis/build/status/microsoft.sarif-sdk?branchName=master)](https://dev.azure.com/mseng/1ES/_build/latest?definitionId=9978&branchName=master)

The SARIF SDK contains .NET code and supporting files for working with the Static Analysis Results Interchange Format (SARIF). For more information about SARIF, see the [SARIF Home Page](http://sarifweb.azurewebsites.net). You can read the [SARIF specification](https://rawgit.com/sarif-standard/sarif-spec/master/Static%20Analysis%20Results%20Interchange%20Format%20(SARIF).html), or file [issues](https://github.com/sarif-standard/sarif-spec/issues) in the [SARIF GitHub repo](https://github.com/sarif-standard/sarif-spec).

## Getting started

To add the SARIF SDK to your project, install the Sarif.Sdk [NuGet package](https://www.nuget.org/packages/Sarif.Sdk). Sarif.Sdk depends on [Newtonsoft.Json](http://www.newtonsoft.com/json), which is installed automatically when you install Sarif.Sdk.

The types in the SARIF SDK are in the `Microsoft.CodeAnalysis.Sarif` namespace.

The SARIF SDK provides a set of classes which represent the elements of the SARIF format. We refer to this as the ""SARIF object model"". The root type that represents a SARIF log file is `SarifLog`. Other types in the SARIF object model are `Result`, `PhysicalLocation`, _etc._.

Note: The SARIF SDK's build process automatically generates the SARIF object model classes from the SARIF JSON schema, which you can find at [`src/Sarif/Schemata/sarif-schema.json`](https://github.com/Microsoft/sarif-sdk/blob/master/src/Sarif/Schemata/sarif-schema.json). Although these files do exist in the repo (under [`src/Sarif/Autogenerated`](https://github.com/Microsoft/sarif-sdk/tree/master/src/Sarif/Autogenerated)), you should never edit them by hand.

In addition to the object model, the SARIF SDK provides a set of helper classes to facilitate using Newtonsoft.Json to read and write SARIF log files.

## Building the SDK

If you want to build the SDK from source, rather than consuming the NuGet package,
proceed as follows:

1. Install .NET Core SDK 2.1 and 3.1 from https://dotnet.microsoft.com/download

2. Ensure that Visual Studio 2019 is installed on your machine.

    You can build in VS 2017 as well.

3. Ensure that your Visual Studio installation includes the components that support
    - C# development

4. Open a Visual Studio 2019 Developer Command Prompt Window.

5. From the root directory of your local repo, run the command `BuildAndTest.cmd`.
    This restores all necessary NuGet packages, builds the SDK, and runs all the tests.

    All build output appears in the `bld\` subdirectory of the repo root directory.

    NOTE: You must run `BuildAndTest.cmd` once _before_ attempting to build in
    Visual Studio, to ensure that all required NuGet packages are available.

6. After you have run `BuildAndTest.cmd` once, you can open any of the solution files
in the `src\` directory in Visual Studio 2017, and build them by running **Rebuild Solution**.


## Accomplishing common tasks

To learn how to accomplish common tasks with the SARIF SDK, such as reading and writing files from disk,
see the [How To](https://github.com/Microsoft/sarif-sdk/blob/master/docs/how-to.md) page.

## Code of conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/),
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
 "
243,microsoft/epic-supervisor,TypeScript,"# @mixer/epic-supervisor

> Project status: this repo is actively maintained and currently used in production code.

[redux-observable](https://github.com/redux-observable/redux-observable) is an RxJS-based side effects module for Redux. Unfortunately, it lacks a built-in way to handle uncaught errors, which by default are unlogged and break all epics in the application. The maintainers [have indicated](https://github.com/redux-observable/redux-observable/issues/94#issuecomment-454968018) that there's no immediate plans to implement a 'first-party' solution for error handling. So, this is ours! This module implements an [Erlang/OTP-style supervisor](http://erlang.org/doc/design_principles/sup_princ.html) pattern for epics. Out of the box it provides a `combineEpics`-compatible function which instruments epics with error logging, and provides an additional `superviseEpics` method which provides additional supervision options for epics.

```ts
// 1. import from this module, instead of redux-observable:
import { combineEpics } from '@mixer/epic-supervisor';

// 2. Define your epics...
const fooEpic = /* ... */;
const barEpic = /* ... */;

// 3. combineEpics() like you normall would:
export const myEpics = combineEpics(fooEpic, barEpic);
```

# API

## `combineEpics(...epics)`

By default, this method functions identically to [`combineEpics`](https://redux-observable.js.org/docs/api/combineEpics.html) from `redux-observable`.

## `superviseEpics(options, ...epics)`

This works like `combineEpics`, except with an options argument in front. The options object can take several parameters, all optional:

- **restart**: defines how epics should be restarted if they error. May be one of `noRestarts`, `oneForOne`, `oneForAll`, or `restForOne`. See the [erlang supervisor page](http://erlang.org/doc/design_principles/sup_princ.html#restart-strategy) for nice diagrams of these. Defaults to `noRestarts`, indicating restarts will not occur.
- **onError**: a function with the same mechanism as RxJS `catchError()`-- it will be invoked with the error context object (see beow) that occurs, followed by the actions, state, and services just like a normal epic. If restart is enabled we'll wait for the returned observable to emit before restarting epics. If an error is thrown or rethrown from this method, it will bubble up to any parent supervisor.
- **onRestart**: a function invoked after epics are restarted. Same call signature as `onError`.

Example:

```ts
import { superviseEpics, oneForOne } from '@mixer/epic-supervisor';
import { timer } from 'rxjs';

const fooEpic = /* ... */;
const barEpic = /* ... */;

const options = {
  restart: oneForOne,
  onError: ({ epicName, error }, actions, state, services) => {
    // Send another action when the error occurs:
    actions.dispatch(doLogErrorAction({ message: `An error occurred in epic ${epicName}`, error }));
    // Wait a second before restarting epics:
    return timer(1000);
  },
  onRestart (_, actions) => {
    actions.dispatch(restartMyService());
  },
};

superviseEpics(options, fooEpic, barEpic);
```

## `configure(options)`

Configures the global/default options for epic-supervisor. The options object can take several parameters, all optional:

- **onAnyError**: A method invoked with any error that gets observed. The first any only argument is the ErrorContext object.
- **onUnhandledError**: A method invoked with any error not handled by (or thrown-from) the user-supplied `onError` method.

```ts
import { configure } from '@mixer/epic-supervisor';

configure({
  onAnyError: context => myLogger.warn(context),
  onUnhandledError: context => myLogger.error(context),
});
```

## Error Context Object

The error context (`IErrorContext` for TypeScript consumers) captures thrown errors along with some metadata--as rxjs stacktraces are often inscrutable. It has the following properties:

- **error**: the original error that was thrown;
- **epicName**: the function name of the epic that the error came from. This may be `null` for epics that lack a function name.
- **epicSource**: the epic source string; useful for tracking down errors from unnamed or minified epics.
- **innerError**: can be set if an error happened whilst handling a previous error. The 'outer' error will be second error, while the innerError will be the original one.
- **innermostError**: gets the deepest `innerError`, the original error that occurred. Returns the current error context if there is no innerError.
"
244,microsoft/windows-rs,Rust,"[![crates.io](https://img.shields.io/crates/v/windows.svg)](https://crates.io/crates/windows)
[![docs.rs](https://docs.rs/windows/badge.svg)](https://docs.rs/windows)
[![Build and Test](https://github.com/microsoft/windows-rs/workflows/Build%20and%20Test/badge.svg?event=push)](https://github.com/microsoft/windows-rs/actions)

## Rust for Windows

The `windows` crate lets you call any Windows API past, present, and future using code generated on the fly directly from the metadata describing the API and right into your Rust package where you can call them as if they were just another Rust module.

The Rust language projection follows in the tradition established by [C++/WinRT](https://github.com/microsoft/cppwinrt) of building language projections for Windows using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs.

Watch the [Getting Started](https://www.youtube.com/watch?v=-oZrsCPKsn4) video! Microsoft Docs also has content on [developing with Rust on Windows](https://docs.microsoft.com/en-us/windows/dev-environment/rust/).

Check out the [FAQ](./docs/FAQ.md) for answers to frequently asked questions.

## Getting started

Start by adding the following to your Cargo.toml file:

```toml
[dependencies]
windows = ""0.9.1""

[build-dependencies]
windows = ""0.9.1""
```

This will allow Cargo to download, build, and cache Windows support as a package. Next, specify which types you need inside of a `build.rs` build script and the `windows` crate will generate the necessary bindings:

```rust
fn main() {
    windows::build!(
        Windows::Data::Xml::Dom::*,
        Windows::Win32::System::Threading::{
            CreateEventW, SetEvent, WaitForSingleObject
        },
        Windows::Win32::System::WindowsProgramming::CloseHandle,
        Windows::Win32::UI::WindowsAndMessaging::MessageBoxA,
    );
}
```

Finally, make use of any Windows APIs as needed.

```rust
mod bindings {
    windows::include_bindings!();
}

use bindings::{
    Windows::Data::Xml::Dom::*,
    Windows::Win32::System::Threading::{CreateEventW, SetEvent, WaitForSingleObject},
    Windows::Win32::System::WindowsProgramming::CloseHandle,
    Windows::Win32::UI::WindowsAndMessaging::{MessageBoxA, MESSAGEBOX_STYLE},
};

fn main() -> windows::Result<()> {
    let doc = XmlDocument::new()?;
    doc.LoadXml(""<html>hello world</html>"")?;

    let root = doc.DocumentElement()?;
    assert!(root.NodeName()? == ""html"");
    assert!(root.InnerText()? == ""hello world"");

    unsafe {
        let event = CreateEventW(std::ptr::null_mut(), true, false, None);
        SetEvent(event).ok()?;
        WaitForSingleObject(event, 0);
        CloseHandle(event).ok()?;

        MessageBoxA(None, ""Text"", ""Caption"", MESSAGEBOX_STYLE::MB_OK);
    }

    Ok(())
}
```

To reduce build time, use a `bindings` crate rather than simply a module. This will allow Cargo to cache the results and build your project far more quickly.

There is an experimental [documentation generator](https://github.com/microsoft/windows-docs-rs) for the Windows API. The documentation [is published here](https://microsoft.github.io/windows-docs-rs/). This can be useful to figure out how the various Windows APIs map to Rust modules and which `use` paths you need to use from within the `build` macro.

More examples [can be found here](examples). Robert Mikhayelyan's [Minesweeper](https://github.com/robmikh/minesweeper-rs) is also a great example.

A more in-depth getting started guide can also be found [here](docs/getting-started.md).
"
245,microsoft/tabster,TypeScript,"# Tabster
*Tabindex on steroids.*

A set of tools and concepts for making a dynamic web application properly accessible and keyboard-navigable.

## About

*This project is pretty much in a work-in-progress proof-of-concept state. More docs and examples are to come.*

The way a browser and the screen readers handle a web application is evolved from the static web era. A process of making a modern dynamic web application accessible presents a number of challenges like, for example, the proper focus management between modal dialogs, popups, lists and other parts of the dynamically changing application. This project is an attempt to solve some of those challenges.

## Dependencies

This project is framework-agnostic. It operates on the DOM level and has no external runtime dependencies. Though it is possible that your framework or application might have own logic to achieve similar result, in that case runtime conflicts and behavioural inconsistencies are definitely possible. At the same time, it does not do things automatically and parts of it should be explicitly enabled.

## Parts

### Focusable

An API for traversing focusable elements.

### Deloser

When you remove, for example, a button which has focus from the DOM, the focus gets lost which is confusing for the screen reader and keyboard navigation users. Deloser is a concept which helps to automatically restore the focus when it gets lost without manually calling `.focus()` method from the application code.

### FocusedElementState

An event and a couple of methods to track and change currently focused element.

### KeyboardNavigationState

An event and a method to determine if the user is using keyboard to navigate through the application.

### Groupper

Keyboard navigation for the lists should allow to avoid going through every list item when the users use Tab key (only one item of the list should be tabbable), also the arrow keys and Home/End/PageUp/PageDown keys should be handled to move between the list items. This is an API to easily make properly behaving lists.

### Modalizer

When you show, for example, a modal dialog, the rest of the application might need to be excluded from the keyboard and screen reader navigation flow. Modalizer is a concept to conveniently make that possible.

### Outline

When people navigate with the keyboard, the currently focused element should be properly highlighted. There is a CSS property called `outline`, which is unfortunately insufficient: the outline of an element gets cropped when a parent element has `overflow: hidden`, there is no way to limit the outline visibility to only the cases when the user is navigating with keyboard. So, we have a custom outline component which is supposed to solve both of the problems.

## Contributing

Contributions are welcome (see the [CONTRIBUTING](./CONTRIBUTING.md) file), though please keep in mind the work-in-progress proof-of-concept state. Might make sense to just observe/discuss until the thing gets stable and well-documented.

The repo now has an examples project powered by Storybook. Just run `npm start`

## License
This project is licensed under the MIT License, see the [LICENSE](LICENSE) file for details.
"
246,microsoft/fluentui-android,Kotlin,"# Fluent UI for Android

##### The Android UI framework for building experiences for Office and Office 365.

Fluent UI for Android is a native library that provides the Office UI experience for the Android platform. It contains information about colors and typography, as well as custom controls and customizations for platform controls, all from the official Fluent design language used in Office and Office 365 products.


### Build status (master branch)

| Build Service   | Status                                                                                                                                                                                                                                                           |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| App Center      | [![Build status](https://build.appcenter.ms/v0.1/apps/7acc51be-c1e6-4351-8fa4-c4536fd42dd8/branches/master/badge)](https://appcenter.ms)                                                                                                                         |
| Azure Pipelines | [![Build Status](https://dev.azure.com/microsoftdesign/fluentui-native/_apis/build/status/fluentui-android/fluentui-android-build?branchName=refs%2Fpull%2F3249%2Fmerge)](https://dev.azure.com/microsoftdesign/fluentui-native/_build/latest?definitionId=145&branchName=master) |

## Contents

- [Colors and typography](#colors-and-typography)
- [Controls](#controls)
- [Install and use Fluent UI](#install-and-use-fluent-ui)
- [Demo app](#demo-app)
- [Contributing](#contributing)
- [License](#license)
- [Changelog](#changelog)

## Colors and typography

Fluent UI for Android provides [colors](FluentUI/src/main/res/values/colors.xml) and [typography](FluentUI/src/main/res/values/styles_font.xml) based on the Fluent design language.

## Controls

Fluent UI for Android includes an expanding library of controls written in Kotlin. These controls implement the Fluent design language and bring consistency across Office app experiences.

Some of the controls available include:
- AvatarView
- Button styles
- BottomSheet
- CalendarView
- CircularProgress styles
- DateTimePickerDialog
- Drawer
- ListItemView
- PeoplePickerView
- PersonaChipView
- PersonaListView
- PersonaView
- Snackbar
- TemplateView
- Tooltip

A full list of currently supported controls can be found here: [FluentUI](FluentUI/src/main/java/com/microsoft/fluentui).

## Install and use Fluent UI

### Requirements

API 21+

### 1. Using Gradle

- Our library is published through JCenter, so make sure the `jcenter()` repository has been added to your project level build.gradle file (which usually is automatic).

- Inside the dependency block in your build.gradle, add this line for the FluentUI library:
```gradle
dependencies {
    ...
    implementation 'com.microsoft.fluentui:FluentUIAndroid:$version'
    ...
}
```
- Make sure you replace `$version` with the latest version of FluentUI.

#### a) Develop for Surface-Duo:
- Please also add the following lines to your repositories section in your gradle script:
```gradle
maven {
    url ""https://pkgs.dev.azure.com/MicrosoftDeviceSDK/DuoSDK-Public/_packaging/Duo-SDK-Feed/maven/v1""
}
```
- Also add the SDK dependency to the module-level build.gradle file(current version may be  different
from what's shown here):
```gradle
implementation ""com.microsoft.device:dualscreen-layout:1.0.0-alpha01""
```

### 2. Using Maven

- Add the FluentUI library as a dependency:
```xml
<dependency>
  <groupId>com.microsoft.fluentui</groupId>
  <artifactId>FluentUIAndroid</artifactId>
  <version>${version}</version>
</dependency>
```

- Make sure you replace `${version}` with the latest version of FluentUI.

### 3. Manual installation

- Download the latest changes from the [Fluent UI Android](https://github.com/microsoft/fluentui-android) repository.

- Follow [these instructions](https://developer.android.com/studio/projects/android-library) to build and output an AAR file from the FluentUI module, import the module to your project, and add it as a dependency. If you're having trouble generating an AAR file for the module, make sure you select it and run ""Make Module 'FluentUI'"" from the Build menu.

- Some components have dependencies you will need to manually add to your app if you are using this library as an AAR artifact because these dependencies do not get included in the output.
  - If using **PeoplePickerView**, include this dependency in your gradle file:
    ```gradle
    implementation 'com.splitwise:tokenautocomplete:2.0.8'
    ```
  - If using **CalendarView** or **DateTimePickerDialog**, include this dependency in your gradle file:
    ```gradle
    implementation 'com.jakewharton.threetenabp:threetenabp:1.1.0'
    ```
  - Double check that these library versions correspond to the latest versions we implement in the FluentUI [build.gradle](FluentUI/build.gradle).

### Import and use the library

In code:
```kotlin
import com.microsoft.fluentui.persona.AvatarView
```

In XML:
```xml
<com.microsoft.fluentui.persona.AvatarView
    android:layout_width=""wrap_content""
    android:layout_height=""wrap_content""
    app:name=""Mona Kane"" />
```

## Demo app

Included in this repository is a demo of currently implemented controls. A full list of implemented controls available in the demo can be found here:  [Demos](FluentUI.Demo/src/main/java/com/microsoft/fluentuidemo/demos).

To see samples of all of our implemented controls and design language, run the [FluentUI.Demo](FluentUI.Demo) module in Android Studio.

## Contributing

Post bug reports, feature requests, and questions in [Issues](https://github.com/microsoft/fluentui-android/issues).

## Changelog

We use [GitHub Releases](https://github.com/blog/1547-release-your-software) to manage our releases, including the changelog between every release. You'll find a complete list of additions, fixes, and changes on the [Releases page](https://github.com/microsoft/fluentui-android/releases).

## License

All files on the Fluent UI for Android GitHub repository are subject to the MIT license. Please read the [LICENSE](LICENSE) file at the root of the project.

Usage of the logos and icons referenced in Fluent UI for Android is subject to the terms of the [assets license agreement](https://aka.ms/fabric-assets-license).
"
247,microsoft/Windows-AppConsult-Samples-UWP,C#,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
248,microsoft/powerquery-language,TypeScript,"# Overview

This repository stores the .tmLanguage file for the Power Query / M language.

## Usage

The Power Query TextMate grammar file ([PowerQuery.tmLanguage](https://raw.githubusercontent.com/Microsoft/powerquery-language/master/PowerQuery.tmLanguage)) can be consumed directly.

### Update and test

Edit the PowerQuery.YAML-tmLanguage file. Running the build process will generate the PowerQuery.tmLanguage file.

Run the following to build and test:

```
npm install
npm run-script build
npm test
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
249,microsoft/ApplicationInsights-node.js-native-metrics,C++,"# ApplicationInsights Node.js Native Metrics
Native Metrics Agent for the Application Insights Node.js SDK

## Getting Started
Your app must be using the [Application Insights Node.js SDK](https://github.com/microsoft/applicationinsights-node.js).

Once your app is using the Application Insights SDK, you can add native metrics capabilities by simply adding the native-metrics package to your app.
No further configuration is required!
```zsh
npm i --save applicationinsights-native-metrics
```

## Contributing
For details on contributing to this repository, see the [contributing guide](https://github.com/microsoft/ApplicationInsights-node.js-native-metrics/master/CONTRIBUTING.md).

This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
250,microsoft/CBL-Mariner,Go,"# CBL-Mariner

| Release Branch | Status                                                                                                                                                                                                 |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1.0            | [![1.0 Status](https://github.com/microsoft/CBL-Mariner/workflows/Verify%20Quickstart%201.0/badge.svg)](https://github.com/microsoft/CBL-Mariner/actions?query=workflow%3A%22Verify+Quickstart+1.0%22) |

CBL-Mariner is an internal Linux distribution for Microsoft’s cloud infrastructure and edge products and services. CBL-Mariner is designed to provide a consistent platform for these devices and services and will enhance Microsoft’s ability to stay current on Linux updates. This initiative is part of Microsoft’s increasing investment in a wide range of Linux technologies, such as [SONiC](https://azure.microsoft.com/en-us/blog/sonic-the-networking-switch-software-that-powers-the-microsoft-global-cloud/), [Azure Sphere OS](https://docs.microsoft.com/en-us/azure-sphere/product-overview/what-is-azure-sphere) and [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/about). CBL-Mariner is being shared publicly as part of Microsoft’s commitment to Open Source and to contribute back to the Linux community. CBL-Mariner does not change our approach or commitment to any existing third-party Linux distribution offerings. 

CBL-Mariner has been engineered with the notion that a small common core set of packages can address the universal needs of first party cloud and edge services while allowing individual teams to layer additional packages on top of the common core to produce images for their workloads. This is made possible by a simple build system that enables:

- **Package Generation:** This produces the desired set of RPM packages from SPEC files and source files. 
- **Image Generation:** This produces the desired image artifacts like ISOs or VHDs from a given set of packages. 

Whether deployed as a container or a container host, CBL-Mariner consumes limited disk and memory resources. The lightweight characteristics of CBL-Mariner also provides faster boot times and a minimal attack surface. By focusing the features in the core image to just what is needed for our internal cloud customers there are fewer services to load, and fewer attack vectors. 

When security vulnerabilities arise, CBL-Mariner supports both a package-based update model and an image based update model.  Leveraging the common [RPM Package Manager](https://rpm.org/) system, CBL-Mariner makes the latest security patches and fixes available for download with the goal of fast turn-around times.   

# Getting Started with CBL-Mariner: 

Instructions for building CBL-Mariner may be found here: [Toolkit Documentation](./toolkit/README.md)

# Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.

# Acknowledgments 

Any Linux distribution, including CBL-Mariner, benefits from contributions by the open software community. We gratefully acknowledge all contributions made from the broader open source community, in particular:

1) The [Photon OS Project](https://vmware.github.io/photon/) for SPEC files originating from the Photon distribution.   

2) [The Fedora Project](https://start.fedoraproject.org/) for SPEC files, particularly with respect to QT, DNF and several of their dependencies. 

3) [GNU](https://www.gnu.org/) and the [Free Software Foundation](https://www.fsf.org/)

4) [Linux from Scratch](http://www.linuxfromscratch.org)

5) [Openmamba](https://openmamba.org/en/) for SPEC files
"
251,microsoft/vscode-dev-containers,Shell,"# VS Code Remote / GitHub Codespaces Container Definitions

<table style=""width: 100%; border-style: none;""><tr>
<td style=""width: 140px; text-align: center;""><a href=""https://aka.ms/vscode-remote/download/extension""><img width=""128px"" src=""https://microsoft.github.io/vscode-remote-release/images/remote-extensionpack.png"" alt=""Visual Studio Code logo""/></a></td>
<td>
<strong>Visual Studio Code Remote Development and GitHub Codespaces</strong><br />
<i>Open your code in the cloud, in a local container, on a remote machine, or in WSL and take advantage of VS Code's full feature set.
</td>
</tr></table>

A **development container** is a running [Docker](https://www.docker.com) container with a well-defined tool/runtime stack and its prerequisites. The [VS Code Remote - Containers](https://aka.ms/vscode-remote/download/containers) extension and [GitHub Codespaces](https://github.com/features/codespaces) allow you to open or clone code in a local or cloud-hosted dev container and take advantage of VS Code's full development feature set.

This repository contains a set of **dev container definitions** to help get you up and running with a containerized environment. The definitions describe the appropriate container image, runtime arguments for starting the container, and VS Code extensions that should be installed. Each provides a container configuration file (`devcontainer.json`) and other needed files that you can drop into any existing folder as a starting point for containerizing your project. You can use the the **Add Development Container Configuration Files...** command to add one to your project or codespace.

The [vscode-remote-try-*](https://github.com/search?q=org%3Amicrosoft+vscode-remote-try-&type=Repositories) repositories may also be of interest if you are looking for complete sample projects.

## Adding a definition to a project or codespace
  
  1. Either [create a codespace for your repository](https://aka.ms/ghcs-open-codespace) or [set up your local machine](https://aka.ms/vscode-remote/containers/getting-started) for use with the Remote - Containers extension, start VS Code, and open your project folder.
  2. Press <kbd>F1</kbd>, and select the **Add Development Container Configuration Files...** command for **Remote-Containers** or **Codespaces**.
  3. Pick one of the recommended definitions from the list or select **Show All Definitions...** to see all of them. You may need to choose the **From a predefined container configuration definition...** option if your project has an existing Dockerfile or Docker Compose file. Answer any questions that appear.
  4. See the definition's `README` for configuration options. A link is available in the `.devcontainer/devcontainer.json` file added to your folder.
  5. Run **Remote-Containers: Reopen in Container** to use it locally, or **Codespaces: Rebuild Container** from within a codespace.

### Adding a definition to a repository

You can share a customized dev container definition for your project by adding the files under `.devcontainer` to source control.

Anyone who then opens a local copy of your repo in VS Code will be prompted to reopen the folder in a container, provided they have the [Remote - Containers](https://aka.ms/vscode-remote/download/containers) extension installed. Additionally, this will be used whenever someone creates a codespace in [GitHub Codespaces](https://github.com/features/codespaces) for the repository.

Your team now has a consistent environment and tool-chain and new contributors or team members can be productive quickly. First-time contributors will require less guidance and there will be fewer issues related to environment setup.

## Sample projects

If you want to try a sample project which already has a dev container, check out one of the following repositories:

- [Node Sample](https://github.com/Microsoft/vscode-remote-try-node)
- [Python Sample](https://github.com/Microsoft/vscode-remote-try-python)
- [Go Sample](https://github.com/Microsoft/vscode-remote-try-go)
- [Java Sample](https://github.com/Microsoft/vscode-remote-try-java)
- [.NET Core Sample](https://github.com/Microsoft/vscode-remote-try-dotnetcore)
- [Rust Sample](https://github.com/microsoft/vscode-remote-try-rust)
- [C++ Sample](https://github.com/microsoft/vscode-remote-try-cpp)
- [PHP Sample](https://github.com/microsoft/vscode-remote-try-php)

## Contents

- [`containers`](containers) - Contains reusable dev container definitions.
- [`script-library`](script-library) - Includes scripts used in this repository to install things. Also useful in your own Dockerfiles.
- [`repository-containers`](repository-containers) - Dev container definitions for working public source code repositories. Only used by Remote - Containers.
- [`container-templates`](container-templates) - Contains templates for creating your own container definitions or to [contribute back](CONTRIBUTING.md#contributing-dev-container-definitions).

## Common Questions

### Can I just reuse an existing container image or Docker / Docker Compose configuration?

Yes! If you have a Dockerfile or Docker Compose file in your project/repository, follow the [same steps to add a definition](#adding) and you'll be prompted to select a Dockerfile or Docker Compose file and customize from there. If you then commit these files to a Git repository, you can use it with [GitHub Codespaces](https://github.com/features/codespaces) as well. If you prefer, you can also start up the container manually and [attach to it](https://aka.ms/vscode-remote/containers/attach). However, note that many images will be missing things like `git` that you will want to use. There are scripts in the [script-library](script-library) like the [common script](script-library/docs/common.md) that can help adding these to your existing Dockerfile or image.

### What is the goal of `devcontainer.json`?

A `devcontainer.json` file is similar to `launch.json` for debugging, but designed to launch (or attach to) a development container instead. At its simplest, all you need is a `.devcontainer/devcontainer.json` file in your project that references an image, `Dockerfile`, or `docker-compose.yml`, and a few properties. You can [adapt it for use](https://aka.ms/vscode-remote/containers/folder-setup) in a wide variety of situations.

### Why do Dockerfiles in this repo use `RUN` statements with commands separated by `&&`?

Each `RUN` statement creates a Docker image ""layer"". If one `RUN` statement adds temporary contents, these contents remain in this layer in the image even if they are deleted in a subsequent `RUN`. This means the image takes more storage locally and results in slower image download times if you publish the image to a registry. You can resolve this problem by using a `RUN` statement that includes any clean up steps (separated by `&&`) after a given operation. See [CONTRIBUTING.md](./CONTRIBUTING.md#why-do-dockerfiles-in-this-repository-use-run-statements-with-commands-separated-by-) for more tips.

## Contributing and feedback

Have a question or feedback?

- Contribute or provide feedback for the [VS Code Remote](https://github.com/Microsoft/vscode-remote-release/blob/master/CONTRIBUTING.md) extensions or [GitHub Codespaces](https://github.com/github/feedback/discussions/categories/codespaces-feedback).
- Search [existing issues](https://github.com/Microsoft/vscode-dev-containers/issues) with dev container definitions or [report a problem](https://github.com/Microsoft/vscode-dev-containers/issues/new).
- Contribute a [development container definition](CONTRIBUTING.md#contributing-dev-container-definitions) to the repository.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Copyright (c) Microsoft Corporation. All rights reserved. <br />
Licensed under the MIT License. See [LICENSE](LICENSE).

For images generated from this repository, see [LICENSE](https://github.com/microsoft/containerregistry/blob/master/legal/Container-Images-Legal-Notice.md) and [NOTICE.txt](NOTICE.txt).
"
252,microsoft/beachball,TypeScript,"<!--
If making changes, don't forget to update the version under packages/beachball/README.md too!
-->

# beachball

the sunniest version bumping tool

## Prerequisites

git and a remote named ""origin""

## Usage

```
beachball [command] [options]
```

## Commands

### change (default)

a tool to help create change files in the change/ folder

### check

checks whether a change file is needed for this branch

### changelog

based on change files, create changelogs and then unlinks the change files

### bump

bumps versions as well as generating changelogs

### publish

bumps, publishes to npm registry (optionally does dist-tags), and pushes changelogs back into master

### sync

synchronizes published versions of packages from a registry, makes local package.json changes to match what is published

## Options

### --config, -c

Explicit configuration file to use instead of the configuration automatically detected by cosmicconfig.

### --registry, -r

registry, defaults to https://registry.npmjs.org

### --tag, -t

- for the publish command: dist-tag for npm publishes
- for the sync command: will use specified tag to set the version

### --branch, -b

target branch from origin (default: master)

### --message, -m

custom message for the checkin (default: applying package updates)

### --no-push

skip pushing changes back to git remote origin

### --no-publish

skip publishing to the npm registry

### --help, -?, -h

show help message

### --yes, -y

skips the prompts for publish

## Examples

```
  $ beachball

  $ beachball check

  $ beachball publish -r http://localhost:4873 -t beta
```

<!--
If making changes, don't forget to update the version under packages/beachball/README.md too!
-->
"
253,microsoft/vscode-test,TypeScript,"# vscode-test

![Test Status Badge](https://github.com/microsoft/vscode-test/workflows/Tests/badge.svg)

This module helps you test VS Code extensions.

Supported:

- Node >= 12.x
- Windows >= Windows Server 2012+ / Win10+ (anything with Powershell >= 5.0)
- macOS
- Linux

## Usage

See [./sample](./sample) for a runnable sample, with [Azure DevOps Pipelines](https://github.com/microsoft/vscode-test/blob/master/sample/azure-pipelines.yml) and [Travis CI](https://github.com/microsoft/vscode-test/blob/master/.travis.yml) configuration.

```ts
async function go() {
	try {
		const extensionDevelopmentPath = path.resolve(__dirname, '../../../')
		const extensionTestsPath = path.resolve(__dirname, './suite')

		/**
		 * Basic usage
		 */
		await runTests({
			extensionDevelopmentPath,
			extensionTestsPath
		})

		const extensionTestsPath2 = path.resolve(__dirname, './suite2')
		const testWorkspace = path.resolve(__dirname, '../../../test-fixtures/fixture1')

		/**
		 * Running another test suite on a specific workspace
		 */
		await runTests({
			extensionDevelopmentPath,
			extensionTestsPath: extensionTestsPath2,
			launchArgs: [testWorkspace]
		})

		/**
		 * Use 1.36.1 release for testing
		 */
		await runTests({
			version: '1.36.1',
			extensionDevelopmentPath,
			extensionTestsPath,
			launchArgs: [testWorkspace]
		})

		/**
		 * Use Insiders release for testing
		 */
		await runTests({
			version: 'insiders',
			extensionDevelopmentPath,
			extensionTestsPath,
			launchArgs: [testWorkspace]
		})

		/**
		 * Noop, since 1.36.1 already downloaded to .vscode-test/vscode-1.36.1
		 */
		await downloadAndUnzipVSCode('1.36.1')

		/**
		 * Manually download VS Code 1.35.0 release for testing.
		 */
		const vscodeExecutablePath = await downloadAndUnzipVSCode('1.35.0')
		await runTests({
			vscodeExecutablePath,
			extensionDevelopmentPath,
			extensionTestsPath,
			launchArgs: [testWorkspace]
		})

		/**
		 * Install Python extension
		 */
		const cliPath = resolveCliPathFromVSCodeExecutablePath(vscodeExecutablePath)
		cp.spawnSync(cliPath, ['--install-extension', 'ms-python.python'], {
			encoding: 'utf-8',
			stdio: 'inherit'
		})

		/**
		 * - Add additional launch flags for VS Code
		 * - Pass custom environment variables to test runner
		 */
		await runTests({
			vscodeExecutablePath,
			extensionDevelopmentPath,
			extensionTestsPath,
			launchArgs: [
				testWorkspace,
				// This disables all extensions except the one being tested
				'--disable-extensions'
			],
			// Custom environment variables for extension test script
			extensionTestsEnv: { foo: 'bar' }
		})

		/**
		 * Use win64 instead of win32 for testing Windows
		 */
		if (process.platform === 'win32') {
			await runTests({
				extensionDevelopmentPath,
				extensionTestsPath,
				version: '1.40.0',
				platform: 'win32-x64-archive'
			});
		}

	} catch (err) {
		console.error('Failed to run tests')
		process.exit(1)
	}
}

go()
```

## Development

- `yarn install`
- Make necessary changes in [`lib`](./lib)
- `yarn compile` (or `yarn watch`)
- In [`sample`](./sample), run `yarn install`, `yarn compile` and `yarn test` to make sure integration test can run successfully

## License

[MIT](LICENSE)

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
254,microsoft/terraform-provider-azuredevops,Go,"# Terraform Provider for Azure DevOps (Devops Resource Manager)

[![Gitter](https://badges.gitter.im/terraform-provider-azuredevops/community.svg)](https://gitter.im/terraform-provider-azuredevops/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![Go Report Card](https://goreportcard.com/badge/github.com/microsoft/terraform-provider-azuredevops)](https://goreportcard.com/report/github.com/microsoft/terraform-provider-azuredevops)

The AzureRM Provider supports Terraform 0.12.x and later.

* [Terraform Website](https://www.terraform.io)
* [Azure DevOps Website](https://azure.microsoft.com/en-us/services/devops/)
* [Provider Documentation](./website/docs/index.html.markdown)
* [Resources Documentation](./website/docs/r/)
* [Data Sources Documentation](./website/docs/d/)
* [Usage Examples](./examples/)
* [Gitter Channel](https://gitter.im/terraform-provider-azuredevops/community)

## Usage Example

```hcl
# Make sure to set the following environment variables:
#   AZDO_PERSONAL_ACCESS_TOKEN
#   AZDO_ORG_SERVICE_URL
terraform {
  required_providers {
    azuredevops = {
      source = ""microsoft/azuredevops""
      version = "">=0.1.0""
    }
  }
}

resource ""azuredevops_project"" ""project"" {
  name = ""My Awesome Project""
  description  = ""All of my awesomee things""
}

resource ""azuredevops_git_repository"" ""repository"" {
  project_id = azuredevops_project.project.id
  name       = ""My Awesome Repo""
  initialization {
    init_type = ""Clean""
  }
}

resource ""azuredevops_build_definition"" ""build_definition"" {
  project_id = azuredevops_project.project.id
  name       = ""My Awesome Build Pipeline""
  path       = ""\\""

  repository {
    repo_type   = ""TfsGit""
    repo_id     = azuredevops_git_repository.repository.id
    branch_name = azuredevops_git_repository.repository.default_branch
    yml_path    = ""azure-pipelines.yml""
  }
}
```

## Developer Requirements

* [Terraform](https://www.terraform.io/downloads.html) version 0.13.x +
* [Go](https://golang.org/doc/install) version 1.16.x (to build the provider plugin)

If you're on Windows you'll also need:

* [Git for Windows](https://git-scm.com/download/win)

If you what to use the `makefile` build strategy on Windows it's required to install

* [Make for Windows](http://gnuwin32.sourceforge.net/packages/make.htm)

For *GNU32 Make*, make sure its bin path is added to PATH environment variable.*

For *Git Bash for Windows*, at the step of ""Adjusting your PATH environment"", please choose ""Use Git and optional Unix tools from Windows Command Prompt"".*

As [described below](#build-using-powerShell-scripts) we provide some PowerShell scripts to build the provider on Windows, without the requiremet to install any Unix based tools aside Go.

## Developing the Provider

If you wish to work on the provider, you'll first need [Go](http://www.golang.org) installed on your machine (version 1.16+ is **required**). You'll also need to correctly setup a [GOPATH](http://golang.org/doc/code.html#GOPATH), as well as adding `$GOPATH/bin` to your `$PATH`.

### Using the GOPATH model

First clone the repository to: `$GOPATH/src/github.com/microsoft/terraform-provider-azuredevops`

```sh
$ mkdir -p $GOPATH/src/github.com/terraform-providers && cd ""$_""
$ git clone git@github.com:microsoft/terraform-provider-azuredevops.git
$ cd terraform-provider-azuredevops
```

Once you've cloned, run the `./scripts/build.sh` and `./scripts/local-install.sh`, as recommended [here](https://github.com/microsoft/terraform-provider-azuredevops/blob/master/docs/contributing.md#3-build--install-provider).
These commands will sideload the plugin for Terraform.

### Using a directory separate from GOPATH

The infrastructure supports building and testing the provider outside `GOPATH` in an arbitrary directory.
In this scenario all required packages of the provider during build will be managed via the `pkg` in `$GOPATH`. As with the [GOPATH Model](#using-the-gopath-model), you can redefine the `GOPATH` environment variable to prevent existing packages in the current `GOPATH` directory from being changed.

### Build using make

Once inside the provider directory, you can run `make tools` to install the dependent tooling required to compile the provider.

At this point you can compile the provider by running `make build`, which will build the provider and put the provider binary in the `$GOPATH/bin` directory.

```sh
$ make build
...
$ $GOPATH/bin/terraform-provider-azuredevops
...
```

You can also cross-compile if necessary:

```sh
GOOS=windows GOARCH=amd64 make build
```

#### Unit tests

In order to run the Unit Tests for the provider, you can run:

```sh
$ make test
```

With VSCode Golang extension you can also run and debug the tests using `run test`, `debug test` `run package tests`, `run file tests` buttons.

#### Acceptance tests

The majority of tests in the provider are acceptance tests - which provisions real resources in Azure Devops and Azure. To run any acceptance tests you need to set `AZDO_ORG_SERVICE_URL`, `AZDO_PERSONAL_ACCESS_TOKEN` environment variables, some test have additional environment variables required to run. You can find out the required environment variables by running the test. Most of these variables can be set to dummy values.

The several options to run the tests are:

* Run the entire acceptance test suite

  ```sh
  make testacc
  ```

* Run a subset using a prefix

  ```sh
  make testacc TESTARGS='-run=TestAccBuildDefinitionBitbucket_Create' TESTTAGS='resource_build_definition'
  ```

* With VSCode Golang extension you can also run the tests using `run test`, `run package tests`, `run file tests` buttons above the test

### Build using PowerShell scripts

If you like to develop on Windows, we provide a set of PowerShell scripts to build and test the provider.
They don't offer the luxury of a Makefile environment but are quite sufficient to develop on Windows.

#### `scripts\build.ps1`

The `build.ps1`is used to build the provider. Aside this the script runs (if not skipped) the defined unit tests and is able to install the compiled provider locally.

| Parameter   | Description                                                                               |
| ----------- | ----------------------------------------------------------------------------------------- |
| -SkipTests  | Skip running unit tests during build                                                      |
| -Install    | Install the provider locally, after a successful build                                    |
| -DebugBuild | Build the provider with extra debugging information                                       |
| -GoMod      | Control the `-mod` build parameter: Valid values: '' (Empty string), 'vendor', 'readonly' |

#### `scripts\unittest.ps1`

The script is used to execute unit tests. The script is also executed by `build.ps1` if the `-SkipTest` are not specified.

| Parameter   | Description                                                                                                                       |
| ----------- | --------------------------------------------------------------------------------------------------------------------------------- |
| -TestFilter | A GO regular expression which filters the test functions to be executed                                                           |
| -Tag        | Tests in the provider project are organized with GO build tags. The parameter accepts a list of tag names which should be tested. |
| -GoMod      | Control the `-mod` build parameter: Valid values: '' (Empty string), 'vendor', 'readonly'                                         |

#### `scripts\acctest.ps1`

The script is used to execute unit tests.

| Parameter   | Description                                                                                                                       |
| ----------- | --------------------------------------------------------------------------------------------------------------------------------- |
| -TestFilter | A GO regular expression which filters the test functions to be executed                                                           |
| -Tag        | Tests in the provider project are organized with GO build tags. The parameter accepts a list of tag names which should be tested. |
| -GoMod      | Control the `-mod` build parameter: Valid values: '' (Empty string), 'vendor', 'readonly'                                         |

#### `scripts\gofmtcheck.ps1`

To validate if all `.go` files adhere to the required formatting rules, execute `gofmtcheck.ps1`

| Parameter | Description                                                                                                    |
| --------- | -------------------------------------------------------------------------------------------------------------- |
| -Fix      | Fix any formatting rule deviations automatically. If the parameter is not set, the script runs in report mode. |

#### `scripts\lint-check-go.ps1`

Like with `gofmtcheck.ps1` the script validate if all `.go` files adhere to the required formatting rules and if any style mistakes exist. In difference to `gofmtcheck.ps1` the script uses Golint instead of Gofmt.

## Environment variables for acceptance tests

The following Environment Variables must be set in your shell prior to running acceptance tests:

- `AZDO_ORG_SERVICE_URL`
- `AZDO_PERSONAL_ACCESS_TOKEN`
- `AZDO_DOCKERREGISTRY_SERVICE_CONNECTION_EMAIL`
- `AZDO_DOCKERREGISTRY_SERVICE_CONNECTION_PASSWORD`
- `AZDO_DOCKERREGISTRY_SERVICE_CONNECTION_USERNAME`
- `AZDO_GITHUB_SERVICE_CONNECTION_PAT`
- `AZDO_TEST_AAD_USER_EMAIL`

**Note:** Acceptance tests create real resources in Azure DevOps which often cost money to run.
"
255,microsoft/adaptivecards-templates,JavaScript,"
# Adaptive Cards Template Service

The Adaptive Cards Template Service is a proof-of-concept service that allows anyone to find, contribute to, and share a broad set card templates. Templates are great if you want to display some data but want to save time by not having to write a custom adaptive card for it.

To learn more about Adaptive Cards visit https://adaptivecards.io

Check this our for an [overview of Adaptive Card Templating](https://docs.microsoft.com/en-us/adaptive-cards/templating/)

> *Terms and agreement* 
> 
> This **preview** service is provided ""as-is"", with all faults and is not supported in any way. The service does not store or collect any data beyond the default Azure Function collection. Any data collection from the service is subject to the [Microsoft privacy statement](https://go.microsoft.com/fwlink/?LinkID=824704).
> 
> These features are **in preview and subject to change**. Your feedback is not only welcome, but  critical to ensure we deliver the features **you** need.

## How does the service help me?

Let's say I just got a piece of data, maybe it's financial data, Microsoft Graph data, schema.org data, or custom data from within my organization. 

Now I want to display the data to a user. 

Traditionally that means writing custom UI code in all of the front-end stacks that I deliver to end-users.

But what if there were a world where my app could ""learn"" new UI templates based on the type of data? A world where anyone could contribute, enhance, and share common UI templates, within their own projects, within an organization, or for the entire internet.

## What is the card template service?

The card template service is a simple REST endpoint that helps:

* **Find** a template by analyzing the structure of your data
* **Get** a template so you can bind it directly on the client, *without sending your data to the server or ever leaving the device*
* **Populate** a template on the server, when client-side data binding isn't appropriate or possible

Behind it all, is:

* A shared, open-source template repository backed by GitHub. *(The repo is currently private but will be made public as soon as we tie up some loose ends)*
* All the templates are flat JSON files in the repo, which makes editing, contributing, and sharing a natural part of a developer workflow.
* The code for the service will be made available so you can host wherever makes the most sense to you. 

## Using the service

### Get all templates 

This endpoint returns a list of all known templates.

> `HTTP GET https://templates.adaptivecards.io/list`

**Response excerpt**

```json
{
  ""graph.microsoft.com"": {
    ""templates"": [
      {
        ""file"": ""Files.json"",
        ""fullPath"": ""graph.microsoft.com/Files.json""
      },
      {
        ""file"": ""Profile.json"",
        ""fullPath"": ""graph.microsoft.com/Profile.json""
      }
   ]
}
```

### Find a template

You can find templates one of two ways.

The first is by `POST`ing your data to the endpoint, which will analyze the structure of your data and see if any templates can be found.

> `HTTP POST https://templates.adaptivecards.io/find`

The other option is by passing an `odata.type` via query string, for example:

> `HTTP GET https://templates.adaptivecards.io/find?odata.type=%23microsoft.graph.user`

#### Example

Let's say I just hit a [Microsoft Graph](https://graph.microsoft.com) endpoint to get organizational data about me.

> `HTTP GET https://graph.microsoft.com/v1.0/me/`

![Graph Explorer screenshot](https://docs.microsoft.com/en-us/adaptive-cards/templating/content/2019-08-01-12-08-13.png)

That API returned **JSON data**, but how do I **display it** to users using Adaptive Cards? 

First I want to see if a template exists for this type of data, so I make an HTTP request to the `/find` endpoint with my data in the `POST body`.

```
HTTP POST https://templates.adaptivecards.io/find

{
    ""@odata.context"": ""https://graph.microsoft.com/v1.0/$metadata#users/$entity"",
    ""businessPhones"": [
        ""+1 412 555 0109""
    ],
    ""displayName"": ""Megan Bowen"",
    ""givenName"": ""Megan"",
    ""jobTitle"": ""Auditor"",
    ""mail"": ""MeganB@M365x214355.onmicrosoft.com"",
    ""mobilePhone"": null,
    ""officeLocation"": ""12/1110"",
    ""preferredLanguage"": ""en-US"",
    ""surname"": ""Bowen"",
    ""userPrincipalName"": ""MeganB@M365x214355.onmicrosoft.com"",
    ""id"": ""48d31887-5fad-4d73-a9f5-3c356e68a038""
}
```

**Response:**

```json
[
  {
    ""templateUrl"": ""graph.microsoft.com/Profile.json"",
    ""confidence"": 1
  }
]
```

The service returns a list of any matching templates, along with a `confidence` indicating how close the match is. Now I can use that template URL to **get** the template, or **populate** it server-side.

### Get a template

A template retrieved from this endpoint can be populated with data at runtime [using the templatng SDKs](sdk.md).

> `HTTP GET https://templates.adaptivecards.io/[TEMPLATE-PATH]`

You can also include ""sample data"" with the template, which makes editing in the designer more friendly:

> `HTTP GET https://templates.adaptivecards.io/[TEMPLATE-PATH]?sampleData=true`

#### Example

Let's get the Microsoft Graph profile template that was returned from `/find` above.

`HTTP GET https://templates.adaptivecards.io/graph.microsoft.com/Profile.json`

**Response excerpt**

```json
{
  ""type"": ""AdaptiveCard"",
  ""version"": ""1.0"",
  ""body"": [
    {
      ""type"": ""TextBlock"",
      ""size"": ""Medium"",
      ""weight"": ""Bolder"",
      ""text"": ""{name}""
    },
    {
        // ...snip
    }
  ]
}
```

Now use this template with the [templating SDKs](https://docs.microsoft.com/en-us/adaptive-cards/templating/sdk) to create a ready-to-render Adaptive Card.

### Populate a template server-side

In some cases it may not make sense to populate a template on the client.  For these use cases, you can have the service return a fully-populated Adaptive Card, ready to be passed to any Adaptive Card Renderer.

> `HTTP POST https://templates.adaptivecards.io/[TEMPLATE-PATH]`

#### Example

Let's populate the Microsoft Graph profile template that was returned from `/find` using the data above.

```
HTTP POST https://templates.adaptivecards.io/graph.microsoft.com/Profile.json

{
    ""@odata.context"": ""https://graph.microsoft.com/v1.0/$metadata#users/$entity"",
    ""businessPhones"": [
        ""+1 412 555 0109""
    ],
    ""displayName"": ""Megan Bowen"",
    ""givenName"": ""Megan"",
    ""jobTitle"": ""Auditor"",
    ""mail"": ""MeganB@M365x214355.onmicrosoft.com"",
    ""mobilePhone"": null,
    ""officeLocation"": ""12/1110"",
    ""preferredLanguage"": ""en-US"",
    ""surname"": ""Bowen"",
    ""userPrincipalName"": ""MeganB@M365x214355.onmicrosoft.com"",
    ""id"": ""48d31887-5fad-4d73-a9f5-3c356e68a038""
}
```

**Response excerpt**

```json
{
  ""type"": ""AdaptiveCard"",
  ""version"": ""1.0"",
  ""body"": [
    {
      ""type"": ""TextBlock"",
      ""size"": ""Medium"",
      ""weight"": ""Bolder"",
      ""text"": ""Megan Bowen""
    },
    {
        // ...snip
    }
  ]
}
```

Notice how the response replaced the text of the first `TextBlock` with `""Megan Bowen""` instead of `""{name}""`, as in the `GET` request. This AdaptiveCard can now be passed to any Adaptive Card renderer without going through client-side templating.

## Contributing templates

All templates are stored in this repo, in the `templates` directory. 

Our hope is that by using GitHub as a backing store for the templates, we can ""democratize"" the process of contributing and sharing templates. Anyone can submit a Pull Request that includes an entirely new template, or make enhancements to existing ones... all within the developer-friendly experience of GitHub.

## Self-hosting the service

We realize that not all types of data are appropriate for the ""central"" Adaptive Cards template service hosted at `https://templates.adaptivecards.io`. 

The source code for the service is authored as a Azure Function in TypeScript. You can take the code as-is and deploy it to your own Function. 

## Building templating service

### Install Azure Functions Tools 

**On macOS**, install using Homebrew

```console
$ brew tap azure/functions
$ brew install azure-functions-core-tools
```

**On Windows**, install using npm.

```console
$ npm install -g azure-functions-core-tools
```

**On Linux**, follow the instructions in the Azure Functions Core Tools [GitHub repository](https://github.com/Azure/azure-functions-core-tools#linux).


### Update `local.settings.json` to point to a Storage account

```json
""AzureWebJobsStorage"": ""DefaultEndpointsProtocol=https;AccountName=XXXXXXX;AccountKey=XXXXXXXXXX"",
```


The JSON template files get copied from the Git repo into Blob storage and the Function serves them from there.

### Build and run 

```console
cd src
func extensions sync
```

Press `F5` to run

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see 
the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
256,microsoft/Reference-Guide-For-Quantum-Computing-A-Microsoft-Garage-Project,Jupyter Notebook,"**Reference Guide for Quantum Computing, a Microsoft Garage project**

**A Hands-On Introduction to Quantum Computing**

**Authors: Kitty Yeung, Pavan Kumar, Michael Beverland, Ravindra Ramouthar,** 
**Alex Talarico, Delbert Murphy, Waldemir Cambiucci, Karl Tietze, Benjamin Tokgöz, Alexandra Schroeder**

**Q# exercise: Pavan Kumar, Mariia Mykhailova, Nafisa Barlaskar, Devika Mehra**

**Illustration: Kitty Yeung**

**Reviewers: the book is written by a community of quantum computing enthusiasts and reviewed by individuals at The Microsoft Garage and Quantum Systems teams**


## Introduction <a name=""Introduction"" /> ##

**Phase 1 – Quantum Computing Basic Concepts**

**Kitty Yeung, Ravindra Ramouthar, Alex Talarico, Delbert Murphy**

## 1.0 BACKGROUND

Now two decades into the new millennium, the field of quantum computing has developed
significantly at an industrial scale over the recent years and made tremendous progress in hardware and
software advancements. Companies and governments across the world are competing and investing
heavily to build the first scalable quantum computer to unlock the unprecedented powers promised by
harnessing the new technology. We often read news about its potential and promises and new discoveries
in the field. However, these articles rarely go beyond a major headline followed by a quick mention about
superposition and entanglement. On the other hand, comprehensive textbooks and research publications
often assume professional knowledge in physics and math, which makes it difficult for beginning hobbyists
to understand.

Our goal is to make quantum computing more accessible and easier to learn. Analogous to open-
source tutorials that helped democratize hardware electronics and programming, the tutorials in this
publication aim to achieve the same – helping anyone interested in the subject get started with the basic
concepts of quantum computing and quickly gain hands-on experience programming in a quantum
computing language. When a quantum computer is available, we would already have the necessary tools
and knowledge to utilize it effectively.

The subject is a combination of physics, math, hardware and software, which will all be discussed
holistically without deviating from the big picture. We will demonstrate concepts and definitions with
tools and representations and divide the contents into three phases. Phase 1 will introduce the basics,
such as quantum states, circuit representations, qubits, gates and measurements. If you do not yet know
these terminologies, do not worry. We will define and explain them – this is what this book is for! Phase
2 will use all the tools obtained from Phase 1 and derive step-by-step algorithms that laid milestone
foundations for quantum computing development. Every session in Phase 1 and 2 has corresponding
programming exercises we will present in Q# (Q-sharp) a domain-specific programming language used for
expressing quantum algorithms. The goal is for you to familiarize yourself with the concepts learned. We
will also cover state-of -the-art hardware systems and real-world applications of quantum computing in
Phase 3.

>_Math and Physics inserts_ ----------------------------------------------------------------------------------
>
>Throughout the book, you will find inserts. These are deeper sections than the
>main texts that function as expansions for interested readers. They can be skipped without
>affecting reading of the main texts.


Quantum computing is a practical subject – engineering applying quantum phenomena in building
a computing system that surpasses classical computing capabilities in certain tasks. It is a sub-field of
quantum information science and has its roots founded in the application of quantum mechanics which
is a physics theory describing nature at its smallest constituent scales of atoms and subatomic particles.
Other sub-fields of quantum information science include: quantum teleportation, quantum dense coding,
quantum communication complexity, quantum cryptography, quantum complexity theory, quantum
information theory and quantum error correction.

You may be familiar with the concept of bits in a classical computer, with each bit being in either
a 1 or 0 position, whereas i n a quantum computer, a sequence of qubits is maintained which can represent
a 1, a 0, or any quantum superposition of those two qubit states. A pair of qubits can be in any quantum
superposition of 4 states and three qubits in any superposition of 8 states, and so on. As a result, quantum
computers can store far more information than classical computers and yield the potential to compute at
speeds exponentially higher than classical computers while consuming much less energy. Elemental
particles such as electrons, photons or ions could be used to manifest a qubit, with either their charge or
polarization representing a 0 and/or 1 state.

The basic principles of a quantum computer take place through operations on its qubits via
quantum logic gates. Measurements are typically observed as a probability outcome, as quantum
algorithms are probabilistic. To run a quantum algorithm, the computer's qubits are first initialized, and
operations are performed on these qubits through a sequence of logic gates that take advantage of
quantum mechanics phenomena. Correct solutions manifest themselves through a favorable probability
result. If no measurement is yielded from the algorithm, we call this an unobserved quantum state. To
yield trusted outcomes, quantum computers must often run their algorithms multiples of time.
Specialized systems are designed to be fault-tolerant through higher quantum volume, namely having
more qubits. More qubits equate to more states that can be manipulated and stored and purposed for
error correction. As systems continue to increase their quantum volume, we can expect to see quantum
computers gain a significant advantage over classical counterparts.

Classical computers based on transistor bits have been used to simulate the laws of physics in
hopes to better understand how the universe works. We can simulate how far a ball will go based on
where it starts and how fast it is thrown. Programming the laws of physics into classical computers, albeit
obsolete ones by today's standards, is ultimately what got us on the moon. But using bits to simulate
physics doesn't always make sense, as the laws of physics at the smallest scale are rooted in the rules of
quantum mechanics. Classical computing simply breaks down and cannot process information fast or
efficiently enough to solve or simulate real world nondeterministic models. Quantum computing takes a
giant leap forward from today's technology – one that will forever alter our economic, industrial, academic,
and societal landscape. In just hours or days, a quantum computer will be able to solve complex problems
that would otherwise take billions of years for today's computers to solve. This yields multi-, inter- and
trans-disciplinary implications for research in healthcare, energy, environmental systems, smart materials,
and more. For example, simulations of quantum interactions between chemical molecules, elementary
particles or materials can help us find solutions in pharmaceutical, agricultural or renewable energy
applications. Quantum computing algorithms that solve certain mathematic problems more efficiently
than classical computing can assist with optimization and cryptography.

Quantum computing was incubated in the early 1980s when Richard Feynman and Yuri Manin
postulated that a quantum computer could perform simulations that a classical computer could not. In
1994, Peter Shor published a cryptography algorithm which could efficiently solve some problems
considered hard for classical computers. A quantum computer can be implemented through analog or
digital means through qubits or quantum bits; analog manifestations include quantum simulation,
quantum annealing and adiabatic quantum computation, whilst digital approaches use quantum logic
gates to perform computation.

You may be surprised to learn that the first-generation quantum computers, were not what we
would call computers or new devices at all. Remarkably, it all began with physicists tinkering with
mathematics and biochemistry equipment for curiosity's sake. ""It was not motivated in any way by making
better computers,"" Neil Gershenfeld, director of MIT's Center for Bits and Atoms and a member of one of
the two teams that first experimentally realized quantum algorithms says. ""It was understanding whether
the universe computes, and how the universe computes. (Quote from [The Unlikely Origins of the First
Quantum Computer](https://gizmodo.com/the-unlikely-origins-of-the-first-quantum-computer-1831054476))""

Several independent groups realized that the medical and biochemistry industry had long been
using a 'quantum computer' in research: Nuclear Magnetic Resonance (NMR) spectrometers. This is the
technology behind Magnetic Resonance Imaging (MRI), which commonly consists of a molecule of interest
dissolved in a liquid solvent, placed in a strong magnetic field. The nuclei of the atoms in these molecules
have an innate quantum mechanical property called ""spin,"" which can be in either of two states, ""up"" or
""down"" as we will explore later. These spins align with the direction of the field when hit with additional
smaller oscillating magnetic fields ( radio-frequency pulses) causing the atoms to release characteristic
signals that offer physical information about the molecule. MRI machines use this signal to create a picture,
but the physicists realized by the late 1990's that they could treat certain molecules in this magnetic field
as quantum computers, where the nuclei served as qubits, the spin states were qubit values, and the
radio-frequency pulses were both the instructions and controllers. These are the operations of quantum
computers, also called logic gates as they are in classical computers.

We don't often hear about NMR quantum computers today, because even then, physicists knew
that the technique had its limits. The techniques relied on special workarounds such that each additional
qubit would make it harder to pick the signal out of the background noise, unable to not scale beyond a
few qubits. Despite notable shortcomings, the experiments gave the field the credibility needed to prove
quantum computing hardware viability and pave the way for research and development investments in a
hardware of the future for quantum computing.

At the time of this writing, nearly 20 years after the NMR experiments, we have seen several
companies heavily invested in the development and promotion of commercially available quantum
computing hardware, luring software developers with the promise to most efficiently compute quantum
circuits and algorithms. Companies across the board are periodically joining the quantum race, as
Honeywell recently announced its own quantum computing program for niche industries (see [Investing
in Quantum Computing – The TQD Guide](https://thequantumdaily.com/2020/01/13/investing-in-quantum-computing-the-tqd-guide/)).

Not all quantum computers are created equal, as you might expect, especially in such a fast-paced
landscape. Some manufacturers, namely IBM and Rigetti, claim their systems are truly universal quantum
computers, also known as Quantum Turing Machines (QTM), capable of handling any quantum algorithm.

Other manufacturers, such as D-Wave Systems, focus strictly on optimization problems – travelling
salesman, for example – and are thereby limited to what sorts of processing they can perform.

Although every hardware manufacturer brings forward a unique architecture to their quantum
computing hardware, one common theme emerges: classical computing is not going to be replaced
anytime soon. For the foreseeable future, we will be using a hybrid of classical computing and quantum
computers as each is optimal for different tasks. We leverage our classical computing investments to setup
algorithms and quantum circuits that are then passed along for the quantum computer to resolve and
produce an output our classical computers can then make sense of and display.

As we have seen throughout history, sometimes technological breakthroughs occur by tinkering
with existing equipment. The technology to build a quantum computer today is certain to evolve over the
next decades, as we have just begun to scratch the surface of hardware design.

## EXERCISES

At the end of each chapter, you will find a set of hands-on excercises to practice knowledge learned from that chapter. They are contained in the [QuantumComputingViaQSharpSolution](https://github.com/microsoft/Reference-Guide-For-Quantum-Computing-A-Microsoft-Garage-Project/tree/main/QuantumComputingViaQSharpSolution) folder of this repository. 
"
257,microsoft/PowerBI-visuals-AttributeSlicer,TypeScript,"[![Build Status](https://travis-ci.org/Microsoft/PowerBI-visuals-AttributeSlicer.svg?branch=develop)](https://travis-ci.org/Microsoft/PowerBI-visuals-AttributeSlicer)

# AttributeSlicer

 Attribute Slicer lets you filter a dataset on a given column by selecting attribute values of interest. The initial display is a helpful overview that lists the most common values first and shows the overall distribution of values as a horizontal bar chart. Whenever you select an attribute value, it is moved to the list of applied filters and all records containing that value are added to the result set for further analysis.

 ![Attribute Slicer](/assets/screenshot.png?raw=true)

> This visual is experimental and not actively being developed, only major issues will be addressed.

## Usage
* Install [node.js 6+](https://nodejs.org)
* Install [yarn](https://yarnpkg.com/lang/en/docs/install)
* Run `yarn` on the project directory, which will install all the dependencies
* Run `yarn test` which will lint, test, and compile the `attribute-slicer` and `attribute-slicer-powerbi` packages.
    * Compiling `attribute-slicer-powerbi` will also create a `.pbiviz` file in the `packages/attribute-slicer/powerbi/dist` directory, which can be imported directly in [Power BI](https://app.powerbi.com/)
* Run `yarn start`, which will load the powerbi visual into live reload mode.
"
258,microsoft/tsyringe,TypeScript,"[![Travis](https://img.shields.io/travis/Microsoft/tsyringe.svg)](https://travis-ci.org/Microsoft/tsyringe/)
[![npm](https://img.shields.io/npm/v/tsyringe.svg)](https://www.npmjs.com/package/tsyringe)
[![npm](https://img.shields.io/npm/dt/tsyringe.svg)](https://www.npmjs.com/package/tsyringe)

# TSyringe

A lightweight dependency injection container for TypeScript/JavaScript for
constructor injection.

<!-- TOC depthFrom:1 depthTo:3 -->

- [TSyringe](#tsyringe)
  - [Installation](#installation)
- [API](#api)
  - [Decorators](#decorators)
    - [injectable()](#injectable)
    - [singleton()](#singleton)
    - [autoInjectable()](#autoinjectable)
    - [inject()](#inject)
    - [injectAll()](#injectall)
    - [injectWithTransform()](#injectWithTransform)
    - [injectAllWithTransform()](#injectAllWithTransform)
    - [scoped()](#scoped)
  - [Container](#container)
    - [Injection Token](#injection-token)
    - [Providers](#providers)
    - [Register](#register)
    - [Registry](#registry)
    - [Resolution](#resolution)
    - [Interception](#interception)
    - [Child Containers](#child-containers)
    - [Clearing Instances](#clearing-instances)
- [Circular dependencies](#circular-dependencies)
  - [The `delay` helper function](#the-delay-helper-function)
  - [Interfaces and circular dependencies](#interfaces-and-circular-dependencies)
- [Full examples](#full-examples)
  - [Example without interfaces](#example-without-interfaces)
  - [Example with interfaces](#example-with-interfaces)
  - [Injecting primitive values (Named injection)](#injecting-primitive-values-named-injection)
- [Non goals](#non-goals)
- [Contributing](#contributing)

<!-- /TOC -->

## Installation

Install by `npm`

```sh
npm install --save tsyringe
```

**or** install with `yarn` (this project is developed using `yarn`)

```sh
yarn add tsyringe
```

Modify your `tsconfig.json` to include the following settings

```json
{
  ""compilerOptions"": {
    ""experimentalDecorators"": true,
    ""emitDecoratorMetadata"": true
  }
}
```

Add a polyfill for the Reflect API (examples below use reflect-metadata). You can use:

- [reflect-metadata](https://www.npmjs.com/package/reflect-metadata)
- [core-js (core-js/es7/reflect)](https://www.npmjs.com/package/core-js)
- [reflection](https://www.npmjs.com/package/@abraham/reflection)

The Reflect polyfill import should only be added once, and before DI is used:

```typescript
// main.ts
import ""reflect-metadata"";

// Your code here...
```

### Babel

If you're using Babel (e.g. using React Native), you will need to configure it to emit TypeScript metadata.

First get the Babel plugin

#### Yarn

```
yarn add --dev babel-plugin-transform-typescript-metadata
```

#### npm

```
npm install --save-dev babel-plugin-transform-typescript-metadata
```

Then add it to your Babel config

```
plugins: [
            'babel-plugin-transform-typescript-metadata',
            /* ...the rest of your config... */
         ]
```

# API

TSyringe performs [Constructor Injection](https://en.wikipedia.org/wiki/Dependency_injection#Constructor_injection)
on the constructors of decorated classes.

## Decorators

### injectable()

Class decorator factory that allows the class' dependencies to be injected at
runtime. TSyringe relies on several decorators in order to collect metadata about classes
to be instantiated.

#### Usage

```typescript
import {injectable} from ""tsyringe"";

@injectable()
class Foo {
  constructor(private database: Database) {}
}

// some other file
import ""reflect-metadata"";
import {container} from ""tsyringe"";
import {Foo} from ""./foo"";

const instance = container.resolve(Foo);
```

### singleton()

Class decorator factory that registers the class as a singleton within the
global container.

#### Usage

```typescript
import {singleton} from ""tsyringe"";

@singleton()
class Foo {
  constructor() {}
}

// some other file
import ""reflect-metadata"";
import {container} from ""tsyringe"";
import {Foo} from ""./foo"";

const instance = container.resolve(Foo);
```

### autoInjectable()

Class decorator factory that replaces the decorated class' constructor with
a parameterless constructor that has dependencies auto-resolved.

**Note** Resolution is performed using the global container.

#### Usage

```typescript
import {autoInjectable} from ""tsyringe"";

@autoInjectable()
class Foo {
  constructor(private database?: Database) {}
}

// some other file
import {Foo} from ""./foo"";

const instance = new Foo();
```

Notice how in order to allow the use of the empty constructor `new Foo()`, we
need to make the parameters optional, e.g. `database?: Database`.

### inject()

Parameter decorator factory that allows for interface and other non-class
information to be stored in the constructor's metadata.

#### Usage

```typescript
import {injectable, inject} from ""tsyringe"";

interface Database {
  // ...
}

@injectable()
class Foo {
  constructor(@inject(""Database"") private database?: Database) {}
}
```

### injectAll()

Parameter decorator for array parameters where the array contents will come from the container.
It will inject an array using the specified injection token to resolve the values.

#### Usage

```typescript
import {injectable, injectAll} from ""tsyringe"";

@injectable
class Foo {}

@injectable
class Bar {
  constructor(@injectAll(Foo) fooArray: Foo[]) {
    // ...
  }
}
```

### injectWithTransform

Parameter decorator which allows for a transformer object to take an action on the resolved object
before returning the result.

```typescript
class FeatureFlags {
  public getFlagValue(flagName: string): boolean {
    // ...
}

class Foo() {}

class FeatureFlagsTransformer implements Transform<FeatureFlags, bool> {
  public transform(flags: FeatureFlags, flag: string) {
    return flags.getFlagValue(flag);
  }
}

@injectable()
class MyComponent(foo: Foo, @injectWithTransform(FeatureFlags, FeatureFlagsTransformer, ""IsBlahEnabled"") blahEnabled: boolean){
  // ...
}
```

### injectAllWithTransform

This parameter decorator allows for array contents to be passed through a transformer. The transformer can return any type, so this
can be used to map or fold an array.

```typescript
@injectable
class Foo {
  public value;
}

class FooTransform implements Transform<Foo[], string[]>{
  public transform(foos: Foo[]): string[]{
    return foos.map(f => f.value));
  }
}

@injectable
class Bar {
  constructor(@injectAllWithTransform(Foo, FooTransform) stringArray: string[]) {
    // ...
  }
}
```

### scoped()

Class decorator factory that registers the class as a scoped dependency within the global container.

#### Available scopes

- Transient
  - The **default** registration scope, a new instance will be created with each resolve
- Singleton
  - Each resolve will return the same instance (including resolves from child containers)
- ResolutionScoped
  - The same instance will be resolved for each resolution of this dependency during a single
    resolution chain
- ContainerScoped
  - The dependency container will return the same instance each time a resolution for this dependency
    is requested. This is similar to being a singleton, however if a child container is made, that child
    container will resolve an instance unique to it.

#### Usage

```typescript
@scoped(Lifecycle.ContainerScoped)
class Foo {}
```

## Container

The general principle behind [Inversion of Control](https://en.wikipedia.org/wiki/Inversion_of_control) (IoC) containers
is you give the container a _token_, and in exchange you get an instance/value. Our container automatically figures out the tokens most of the time, with 2 major exceptions, interfaces and non-class types, which require the `@inject()` decorator to be used on the constructor parameter to be injected (see above).

In order for your decorated classes to be used, they need to be registered with the container. Registrations take the
form of a Token/Provider pair, so we need to take a brief diversion to discuss tokens and providers.

### Injection Token

A token may be either a string, a symbol, a class constructor, or a instance of [`DelayedConstructor`](#circular-dependencies).

```typescript
type InjectionToken<T = any> =
  | constructor<T>
  | DelayedConstructor<T>
  | string
  | symbol;
```

### Providers

Our container has the notion of a _provider_. A provider is registered with the DI
container and provides the container the information
needed to resolve an instance for a given token. In our implementation, we have the following 4
provider types:

#### Class Provider

```TypeScript
{
  token: InjectionToken<T>;
  useClass: constructor<T>;
}
```

This provider is used to resolve classes by their constructor. When registering a class provider
you can simply use the constructor itself, unless of course you're making an alias (a
class provider where the token isn't the class itself).

#### Value Provider

```TypeScript
{
  token: InjectionToken<T>;
  useValue: T
}
```

This provider is used to resolve a token to a given value. This is useful for registering
constants, or things that have a already been instantiated in a particular way.

#### Factory provider

```TypeScript
{
  token: InjectionToken<T>;
  useFactory: FactoryFunction<T>;
}
```

This provider is used to resolve a token using a given factory. The factory has full access
to the dependency container.

We have provided 2 factories for you to use, though any function that matches the `FactoryFunction<T>` signature
can be used as a factory:

```typescript
type FactoryFunction<T> = (dependencyContainer: DependencyContainer) => T;
```

##### instanceCachingFactory

This factory is used to lazy construct an object and cache result, returning the single instance for each subsequent
resolution. This is very similar to `@singleton()`

```typescript
import {instanceCachingFactory} from ""tsyringe"";

{
  token: ""SingletonFoo"";
  useFactory: instanceCachingFactory<Foo>(c => c.resolve(Foo));
}
```

##### predicateAwareClassFactory

This factory is used to provide conditional behavior upon resolution. It caches the result by default, but
has an optional parameter to resolve fresh each time.

```typescript
import {predicateAwareClassFactory} from ""tsyringe"";

{
  token: useFactory: predicateAwareClassFactory<Foo>(
    c => c.resolve(Bar).useHttps, // Predicate for evaluation
    FooHttps, // A FooHttps will be resolved from the container if predicate is true
    FooHttp // A FooHttp will be resolved if predicate is false
  );
}
```

#### Token Provider

```TypeScript
{
  token: InjectionToken<T>;
  useToken: InjectionToken<T>;
}
```

This provider can be thought of as a redirect or an alias, it simply states that given token _x_,
resolve using token _y_.

### Register

The normal way to achieve this is to add `DependencyContainer.register()` statements somewhere
in your program some time before your first decorated class is instantiated.

```typescript
container.register<Foo>(Foo, {useClass: Foo});
container.register<Bar>(Bar, {useValue: new Bar()});
container.register<Baz>(""MyBaz"", {useValue: new Baz()});
```

#### Registration options

As an optional parameter to `.register()` you may provide [`RegistrationOptions`](./src/types/registration-options.ts)
which customize how the registration behaves. See the linked source code for up to date documentation
on available options.

### Registry

You can also mark up any class with the `@registry()` decorator to have the given providers registered
upon importing the marked up class. `@registry()` takes an array of providers like so:

```TypeScript
@registry([
  { token: Foobar, useClass: Foobar },
  { token: ""theirClass"", useFactory: (c) => {
       return new TheirClass( ""arg"" )
    },
  }
])
class MyClass {}
```

This is useful when you want to [register multiple classes for the same token](#register).
You can also use it to register and declare objects that wouldn't be imported by anything else,
such as more classes annotated with `@registry` or that are otherwise responsible for registering objects.
Lastly you might choose to use this to register 3rd party instances instead of the `container.register(...)` method.
note: if you want this class to be `@injectable` you must put the decorator before `@registry`, this annotation is not
required though.

### Resolution

Resolution is the process of exchanging a token for an instance. Our container will recursively fulfill the
dependencies of the token being resolved in order to return a fully constructed object.

The typical way that an object is resolved is from the container using `resolve()`.

```typescript
const myFoo = container.resolve(Foo);
const myBar = container.resolve<Bar>(""Bar"");
```

You can also resolve all instances registered against a given token with `resolveAll()`.

```typescript
interface Bar {}

@injectable()
class Foo implements Bar {}
@injectable()
class Baz implements Bar {}

@registry([
  // registry is optional, all you need is to use the same token when registering
  {token: ""Bar"", useToken: Foo}, // can be any provider
  {token: ""Bar"", useToken: Baz}
])
class MyRegistry {}

const myBars = container.resolveAll<Bar>(""Bar""); // myBars type is Bar[]
```

### Interception

Interception allows you to register a callback that will be called before or after the resolution of a specific token.
This callback can be registered to execute only once (to perform initialization, for example),
on each resolution to do logging, for example.

`beforeResolution` is used to take an action before an object is resolved.

```typescript
class Bar {}

container.beforeResolution(
  Bar,
  // Callback signature is (token: InjectionToken<T>, resolutionType: ResolutionType) => void
  () => {
    console.log(""Bar is about to be resolved!"");
  },
  {frequency: ""Always""}
);
```

`afterResolution` is used to take an action after the object has been resolved.

```typescript
class Bar {
  public init(): void {
    // ...
  }
}

container.afterResolution(
  Bar,
  // Callback signature is (token: InjectionToken<T>, result: T | T[], resolutionType: ResolutionType)
  (_t, result) => {
    result.init();
  },
  {frequency: ""Once""}
);
```

### Child Containers

If you need to have multiple containers that have disparate sets of registrations, you can create child containers:

```typescript
const childContainer1 = container.createChildContainer();
const childContainer2 = container.createChildContainer();
const grandChildContainer = childContainer1.createChildContainer();
```

Each of the child containers will have independent registrations, but if a registration is absent in the child container at resolution, the token will be resolved from the parent. This allows for a set of common services to be registered at the root, with specialized services registered on the child. This can be useful, for example, if you wish to create per-request containers that use common stateless services from the root container.

### Clearing Instances

The `container.clearInstances()` method allows you to clear all previously created and registered instances:

```typescript
class Foo {}
@singleton()
class Bar {}

const myFoo = new Foo();
container.registerInstance(""Test"", myFoo);
const myBar = container.resolve(Bar);

container.clearInstances();

container.resolve(""Test""); // throws error
const myBar2 = container.resolve(Bar); // myBar !== myBar2
const myBar3 = container.resolve(Bar); // myBar2 === myBar3
```

Unlike with `container.reset()`, the registrations themselves are not cleared.
This is especially useful for testing:

```typescript
@singleton()
class Foo {}

beforeEach(() => {
  container.clearInstances();
});

test(""something"", () => {
  container.resolve(Foo); // will be a new singleton instance in every test
});
```

# Circular dependencies

Sometimes you need to inject services that have cyclic dependencies between them. As an example:

```typescript
@injectable()
export class Foo {
  constructor(public bar: Bar) {}
}

@injectable()
export class Bar {
  constructor(public foo: Foo) {}
}
```

Trying to resolve one of the services will end in an error because always one of the constructor will not be fully defined to construct the other one.

```typescript
container.resolve(Foo);
```

```
Error: Cannot inject the dependency at position #0 of ""Foo"" constructor. Reason:
    Attempted to construct an undefined constructor. Could mean a circular dependency problem. Try using `delay` function.
```

### The `delay` helper function

The best way to deal with this situation is to do some kind of refactor to avoid the cyclic dependencies. Usually this implies introducing additional services to cut the cycles.

But when refactor is not an option you can use the `delay` function helper. The `delay` function wraps the constructor in an instance of `DelayedConstructor`.

The _delayed constructor_ is a kind of special `InjectionToken` that will eventually be evaluated to construct an intermediate proxy object wrapping a factory for the real object.

When the proxy object is used for the first time it will construct a real object using this factory and any usage will be forwarded to the real object.

```typescript
@injectable()
export class Foo {
  constructor(@inject(delay(() => Bar)) public bar: Bar) {}
}

@injectable()
export class Bar {
  constructor(@inject(delay(() => Foo)) public foo: Foo) {}
}

// construction of foo is possible
const foo = container.resolve(Foo);

// property bar will hold a proxy that looks and acts as a real Bar instance.
foo.bar instanceof Bar; // true
```

### Interfaces and circular dependencies

We can rest in the fact that a `DelayedConstructor` could be used in the same contexts that a constructor and will be handled transparently by tsyringe. Such idea is used in the next example involving interfaces:

```typescript
export interface IFoo {}

@injectable()
@registry([
  {
    token: ""IBar"",
    // `DelayedConstructor` of Bar will be the token
    useToken: delay(() => Bar)
  }
])
export class Foo implements IFoo {
  constructor(@inject(""IBar"") public bar: IBar) {}
}
export interface IBar {}

@injectable()
@registry([
  {
    token: ""IFoo"",
    useToken: delay(() => Foo)
  }
])
export class Bar implements IBar {
  constructor(@inject(""IFoo"") public foo: IFoo) {}
}
```

# Full examples

## Example without interfaces

Since classes have type information at runtime, we can resolve them without any
extra information.

```typescript
// Foo.ts
export class Foo {}
```

```typescript
// Bar.ts
import {Foo} from ""./Foo"";
import {injectable} from ""tsyringe"";

@injectable()
export class Bar {
  constructor(public myFoo: Foo) {}
}
```

```typescript
// main.ts
import ""reflect-metadata"";
import {container} from ""tsyringe"";
import {Bar} from ""./Bar"";

const myBar = container.resolve(Bar);
// myBar.myFoo => An instance of Foo
```

## Example with interfaces

Interfaces don't have type information at runtime, so we need to decorate them
with `@inject(...)` so the container knows how to resolve them.

```typescript
// SuperService.ts
export interface SuperService {
  // ...
}
```

```typescript
// TestService.ts
import {SuperService} from ""./SuperService"";
export class TestService implements SuperService {
  //...
}
```

```typescript
// Client.ts
import {injectable, inject} from ""tsyringe"";

@injectable()
export class Client {
  constructor(@inject(""SuperService"") private service: SuperService) {}
}
```

```typescript
// main.ts
import ""reflect-metadata"";
import {Client} from ""./Client"";
import {TestService} from ""./TestService"";
import {container} from ""tsyringe"";

container.register(""SuperService"", {
  useClass: TestService
});

const client = container.resolve(Client);
// client's dependencies will have been resolved
```

## Injecting primitive values (Named injection)

Primitive values can also be injected by utilizing named injection

```typescript
import {singleton, inject} from ""tsyringe"";

@singleton()
class Foo {
  private str: string;
  constructor(@inject(""SpecialString"") value: string) {
    this.str = value;
  }
}

// some other file
import ""reflect-metadata"";
import {container} from ""tsyringe"";
import {Foo} from ""./foo"";

const str = ""test"";
container.register(""SpecialString"", {useValue: str});

const instance = container.resolve(Foo);
```

# Non goals

The following is a list of features we explicitly plan on not adding:

- Property Injection

# Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit [https://cla.microsoft.com](https://cla.microsoft.com).

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
259,microsoft/gatsby-starter-uifabric,TypeScript,"<h1 align=""center"">
  UI Fabric Gatsby.js
</h1>

Kick off your project with this UI Fabric boilerplate. This starter ships with the main Gatsby configuration files you might need to get up and running blazing fast with the blazing fast app generator for React.

## 🚀 Quick start

1.  **Create a Gatsby site.**

    Use the Gatsby CLI to create a new site, specifying the UI Fabric starter.

    ```sh
    # create a new Gatsby site using the UI Fabric starter
    gatsby new my-uifabric-starter https://github.com/microsoft/gatsby-starter-uifabric
    ```

1.  **Start developing.**

    Navigate into your new site’s directory and start it up.

    ```sh
    cd my-uifabric-starter/
    gatsby develop
    ```

1.  **Open the source code and start editing!**

    Your site is now running at `http://localhost:8000`!

    _Note: You'll also see a second link: _`http://localhost:8000/___graphql`_. This is a tool you can use to experiment with querying your data. Learn more about using this tool in the [Gatsby tutorial](https://www.gatsbyjs.org/tutorial/part-five/#introducing-graphiql)._

    Open the `my-uifabric-starter` directory in your code editor of choice and edit `src/pages/index.js`. Save your changes and the browser will update in real time!

## 🧐 What's inside?

A quick look at the top-level files and directories you'll see in a Gatsby project.

    .
    ├── node_modules
    ├── src
    ├── .gitignore
    ├── .prettierrc
    ├── gatsby-browser.js
    ├── gatsby-config.js
    ├── gatsby-node.js
    ├── gatsby-ssr.js
    ├── LICENSE
    ├── package-lock.json
    ├── package.json
    └── README.md

1.  **`/node_modules`**: This directory contains all of the modules of code that your project depends on (npm packages) are automatically installed.

2.  **`/src`**: This directory will contain all of the code related to what you will see on the front-end of your site (what you see in the browser) such as your site header or a page template. `src` is a convention for “source code”.

3.  **`.gitignore`**: This file tells git which files it should not track / not maintain a version history for.

4.  **`.prettierrc`**: This is a configuration file for [Prettier](https://prettier.io/). Prettier is a tool to help keep the formatting of your code consistent.

5.  **`gatsby-browser.js`**: This file is where Gatsby expects to find any usage of the [Gatsby browser APIs](https://www.gatsbyjs.org/docs/browser-apis/) (if any). These allow customization/extension of default Gatsby settings affecting the browser.

6.  **`gatsby-config.js`**: This is the main configuration file for a Gatsby site. This is where you can specify information about your site (metadata) like the site title and description, which Gatsby plugins you’d like to include, etc. (Check out the [config docs](https://www.gatsbyjs.org/docs/gatsby-config/) for more detail).

7.  **`gatsby-node.js`**: This file is where Gatsby expects to find any usage of the [Gatsby Node APIs](https://www.gatsbyjs.org/docs/node-apis/) (if any). These allow customization/extension of default Gatsby settings affecting pieces of the site build process.

8.  **`gatsby-ssr.js`**: This file is where Gatsby expects to find any usage of the [Gatsby server-side rendering APIs](https://www.gatsbyjs.org/docs/ssr-apis/) (if any). These allow customization of default Gatsby settings affecting server-side rendering.

9.  **`LICENSE`**: Gatsby is licensed under the MIT license.

10. **`package-lock.json`** (See `package.json` below, first). This is an automatically generated file based on the exact versions of your npm dependencies that were installed for your project. **(You won’t change this file directly).**

11. **`package.json`**: A manifest file for Node.js projects, which includes things like metadata (the project’s name, author, etc). This manifest is how npm knows which packages to install for your project.

12. **`README.md`**: A text file containing useful reference information about your project.

## 🎓 Learning Gatsby

Looking for more guidance? Full documentation for Gatsby lives [on the website](https://www.gatsbyjs.org/). Here are some places to start:

- **For most developers, we recommend starting with our [in-depth tutorial for creating a site with Gatsby](https://www.gatsbyjs.org/tutorial/).** It starts with zero assumptions about your level of ability and walks through every step of the process.

- **To dive straight into code samples, head [to our documentation](https://www.gatsbyjs.org/docs/).** In particular, check out the _Guides_, _API Reference_, and _Advanced Tutorials_ sections in the sidebar.

## 💫 Deploy

[![Deploy to Azure](http://azuredeploy.net/deploybutton.png)](https://azuredeploy.net/?repository=https://github.com/kenotron/gatsby-starter-uifabric)

[![Deploy to Netlify](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/kenotron/gatsby-starter-uifabric)


# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
260,microsoft/MSMARCO-Document-Ranking-Submissions,Python,"# MS MARCO Document Ranking Submissions

This repo holds the official MS MARCO document ranking leaderboard and describes the process for submitting runs.
All associated data for the task (corpus, training data, eval queries, etc.) are held in [this repo](https://github.com/microsoft/MSMARCO-Document-Ranking).

## Submission Instructions

To make a submission, please follow these instructions:

1. Decide on a submission id, which will be a permanent (public) unique key. The submission id should be of the form `yyyymmdd-foo`, where `foo` can be a suffix of your choice, e.g., your group's name.
Please keep the length reasonable.
See [here](https://github.com/microsoft/MSMARCO-Document-Ranking-Archive/tree/main/submissions) for examples.
`yyyymmdd` should correspond to the submission date of your run.

2. In the directory `submissions/`, create the following files:
   1. `submissions/yyyymmdd-foo/dev.txt.bz2` - run file on the dev queries (`msmarco-docdev-queries.tsv`), bz2-compressed
   2. `submissions/yyyymmdd-foo/eval.txt.bz2` - run file on the eval queries (`docleaderboard-queries.tsv`), bz2-compressed
   3. `submissions/yyyymmdd-foo-metadata.json`, in the following format:

       ```
        {
          ""team"": ""team name"",
          ""model_description"": ""model description"",
          ""paper"": ""url"",              // URL to paper
          ""code"": ""url"",               // URL to code
          ""type"": ""full ranking""       // either 'full ranking' or 'reranking'
        }
       ```
       Leave the value of `paper` and `code` empty (i.e., the empty string) if not available.
       These fields correspond to what is shown on the leaderboard.

3. Run our evaluation script to make sure everything is in order (and fix any errors):
   ```bash
   $ python eval/run_eval.py --id yyyymmdd-foo
   ```

4. Package (i.e., encrypt) the submission using the following script:
   ```bash
   $ eval/pack.sh yyyymmdd-foo
   ```

5. Open a pull request against this repository.
The subject (title) of the pull request should be ""Submission yyyymmdd-foo"", where `yyyymmdd-foo` is the submission id you decided on.
This pull request should contain exactly three files:
   1. `submissions/yyyymmdd-foo.key.bin.enc` - the encrypted key
   2. `submissions/yyyymmdd-foo.tar.enc` - the encrypted tarball
   3. `submissions/yyyymmdd-foo-metadata.json.enc` - the encrypted metadata

**IMPORTANT NOTE:**
You might want to save the _unencrypted_ version of the key you've generated, i.e., `submissions/yyyymmdd-foo.key.bin`.
You'll need it if you want to, for example, change your metadata later on.
If you don't keep it, you'll lose it forever, because the `pack.sh` script generates a random key each time, see [here](https://github.com/microsoft/MSMARCO-Document-Ranking-Submissions/blob/main/eval/pack.sh#L6).

## Additional Submission Guidelines

The goal of the MS MARCO leaderboard is to encourage [coopetition](https://en.wikipedia.org/wiki/Coopetition) (cooperation + competition) among various groups working on deep learning and other methods for search that requires or benefits from large-scale training data.
So, while we encourage friendly competition between different participating groups for top positions on the leaderboard, our core motivation is to ensure that over time the leaderboard provides meaningful scientific insights about how different methods compare to each other and answer questions like whether we are making real progress as a research community.
All participants are requested to abide by this spirit of coopetition and strictly observe good scientific principles when participating.
We will follow an honour system and expect participants to ensure that they are acting in compliance with both the policies and the spirit of this leaderboard.
We will also periodically audit all submissions ourselves and may flag issues as appropriate. 

### Frequency of Submission
The eval set is meant to be a blind set.
We want to discourage modeling decisions based eval numbers to avoid overfitting to the set.
To ensure this, we request participants to submit:

1. No more than 2 runs in any given period of 30 days.
2. No more than 1 run with very small changes, such as different random seeds or different hyper-parameters (e.g., small changes in number of layers or number of training epochs).

Participants who may want to run ablation studies on their models are encouraged to do so on the dev set, but not on the eval set.

### Metadata Updates

The metadata you provide during run submission is meant to be permanent.
However, we do allow ""reasonable"" updates to the metadata as long as it abides by the spirit of the leaderboard (see above).
These reasons might include adding links to a paper or a code repository, fixing typos, clarifying the description of a run, etc.
However, we reserve the right to reject any changes.

It is generally expected that the team description in the metadata file will include the name of the organization (e.g., university or company).
In many cases, submissions explicitly list the contributors of the run.
It is _not_ permissible to submit a run under an alias (or a generic, nondescript team) to first determine ""how you did"", and then ask for a metadata change only after you've been shown to ""do well"".
We will reject metadata change requests in these circumstances.
Thus, you're advised to make the team description as specific as possible, so that you can claim ""credit"" for doing well.

To update the metadata of a particular run, you'll need to encrypt a new metadata JSON file _with the same key_ that you used in the original submission.
The command to encrypt the metadata is [here](https://github.com/microsoft/MSMARCO-Document-Ranking-Submissions/blob/main/eval/pack.sh#L11).
Hopefully, you've saved the key?
If you've lost it, get in touch with us and we'll send you the key back via another channel (e.g., email).
Once you've created a new metadata JSON file (i.e., `submissions/yyyymmdd-foo-metadata.json.enc`), send us a pull request with it.
Please make the subject of the pull request something obvious like ""Metadata change for yyyymmdd-foo"".
Also, please make it clear to us that _you_ have ""permission"" to change the metadata, e.g., the person making the change request is the same person who performed the original submission. 

### Anonymous Submissions

We allow anonymous submissions.
Note that the purpose of an anonymous submission is to support blind reviewing for corresponding publications, not as a probing mechanism to see how well you do, and then only make your identity known if you do well.

Anonymous submissions should still contain accurate team and model information in the metadata JSON file, but on the leaderboard we will anonymize your entry.
By default, we allow an embargo period of anonymous submissions for up to nine months.
That is, after nine months, your identity will be revealed and the leaderboard will be updated accordingly.
Additional extensions to the embargo period based on exceptional circumstances can be discussed on a case-by-case basis; please get in touch with the organizers.

For an anonymous submission, the metadata JSON file should have an additional field:

```
""embargo_until"": ""yyyy/mm/dd""
```

Where the date in `yyyy/mm/dd` format cannot be more than nine months past the submission date.
For example, if the submission date is 2020/11/01, the longest possible embargo period is 2021/07/31.
Of course, you are free to specify a shorter embargo period if you wish.

Note that even with an anonymous submission, the submission id is publicly known, as well as the person performing the submission.
You might consider using a random string as the submission id, and you might consider creating a separate GitHub account for the sole purpose of submitting an anonymous run.
Neither is necessary; we only provide this information for your reference.


## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft's general trademark guidelines can be found at http://go.microsoft.com/fwlink/?LinkID=254653.

Privacy information can be found at https://privacy.microsoft.com/en-us/

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel or otherwise.
"
261,microsoft/IgniteTheTour,C#,"
# Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
262,microsoft/react-popout-component,TypeScript,"# React Popout Component

[![Build Status](https://travis-ci.org/Microsoft/react-popout-component.svg?branch=v1.0.0)](https://travis-ci.org/Microsoft/react-popout-component) [![npm](https://img.shields.io/npm/v/react-popout-component.svg)](https://www.npmjs.com/package/react-popout-component)

This is a React component designed for React 16 with complete Typescript support.

## Features

1. This is developed along side with the React 16 fix to allow mounting across frames *even for Edge and IE* browsers
2. Typescript support for all the options (especially hard to remember window features)
3. Reflects style-loader injected styles from the main window to the children window

## Installation

```sh
npm install react-popout-component
```

or

```sh
yarn add react-popout-component
```

## Usage

```tsx
import * as React from 'react';
import {Popout} from 'react-popout-component';

export default class App extends React.Component<any, any> {
    constructor(props: any) {
        super(props);
        this.state = {showPopout: false};
    }

    onClick = () => {
        this.setState({showPopout: true});
    }

    render() {
        return (
            <div>
                <h1>Now you too have the power to POP OUT</h1>
                <button onClick={this.onClick}>POP IT OUT!</button>
                {this.state.showPopout && (
                    <Popout>
                        <div>You can put anything here!</div>
                    </Popout>
                )}
            </div>
        );
    }
}

```

## API

PopOut Component has the following props:

```ts
export interface PopoutProps {
    hidden?: boolean;
    name?: string;
    onClose?: () => void;
    onBeforeUnload?: (evt: BeforeUnloadEvent) => string | null | undefined;
    children?: any;
    options?: Partial<WindowFeaturesOptions>;
    html?: string;
}
```

The `options` prop is of the following type:

```ts
export interface WindowFeaturesOptions {
    left: number;
    top: number;
    height: number;
    width: number;
    menubar: boolean;
    toolbar: boolean;
    location: boolean;
    status: boolean;
    resizable: boolean;
    scrollbars: boolean;
}
```

## Injection Mode

This component works well for both modes of style loading:
1. Appending Style blocks (e.g. style-loader)
2. Manual insertRule() into a CSSStyleSheet

For the second case with insertRule(), since there is nothing that can observe the insert event, a callback must be registered when a
rule is inserted. For an example usage with the Microsoft [Office Fabric](https://github.com/officedev/office-ui-fabric-react), 
set it up as a global like so:

```js
import {insertPopoutStylesheetRule} from 'react-popout-component';

window.FabricConfig = {
    mergeStyles: {
        onInsertRule: insertPopoutStylesheetRule
    }
}
```

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.
"
263,microsoft/DeepSpeed,Python,"[![Build Status](https://github.com/microsoft/deepspeed/workflows/Build/badge.svg)](https://github.com/microsoft/DeepSpeed/actions)
[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)
[![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)
[![License MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)
[![Downloads](https://pepy.tech/badge/deepspeed/month)](https://pepy.tech/project/deepspeed)

### 03/2021: DeepSpeed is hiring! Come join us: [SDE 2](https://careers.microsoft.com/us/en/job/1013160/Software-Engineer-2), [Sr. SDE](https://careers.microsoft.com/us/en/job/1017151/Senior-Software-Engineer), [Sr. Researcher](https://careers.microsoft.com/us/en/job/1016440/Senior-Researcher)

[DeepSpeed](https://www.deepspeed.ai/) is a deep learning optimization
library that makes distributed training easy, efficient, and effective.

<p align=""center""><i><b>10x Larger Models</b></i></p>
<p align=""center""><i><b>10x Faster Training</b></i></p>
<p align=""center""><i><b>Minimal Code Change</b></i></p>

DeepSpeed delivers extreme-scale model training for everyone, from data scientists training on massive supercomputers to those training on low-end clusters or even on a single GPU:
* Extreme scale: Using current generation of GPU clusters with hundreds of devices,  3D parallelism of DeepSpeed can efficiently train deep learning models with trillions of parameters.  
* Extremely memory efficient: With just a single GPU, ZeRO-Offload of DeepSpeed can train models with over 10B parameters, 10x bigger than the state of arts, democratizing multi-billion-parameter model training such that many deep learning scientists can explore bigger and better models.
* Extremely long sequence length: Sparse attention of DeepSpeed powers an order-of-magnitude longer input sequence and obtains up to 6x faster execution comparing with dense transformers.  
* Extremely communication efficient: 3D parallelism improves communication efficiency allows users to train multi-billion-parameter models 2–7x faster on clusters with limited network bandwidth.  1-bit Adam/1-bit LAMB reduce communication volume by up to 5x while achieving similar convergence efficiency to Adam/LAMB, allowing for scaling to different types of GPU clusters and networks.

Early adopters of DeepSpeed have already produced
a language model (LM) with over 17B parameters called
[Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft),
establishing a new SOTA in the LM category.

DeepSpeed is an important part of Microsoft’s new
[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)
initiative to enable next-generation AI capabilities at scale, where you can find more
information [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).

**_For further documentation, tutorials, and technical deep-dives please see [deepspeed.ai](https://www.deepspeed.ai/)!_**


# News
* [2021/04/20] [1-bit LAMB: up to 4.6x less communication and 2.8x faster training, together with LAMB's convergence speed at large batch sizes](https://www.deepspeed.ai/tutorials/onebit-lamb/)
* [2021/04/19] [ZeRO-Infinity unlocks unprecedented model scale for deep learning training](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)
  * [Tutorial on how to use different stages of ZeRO](https://www.deepspeed.ai/tutorials/zero/)
* [2021/04/01] [[DeepSpeed on AzureML] Transformers and CIFAR examples are now available on AzureML GitHub](https://github.com/Azure/azureml-examples/tree/main/workflows/train/deepspeed)
* [2021/03/30] [[PyTorch Lightning Blog] Accessible Multi-Billion Parameter Model Training with PyTorch Lightning + DeepSpeed](https://medium.com/pytorch-lightning/accessible-multi-billion-parameter-model-training-with-pytorch-lightning-deepspeed-c9333ac3bb59)
* [2021/03/16] [1-bit Adam v2: NCCL-based implementation and more](https://www.deepspeed.ai/tutorials/onebit-adam/)
* [2021/03/08] [ZeRO-3 Offload: Scale your models to trillion parameters without code changes while leveraging both CPUs & GPUs](https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html)
* [2021/01/19] [[🤗Hugging Face Blog] Fit More and Train Faster With ZeRO via DeepSpeed and FairScale](https://huggingface.co/blog/zero-deepspeed-fairscale)
* [2020/11/12] [Simplified install, JIT compiled ops, PyPI releases, and reduced dependencies](#installation)
* [2020/11/10] [Efficient and robust compressed training through progressive layer dropping](https://www.deepspeed.ai/news/2020/10/28/progressive-layer-dropping-news.html)
* [2020/09/10] [DeepSpeed v0.3: Extreme-scale model training for everyone](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)


# Table of Contents
| Section                                 | Description                                 |
| --------------------------------------- | ------------------------------------------- |
| [Why DeepSpeed?](#why-deepspeed)        |  DeepSpeed overview                         |
| [Install](#installation)                |  Installation details                       |
| [Features](#features)                   |  Feature list and overview                  |
| [Further Reading](#further-reading)     |  Documentation, tutorials, etc.             |
| [Contributing](#contributing)           |  Instructions for contributing              |
| [Publications](#publications)           |  Publications related to DeepSpeed          |
| [Videos](#videos)                       |  Videos related to DeepSpeed                |

# Why DeepSpeed?
Training advanced deep learning models is challenging. Beyond model design,
model scientists also need to set up the state-of-the-art training techniques
such as distributed training, mixed precision, gradient accumulation, and
checkpointing. Yet still, scientists may not achieve the desired system
performance and convergence rate. Large model sizes are even more challenging:
a large model easily runs out of memory with pure data parallelism and it is
difficult to use model parallelism. DeepSpeed addresses these challenges to
accelerate model development *and* training.

# Installation

The quickest way to get started with DeepSpeed is via pip, this will install
the latest release of DeepSpeed which is not tied to specific PyTorch or CUDA
versions. DeepSpeed includes several C++/CUDA extensions that we commonly refer
to as our 'ops'.  By default, all of these extensions/ops will be built
just-in-time (JIT) using [torch's JIT C++ extension loader that relies on
ninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and
dynamically link them at runtime.

**Note:** [PyTorch](https://pytorch.org/) must be installed _before_ installing
DeepSpeed.

```bash
pip install deepspeed
```

After installation, you can validate your install and see which extensions/ops
your machine is compatible with via the DeepSpeed environment report.

```bash
ds_report
```

If you would like to pre-install any of the DeepSpeed extensions/ops (instead
of JIT compiling) or install pre-compiled ops via PyPI please see our [advanced
installation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).

# Features
Below we provide a brief feature list, see our detailed [feature
overview](https://www.deepspeed.ai/features/) for descriptions and usage.

* [Distributed Training with Mixed Precision](https://www.deepspeed.ai/features/#distributed-training-with-mixed-precision)
  * 16-bit mixed precision
  * Single-GPU/Multi-GPU/Multi-Node
* [Model Parallelism](https://www.deepspeed.ai/features/#model-parallelism)
  * Support for Custom Model Parallelism
  * Integration with Megatron-LM
* [Pipeline Parallelism](https://www.deepspeed.ai/tutorials/pipeline/)
  * 3D Parallelism
* [The Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)
  * Optimizer State and Gradient Partitioning
  * Activation Partitioning
  * Constant Buffer Optimization
  * Contiguous Memory Optimization
* [ZeRO-Offload](https://www.deepspeed.ai/tutorials/zero-offload/)
  * Leverage both CPU/GPU memory for model training
  * Support 10B model training on a single GPU
* [Ultra-fast dense transformer kernels](https://www.deepspeed.ai/news/2020/05/18/bert-record.html)
* [Sparse attention](https://www.deepspeed.ai/news/2020/09/08/sparse-attention.html)
  * Memory- and compute-efficient sparse kernels
  * Support 10x longer sequences than dense
  * Flexible support to different sparse structures
* [1-bit Adam](https://www.deepspeed.ai/news/2020/09/08/onebit-adam-blog-post.html) and [1-bit LAMB](https://www.deepspeed.ai/tutorials/onebit-lamb/)
  * Custom communication collective
  * Up to 5x communication volume saving
* [Additional Memory and Bandwidth Optimizations](https://www.deepspeed.ai/features/#additional-memory-and-bandwidth-optimizations)
  * Smart Gradient Accumulation
  * Communication/Computation Overlap
* [Training Features](https://www.deepspeed.ai/features/#training-features)
  * Simplified training API
  * Gradient Clipping
  * Automatic loss scaling with mixed precision
* [Training Optimizers](https://www.deepspeed.ai/features/#training-optimizers)
  * Fused Adam optimizer and arbitrary `torch.optim.Optimizer`
  * Memory bandwidth optimized FP16 Optimizer
  * Large Batch Training with LAMB Optimizer
  * Memory efficient Training with ZeRO Optimizer
  * CPU-Adam
* [Training Agnostic Checkpointing](https://www.deepspeed.ai/features/#training-agnostic-checkpointing)
* [Advanced Parameter Search](https://www.deepspeed.ai/features/#advanced-parameter-search)
  * Learning Rate Range Test
  * 1Cycle Learning Rate Schedule
* [Simplified Data Loader](https://www.deepspeed.ai/features/#simplified-data-loader)
* [Performance Analysis and Debugging](https://www.deepspeed.ai/features/#performance-analysis-and-debugging)



# Further Reading

All DeepSpeed documentation can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)


| Article                                                                                        | Description                                  |
| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |
| [DeepSpeed Features](https://www.deepspeed.ai/features/)                                       |  DeepSpeed features                          |
| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                         |
| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |
| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |
| [CIFAR-10 Tutorial](https://www.deepspeed.ai/tutorials/cifar-10)                               |  Getting started with CIFAR-10 and DeepSpeed |
| [Megatron-LM Tutorial](https://www.deepspeed.ai/tutorials/megatron/)                           |  Train GPT2 with DeepSpeed and Megatron-LM   |
| [BERT Pre-training Tutorial](https://www.deepspeed.ai/tutorials/bert-pretraining/)             |  Pre-train BERT with DeepSpeed |
| [Learning Rate Range Test Tutorial](https://www.deepspeed.ai/tutorials/lrrt/)                  |  Faster training with large learning rates   |
| [1Cycle Tutorial](https://www.deepspeed.ai/tutorials/1Cycle/)                                  |  SOTA learning schedule in DeepSpeed         |



# Contributing
DeepSpeed welcomes your contributions! Please see our
[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,
etc.

## Contributor License Agreement
This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to, and
actually do, grant us the rights to use your contribution. For details, visit
https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need
to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply
follow the instructions provided by the bot. You will only need to do this once across
all repos using our CLA.

## Code of Conduct
This project has adopted the [Microsoft Open Source Code of
Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

# Publications
1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).
2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).
3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).
4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840).
5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888).
6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857).
7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069).

# Videos
1. DeepSpeed KDD 2020 Tutorial
    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)
    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)
    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)
    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)
    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)
    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)
2. Microsoft Research Webinar
    * Registration is free and all videos are available on-demand.
    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).
3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)
4. Community Tutorials
    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)
    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)
"
264,microsoft/vscode-html-languageservice,TypeScript,"# vscode-html-languageservice
HTML language service extracted from VSCode to be reused, e.g in the Monaco editor.

[![npm Package](https://img.shields.io/npm/v/vscode-html-languageservice.svg?style=flat-square)](https://www.npmjs.org/package/vscode-html-languageservice)
[![NPM Downloads](https://img.shields.io/npm/dm/vscode-html-languageservice.svg)](https://npmjs.org/package/vscode-html-languageservice)
[![Azure DevOps Build Status](https://img.shields.io/azure-devops/build/vscode/4c3636fe-3a50-40b9-b8b4-f820ca92886f/22.svg?label=Azure%20DevOps)](https://dev.azure.com/vscode/vscode-html-languageservice/_build?definitionId=22)
[![Travis Build Status](https://travis-ci.org/Microsoft/vscode-html-languageservice.svg?branch=master)](https://travis-ci.org/Microsoft/vscode-html-languageservice)


Why?
----

The _vscode-html-languageservice_ contains the language smarts behind the HTML editing experience of Visual Studio Code
and the Monaco editor.

 - *doComplete* / *doComplete2* (async) provide completion proposals for a given location.
 - *setCompletionParticipants* allows participant to provide suggestions for specific tokens.
 - *doHover* provides hover information at a given location.
 
 - *format* formats the code at the given range.
 - *findDocumentLinks* finds all links in the document.
 - *findDocumentSymbols* finds all the symbols in the document.
 - *getFoldingRanges* return folding ranges for the given document.
 - *getSelectionRanges* return the selection ranges for the given document.
 ...

 For the complete API see [htmlLanguageService.ts](./src/htmlLanguageService.ts) and [htmlLanguageTypes.ts](./src/htmlLanguageTypes.ts) 

Installation
------------

    npm install --save vscode-html-languageservice

Development
-----------

- clone this repo, run yarn
- `yarn test` to compile and run tests


How can I run and debug the service?

- open the folder in VSCode.
- set breakpoints, e.g. in `htmlCompletion.ts`
- run the Unit tests from the run viewlet and wait until a breakpoint is hit:
![image](https://user-images.githubusercontent.com/6461412/94239202-bdad4e80-ff11-11ea-99c3-cb9dbeb1c0b2.png)


How can I run and debug the service inside an instance of VSCode?

- run VSCode out of sources setup as described here: https://github.com/Microsoft/vscode/wiki/How-to-Contribute
- link the fodler of the `vscode-html-languageservice` repo to `vscode/extensions/html-language-features/server` to run VSCode with the latest changes from that folder:
  - cd `vscode-html-languageservice`, `yarn link`
  - cd `vscode/extensions/html-language-features/server`, `yarn link vscode-html-languageservice`
- run VSCode out of source (`vscode/scripts/code.sh|bat`) and open a `.html` file
- in VSCode window that is open on the `vscode-html-languageservice` sources, run command `Debug: Attach to Node process` and pick the `code-oss` process with the `html-language-features` path
![image](https://user-images.githubusercontent.com/6461412/94239296-dfa6d100-ff11-11ea-8e30-6444cf5defb8.png)
- set breakpoints, e.g. in `htmlCompletion.ts`
- in the instance run from sources, invoke code completion in the `.html` file


License
-------

(MIT License)

Copyright 2016-2020, Microsoft

With the exceptions of `data/*.json`, which is built upon content from [Mozilla Developer Network](https://developer.mozilla.org/en-US/docs/Web)
and distributed under CC BY-SA 2.5.
"
265,microsoft/satcheljs,TypeScript,"# Satchel

Satchel is a dataflow framework based on the [Flux architecture](http://facebook.github.io/react/blog/2014/05/06/flux.html).  It is characterized by exposing an observable state that makes view updates painless and efficient.

[![npm](https://img.shields.io/npm/v/satcheljs.svg)](https://www.npmjs.com/package/satcheljs)
[![Build Status](https://travis-ci.org/Microsoft/satcheljs.svg?branch=master)](https://travis-ci.org/Microsoft/satcheljs)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Influences

Satchel is an attempt to synthesize the best of several dataflow patterns typically used to drive a React-based UI.  In particular:

* [Flux](http://facebook.github.io/react/blog/2014/05/06/flux.html) is not a library itself, but is a dataflow pattern conceived for use with React.  In Flux, dataflow is unidirectional, and the only way to modify state is by dispatching actions through a central dispatcher.
* [Redux](http://redux.js.org/index.html) is an implementation of Flux that consolidates stores into a single state tree and attempts to simplify state changes by making all mutations via pure functions called reducers.  Ultimately, however, we found reducers and immutable state cumbersome to deal with, particularly in a large, interconnected app.
* [MobX](http://mobxjs.github.io/mobx/index.html) provides a seamless way to make state observable, and allows React to listen to state changes and rerender in a very performant way.  Satchel uses MobX under the covers to allow React components to observe the data they depend on.

## Advantages

There are a number of advantages to using Satchel to maintain your application state:

* Satchel enables a very **performant UI**, only rerendering the minimal amount necessary.  MobX makes UI updates very efficient by automatically detecting specifically what components need to rerender for a given state change.
* Satchel's datastore allows for **isomorphic JavaScript** by making it feasible to render on the server and then serialize and pass the application state down to the client.
* Satchel supports **middleware** that can act on each action that is dispatched.  (For example, for tracing or performance instrumentation.)
* Satchel is **type-safe** out of the box, without any extra effort on the consumer's part.

## Installation

Install via NPM:

`npm install satcheljs --save`

In order to use Satchel with React, you'll also need MobX and the MobX React bindings:

`npm install mobx --save`

`npm install mobx-react --save`

## Usage

The following examples assume you're developing in Typescript.

### Create a store with some initial state

```typescript
import { createStore } from 'satcheljs';

let getStore = createStore(
    'todoStore',
    { todos: [] }
);
```

### Create a component that consumes your state

Notice the `@observer` decorator on the component—this is what tells MobX to rerender the component whenever the data it relies on changes.

```javascript
import { observer } from 'mobx-react';

@observer
class TodoListComponent extends React.Component<any, any> {
    render() {
        return (
            <div>
                {getStore().todos.map(todo => <div>{todo.text}</div>)}
            </div>
        );
    }
}
```

### Implement an action creator

Note that, as a convenience, Satchel action creators created with the `action` API both *create* and *dispatch* the action.
This is typically how you want to use action creators.
If you want to create and dispatch the actions separately you can use the `actionCreator` and `dispatch` APIs.

```typescript
import { action } from 'satcheljs';

let addTodo = action(
    'ADD_TODO',
    (text: string) => ({ text: text })
);

// This creates and dispatches an ADD_TODO action
addTodo('Take out trash');
```

### Implement a mutator

You specify what action a mutator subscribes to by providing the corresponding action creator.
If you're using TypeScript, the type of `actionMessage` is automatically inferred.

```typescript
import { mutator } from 'satcheljs';

mutator(addTodo, (actionMessage) => {
    getStore().todos.push({
        id: Math.random(),
        text: actionMessage.text
    });
};
```

### Orchestrators

Orchestrators are like mutators—they subscribe to actions—but they serve a different purpose.
While mutators modify the store, orchestrators are responsible for side effects.
Side effects might include making a server call or even dispatching further actions.

The following example shows how an orchestrator can persist a value to a server before updating the store.

```typescript
import { action, orchestrator } from 'satcheljs';

let requestAddTodo = action(
    'REQUEST_ADD_TODO',
    (text: string) => ({ text: text })
);

orchestrator(requestAddTodo, async (actionMessage) => {
    await addTodoOnServer(actionMessage.text);
    addTodo(actionMessage.text);
});
```

### mutatorAction

In many cases a given action only needs to be handled by one mutator.
Satchel provides this utility API which encapsulates action creation, dispatch, and handling in one simple function call.

The `addTodo` mutator above could be implemented as follows:

```typescript
let addTodo = mutatorAction(
    'ADD_TODO',
    function addTodo(text: string) {
        getStore().todos.push({
            id: Math.random(),
            text: actionMessage.text
        });
    });
```

This is a succinct and easy way to write mutators, but it comes with a restriction:
the action creator is not exposed, so no *other* mutators or orchestrators can subscribe to it.
If an action needs multiple handlers then it must use the full pattern with action creators and handlers implemented separately.

## License - MIT
"
266,microsoft/unilm,Python,"# UniLM
**Pre-trained models for natural language understanding (NLU) and generation (NLG) tasks**

The family of UniLM:
> [**UniLM**](https://github.com/microsoft/unilm/tree/master/unilm) (```v1@NeurIPS'19 | v2@ICML'20 | v3@ACL'21```): **unified pre-training for language understanding and generation**

> [**InfoXLM**](https://github.com/microsoft/unilm/tree/master/infoxlm) (```v1@NAACL'21 | v2@ACL'21```): **multilingual/cross-lingual pre-trained models for language understanding and generation**

> [**MiniLM**](https://github.com/microsoft/unilm/tree/master/minilm) (```v1@NeurIPS'20 | v2@ACL'21```): **small and fast pre-trained models for language understanding and generation**

> [**AdaLM**](https://github.com/microsoft/unilm/tree/master/adalm) (```v1@ACL'21```): **domain, language, and task adaptation of pre-trained models**

> [**LayoutLM**](https://github.com/microsoft/unilm/tree/master/layoutlm) (```v1@KDD'20 | v2@ACL'21```): **multimodal (text + layout/format + image) pre-training for document understanding** (e.g. scanned documents, PDF, etc.)

> [**LayoutXLM**](https://github.com/microsoft/unilm/tree/master/layoutxlm) (```NEW```): **multimodal (text + layout/format + image) pre-training for multilingual document understanding**

> [**s2s-ft**](https://github.com/microsoft/unilm/tree/master/s2s-ft): **sequence-to-sequence fine-tuning toolkit**

> [**XLM-T**](https://github.com/microsoft/unilm/tree/master/xlmt) (```NEW```): **Multilingual NMT w/ pretrained cross-lingual encoders**


## News
- May, 2021: [LayoutLMv2](https://github.com/microsoft/unilm/tree/master/layoutlmv2), InfoXLMv2, MiniLMv2, UniLMv3, and AdaLM were accepted by ACL 2021.
- April, 2021: [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) is coming by extending the LayoutLM into multilingual support! A multilingual form understanding benchmark XFUN is also introduced, which includes forms with human labeled key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese).
- March, 2021: [InfoXLM](https://github.com/microsoft/unilm/tree/master/infoxlm) was accepted by NAACL 2021.
- December 29th, 2020: [LayoutLMv2](https://arxiv.org/abs/2012.14740) is coming with the new SOTA on a wide varierty of document AI tasks, including [DocVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) and [SROIE](https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3) leaderboard.
- October 8th, 2020: T-ULRv2 (aka [InfoXLM](https://arxiv.org/abs/2007.07834)) as the SOTA on the [XTREME](https://sites.research.google/xtreme) leaderboard. // [Blog](https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv2-tops-xtreme-leaderboard/)
- September, 2020: [MiniLM](https://github.com/microsoft/unilm/tree/master/minilm) was accepted by NeurIPS 2020.
- July 16, 2020: [**InfoXLM** (Multilingual UniLM)](https://github.com/microsoft/unilm/tree/master/infoxlm) [arXiv](https://arxiv.org/pdf/2007.07834.pdf)
- June, 2020: [UniLMv2](https://github.com/microsoft/unilm/tree/master/unilm) was accepted by ICML 2020; [LayoutLM](https://github.com/microsoft/unilm/tree/master/layoutlm) was accepted by KDD 2020.
- April 5, 2020: [**Multilingual MiniLM**](https://github.com/microsoft/unilm/tree/master/minilm) released!
- September, 2019: [UniLMv1](https://github.com/microsoft/unilm/tree/master/unilm-v1) was accepted by NeurIPS 2019.

## Release

**\*\*\*\*\* ```New May, 2021```: [LayoutLMv2](https://github.com/microsoft/unilm/tree/master/layoutlmv2) | [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) release \*\*\*\*\***

- [x] [**LayoutLM 2.0**](https://github.com/microsoft/unilm/tree/master/layoutlmv2) (December 29, 2020): multimodal pre-training for visually-rich document understanding by leveraging text, layout and image information in a single framework. It is coming with new SOTA on a wide range of document understanding tasks, including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852), RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). ""[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) ```ACL 2021```""
- [x] [**LayoutXLM**](https://github.com/microsoft/unilm/tree/master/layoutxlm) (April, 17, 2021): multimodal pre-training for multilingual visually-rich document understanding. The pre-trained LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the FUNSD and multilingual XFUN dataset including 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). ""[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)""

**\*\*\*\*\* ```February, 2020```: [UniLM v2](https://github.com/microsoft/unilm/tree/master/unilm) | [MiniLM v1](https://github.com/microsoft/unilm/tree/master/minilm) | [LayoutLM v1](https://github.com/microsoft/unilm/tree/master/layoutlm) | [s2s-ft v1](https://github.com/microsoft/unilm/tree/master/s2s-ft) release \*\*\*\*\***

- [x] [**LayoutLM 1.0**](https://github.com/microsoft/unilm/tree/master/layoutlm) (February 18, 2020): pre-trained models for document (image) understanding (e.g. receipts, forms, etc.) . It achieves new SOTA results in several downstream tasks, including form understanding (the FUNSD dataset from 70.72 to 79.27), receipt understanding (the [ICDAR 2019 SROIE leaderboard](https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3) from 94.02 to 95.24) and document image classification (the RVL-CDIP dataset from 93.07 to 94.42). ""[LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) ```KDD 2020```""
- [x] [**s2s-ft 1.0**](https://github.com/microsoft/unilm/tree/master/s2s-ft) (February 26, 2020): A PyTorch package used to fine-tune pre-trained Transformers for sequence-to-sequence language generation. ""[s2s-ft: Fine-Tuning Pre-Trained Transformers for Sequence-to-Sequence Learning](#)""
- [x] [**MiniLM 1.0**](https://github.com/microsoft/unilm/tree/master/minilm) (February 26, 2020): deep self-attention distillation is all you need (for task-agnostic knowledge distillation of pre-trained Transformers). MiniLM (12-layer, 384-hidden) achieves 2.7x speedup and comparable results over BERT-base (12-layer, 768-hidden) on NLU tasks as well as strong results on NLG tasks. The even smaller MiniLM (6-layer, 384-hidden) obtains 5.3x speedup and produces very competitive results. ""[MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957) ```NeurIPS 2020```""
- [x] [**UniLM 2.0**](https://github.com/microsoft/unilm/tree/master/unilm) (February 28, 2020): **unified pre-training** of bi-directional LM (via autoencoding) and sequence-to-sequence LM (via partially autoregressive) w/ **Pseudo-Masked Language Model** for language understanding and generation. UniLM v2 achieves new SOTA in a wide range of natural language understanding and generation tasks. ""[UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training](https://arxiv.org/abs/2002.12804) ```ICML 2020```""



**\*\*\*\*\* October 1st, 2019: UniLM v1 release \*\*\*\*\***

- [x] [**UniLM v1**](https://github.com/microsoft/unilm/tree/master/unilm-v1) (September 30, 2019): the code and pre-trained models for the ```NeurIPS 2019``` paper entitled ""[Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)"". UniLM (v1) achieves the **new SOTA results** in **NLG** (especially **sequence-to-sequence generation**) tasks, including abstractive summarization (the Gigaword and CNN/DM datasets), question generation (the SQuAD QG dataset), etc. 

## License
This project is licensed under the license found in the LICENSE file in the root directory of this source tree.
Portions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.

[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)

### Contact Information

For help or issues using UniLM, please submit a GitHub issue.

For other communications related to UniLM, please contact Li Dong (`lidong1@microsoft.com`), Furu Wei (`fuwei@microsoft.com`).

"
267,microsoft/vscode-java-test,Java,"# Java Test Runner

> Run and debug Java test cases in Visual Studio Code

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/resources/logo.png"" width=""128"" height=""128"" alt="""">
</p>
<p align=""center"">
  <a href=""https://github.com/microsoft/vscode-java-test/actions?query=workflow%3ACI+branch%3Amaster"">
    <img src=""https://img.shields.io/github/workflow/status/microsoft/vscode-java-test/CI/master?style=flat-square"" alt="""">
  </a>
  <a href=""https://lgtm.com/projects/g/microsoft/vscode-java-test/alerts/?mode=list"">
    <img src=""https://img.shields.io/lgtm/alerts/g/microsoft/vscode-java-test.svg?style=flat-square"" alt="""">
  </a>
  <a href=""https://gitter.im/microsoft/vscode-java-test"">
    <img src=""https://img.shields.io/gitter/room/microsoft/vscode-java-test.svg?style=flat-square"" alt="""">
  </a>
  <a href=""https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-test"">
    <img src=""https://img.shields.io/visual-studio-marketplace/d/vscjava.vscode-java-test.svg?style=flat-square"" alt="""">
  </a>
</p>

## Overview

A lightweight extension to run and debug Java test cases in Visual Studio Code. The extension support following test frameworks:

- JUnit 4 (v4.8.0+)
- JUnit 5 (v5.1.0+)
- TestNG (v6.8.0+)

> Note: JUnit 3 styled tests are not supported in this extension (i.e. extends `junit.framework.TestCase`).

The [Java Test Runner](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-test) works with [Language Support for Java by Red Hat](https://marketplace.visualstudio.com/items?itemName=redhat.java) and [Debugger for Java](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-debug) to provide the following features:

- Run/Debug test cases
- Customize test configurations
- View test report
- View tests in Test Explorer
- Show test logs


## Requirements

- JDK (version 11 or later)
- VS Code (version 1.44.0 or later)
- [Language Support for Java by Red Hat](https://marketplace.visualstudio.com/items?itemName=redhat.java)
- [Debugger for Java](https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-java-debug)

## Quickstart

![Run/debug JUnit test](demo/demo.gif)

### Getting Started for JUnit 5

Please refer to [Getting Started](https://junit.org/junit5/docs/current/user-guide/#overview-getting-started) from the JUnit 5's official document for getting started guide.

> Note: You can use [junit-platform-console-standalone.jar](https://search.maven.org/search?q=g:org.junit.platform%20AND%20a:junit-platform-console-standalone) in projects that manually manage their dependencies similar to the [plain-old JAR known from JUnit 4](https://github.com/junit-team/junit4/wiki/Download-and-Install#plain-old-jar).

### Getting Started for JUnit 4
Please refer to [Download and Install](https://github.com/junit-team/junit4/wiki/Download-and-Install) from the JUnit 4's official document for the getting started guide.

### Getting Started for TestNG

Please refer to [TestNG Docs](https://testng.org/doc/) from the TestNG's official document for getting started guide.

## Features

### Run/Debug Test Cases
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/demo/run_codelens.png"" style=""border-radius: 15px"" alt=""Run Code Lens""/>
</p>

- The extension will generate `Run Test` and `Debug Test` shortcuts (also known as Code Lens) above the class and method definition. Simply click on them will start running or debugging the target test cases.

> Note: If you cannot see the Code Lens in your editor, please refer to this [issue comment](https://github.com/Microsoft/vscode-java-test/issues/470#issuecomment-444681714) as a workaround.

---

### Test Explorer

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/demo/run_explorer.png"" style=""border-radius: 15px"" alt=""Run Explorer""/>
</p>

- The Test Explorer is the place to show all the test cases in your project. You can also run/debug your test cases from here.
- Click the node in the Test Explorer will navigate to the location of the source code.

> Note: If the Test Explorer is empty, please refer to this [issue comment](https://github.com/Microsoft/vscode-java-test/issues/470#issuecomment-444681714) as a workaround.

---

### Customize Test Configurations
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/demo/configuration.png"" style=""border-radius: 15px"" alt=""Customize Test Configurations""/>
</p>

- Sometimes you may want to customize the configuration for running the test cases. To achieve this, you can add it into your workspace settings under the section: `java.test.config`.

> Note: More details can be found [here](https://github.com/Microsoft/vscode-java-test/wiki/Run-with-Configuration).

---

### View Test Report

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/demo/status_bar.png"" style=""border-radius: 15px"" alt=""Status Bar""/>
</p>

- After running/debugging the test cases, the status bar will show the final results. Simply click on it to show the Test Report.
- You can also click the ✔️ or ❌ mark in Code Lens to open the Test Report.

<p align=""center"">
  <img src=""https://raw.githubusercontent.com/Microsoft/vscode-java-test/master/demo/report_navigate.png"" style=""border-radius: 15px"" alt=""Status Bar""/>
</p>

- You can navigate to the source location of the target test case by clicking the navigate button.

> Note: You can use `java.test.report.showAfterExecution` to configure whether to automatically show the test report after execution. By default, it will be shown when there are failed tests. 


## Settings

| Setting Name | Description | Default Value |
|---|---|---|
| `java.test.report.position` | Specify where to show the test report. Supported values are: `sideView`, `currentView`. | `sideView` |
| `java.test.report.showAfterExecution` | Specify if the test report will automatically be shown after execution. Supported values are: `always`, `onFailure`, `never`. | `onFailure` |
| `java.test.editor.enableShortcuts` | Specify whether to show the Code Lenses in editor or not. | `true` |
| `java.test.log.level` | Specify the level of the test logs. Supported values are: `error`, `info`, `verbose`. | `info` |
| `java.test.config` | Specify the configuration for the test cases to run with. [More details](https://aka.ms/java-test-config). | `{}` |
| `java.test.defaultConfig` | Specify the name of the default test configuration. | `""""` |

## FAQ
If you meet any problem when using the extension, please refer to the [FAQ](https://github.com/microsoft/vscode-java-test/wiki/FAQ) to check if there is an answer to your problem.

## Contributing and Feedback

If you are interested in providing feedback or contributing directly to the code base, please check the document [Contributing to Java Test Runner](https://github.com/Microsoft/vscode-java-test/blob/master/CONTRIBUTING.md), which covers the following parts:
- [Questions and Feedback](https://github.com/Microsoft/vscode-java-test/blob/master/CONTRIBUTING.md#questions-and-feedback)
- [Reporting Issues](https://github.com/Microsoft/vscode-java-test/blob/master/CONTRIBUTING.md#reporting-issues)
- [Contributing Fixes](https://github.com/Microsoft/vscode-java-test/blob/master/CONTRIBUTING.md#contributing-fixes)

## License

This extension is licensed under [MIT License](LICENSE.txt).

## Telemetry

This extension collects telemetry data to help improve our products. Please read [Microsoft privacy statement](https://privacy.microsoft.com/en-us/privacystatement) to learn more. If you opt out to send telemetry data to Microsoft, please set below configuration in settings.json: `telemetry.enableTelemetry = false`. Learn more in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).
"
268,microsoft/AirSim,C++,"# Welcome to AirSim

AirSim is a simulator for drones, cars and more, built on [Unreal Engine](https://www.unrealengine.com/) (we now also have an experimental [Unity](https://unity3d.com/) release). It is open-source, cross platform, and supports software-in-the-loop simulation with popular flight controllers such as PX4 & ArduPilot and hardware-in-loop with PX4 for physically and visually realistic simulations. It is developed as an Unreal plugin that can simply be dropped into any Unreal environment. Similarly, we have an experimental release for a Unity plugin.

Our goal is to develop AirSim as a platform for AI research to experiment with deep learning, computer vision and reinforcement learning algorithms for autonomous vehicles. For this purpose, AirSim also exposes APIs to retrieve data and control vehicles in a platform independent way.

**Check out the quick 1.5 minute demo**

Drones in AirSim

[![AirSim Drone Demo Video](docs/images/demo_video.png)](https://youtu.be/-WfTr1-OBGQ)

Cars in AirSim

[![AirSim Car Demo Video](docs/images/car_demo_video.png)](https://youtu.be/gnz1X3UNM5Y)


## How to Get It

[![Build Status](https://travis-ci.org/Microsoft/AirSim.svg?branch=master)](https://travis-ci.org/Microsoft/AirSim)

### Windows
* [Download binaries](https://github.com/Microsoft/AirSim/releases)
* [Build it](https://microsoft.github.io/AirSim/build_windows)

### Linux
* [Download binaries](https://github.com/Microsoft/AirSim/releases)
* [Build it](https://microsoft.github.io/AirSim/build_linux)

### macOS
* [Build it](https://microsoft.github.io/AirSim/build_linux)

For more details, see the [use precompiled binaries](docs/use_precompiled.md) document. 

## How to Use It

### Documentation

View our [detailed documentation](https://microsoft.github.io/AirSim/) on all aspects of AirSim.

### Manual drive

If you have remote control (RC) as shown below, you can manually control the drone in the simulator. For cars, you can use arrow keys to drive manually.

[More details](https://microsoft.github.io/AirSim/remote_control/)

![record screenshot](docs/images/AirSimDroneManual.gif)

![record screenshot](docs/images/AirSimCarManual.gif)


### Programmatic control

AirSim exposes APIs so you can interact with the vehicle in the simulation programmatically. You can use these APIs to retrieve images, get state, control the vehicle and so on. The APIs are exposed through the RPC, and are accessible via a variety of languages, including C++, Python, C# and Java.

These APIs are also available as part of a separate, independent cross-platform library, so you can deploy them on a companion computer on your vehicle. This way you can write and test your code in the simulator, and later execute it on the real vehicles. Transfer learning and related research is one of our focus areas.

Note that you can use [SimMode setting](https://microsoft.github.io/AirSim/settings#simmode) to specify the default vehicle or the new [ComputerVision mode](https://microsoft.github.io/AirSim/image_apis#computer-vision-mode-1) so you don't get prompted each time you start AirSim.

[More details](https://microsoft.github.io/AirSim/apis/)

### Gathering training data

There are two ways you can generate training data from AirSim for deep learning. The easiest way is to simply press the record button in the lower right corner. This will start writing pose and images for each frame. The data logging code is pretty simple and you can modify it to your heart's content.

![record screenshot](docs/images/record_data.png)

A better way to generate training data exactly the way you want is by accessing the APIs. This allows you to be in full control of how, what, where and when you want to log data.

### Computer Vision mode

Yet another way to use AirSim is the so-called ""Computer Vision"" mode. In this mode, you don't have vehicles or physics. You can use the keyboard to move around the scene, or use APIs to position available cameras in any arbitrary pose, and collect images such as depth, disparity, surface normals or object segmentation.

[More details](https://microsoft.github.io/AirSim/image_apis/)

### Weather Effects

Press F10 to see various options available for weather effects. You can also control the weather using [APIs](https://microsoft.github.io/AirSim/apis#weather-apis). Press F1 to see other options available.

![record screenshot](docs/images/weather_menu.png)

## Tutorials

- [Video - Setting up AirSim with Pixhawk Tutorial](https://youtu.be/1oY8Qu5maQQ) by Chris Lovett
- [Video - Using AirSim with Pixhawk Tutorial](https://youtu.be/HNWdYrtw3f0) by Chris Lovett
- [Video - Using off-the-self environments with AirSim](https://www.youtube.com/watch?v=y09VbdQWvQY) by Jim Piavis
- [Reinforcement Learning with AirSim](https://microsoft.github.io/AirSim/reinforcement_learning) by Ashish Kapoor
- [The Autonomous Driving Cookbook](https://aka.ms/AutonomousDrivingCookbook) by Microsoft Deep Learning and Robotics Garage Chapter
- [Using TensorFlow for simple collision avoidance](https://github.com/simondlevy/AirSimTensorFlow) by Simon Levy and WLU team

## Participate

### Paper

More technical details are available in [AirSim paper (FSR 2017 Conference)](https://arxiv.org/abs/1705.05065). Please cite this as:
```
@inproceedings{airsim2017fsr,
  author = {Shital Shah and Debadeepta Dey and Chris Lovett and Ashish Kapoor},
  title = {AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
  year = {2017},
  booktitle = {Field and Service Robotics},
  eprint = {arXiv:1705.05065},
  url = {https://arxiv.org/abs/1705.05065}
}
```

### Contribute

Please take a look at [open issues](https://github.com/microsoft/airsim/issues) if you are looking for areas to contribute to.

* [More on AirSim design](https://microsoft.github.io/AirSim/design)
* [More on code structure](https://microsoft.github.io/AirSim/code_structure)
* [Contribution Guidelines](CONTRIBUTING.md)

### Who is Using AirSim?

We are maintaining a [list](https://microsoft.github.io/AirSim/who_is_using) of a few projects, people and groups that we are aware of. If you would like to be featured in this list please [make a request here](https://github.com/microsoft/airsim/issues).

## Contact

Join our [GitHub Discussions group](https://github.com/microsoft/AirSim/discussions) to stay up to date or ask any questions.

We also have an AirSim group on [Facebook](https://www.facebook.com/groups/1225832467530667/). 


## What's New

- [Python wrapper for Open AI gym interfaces.](https://github.com/microsoft/AirSim/pull/3215)
- [Python wrapper for Event camera simulation](https://github.com/microsoft/AirSim/pull/3202)
- [Voxel grid construction](https://github.com/microsoft/AirSim/pull/3209)
- [Programmable camera distortion](https://github.com/microsoft/AirSim/pull/3039)
- [Wind simulation](https://github.com/microsoft/AirSim/pull/2867)
- [Azure development environment with documentation](https://github.com/microsoft/AirSim/pull/2816)
- ROS wrapper for [multirotor](https://github.com/microsoft/AirSim/blob/master/docs/airsim_ros_pkgs.md) and [car](https://github.com/microsoft/AirSim/pull/2743).

For complete list of changes, view our [Changelog](docs/CHANGELOG.md)

## FAQ

If you run into problems, check the [FAQ](https://microsoft.github.io/AirSim/faq) and feel free to post issues in the  [AirSim](https://github.com/Microsoft/AirSim/issues) repository.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.


## License

This project is released under the MIT License. Please review the [License file](LICENSE) for more details.


"
269,microsoft/vscode-cosmosdb,TypeScript,"
# Azure Databases for VS Code (Preview)

<!-- region exclude-from-marketplace -->

[![Version](https://vsmarketplacebadge.apphb.com/version/ms-azuretools.vscode-cosmosdb.svg)](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-cosmosdb) [![Installs](https://vsmarketplacebadge.apphb.com/installs-short/ms-azuretools.vscode-cosmosdb.svg)](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-cosmosdb) [![Build Status](https://dev.azure.com/ms-azuretools/AzCode/_apis/build/status/vscode-cosmosdb)](https://dev.azure.com/ms-azuretools/AzCode/_build/latest?definitionId=7)

<!-- endregion exclude-from-marketplace -->

Browse and query your Azure databases both locally and in the cloud using [_scrapbooks_](#mongo-scrapbooks) with rich Intellisense then connect to Azure to manage your PostgreSQL and Cosmos DB databases with support for MongoDB, Graph (Gremlin), and SQL (previously known as DocumentDB).

![Azure Databases Extension](resources/features.png)

# Prerequisites

- Some less-common commands in the Mongo [scrapbook](#mongo-scrapbooks) and use of the Mongo shell require installing [Mongo DB and Mongo shell](https://docs.mongodb.com/manual/installation/).

# Features

## Azure Databases Explorer

- Create a database server by clicking the `+` button in the title
- View database servers and open directly in the portal
- View/Create/Delete databases, collections, graphs, stored procedures, documents, and queries
- Click on a document, stored procedure, or query to open in the editor
- Click on a graph to visualize data
- Query graph using [Gremlin](https://docs.microsoft.com/azure/cosmos-db/gremlin-support)
- Edit a document and persist changes to the cloud
- Attach a Mongo server by clicking the plug icon in the title

![Browse PostgreSQL, CosmosDB, and MongoDB databases](resources/Browse.png)

## Mongo Scrapbooks
### Run Mongo Commands with Rich Intellisense

- View your MongoDB database account by [signing in to Azure](#managing-azure-subscriptions) or using ""Attach Database Account"" to connect via a connection string
- Optionally configure the settings `mongo.shell.path` and `mongo.shell.args` if your mongo executable is not already on your system's PATH (many of the common commands have built-in support and do not require the Mongo shell to be installed - see [Prerequisites](#prerequisites))
- Click on ""New Mongo Scrapbook"" in the tree title bar
- Click on ""Connect to a database"" to indicate which database to run the commands against
- Enter your commands and/or comments, eg: `db.<collectionName>.find()`
- IntelliSense (auto-completions) will be provided
- Click on ""Execute"" above a command to execute it, or press `CMD+""` (Mac) or `CTRL+""` (Windows and Linux) to execute the line with the cursor
- To run all commands, click on ""Execute All"", or press `CMD+:` or `Ctrl+:`
- Save and re-use later
![Mongo Scrapbook](resources/Scrapbook.gif)

## Import into Cosmos DB

- You can now import documents from your workspace into CosmosDB. Use the context menu of a collection or a document file (json) to get started!
![Import documents](resources/import_documents.gif)

## Use [Gremlin](https://docs.microsoft.com/azure/cosmos-db/gremlin-support) to query graphs

![Query Graphs](resources/Graph.gif)

- <a name=""graphSettings""></a>Configure the user setting `cosmosDB.graph.viewSettings` to customize which properties to display and which colors to use based on vertex label.
```javascript
    ""cosmosDB.graph.viewSettings"": [
        {
            ""vertexSettings"": [
                {
                    // Default settings for all vertices
                    ""displayProperty"": [
                        // Display name property if exists, otherwise firstName if it exists, otherwise ID
                        ""name"",
                        ""firstName""
                    ],
                    // Auto-choose color by label
                    ""color"": ""auto"",
                    // Show label after display property
                    ""showLabel"": true
                },
                {
                    // These setting apply to vertices with the label 'person'
                    ""appliesToLabel"": ""person"",
                    ""color"": ""blue""
                }
            ]
        }
    ]
```

## Create an Azure Databases Server

![Create Azure Databases Server](resources/create.gif)

## Attach to the Cosmos DB Emulator

* Install and run the [Cosmos DB Emulator](https://docs.microsoft.com/azure/cosmos-db/local-emulator) on your local machine
* Right click 'Attached Database Accounts' and select 'Attach Emulator'

![Attach Emulator](resources/attachEmulator.png)

## Managing Azure Subscriptions

If you are not signed in to Azure, you will see a ""Sign in to Azure..."" link. Alternatively, you can select ""View->Command Palette"" in the VS Code menu, and search for ""Azure: Sign In"".

![Sign in to Azure](resources/SignIn.gif)

If you don't have an Azure Account, you can sign up for one today for free and receive $200 in credits by selecting ""Create a Free Azure Account..."" or selecting ""View->Command Palette"" and searching for ""Azure: Create an Account"".

You may sign out of Azure by selecting ""View->Command Palette"" and searching for ""Azure: Sign Out"".

To select which subscriptions show up in the extension's explorer, click on the ""Select Subscriptions..."" button on any subscription node (indicated by a ""filter"" icon when you hover over it), or select ""View->Command Palette"" and search for ""Azure: Select Subscriptions"". Note that this selection affects all VS Code extensions that support the [Azure Account and Sign-In](https://github.com/Microsoft/vscode-azure-account) extension.

![Select Azure Subscriptions](resources/SelectSubscriptions.gif)

## Known Issues

- Azure no longer supports gremlin queries on pre-GA graph accounts. If you see the error ""Could not find a valid gremlin endpoint for *graph*"", then choose ""Open Portal"" on the graph node and check the ""Gremlin Endpoint"" in the Overview tab. If it does not take the form of '...[graph-name].***gremlin***.cosmosdb.azure.com...', then you will need to create a new graph account using the Azure portal or the current version of the extension.
- Graphs are not currently supported with the emulator
- Viewing/editing tables is not currently supported
- Support for escapes in the scrapbooks is preliminary. We currently do not support escaped characters as is inside a string - the characters need to be double escaped. For example, newlines in the string should be  '\\\\n' instead of '\\n' to be recognized correctly. If you find any issues with how the scrapbook handles escapes, please add to issue [#937](https://github.com/Microsoft/vscode-cosmosdb/issues/937).

<!-- region exclude-from-marketplace -->

# Contributing
There are several ways you can contribute to our [repo](https://github.com/Microsoft/vscode-cosmosdb):

* **Ideas, feature requests and bugs**: We are open to all ideas and we want to get rid of bugs! Use the [Issues](https://github.com/Microsoft/vscode-cosmosdb/issues) section to report a new issue, provide your ideas or contribute to existing threads.
* **Documentation**: Found a typo or strangely worded sentences? Submit a PR!
* **Code**: Contribute bug fixes, features or design changes:
  * Clone the repository locally and open in VS Code.
  * Run ""Extensions: Show Recommended Extensions"" from the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette) and install all extensions listed under ""Workspace Recommendations""
  * Open the terminal (press <kbd>CTRL</kbd>+ <kbd>\`</kbd>) and run `npm install`.
  * To build, press <kbd>F1</kbd> and type in `Tasks: Run Build Task`.
  * Debug: press <kbd>F5</kbd> to start debugging the extension.

## Legal
Before we can accept your pull request you will need to sign a **Contribution License Agreement**. All you need to do is to submit a pull request, then the PR will get appropriately labelled (e.g. `cla-required`, `cla-norequired`, `cla-signed`, `cla-already-signed`). If you already signed the agreement we will continue with reviewing the PR, otherwise system will tell you how you can sign the CLA. Once you sign the CLA all future PR's will be labeled as `cla-signed`.

## Code of Conduct
This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

<!-- endregion exclude-from-marketplace -->

# Telemetry
VS Code collects usage data and sends it to Microsoft to help improve our products and services. Read our [privacy statement](https://go.microsoft.com/fwlink/?LinkID=528096&clcid=0x409) to learn more. If you don’t wish to send usage data to Microsoft, you can set the `telemetry.enableTelemetry` setting to `false`. Learn more in our [FAQ](https://code.visualstudio.com/docs/supporting/faq#_how-to-disable-telemetry-reporting).

# License
[MIT](LICENSE.md)
"
